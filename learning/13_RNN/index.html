
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mydomain.org/mysite/learning/13_RNN/">
      
      
        <link rel="prev" href="../12_KLdivergence/">
      
      
        <link rel="next" href="../14_LSTM/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>RNN - 溶err</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/timeago.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
      <link rel="stylesheet" href="../../mkdocs/css/no-footer.css">
    
      <link rel="stylesheet" href="../../mkdocs/css/unordered-list-symbols.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#rnn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="溶err" class="md-header__button md-logo" aria-label="溶err" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            溶err
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              RNN
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../sticks/mkdocs_learn/" class="md-tabs__link">
          
  
    
  
  便签

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../bagu/questions/1_questions/" class="md-tabs__link">
          
  
    
  
  面试

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Error/github/" class="md-tabs__link">
          
  
    
  
  捉个虫

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../3_ViT/" class="md-tabs__link">
          
  
    
  
  笔记

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../literature/" class="md-tabs__link">
          
  
    
  
  文献

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../logs/" class="md-tabs__link">
          
  
    
  
  杂

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="溶err" class="md-nav__button md-logo" aria-label="溶err" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    溶err
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    便签
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            便签
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/mkdocs_learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MkDocs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/markdwon_learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    markdown
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/latex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LaTex
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/GitHub/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GitHub
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/MacOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MacOS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/shell/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/screen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    screen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/docker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/writting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    写作
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/1_github_v1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    github v1.0
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/2_python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/3_vscode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    面试
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            面试
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    题目
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            题目
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/questions/1_questions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    面试问题
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../bagu/leetcode/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    力扣
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            力扣
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/leetcode/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1 两数之和
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/leetcode/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2 两数相加
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../bagu/deeplearning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    深度学习
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            深度学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕Transformer代码
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/former1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    空
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/former2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    空
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/pytorch_shape_function/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pytorch的维度变换函数
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visionTransformer代码
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    机器学习
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/machinelearning/kmeans/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕kmeans
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/machinelearning/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕反向传播
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    捉个虫
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            捉个虫
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/github/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    github
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/latex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latex
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/macos/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/docker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    docker
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    笔记
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            笔记
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_ViT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ViT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2_MOCO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MOCO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    图解LayerNorm &amp; BatchNorm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5种归一化方法
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision Transformer的原理与难点源码实现
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../swintransformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SwinTransformer 学习笔记
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4种位置编码
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    卷积
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    李沐 目标检测部分
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_GAN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_Bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BERT从零详细解读
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_Diffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDPM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_Diffusion1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_Clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Clip
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_WeightNorm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    WeightNorm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9_cGAN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN 变体
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_ResNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    项目实战：ResNet果蔬分类
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_excelcsvtensor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基础：excel\csv文件→tensor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_KLdivergence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KL divergence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    RNN
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    RNN
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#k1" class="md-nav__link">
    <span class="md-ellipsis">
      k1 记忆单元分类
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k2" class="md-nav__link">
    <span class="md-ellipsis">
      k2 模型的分类
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k3" class="md-nav__link">
    <span class="md-ellipsis">
      k3 语音识别模型性能比较
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k4" class="md-nav__link">
    <span class="md-ellipsis">
      k4 循环神经网络的优缺点
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k5-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      k5 RNN 的应用场景
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k6-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      k6 RNN框图
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#torchnnrnn" class="md-nav__link">
    <span class="md-ellipsis">
      torch.nn.RNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      代码示例
    </span>
  </a>
  
    <nav class="md-nav" aria-label="代码示例">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      1 单层单向 RNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      2 双向、单层RNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-rnn-api" class="md-nav__link">
    <span class="md-ellipsis">
      3 RNN api 代码汇总
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-rnnrnn" class="md-nav__link">
    <span class="md-ellipsis">
      4 单向RNN&amp;双向RNN 从矩阵运算的角度实现
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchtile" class="md-nav__link">
    <span class="md-ellipsis">
      torch.tile函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5 验证
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 验证">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn_1" class="md-nav__link">
    <span class="md-ellipsis">
      自定义 RNN代码
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      6 验证双向RNN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6 验证双向RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn_2" class="md-nav__link">
    <span class="md-ellipsis">
      自定义双向 RNN代码
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      汇总所有代码
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_LSTM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LSTM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_ContrastiveLearning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对比学习
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_YOLO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    YOLO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_DETR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DETR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_DINO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DINO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_GPT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_distill/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    知识蒸馏
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_FastRCNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    21 FastRCNN
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    文献
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            文献
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/TSP/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    时间序列预测
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            时间序列预测
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/0_note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NOTE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/1_SegRNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/2_DLinear/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DLinear
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/3_TimesNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TimesNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/4_Informer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021|Informer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/5_Autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021|Autoformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/ObejectCounting/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    目标计数
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            目标计数
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank1%20CountGD/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank1 CountGD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank2%20GeCo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank2 GeCo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank3%20DAVE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank3 DAVE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank4%20CACViT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank4 CACViT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank5%20SSD/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank5 SSD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank6%20LOCA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank6 LOCA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank7%20SemAug_CountTR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank7 SemAug CountTR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank8%20CounTR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank8 CounTR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank9%20SemAug_SAFECount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank9 SemAug SAFECount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank10%20SPDCN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank10 SPDCN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank11%20GCA_SUN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank11 GCA SUN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank12%20SAFECount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank12 SAFECount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank13%20BMNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank13 BMNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank14%20LaoNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank14 LaoNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank15%20CounTX/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank15 CounTX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank16%20Counting_DETR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank16 Counting DETR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank17%20RCC/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank17 RCC
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank18%20Omnicount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank18 Omnicount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank19%20FamNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank19 FamNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/Reproduction/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    复现&代码
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            复现&代码
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/DAVE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DAVE复现
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一些模块
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    特征融合方式
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一些感悟
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    预训练权重
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/5_SegRNN_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/5_SegRNN_v1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    复现SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/5_SegRNN_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    复现 SegRNN_v2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/6_AutoFormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autoformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/ObjectDetection/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    目标检测
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_5">
            <span class="md-nav__icon md-icon"></span>
            目标检测
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObjectDetection/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    目标检测基础知识
    
  </span>
  

      </a>
    </li>
  

              
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObjectDetection/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DETR论文系列
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObjectDetection/3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    （DETR）End-to-End Object Detection with Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObjectDetection/4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/MultiModal/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    多模态
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_6" id="__nav_5_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6">
            <span class="md-nav__icon md-icon"></span>
            多模态
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/MultiModal/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../logs/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    杂
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            杂
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../logs/diary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    乐观 &amp; 坚强
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#k1" class="md-nav__link">
    <span class="md-ellipsis">
      k1 记忆单元分类
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k2" class="md-nav__link">
    <span class="md-ellipsis">
      k2 模型的分类
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k3" class="md-nav__link">
    <span class="md-ellipsis">
      k3 语音识别模型性能比较
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k4" class="md-nav__link">
    <span class="md-ellipsis">
      k4 循环神经网络的优缺点
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k5-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      k5 RNN 的应用场景
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k6-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      k6 RNN框图
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#torchnnrnn" class="md-nav__link">
    <span class="md-ellipsis">
      torch.nn.RNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      代码示例
    </span>
  </a>
  
    <nav class="md-nav" aria-label="代码示例">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      1 单层单向 RNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      2 双向、单层RNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-rnn-api" class="md-nav__link">
    <span class="md-ellipsis">
      3 RNN api 代码汇总
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-rnnrnn" class="md-nav__link">
    <span class="md-ellipsis">
      4 单向RNN&amp;双向RNN 从矩阵运算的角度实现
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchtile" class="md-nav__link">
    <span class="md-ellipsis">
      torch.tile函数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5 验证
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 验证">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn_1" class="md-nav__link">
    <span class="md-ellipsis">
      自定义 RNN代码
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      6 验证双向RNN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6 验证双向RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn_2" class="md-nav__link">
    <span class="md-ellipsis">
      自定义双向 RNN代码
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      汇总所有代码
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="rnn">RNN<a class="headerlink" href="#rnn" title="Permanent link">&para;</a></h1>
<p>ref：<a href="https://www.bilibili.com/video/BV13i4y1R7jB/?share_source=copy_web&amp;vd_source=5cbbeafd6fa2338b041c25f100ea6483">【29、PyTorch RNN的原理及其手写复现】</a></p>
<p><img alt="image-20241220115043569" src="../images/image-20241220115043569.png" /></p>
<p>topic：</p>
<p>（1）不同类型的RNN的图示以及应用场景的图示</p>
<p>（2）介绍pytorch中RNN的api的使用</p>
<p>（3）通过代码验证 RNN 内部是如何计算的，通过代码来 验证 pytorch的RNN的api 并对比结果</p>
<h2 id="k1">k1 记忆单元分类<a class="headerlink" href="#k1" title="Permanent link">&para;</a></h2>
<p><img alt="image-20241220115210956" src="../images/image-20241220115210956.png" /></p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 什么是记忆单元？</li>
</ul>
<p>记忆单元就是 存储的 过去的历史信息</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 什么是循环神经网络？</li>
</ul>
<p>所谓循环神经网络 就是说，在对序列进行建模的时候，在算每一时刻的表征的时候，一般考虑过去的 历史信息。这个历史信息 就是通过 记忆单元 保存的。然后每个时刻 我们都会从 记忆单元中 获取 过去的 历史信息，然后辅助当前时刻 做预测。</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 记忆单元分类</li>
</ul>
<p>关于记忆单元 一般有三类</p>
<ol>
<li>RNN</li>
<li>LSTM</li>
<li>GRU </li>
</ol>
<blockquote>
<p>一类 比如说 RNN，比如说 Simple RNN，简单的RNN 结构，等下实现的也是 简单的RNN结构</p>
<p>另外两种是 GRU和LSTM，这两种网络的记忆性会更强一点；计算复杂度也会更高一点；使用频率也会更高一点，就是说 现在很多的实际应用中，我们基本使用的是LSTM或者GRU；但是它们都是RNN的一个变体，所以RNN是基础；</p>
</blockquote>
<h2 id="k2">k2 模型的分类<a class="headerlink" href="#k2" title="Permanent link">&para;</a></h2>
<p><img alt="image-20241220115515504" src="../images/image-20241220115515504.png" /></p>
<p>（1）单向循环</p>
<blockquote>
<p>循环神经网络也可分为单向循环，所谓单向循环就是，当前时刻的预测 只跟 过去有关，从左到右 递归的计算。</p>
</blockquote>
<p>（2）双向循环</p>
<blockquote>
<p>双向循环，双向循环就是说 不只有 从左到右的 也有 从右到左的，就是说有两条链，另外一条链，在计算当前时刻的预测的时候 会考虑 未来信息。</p>
</blockquote>
<p>（3）多个单向 、 多个双向</p>
<blockquote>
<p>这个就是双向循环；那还可以把 多个单向 或者说 多个双向 叠加起来，也就是deep RNN 深度循环神经网络</p>
</blockquote>
<p><img alt="image-20241220115718028" src="../images/image-20241220115718028.png" /></p>
<p>（1）单向的循环神经网络</p>
<p><img alt="image-20241220115802397" src="../images/image-20241220115802397.png" /></p>
<p>可以分为三层：</p>
<ol>
<li>最下面一层是 input layer，也就是输入层；</li>
<li>中间是隐含层；</li>
<li>最后是输出层；</li>
</ol>
<blockquote>
<p>下面的输入层每一个神经元 可以看做 每一个时刻；</p>
<p>也就是说 每一个时刻 不仅跟当前时刻的输入有关，还跟上一时刻的记忆单元有关；</p>
<p>并且在单向循环神经网络 中 始终是 从左到右的；</p>
<p>就是说当前时刻的预测 只跟 过去的记忆单元 有关，跟未来的 是无关的；</p>
</blockquote>
<p>（2）双向的循环神经网络</p>
<p><img alt="image-20241220120003057" src="../images/image-20241220120003057.png" /></p>
<ol>
<li>有两条链</li>
<li>分为4个部分：  <strong>input layer、output layer、forward layer、backward layer</strong></li>
<li><strong>（forward layer）</strong>  forward layer是从左到右的循环 ，意思就是说 在 forward layer的输出中，它的输出不仅跟当前输入有关 也跟过去的记忆单元有关；</li>
<li><strong>（backward layer）</strong>  backward layer当中，它的当前时刻的输出 不仅跟当前时刻的输入有关，还跟未来时刻的记忆单元有关，所以是 从右到左的 递归运算的。</li>
<li>**（将forward和backward结合）**起来有什么好处呢？ 就是说 既能看到过去 又能看到未来</li>
</ol>
<h2 id="k3">k3 语音识别模型性能比较<a class="headerlink" href="#k3" title="Permanent link">&para;</a></h2>
<p><img alt="image-20241220120329113" src="../images/image-20241220120329113.png" /></p>
<blockquote>
<p>这张表格 来自某篇论文，这张表格 很好的 展示了 RNN、LSTM、 双向 单向、MLP、以及是否delay等 在参数数量相等的情况下 在语音识别上的表现；可以看到 第二列 第三列 分别是训练误差和测试误差；
</p>
</blockquote>
<p>通过表格 可以看到 不同的模型在 语音识别 这种 序列建模，序列分类这个任务上的表现</p>
<p><img alt="image-20241220123407253" src="../images/image-20241220123407253.png" /></p>
<p>（1）第一行是MLP，MLP就是简单的DNN 是no window的（什么意思？）</p>
<blockquote>
<p>我们把语音 分成很多帧，比方说一帧是 15毫秒 或者 20毫秒，对于每一帧 提取一个特征 比如说 傅里叶变换 得到一个频谱特征，然后 我们对每一帧 进行单独建模，所谓 no window就是 我们不考虑 周围的帧，只考虑 当前这个15毫秒，然后 我们 把它送入 DNN中，来去 进行一个 预测 分类，这样做的话 它的 训练误差 和测试误差 大概都是在 40% 左右；</p>
</blockquote>
<p><img alt="image-20241220120448060" src="../images/image-20241220120448060.png" /></p>
<p>（2）（10 frame window、stride）</p>
<blockquote>
<p>第二行 MLP 10帧作为一个窗 意思是 我们现在 同样还是MLP，但是 现在MLP的 输入 不仅是 只有一帧的特征，而是把 每10帧 放到一起，那么这里是否有stride，就是说 这10帧 到底有没有交叠 并没有介绍，总之 第二行这个 输入 比 第一行 覆盖的 时间窗口 会更大一点 ；</p>
<p>那么这样可以看到 这个误差，显著的从 46% 降到 32%，这个结果说明 在语音识别 这个序列建模 任务中，当我们把 上下文特征 一起考虑的话 效果会 更好；这是第二行。</p>
</blockquote>
<p><img alt="image-20241220123237089" src="../images/image-20241220123237089.png" /></p>
<p><strong>（3）delay</strong></p>
<p>第三行，将MLP换成了 循环神经网络，一个简单的RNN 模型，并且括号 delay 0，等下会解释 什么叫delay，这里的意思就说，就是说 把 每一帧特征 像 第一幅图一样，比如说</p>
<p><img alt="image-20241220123710787" src="../images/image-20241220123710787.png" /></p>
<p>这里是第一帧的特征，这里是第二帧的特征，这里是第三帧的特征，我们把每一帧的特征 送入到RNN中，通过中间的隐含层 对历史信息 进行更新，这样的网络 错误率也是相比MLP 更进一步，看到训练误差到30%，测试误差是35%，相比于上面 10帧的MLP，效果更好。</p>
<p><strong>（4）LSTM</strong></p>
<p><img alt="image-20241220123805856" src="../images/image-20241220123805856.png" /></p>
<ul>
<li>接下来 如果我们把RNN，替换成LSTM，效果更进一步</li>
<li>都是delay 0</li>
</ul>
<p><strong>（5）LSTM+backwards</strong></p>
<p><img alt="image-20241220123952932" src="../images/image-20241220123952932.png" /></p>
<p>再下面一步，还是LSTM，只是把输入 翻转过来，也就是把input序列倒过来，再输入到网络中，误差是差不多的，所以 仅仅是一条链的话，不论是正向识别，还是反向识别 其实效果是差不多的</p>
<p><strong>（6）RNN delay 3</strong></p>
<p><img alt="image-20241220124429146" src="../images/image-20241220124429146.png" /></p>
<p>对输入进行改造，首先可以看到 同样是用 RNN网络，这里 对它 进行 delay 三帧，然后可以看到 它的效果 相比于原本的 RNN 从30% 降低到 29%，测试误差 也是从 35% 降低到 34%；</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 那么这个 delay 3 帧是什么意思呢？</li>
</ul>
<p><img alt="image-20241220124532087" src="../images/image-20241220124532087.png" /></p>
<p>delay 3 帧的意思就是说，当 喂入 三帧 作为 输入的时候，前面 这三个输出，先不要，</p>
<p>就是说 先拿 三帧输入 送入到网络中 让它先对记忆单元 去 更新三步 ，然后到第四步（帧）的 输入的时候，才 把 输出拿出来， 作为 第一帧的预测值，这个就是delay 3的意思</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 为什么 delay 3 帧效果有效？</li>
</ul>
<p>如果 不做delay的话 ，在 输入 第一帧的 特征的时候，它的预测的输出 只能 看到当前的第一帧，范围就很小；</p>
<p>但是当 delay 三帧的时候 预测第一帧的输出 其实就看到了 三帧，它看到了 第一帧、第二帧、第三帧 都进入了 记忆单元中；</p>
<p>以上就是 delay RNN的结构；</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 再次解释 delay</li>
</ul>
<p>delay 能够在 短暂 的 牺牲 时延的情况下，提高精度，看到更宽的上下文</p>
<blockquote>
<p>有 delay 的话，在预测第一帧的输出的时候 肯定会 稍微 延迟一点，因为 如果 不做 delay的话，我们就直接 算一步就好了，如果delay 三帧的话，那在预测第一帧的输出的时候，需要计算 三步，所以肯定会有 一定时延的。但是这个时延 确实能够 使得 预测的效果更好，因为它看到的上下文 会 更宽一点；以上是delay的意思。</p>
</blockquote>
<p><strong>（7）B</strong> </p>
<p><img alt="image-20241220124913764" src="../images/image-20241220124913764.png" /></p>
<p><strong>双向的LSTM、RNN</strong></p>
<p><img alt="image-20241220124945634" src="../images/image-20241220124945634.png" /></p>
<ul>
<li>RNN delay三帧 和LSTM delay 五帧 效果都有不同程度的增加；</li>
<li>双向的结果比delay 和 单向的 效果都要好；</li>
<li>训练集 错误率从29%降低到24%；</li>
<li>测试集错误率也是明显降低；</li>
</ul>
<p><u>双向、delay</u> </p>
<ul>
<li>表示 看到了未来的信息；</li>
<li>当 delay三帧的话，在预测第一帧的输出的时候 其实是看到了第二帧、第三帧、第四帧  指的是 看到了未来的三帧的</li>
<li>当预测 第二帧的输出的时候 同样 看到第三帧、第四帧、第五帧</li>
<li>
<p>虽然也看到了未来的信息，但看到未来的信息还是不够长；</p>
</li>
<li>
<p>如果把单向 换成双向的网络的话，那么整个未来的特征 和 过去的 特征，网络都能看到，这就是说双向的范围 更大一点；</p>
</li>
</ul>
<blockquote>
<ul>
<li>
<p>单向delay 3：输出第一帧看到的是 输入第一帧、第二帧、第三帧</p>
</li>
<li>
<p>双向delaye 3：输出第一帧，看到的是第一帧、第二帧、第三帧+第四帧、第五帧、第六帧 </p>
</li>
</ul>
</blockquote>
<p><u>双向的缺点</u></p>
<p>需要完全的 把整个输入特征序列 送入到网络中 ，最后才能得到输出</p>
<blockquote>
<p>而单向带时延的情况就不需要把整个特征 都算出来 才能预测第一帧，只要有三帧了，就可以预测第一帧了；</p>
<p>所以单向带时延的，响应速度会更快；</p>
<p>双向的响应速度肯定是最慢的；</p>
<p>所以在速度 和效果上 需要 取得一个比较好的平衡 才能满足具体的业务需求。</p>
</blockquote>
<h2 id="k4">k4 循环神经网络的优缺点<a class="headerlink" href="#k4" title="Permanent link">&para;</a></h2>
<p><strong>一、优点</strong></p>
<blockquote>
<p>（1）权重共享可以处理变长序列</p>
<p>（2）模型的大小 与 序列长度无关</p>
<p>（3）计算量与序列长度呈现线性关系</p>
<p>（4）考虑历史信息</p>
<p>（5）便于流式输出</p>
<p>（6）权重时不变</p>
</blockquote>
<p><strong>二、缺点</strong></p>
<blockquote>
<p>（1）串行计算速度慢</p>
<p>（2）无法获取太长的历史信息</p>
</blockquote>
<p><strong>第一点</strong></p>
<p>优点可以处理变长序列</p>
<blockquote>
<p>这个是DNN和CNN处理不了的，比如DNN，输入的特征是固定的，而CNN的不仅和kernel size有关，还跟输入的通道数有关，所以如果CNN 输入通道数有变化的话 还需要重新搭建一个网络，而RNN 是可以处理变长序列的</p>
</blockquote>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 为什么RNN 能处理变长序列呢？</li>
</ul>
<p><img alt="image-20241220125908343" src="../images/image-20241220125908343.png" /></p>
<p>原因是因为，可以看到图中 有一个w</p>
<blockquote>
<p>也就是 权重，这个w在每个时刻 都是相等的，正是因为 所有的权重，在每一个时刻都是相等的；不论是 输入 跟既有单元的连接，还是历史信息 跟当前的神经元的连接 它的权重都是固定的，正是因为 权重 在每一时刻 共享，所以 RNN 能够处理变长序列；</p>
</blockquote>
<p>一旦去掉了 <u>权重 共享</u> 这个归纳偏置的话，就是说，如果每一时刻 都有一个 不一样的 w的话，这个时候 就不能处理 变长序列了，就类似 position embedding 一样，只要遇到了 长度 比训练集大的，那就处理不了了（也不是，三角变换）；</p>
<p><strong>第二点</strong></p>
<p><img alt="image-20241220130243673" src="../images/image-20241220130243673.png" /></p>
<p>第二点，模型的大小 与 序列长度无关，这里说的是 模型的大小，是说模型的参数数量 与 长度无关，模型的全部参数 和序列长度 都是无关的，只输入特征 和输入通道数 以及RNN的隐含单元有关</p>
<p><strong>第三点</strong></p>
<p><img alt="image-20241220130406832" src="../images/image-20241220130406832.png" /></p>
<p>第三个优点就是 RNN的计算量 跟 序列长度 呈线性增长，类比Transformer，在原本的Transformer中 最大的一个 诟病的地方 就是 计算复杂度 跟序列长度 是呈一个平方关系的，但是在RNN中，计算量 是跟长度 呈现 线性增长的；</p>
<blockquote>
<p>举例子：</p>
<p>当 序列长度 为2的 时候，计算量 可能就是2t</p>
<p>（t指的是时间？模型 固有的计算量）</p>
<p>当序列长度为3 的时候，计算量 就是3t，就不是说 从 4变成9，呈现 平方关系。</p>
<p>在RNN中 呈现 线性关系；这是跟 Transformer 在计算量上 一个明显的区别。</p>
</blockquote>
<p><strong>第四点</strong></p>
<p><img alt="image-20241220130603036" src="../images/image-20241220130603036.png" /></p>
<p>相比DNN而言，RNN是可以考虑到 历史信息的，因为有链式的结构，可以通过隐含层 来积累 历史信息；</p>
<p><strong>第五点</strong></p>
<p><img alt="image-20241220130731325" src="../images/image-20241220130731325.png" /></p>
<p>流式 输出，可以看到：</p>
<p><img alt="image-20241220130819372" src="../images/image-20241220130819372.png" /></p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 流式输出是什么？</li>
</ul>
<p>每 计算一步，都可以得到 一个输出，这个输出 可以直接 送给 用户，这就是 流式 的意思。</p>
<blockquote>
<p>但是对于 Transformer而言的话，由于它是考虑到全局的信息 计算一个 全局的self attention，所以就不能单步 的计算 每一步的 输出，这就是 Transformer的一个缺点，不能直接的 应用到 流式的场景；</p>
<p>但是在循环神经网络中，只要每算一次 递归运算，就可以得到一个输出，这个 输出就可以直接返回给用户，这就是流式的，也就是 不需要 把 整个序列 都算完 才返回给用户，而是说 每算出一个 时刻 都可以返回给用户</p>
</blockquote>
<p><strong>第六点</strong></p>
<p><img alt="image-20241220131059489" src="../images/image-20241220131059489.png" /></p>
<p>权重时不变</p>
<blockquote>
<p>权重是 时不变的，正是因为RNN 权重 时不变，所以RNN 可以处理 变长序列；</p>
</blockquote>
<p><strong>二、缺点</strong></p>
<p><img alt="image-20241220131203763" src="../images/image-20241220131203763.png" /></p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 为什么说 串行计算慢</li>
</ul>
<p>因为 在算 每一时刻的时候 都需要等 上一时刻的历史信息，等上一时刻的算出来 才能算 下一时刻，是一个 串行的过程，比较慢</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 怎么理解 RNN 也是无法获取太长的历史信息</li>
</ul>
<p>也就是说 由于梯度消失的问题，导致RNN无法 从当前时刻 获取很久远的信息</p>
<blockquote>
<p>RNN 由于梯度消失的问题，无法获得太长的历史信息。</p>
<p>这一点正是Transformer的优点。</p>
<p>Transformer的归纳偏置 是比较弱的，是通过一个 全局的self attention，来计算 两两位置之间的一个相关性，所以Transformer是可以上下去捕捉 很长的历史关联性的。</p>
</blockquote>
<h2 id="k5-rnn">k5 RNN 的应用场景<a class="headerlink" href="#k5-rnn" title="Permanent link">&para;</a></h2>
<p><img alt="image-20241220131740414" src="../images/image-20241220131740414.png" /></p>
<p><strong>（1）生成任务</strong></p>
<p>生成任务，比如歌词生成、对联生成、像GPT一样写小说</p>
<p>生成任务，如果用一幅图来表示：</p>
<p><img alt="image-20241220131838603" src="../images/image-20241220131838603.png" /></p>
<p>1、如图表示RNN在诗歌、语音、符号生成中的表示</p>
<p>2、这类任务可以看成one to many的过程，也就是说 只要给了 一个输入，或者一个很短的 输入，RNN就可以利用自己的 递归机制 不断的预测 新的输出，就比如 给出 一两句话，RNN 写出一段话 或者 一篇文章，就是 one to many，RNN在生成任务上的应用</p>
<p><strong>（2）情感分类</strong></p>
<p>RNN也能做情感分类</p>
<blockquote>
<p>比如说很古老的一个情感分类任务，对影评进行分类，判断一句话是正向情感还是负向情感，对于一个情感分类任务，可以看成many to one的任务</p>
</blockquote>
<p><img alt="image-20241220134939061" src="../images/image-20241220134939061.png" /></p>
<p>输入是一段话或者说一篇文章，但是输出 只有一个，只需要对一段话预测一个类别就好了，这个就是many to one的任务，典型的应用场景就是去情感分类</p>
<p><img alt="image-20241220135039924" src="../images/image-20241220135039924.png" /></p>
<p>many to many的任务：</p>
<ul>
<li>词法识别</li>
<li>机器翻译</li>
</ul>
<p>词法识别就是识别当前这个词是名词还是动词，当前这个单词多音字等等</p>
<p>机器翻译，在Transformer中是应用比较多的；</p>
<p>但是这两种 many  to many的模型结构还是有一些区别的，可以看到下面两幅图：</p>
<p>（一）词法识别</p>
<p><img alt="image-20241220135218541" src="../images/image-20241220135218541.png" /></p>
<ul>
<li>
<p>识别一句话中，每个字的拼音是什么，或者识别每个词的词性，这种就是many to many</p>
</li>
<li>
<p>属于直进直出的many to many</p>
</li>
</ul>
<p>（二）机器翻译</p>
<p><img alt="image-20241220135306632" src="../images/image-20241220135306632.png" /></p>
<ul>
<li>sequence to sequence 结构；</li>
<li>有编码器，有解码器，中间依靠注意力机制，来帮助解码器预测每一时刻的输出，也是many to many；</li>
<li>常见的应用场景：机器翻译、语音合成等</li>
</ul>
<p><img alt="image-20241220135405098" src="../images/image-20241220135405098.png" /></p>
<p>语言模型 RNNLM；</p>
<p>总之就是</p>
<ul>
<li>one to one</li>
<li>Many to one</li>
<li>many to many</li>
</ul>
<h2 id="k6-rnn">k6  RNN框图<a class="headerlink" href="#k6-rnn" title="Permanent link">&para;</a></h2>
<p><img alt="image-20241220135618631" src="../images/image-20241220135618631.png" /></p>
<h2 id="torchnnrnn">torch.nn.RNN<a class="headerlink" href="#torchnnrnn" title="Permanent link">&para;</a></h2>
<p><img alt="image-20241220135729914" src="../images/image-20241220135729914.png" /></p>
<ul class="task-list">
<li>
<p>可以用来构造一层 或者多层 简单的RNN结构； </p>
</li>
<li>
<p>RNN还有另外一种结构：激活函数，可以用tanh激活函数 或者 ReLU激活函数，使得RNN有更强的非线性建模能力；</p>
</li>
<li class="task-list-item">
<p><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> RNN 计算公式是什么呢？</p>
</li>
</ul>
<p><img alt="image-20241220135820029" src="../images/image-20241220135820029.png" /></p>
<ul>
<li>
<p>每一时刻的输出，或者说每一时刻的状态</p>
</li>
<li>
<p>在简单RNN中，输出是等于状态的， <span class="arithmatex">\(h_t\)</span>也就是 <span class="arithmatex">\(t\)</span> 时刻的输出；</p>
</li>
<li>
<p>或者说 t 时刻RNN的状态 等于 tanh函数，就是非线性激活函数，里面分别是 <span class="arithmatex">\(W_{ih}×x_t\)</span> 再加上 <span class="arithmatex">\(b_{ih}\)</span>，那么这里的<span class="arithmatex">\(x_t\)</span>，就是当前时刻的输入，然后<span class="arithmatex">\(w_{ih}\)</span>，就是在这个RNN中，它对输入的权重矩阵，就是 会用这个矩阵 来对权重 做一个映射，然后整体上，这个东西 可以看做 linear层，有权重 还有 偏置，<span class="arithmatex">\(b_{ih}\)</span>，就是关于 权重的一个偏置</p>
</li>
<li>
<p>后面 还有一项，跟 历史状态有关的，跟 <span class="arithmatex">\(h_{t-1}\)</span> 有关的</p>
</li>
<li>也就是说，需要将 上一时刻的 输出 或者说 上一时刻的隐含状态 拿过来，然后对它进行一个 映射，用 <span class="arithmatex">\(w_{hh}\)</span> 的权重 来进行相乘，来进行映射，然后再加上一个偏置</li>
<li>总体而言 就是说 每一时刻的输出 或者说 隐含状态 不光跟当前时刻 的 输入 <span class="arithmatex">\(x_t\)</span> 有关，同时也跟上一时刻的记忆单元  <span class="arithmatex">\(h_{t-1}\)</span>有关，并且都是线性组合的关系，最后通过一个非线性激活函数就能得到当前时刻的隐含状态；</li>
</ul>
<p><img alt="image-20241220140258663" src="../images/image-20241220140258663.png" /></p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 解释：</li>
</ul>
<p><span class="arithmatex">\(h_t\)</span> 是 <span class="arithmatex">\(t\)</span>时刻的隐含状态</p>
<p><span class="arithmatex">\(x_t\)</span>是 t 时刻的输入</p>
<p><span class="arithmatex">\(h_{t-1}\)</span>是  <span class="arithmatex">\(t-1\)</span>时刻的隐含状态</p>
<p><span class="arithmatex">\(h_0\)</span> 表示初始时刻的隐含状态</p>
<p>pytorch中也提供了两种 非线性激活函数：tanh和relu激活函数，默认用tanh激活函数</p>
<p><img alt="image-20241220140410079" src="../images/image-20241220140410079.png" /></p>
<ul class="task-list">
<li>这是一个 class</li>
<li>在用RNN时候，首先要 实例化 这个class</li>
<li>实例化 class以后，得到RNN的一个模型</li>
<li>然后 再把 输入 喂入到 模型中，而不直接把 输入 喂入到模型中；</li>
<li>
<p>一般所有模型的 class，都需要 先进行一个实例化，然后才能得到一个layer；</p>
</li>
<li class="task-list-item">
<p><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 实例化RNN所需要的参数</p>
</li>
</ul>
<p><img alt="image-20241220140648773" src="../images/image-20241220140648773.png" /></p>
<ul>
<li>
<p>第一个参数是 <code>input_size</code>,也就是 输入特征的大小，也就是 <code>x</code> 的特征的维度</p>
</li>
<li>
<p>第二个参数是 <code>hidden_size</code>，<code>hidden_size</code>决定了 <span class="arithmatex">\(h_t\)</span>的大小，就是每一时刻的 <span class="arithmatex">\(h_t\)</span>就是一个向量，对于单一样本而言，每一时刻 <span class="arithmatex">\(h_t\)</span>就是一个向量，那么这个向量长度是多少呢？就是由 <code>hidden_size</code> 来决定</p>
</li>
<li>
<p>第三个参数 就是 <code>num_layers</code>，就是说 这个RNN，可以默认实例化的时候 只有一层，但是也可以改变 <code>num_layers</code>的值，变成多层，堆叠起来的结构，之前在介绍的时候也讲过，可以堆叠起来，单向的可以堆叠，双向的 也可 堆叠</p>
</li>
<li>
<p>第四个参数 就是 非线性激活函数，这里默认是<code>tanh</code>函数，也可以改成 <code>ReLu</code>函数</p>
</li>
<li>第五个是<code>bias</code> 一般会加上 这两个bias</li>
<li>第六个参数是 <code>batch first</code>，这个需要注意一下，这个参数就决定了 输入和输出的格式</li>
</ul>
<blockquote>
<ul>
<li>如果设置 <code>batch first=true</code>的话：</li>
</ul>
<p>提供的输入张量 和 输出张量的 格式就是 <code>batch × sequence length×feature</code> 这样的格式</p>
<p>默认是<code>false</code>的，如果是 <code>false</code>的情况下：</p>
<p>需要保证 输入的格式是 <code>sequence length</code>，也就是序列长度 在第一个维度，<code>batch size</code>在第二个维度，<code>feature size</code>在第三个维度</p>
</blockquote>
<ul>
<li>第七个参数 <code>dropout</code></li>
<li>最后一个参数<code>bidirectional</code>，最后一个参数 表示 双向的意思</li>
</ul>
<blockquote>
<p>也就是把这个参数设置为 <code>true</code>的话，就可以构建一个双向的RNN结构；</p>
<p>既然是 双向RNN结构，输出的特征大小就是<code>2×feature size</code>，就是2倍的<code>feature size</code>；</p>
</blockquote>
<p><u>双向结构图</u></p>
<p><img alt="image-20241220141145243" src="../images/image-20241220141145243.png" /></p>
<ul>
<li>
<p>这幅图 就是 双向的，一旦把RNN设置成 双向的话，最终的输出 是由<code>forward输出</code>和<code>backward输出</code>一起拼起来的，所以这个 输出状态是 二倍的 <code>hidden size</code>，可以指定 <code>concat</code>和<code>sum</code>，一般用 <code>concat</code> 更多一点</p>
</li>
<li>
<p>也就是说 如果 设置 <code>hidden size是16</code>的话，那么 <code>output layer</code>大小，就是32，如果是双向的话</p>
</li>
</ul>
<p>以上是RNN实例化的参数讲解；</p>
<ul>
<li>当实例化完以后，就得到了RNN层</li>
<li>然后就可以 提供 输入 和 初始的隐含状态，来去递归的算出 每一时刻的 输入 所对应的输出是什么；</li>
</ul>
<p>当实例化完 一个RNN，就可以 提供 <code>input</code> 和 <span class="arithmatex">\(h_0\)</span>，来给出真正的输入序列：</p>
<p><img alt="image-20241220141454221" src="../images/image-20241220141454221.png" /></p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 解释input</li>
</ul>
<p>输入一般是三维的：</p>
<p><img alt="image-20241220141550442" src="../images/image-20241220141550442.png" /></p>
<p>如果设置的<code>batch size first等于true</code>的话，那对应的输入格式就是 <code>batch size×sequence length×hidden size</code>；</p>
<p>反之 如果<code>没有设置batch size等于true</code>的话，提供的格式就是 <code>sequence length×batch size×hidden size</code></p>
<ul class="task-list">
<li class="task-list-item">
<p><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 解释 <span class="arithmatex">\(h_0\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(h_0\)</span>的格式是 (<span class="arithmatex">\(d×{num\_layers}\)</span>， <span class="arithmatex">\(N\)</span>，<span class="arithmatex">\(H_{out}\)</span> )</p>
</li>
<li>
<p><span class="arithmatex">\(h_0\)</span> 是 初始状态，只有 这一个时刻，所以这里不需要考虑 <code>sequence_length</code> 这个维度</p>
</li>
<li class="task-list-item">
<p><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 那这里也是 三个维度，为什么呢？</p>
</li>
</ul>
<p>因为  RNN 可以是 多层 也可以是 双向，所以第一个维度 其实就是 是否是 双向  跟 多层 这两个因素 决定的；</p>
<p><code>case1：</code>如果模型是一层，并且是单向的话，那么第一个维度 就是 1 ；</p>
<p><code>case2：</code>如果是 有两层，并且是 单向的话，那么就是 1×2；</p>
<p><code>case3：</code>如果是双向 并且是 两层的话，那就是 2×2=4；</p>
<p>所以这里的 第一个维度 <span class="arithmatex">\(d \times num\_layers\)</span> 由是否双向 以及 层数有关</p>
<p><img alt="image-20241220142609695" src="../images/image-20241220142609695.png" /></p>
<p>第二个维度 <span class="arithmatex">\(N\)</span>，就是 <code>batch size</code>，每个样本 都可以 设置一个 初始状态</p>
<p>第三个维度 <span class="arithmatex">\(H_{out}\)</span> 就是 <code>hidden size</code>的大小，因为 初始状态 就是一个向量,第三维 就是 向量的长度</p>
<h2 id="_1">代码示例<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<h3 id="1-rnn">1 单层单向 RNN<a class="headerlink" href="#1-rnn" title="Permanent link">&para;</a></h3>
<blockquote>
<p>这个RNN 是一个 class</p>
<p>所以，首先实例化一个单向单层的RNN</p>
</blockquote>
<p>step1：import  torch.nn as nn</p>
<p>step2：实例化 nn.RNN</p>
<p>step3：传入 实例化参数；</p>
<blockquote>
<ul>
<li>
<p>input_size=4</p>
</li>
<li>
<p>hidden_size也可以 随便写一个 hidden_size=3 </p>
</li>
<li>
<p>num_layers可以传入1</p>
</li>
<li>batch first设置成true</li>
<li>定义为<code>single_rnn</code></li>
</ul>
</blockquote>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-0-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-0-3"><span class="c1"># 1.单向、单层RNN</span>
</span><span id="__span-0-4"><span class="n">single_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>以上是 单层单向RNN，接下来构建一个输入</p>
<p>输入的维度一般是 <code>batch_size×sequence length×输入特征</code></p>
<p>输入特征就是RNN实例化时的 <code>input size=4，batch size=1，sequence length=2，特征维度=4</code></p>
<p>以上构建好了input序列，分别是： <code>batch_size × sequence length×输入特征</code></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-1-1"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> 
</span><span id="__span-1-2"><span class="c1"># batch_size*sequence_length*feature_size</span>
</span></code></pre></div></td></tr></table></div>
<p>把这个<code>input</code>作为 <code>single_rnn</code>的输入；</p>
<p>也可以不传入<span class="arithmatex">\(h_0\)</span>,它默认以<span class="arithmatex">\(0\)</span>向量填充</p>
<p><img alt="image-20241220143813357" src="../images/image-20241220143813357.png" /></p>
<p>同时也可以看看 官网 api 输出是什么</p>
<p><img alt="image-20241220143851387" src="../images/image-20241220143851387.png" /></p>
<p>输出是两个值，一个是整个的，所有时刻的输出；</p>
<p>另外一个输出的量就是最后一个时刻的状态，要定义变量接收输出</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-2-1"><span class="n">output</span><span class="p">,</span><span class="n">h_n</span> <span class="o">=</span> <span class="n">single_rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>这样整个输出就算完了，接下来看一下<span class="arithmatex">\(output\)</span>和 <span class="arithmatex">\(h_n\)</span></p>
<p><img alt="image-20241220144015618" src="../images/image-20241220144015618.png" /></p>
<p>代码解读：</p>
<p>（1） <code>input</code>的形状 <code>1×2×4 = batch size×sequence length×feature dim</code></p>
<p>（2）<code>single_rnn</code> 的参数含义：<code>4,3,1=input_size,hidden_size;num_layers</code></p>
<p>（3）<code>output</code>大小就是 <code>1×2×3</code></p>
<ul>
<li>1表示 batch size，输入batch size=1，输出 batch size也是1，没有改变</li>
<li>2是 sequence length，序列长度，我们喂入的输入长度是2，所以输出的长度也是2</li>
<li>3，第三个维度为什么是3呢？因为我们设置的hidden size=3，也就是说 每个输出的状态向量 长度是3</li>
</ul>
<p>（4）<span class="arithmatex">\(h_n\)</span>： 最后一个时刻的隐含状态，在简单RNN中，最后一个时刻的隐含状态等于最后时刻的输出的，output最后一行的值 等于 <span class="arithmatex">\(h_n\)</span></p>
<p><img alt="image-20241221102700631" src="../images/image-20241221102700631.png" /></p>
<h3 id="2-rnn">2 双向、单层RNN<a class="headerlink" href="#2-rnn" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-3-1"><span class="n">single_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>
<p>input size不变</p>
</li>
<li>
<p>hidden size不变</p>
</li>
<li>num_layers不变</li>
<li>batch first也不变</li>
<li>但是需要新增一个参数，叫做：</li>
</ul>
<p><img alt="image-20241221103009943" src="../images/image-20241221103009943.png" /></p>
<p>：bidirectional，这个参数默认是false，把它置成true</p>
<p>然后命名为 bidirectional_rnn：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-4-1"><span class="n">bidirectional_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>以上是实例化的双向RNN</p>
<ul>
<li>输入特征大小是4</li>
<li>输出 or 隐含层大小是3</li>
<li>只有一层</li>
<li>batch first=true</li>
<li>并且还是双向的</li>
</ul>
<p>同样把上面的输入 送入双向RNN中，以<code>input</code>作为输入<code>bidirectional_rnn(input)</code>，因为无论双向、单向，输出都是一样的，都是<code>output</code> 和 <code>h_n</code>，表示区别加前缀<code>bi</code></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-5-1"><span class="n">bi_output</span><span class="p">,</span><span class="n">bi_h_n</span> <span class="o">=</span> <span class="n">bidirectional_rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>首先 打印 output的形状</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-6-1"><span class="n">bi_output</span><span class="o">.</span><span class="n">shape</span>
</span></code></pre></div></td></tr></table></div>
<p><img alt="image-20241221103251339" src="../images/image-20241221103251339.png" /></p>
<p>还有h_n的形状：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-7-1"><span class="n">bi_h_n</span><span class="o">.</span><span class="n">shape</span>
</span></code></pre></div></td></tr></table></div>
<p><img alt="image-20241221103335046" src="../images/image-20241221103335046.png" /></p>
<p>对比，把单向单层RNN的output的形状，h_n的形状，都打印出来：</p>
<p><img alt="image-20241221103356696" src="../images/image-20241221103356696.png" /></p>
<ul>
<li>首先从输出上来讲：</li>
</ul>
<p>（1）单向的输出大小是 1×2×3的</p>
<p>（2）双向的话变成了 1×2×6（一个batch size；2个sequence length；6个特征维度）</p>
<blockquote>
<p>这是为什么呢？</p>
<p>这是因为在双向RNN中最后是把<code>forward layer</code>和<code>backward layer</code>两个输出拼起来，所以特征大小变成了两倍的<code>hidden size</code>；</p>
</blockquote>
<ul>
<li>最后一个时刻的状态也是不一样的</li>
</ul>
<p>（1）在双向RNN中，它的维度是 2×1×3（前向的输出是个 1×3，后向的输出也是一个1×3）</p>
<p>（2）在单向中，维度是1×1×3</p>
<blockquote>
<p>为什么呢？</p>
<p>因为双向中，其实是有两个层的最后一个时刻状态，有一个<code>forward layer</code>和一个<code>backward layer，</code>这两个状态在第一个维度上拼起来了，但是在单向中，只有一层的最后一个状态；</p>
</blockquote>
<h3 id="3-rnn-api">3 RNN api 代码汇总<a class="headerlink" href="#3-rnn-api" title="Permanent link">&para;</a></h3>
<p><img alt="image-20241221103816446" src="../images/image-20241221103816446.png" /></p>
<p><img alt="image-20241221103830961" src="../images/image-20241221103830961.png" /></p>
<h3 id="4-rnnrnn">4 单向RNN&amp;双向RNN 从矩阵运算的角度实现<a class="headerlink" href="#4-rnnrnn" title="Permanent link">&para;</a></h3>
<p>注意：以下演示中，没有设置多层， num layers都定义的1层</p>
<p>（1）引入库，可以使用常见的pytorch函数</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-8-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-8-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span>
</span></code></pre></div></td></tr></table></div>
<p>（2）定义常量</p>
<p>然后定义一些常量，比如batch size、序列长度</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-9-1"><span class="n">bs</span><span class="p">,</span><span class="n">T</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="mi">3</span>  <span class="c1">#批大小 和 序列长度</span>
</span></code></pre></div></td></tr></table></div>
<p>还需要定义 input size和hidden size，分别表示输入特征大小 和 隐含层 特征大小</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-10-1"><span class="n">input_size</span><span class="p">,</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span> <span class="c1">#输入特征大小，隐含层特征大小</span>
</span></code></pre></div></td></tr></table></div>
<p>有一个问题：怎么理解 时序模型中的 batchsize？</p>
<p>（3）生成 input</p>
<p>有了这些量以后，生成一个  input ，还是考虑batch first等于true的情况：第一个位置写batch size、第二个位置写序列长度、第三个位置写feature dim，也就是 input size</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-11-1"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">input_size</span><span class="p">)</span> <span class="c1"># 随机初始化一个输入特征序列</span>
</span></code></pre></div></td></tr></table></div>
<p>（4）初始化隐状态</p>
<p>初始化一个初始的隐含状态 <code>h_0</code>，初始的隐含状态一般是一个向量，如果考虑了<code>batch size</code>，就应该是 <code>batch size</code>个这样的状态，也可以先写成0：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-12-1"><span class="n">h_prev</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># 每一个状态向量大小是 hidden size</span>
</span></code></pre></div></td></tr></table></div>
<p>也就是在第一个时刻的时候，需要一个初始的隐含状态来，来作为第0时刻的初始状态</p>
<p><img alt="image-20241221134624116" src="../images/image-20241221134624116.png" /></p>
<p>（5）调用pytorch RNN的API</p>
<p>还是用<code>nn.RNN()</code>的api，需要传入<code>input_size</code>，<code>hidden size</code>还有<code>batch first=True</code>，这样我们得到一个rnn</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-13-1"><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>（6）传入参数</p>
<p>需要把 input 以及初始状态也传入RNN中，但是需要注意的是，api中初始状态是三维的</p>
<p><img alt="image-20241221134803507" src="../images/image-20241221134803507.png" /></p>
<p>刚刚初始化的是 后面两维，第三维 我们没有初始化，因为这里是单向的 并且 只有一层的，所以对它扩一维就好了，扩0维，得到rnn output和h_finall，最后一个时刻的状态，或者叫state_finall</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-14-1"><span class="n">rnn_output</span><span class="p">,</span><span class="n">state_finall</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">h_prev</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></code></pre></div></td></tr></table></div>
<p>这个是调用pytorch 官方的api，运行打印，看结果</p>
<p><img alt="image-20241221134949743" src="../images/image-20241221134949743.png" /></p>
<p>（7）手写RNN forward 函数</p>
<p>定义<code>RNN forward</code>函数，实现RNN计算原理 <code>def rnn_forward():</code>，对于这个函数 首先要传入参数：</p>
<p><img alt="image-20241221135123756" src="../images/image-20241221135123756.png" /></p>
<p>根据公式，要想算出<span class="arithmatex">\(h_t\)</span>的话：</p>
<ul>
<li>需要有<span class="arithmatex">\(x\)</span>，<span class="arithmatex">\(x\)</span>就是输入，所以第一个参数，需要写的是<span class="arithmatex">\(input\)</span></li>
<li>输入需要一个投影矩阵，就是<span class="arithmatex">\(W_{ih}\)</span>，需要一个weight</li>
<li>同时还需要偏置项<span class="arithmatex">\(\mathrm{bias_{ih}}\)</span></li>
<li>还有上一时刻的隐含状态 ： <span class="arithmatex">\(W_{hh}\)</span> </li>
<li>还有 <span class="arithmatex">\(b_{hh}\)</span></li>
<li>公式中还有 <span class="arithmatex">\(h_{t-1}\)</span> ，写成 <code>h_prev</code> ，就是前一时刻的状态</li>
</ul>
<p>以上，就能算出RNN的输出</p>
<p><strong>第一步：获取当前时刻的输入特征得到<code>x</code></strong></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-15-1"><span class="k">def</span><span class="w"> </span><span class="nf">rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">weight_ih</span><span class="p">,</span><span class="n">weight_hh</span><span class="p">,</span><span class="n">bias_ih</span><span class="p">,</span><span class="n">bias_hh</span><span class="p">,</span><span class="n">h_prev</span><span class="p">):</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>input 默认 三维的结构，先把input的形状拆解出来，形状应该是 <code>batch size×sequence length×input size</code> ，调用 <code>input.shape</code></li>
</ul>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-16-1"><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>通过拆解 <code>input</code> ，还可以知道 <code>hidden size</code>，也就是<code>h_dim</code>，也就是 <code>weight_ih</code>，可以根据它的权重所得到，也就是<code>weight_ih.shape</code>，那到底是<code>shape[0]</code>还是 <code>shape[1]</code>呢？看公式：</li>
</ul>
<p><img alt="image-20241221135636390" src="../images/image-20241221135636390.png" /></p>
<p><code>weight ih</code>跟 <code>xt</code> 是左乘的关系，所以<code>weight</code>的第2个维度跟<code>x</code>是相同的，所以第一个维度 就是隐含单元的维度，所以写成<code>.shape[0]</code>，得到<code>hidden dim ：</code></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-17-1"><span class="n">h_dim</span> <span class="o">=</span> <span class="n">weight_ih</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></code></pre></div></td></tr></table></div>
<p>以上是得到了一些维度，接下来，可以写出 <code>h out</code>，首先 初始化一个 输出，输出大小是 <code>batch size×T×h dim</code>，初始化一个输出矩阵 或者 状态矩阵</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-18-1"><span class="n">h_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">h_dim</span><span class="p">)</span>  <span class="c1"># 初始化一个输出（状态）矩阵</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>
<p><code>bs</code>跟输入是一样的</p>
</li>
<li>
<p>序列长度 或者叫 时间长度 也是跟 输入一样的维度</p>
</li>
<li>需要改成 <code>hidden size</code>这个维度</li>
</ul>
<p>接下来 根据这 6 个参数，算出 <code>h out</code></p>
<p><img alt="image-20241221140038308" src="../images/image-20241221140038308.png" /></p>
<p>RNN是一个递归的计算，所以需要根据<code>x1</code>计算<code>h1</code>，根据<code>x2</code>计算<code>h2</code>等等，因此需要一个<code>for</code>循环 <code>for t in range(T):</code> </p>
<p>因为RNN的计算复杂度 跟序列长度 呈线性关系，所以对序列长度进行遍历就好了</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-19-1"><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
</span></code></pre></div></td></tr></table></div>
<p>首先得到当前时刻的输入向量，<code>input</code>，因为input是三维：</p>
<ul>
<li>第一个维度是 batch size，全都取出来</li>
<li>第二个维度是时间，就拿当前 t 时刻的输入向量</li>
<li>第三维是特征维度，也是全部拿出来</li>
</ul>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-20-1"><span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span><span class="n">t</span><span class="p">,:]</span>  <span class="c1"># 获取当前时刻输入特征，bs*input_size</span>
</span></code></pre></div></td></tr></table></div>
<p>以上是第一步：获取当前时刻的输入特征得到<code>x</code></p>
<p>第二步：扩充 batch 维度</p>
<p>根据公式，让<code>w</code>跟<code>x</code>进行相乘</p>
<ul>
<li>这里<code>weight</code>一般默认传入 是二维的</li>
<li>而<code>x</code>的大小，默认是 <code>batch size×input size</code></li>
</ul>
<p><code>weight ih</code>的形状是 <code>h dim×input size</code></p>
<p>所以为了进行<code>batch</code>维度无关的乘法运算的话：</p>
<blockquote>
<p>首先对<code>weight ih</code>进行一个扩充，把<code>weight</code>变成一个 <code>batch</code>的形式，<code>weight ih</code>是<code>hidden size×input size</code>的大小，对它 增加一维，<code>batch</code> 维度，对它进行复制，复制成跟<code>input</code>一样的大小，大小就变成了 <code>batch size×h dim×input size</code></p>
</blockquote>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-21-1"><span class="n">w_ih_batch</span> <span class="o">=</span> <span class="n">weight_ih</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> 
</span><span id="__span-21-2"><span class="c1"># bs*h_dim*input_size</span>
</span></code></pre></div></td></tr></table></div>
<p>这是 <code>w ih</code>，变成 <code>batch</code>的形状</p>
<p>同样对于<code>weight hh</code>，也是一样的，也转换一下，对它增加一个<code>batch</code>维度，然后把它的<code>batch</code>维度扩充成 <code>batch size</code>维度大小</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-22-1"><span class="n">w_hh_batch</span> <span class="o">=</span> <span class="n">weight_hh</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-22-2"><span class="c1"># bs * h_dim * h_dim</span>
</span></code></pre></div></td></tr></table></div>
<p>这里 <code>w hh</code>大小就是 <code>batch size× h dim×h dim</code>，因为跟<code>hidden state</code>相连的，所以是一个方阵</p>
<p><span class="arithmatex">\(h_t = \mathrm{tanh(W_{ih}x_t+b_{ih}+W_{hh}h_{t-1}+b_{hh})}\)</span></p>
<p>第三步：开始计算 <span class="arithmatex">\(w_{ih}× x_t、w_{hh}× h_{t-1}\)</span></p>
<p><strong>第一项：<code>w_times_x</code></strong></p>
<p>首先计算 <code>x</code>，就是计算 <code>Wih</code>乘以<code>x</code>这个量 <code>w_times_x</code>这个量，可以调用 <code>torch.bmm</code>这个函数</p>
<blockquote>
<p><code>batch matrix multiplication</code>，是含有批大小的矩阵相乘，与 批 无关的 计算矩阵相乘</p>
</blockquote>
<ul>
<li>第一个位置传入 <code>w ih batch</code></li>
<li>第二个位置 传入 <code>x</code></li>
</ul>
<blockquote>
<p>当前这个<code>x</code>是<code>batch size× input_size</code>的，为了跟 <code>w ih batch</code>相乘，需要将它 扩充一维，扩充成 <code>batch size×input size×1</code>的，这里 需要 对它 扩充一下，调用一下<code>unsqueeze</code></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-23-1"><span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span><span class="n">t</span><span class="p">,:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
</blockquote>
<p>本来是二维的，现在在第三个维度上进行扩充，变成 <code>batch size×input size×1</code>，此时跟<code>x</code>相乘，得到 <code>batch size× h dim×1</code>，最后<code>1</code>的维度去掉，调用<code>unsqueeze</code>函数，得到的结果 <code>batch size×h dim</code>，得到<code>w times x</code>的结果，偏置最后再加</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-24-1"><span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span><span class="n">t</span><span class="p">,:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 获取当前时刻的输入特征 bs*input_size*1</span>
</span><span id="__span-24-2"><span class="n">w_ih_batch</span> <span class="o">=</span> <span class="n">weight_ih</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#bs*h_dim*input_size</span>
</span><span id="__span-24-3"><span class="n">w_hh_batch</span> <span class="o">=</span> <span class="n">weight_hh</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#bs*h_dim*h_dim</span>
</span><span id="__span-24-4">
</span><span id="__span-24-5"><span class="n">w_times_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">w_ih_batch</span><span class="p">,</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># bs*h_dm</span>
</span></code></pre></div></td></tr></table></div>
<p><strong>第二项 <code>w_times_h</code></strong></p>
<p><code>Whh</code> 矩阵 跟上一时刻的状态相乘的结果</p>
<p>同样调用 <code>torch.bmm</code>函数，带有批大小的矩阵相乘，跟上一时刻的隐含状态 进行相乘</p>
<p>同样对<code>h prev</code>进行扩充，<code>h_prev.unsqueeze(2)</code>，把它扩充三维</p>
<p>因为<code>h prev</code>本来是，<code>batch size×hidden size</code>，现在变成 <code>batch size×hidden size×1</code>，乘出来以后 是 <code>batch size×hidden size ×1</code>，最后再把1 去掉，挤掉</p>
<p>这里乘的权重是方阵，不改变大小，所以还是 <code>h prev</code>的形状</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-25-1"><span class="n">w_times_h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">w_hh_batch</span><span class="p">,</span><span class="n">h_prev</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>调用 <code>squeeze</code>函数，把最后的1去掉 最后变成了 <code>batch size× h dim</code></p>
<p>这是这两个量，最后把这些东西全部加起来，跟<code>bias</code>加起来，然后通过两个tanh函数</p>
<p><img alt="image-20241221195057262" src="../images/image-20241221195057262.png" /></p>
<p>首先是 <code>w_times_x</code>这个量 然后加上 <code>bias ih</code>，最后加上 <code>w times h</code>，上一时刻隐含状态相关的，最后是<code>bias hh</code>，然后过一个 <code>tanh</code>激活函数，最终得到当前时刻的这一状态</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-26-1"><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">w_times_x</span> <span class="o">+</span> <span class="n">bias_ih</span> <span class="o">+</span> <span class="n">w_times_h</span> <span class="o">+</span> <span class="n">bias_hh</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>定义为<code>h_prev</code>，因为进行的是递归的运算</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-27-1"><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">w_times_x</span> <span class="o">+</span> <span class="n">bias_ih</span> <span class="o">+</span> <span class="n">w_times_h</span> <span class="o">+</span> <span class="n">bias_hh</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>现在计算了<span class="arithmatex">\(t\)</span>时刻的输出，接着把<span class="arithmatex">\(t\)</span>时刻的输出，放入到 <code>h out</code>中，</p>
<p>怎么放，只要放到时间长度这一维，<span class="arithmatex">\(t\)</span>行即可</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-28-1"><span class="n">h_out</span><span class="p">[:,</span><span class="n">t</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">h_prev</span>
</span></code></pre></div></td></tr></table></div>
<p>以上完成了递归的运算，最后返回 跟 pytorch官方api一样</p>
<p>首先返回<code>h_out</code></p>
<p>然后返回 最后一个时刻的隐含状态，其实也就是<code>h_prev</code></p>
<p>但是这里的<code>h_prev</code>是二维的，官方api是三维的，所以要 扩一维，扩一维的原因就是因为 自己实现的是 单向、单层的，所以在 第0维 扩充一个1 就好了</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-29-1"><span class="k">return</span> <span class="n">h_out</span><span class="p">,</span><span class="n">h_prev</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>以上是所有全手写的RNN forward函数，其实就是单向的RNN</p>
<h3 id="torchtile">torch.tile函数<a class="headerlink" href="#torchtile" title="Permanent link">&para;</a></h3>
<p>补充 torch.tile函数：沿指定维度重复张量函数</p>
<p>例子：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-30-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-30-2">
</span><span id="__span-30-3"><span class="c1"># 创建一个张量</span>
</span><span id="__span-30-4"><span class="n">weight_hh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
</span><span id="__span-30-5">
</span><span id="__span-30-6"><span class="c1"># 假设批量大小为3</span>
</span><span id="__span-30-7"><span class="n">bs</span> <span class="o">=</span> <span class="mi">3</span>
</span><span id="__span-30-8">
</span><span id="__span-30-9"><span class="c1"># 使用 unsqueeze 在第0维度增加一个维度，然后使用 tile 沿第0维度重复 bs 次</span>
</span><span id="__span-30-10"><span class="n">w_hh_batch</span> <span class="o">=</span> <span class="n">weight_hh</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-30-11">
</span><span id="__span-30-12"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;原始张量:&quot;</span><span class="p">)</span>
</span><span id="__span-30-13"><span class="nb">print</span><span class="p">(</span><span class="n">weight_hh</span><span class="p">)</span>
</span><span id="__span-30-14"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;增加维度并重复后的张量:&quot;</span><span class="p">)</span>
</span><span id="__span-30-15"><span class="nb">print</span><span class="p">(</span><span class="n">w_hh_batch</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>在这个示例中：</p>
<ol>
<li><a href="vscode-file://vscode-app/Applications/Visual Studio Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.esm.html"><code>weight_hh</code></a> 是一个形状为 <code>[2, 2]</code> 的张量。</li>
<li><a href="vscode-file://vscode-app/Applications/Visual Studio Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.esm.html"><code>weight_hh.unsqueeze(0)</code></a> 在第0维度增加一个维度，使其形状变为 <code>[1, 2, 2]</code>。</li>
<li><a href="vscode-file://vscode-app/Applications/Visual Studio Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.esm.html"><code>tile(bs, 1, 1)</code></a> 沿第0维度重复 <a href="vscode-file://vscode-app/Applications/Visual Studio Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.esm.html"><code>bs</code></a> 次（这里 <a href="vscode-file://vscode-app/Applications/Visual Studio Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.esm.html"><code>bs</code></a> 为3），使其形状变为 <code>[3, 2, 2]</code>。</li>
</ol>
<p>输出结果：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-31-1"><span class="n">原始张量</span><span class="p">:</span>
</span><span id="__span-31-2"><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
</span><span id="__span-31-3">        <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
</span><span id="__span-31-4"><span class="n">增加维度并重复后的张量</span><span class="p">:</span>
</span><span id="__span-31-5"><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
</span><span id="__span-31-6">         <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
</span><span id="__span-31-7">
</span><span id="__span-31-8">        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
</span><span id="__span-31-9">         <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
</span><span id="__span-31-10">
</span><span id="__span-31-11">        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
</span><span id="__span-31-12">         <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]])</span>
</span></code></pre></div></td></tr></table></div>
<p>这样，<a href="vscode-file://vscode-app/Applications/Visual Studio Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.esm.html"><code>w_hh_batch</code></a> 就是一个形状为 <code>[3, 2, 2]</code> 的张量，其中每个批次都包含原始的 <a href="vscode-file://vscode-app/Applications/Visual Studio Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.esm.html"><code>weight_hh</code></a> 张量</p>
<h3 id="5">5 验证<a class="headerlink" href="#5" title="Permanent link">&para;</a></h3>
<p>验证思路：</p>
<blockquote>
<p>把之前实例化的RNN网络，参数拿出来，填充到自定义的网络中</p>
<p>然后算出来的结果 如果是跟官方API结果一致的话，就表明自定义的函数是正确的</p>
</blockquote>
<p><strong>首先，拿出RNN的参数：</strong></p>
<p>（1）RNN有哪些参数？</p>
<p><code>nn.Module</code>这个类，</p>
<p>① 在<code>pytorch</code>中 所有的层，都是继承自<code>nn.Module</code>这个类</p>
<p>② <code>nn.Module</code>的函数：<code>name.parameters</code>这个函数，查看 RNN中 有哪些参数</p>
<blockquote>
<p><code>name.parameters</code>是一个生成器，可以用循环得到 <code>for p,n in</code> </p>
<p>p：参数</p>
<p>n：name</p>
<p><code>in rnn.named_parameters():</code> 就能看到 rnn有哪些参数</p>
</blockquote>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-32-1"><span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">n</span> <span class="ow">in</span> <span class="n">rnn</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
</span></code></pre></div></td></tr></table></div>
<p>打印查看结果：RNN有哪些参数 以及 它的名称</p>
<p><img alt="image-20241221200129294" src="../images/image-20241221200129294.png" /></p>
<p>可以看到 RNN的所有的参数、名称、具体地张量的数值 </p>
<p>一共有四个参数，分别是</p>
<p>①第一个参数： <code>weight ih l0</code></p>
<ul>
<li><code>weight ih</code> ： 公式里的 <code>wih</code></li>
<li><code>l0</code>：网络定义只有一层，层数是从<span class="arithmatex">\(0\)</span>开始的，所以是从<code>l0</code></li>
</ul>
<p>② 第二个参数：<code>weight hh l0</code></p>
<p>表示当前层 <code>w hh</code>的参数</p>
<p>另外两个就是偏置了，分别是</p>
<p>③第三个参数： <code>bias ih</code></p>
<p>④第四个参数： <code>bias hh</code>  </p>
<p>需要注意的是：</p>
<ul>
<li>前面两个权重张量 是 二维张量</li>
<li>后面两个偏置是 一维的向量</li>
</ul>
<p>（2）现在把这些参数 代入到自己写的<code>RNN forward</code>函数中</p>
<p>首先，复制一下 自己写的函数签名</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-33-1"><span class="n">rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">weight_ih</span><span class="p">,</span><span class="n">weight_hh</span><span class="p">,</span><span class="n">bias_ih</span><span class="p">,</span><span class="n">bias_hh</span><span class="p">,</span><span class="n">h_prev</span><span class="p">):</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li><code>input</code>还是<code>input</code></li>
<li><code>weight ih</code>可以改成 <code>rnn.</code>，直接用<code>rnn.参数名称</code>，就可以访问这个参数 ，<code>rnn.weight_ih_l0</code></li>
<li><code>weight hh</code>也是一样，用<code>rnn.</code>来进行访问：<code>rnn.weight_hh_l0</code></li>
<li><code>bias</code>也是一样的 对应的是 <code>rnn.bias_ih_l0</code></li>
<li>同样<code>hh bias</code>也是一样的 <code>rnn.bias_hh_l0</code></li>
<li><code>h_prev</code>，就是自定义好的，就直接用<code>h_prev</code></li>
</ul>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-34-1"><span class="n">rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">rnn</span><span class="o">.</span><span class="n">weight_ih_l0</span><span class="p">,</span><span class="n">rnn</span><span class="o">.</span><span class="n">weight_hh_l0</span><span class="p">,</span><span class="n">rnn</span><span class="o">.</span><span class="n">bias_ih_l0</span><span class="p">,</span><span class="n">rnn</span><span class="o">.</span><span class="n">bias_hh_l0</span><span class="p">,</span><span class="n">h_prev</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p><strong>变量名命令：</strong></p>
<p>前面写的是<code>rnn output</code>和<code>state finall</code></p>
<p><img alt="image-20241221201523163" src="../images/image-20241221201523163.png" /></p>
<p>加前缀 <code>custom</code>，表示自己写的</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-35-1"><span class="n">custom_rnn_output</span><span class="p">,</span><span class="n">custom_state_finall</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">rnn</span><span class="o">.</span><span class="n">weight_ih_l0</span><span class="p">,</span><span class="n">rnn</span><span class="o">.</span><span class="n">weight_hh_l0</span><span class="p">,</span><span class="n">rnn</span><span class="o">.</span><span class="n">bias_ih_l0</span><span class="p">,</span><span class="n">rnn</span><span class="o">.</span><span class="n">bias_hh_l0</span><span class="p">,</span><span class="n">h_prev</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>这样就调用了自己写的<code>RNN forward</code>函数</p>
<p>然后对比<code>pytorch api</code>的结果 和 自己写的结果</p>
<p><img alt="image-20241221201653310" src="../images/image-20241221201653310.png" /></p>
<p>第一个张量 整体RNN 预测的输出，是一致的</p>
<p>第二个张量是最后一个时刻的输出</p>
<p>官方的结果 和 自定义的结果一样 </p>
<h4 id="rnn_1">自定义 RNN代码<a class="headerlink" href="#rnn_1" title="Permanent link">&para;</a></h4>
<p><img alt="image-20241221201752393" src="../images/image-20241221201752393.png" /></p>
<h3 id="6-rnn">6 验证双向RNN<a class="headerlink" href="#6-rnn" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-36-1"><span class="n">h_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>双向的话调用单向的函数</p>
<p>双向需要注意 所有的参数 都double了，所有的weight和bias 都有两个</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-37-1"><span class="c1"># step3 手写一个bidirectional_rnn_forward函数，实现双向RNN的计算原理</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>双向要考虑两倍的 <code>forward函数</code>和<code>backward层</code></li>
<li><code>weight</code>有<code>forward</code>层和<code>backward</code>层</li>
<li><code>bias</code>也有<code>forward</code>层和<code>backward</code>层</li>
<li><code>h prev</code>也是有两份的</li>
</ul>
<p>第一份是 <code>forward layer</code>，还有 <code>backward</code>，复制然后改名，按照官方的名称，改成<code>reverse</code></p>
<p>这时候所有的参数都是两份的：</p>
<p><code>forward</code>一份，<code>backward</code>一份</p>
<p>RNN是比较简单的，如果是<code>LSTM</code> 、<code>GRU</code> 更复杂</p>
<p>函数签名写成：</p>
<p><img alt="image-20241221205304685" src="../images/image-20241221205304685.png" /></p>
<p>接下来，还是一样的，得到一些基本的信息</p>
<p>首先，上面复制下来：</p>
<p><img alt="image-20241221205534221" src="../images/image-20241221205534221.png" /></p>
<p>第一步 <code>batch size</code>，<code>时间</code> 和 <code>input size</code></p>
<p>然后，得到 <code>hidden size</code>  、<code>h_dim</code></p>
<p>关于<code>h_out</code>，这里<code>batch size</code>不变，<code>T</code>不变，但是<code>h dim</code>要变成两倍，因为是双向的结构：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-38-1"><span class="n">h_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">h_dim</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-38-2"><span class="c1"># 初始化输出状态矩阵，注意双向是两倍的特征大小</span>
</span></code></pre></div></td></tr></table></div>
<p>该定义的定义好了，接下来 调用<code>RNN forward</code>函数</p>
<p>调用两次<code>RNN forward</code>函数</p>
<p>第一步一模一样</p>
<p><img alt="image-20241221205736872" src="../images/image-20241221205736872.png" /></p>
<p>红框是需要传入的参数</p>
<p>这是<code>forward</code>层的调用，取名为 <code>forward_output</code>，这里只取 第一个返回值，所以加个[0]</p>
<p><img alt="image-20241221205826587" src="../images/image-20241221205826587.png" /></p>
<p>得到 <code>forward layer</code></p>
<p>下面 <code>backward layer</code></p>
<p>这里要变换一下，除了所有的参数都用reverse版本的，对input 也要reverse一下，就是因为如果是反向的话，要保证第一个位置上，input是最后一个元素；对input 需要 在长度这一维 进行翻转：</p>
<p><img alt="image-20241221205927963" src="../images/image-20241221205927963.png" /></p>
<p>调用 <code>torch.flip  api</code>，这个<code>api</code>，对张量进行翻转：</p>
<p><img alt="image-20241221210032831" src="../images/image-20241221210032831.png" /></p>
<p>有两个参数：</p>
<ul>
<li>
<p>一个是input</p>
</li>
<li>
<p>一个是dim，也就是说 传入的是 哪个 dim，就会对哪个 dim 进行翻转，完全相反的顺序</p>
</li>
</ul>
<p>还是先拷贝所有的参数，调用 rnn_forward函数：</p>
<p><img alt="image-20241221210122850" src="../images/image-20241221210122850.png" /></p>
<ul>
<li>第一个参数 <code>input</code>，进行翻转，调用 <code>torch.flip</code>，<code>flip</code>的第一个参数是 <code>input</code>，第二个参数是<code>维度</code>，维度官方api中规定：</li>
</ul>
<blockquote>
<p><img alt="image-20241221210222846" src="../images/image-20241221210222846.png" /></p>
<p>要么是列表 要么是元组</p>
</blockquote>
<p>这里的input是三维，要翻转的是 中间这一维，<code>T</code>这维：</p>
<p><img alt="image-20241221210258044" src="../images/image-20241221210258044.png" /></p>
<p>传入一个列表，<code>1</code>这一维度，表示中间这一维度，进行翻转</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-39-1"><span class="n">rnn_forward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="nb">input</span><span class="p">,[</span><span class="mi">1</span><span class="p">]),</span>
</span><span id="__span-39-2">            <span class="n">weight_ih_reverse</span><span class="p">,</span>
</span><span id="__span-39-3">            <span class="n">weight_hh_reverse</span><span class="p">,</span>
</span><span id="__span-39-4">            <span class="n">bias_ih_reverse</span><span class="p">,</span>
</span><span id="__span-39-5">            <span class="n">bias_hh_reveerse</span><span class="p">,</span>
</span><span id="__span-39-6">            <span class="n">h_prev_reverse</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>同样 对它的调用 也只取 <code>output</code>，定义为 <code>backward output</code></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-40-1"><span class="n">backward_output</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="nb">input</span><span class="p">,[</span><span class="mi">1</span><span class="p">]),</span>
</span><span id="__span-40-2">                              <span class="n">weight_ih_reverse</span><span class="p">,</span>
</span><span id="__span-40-3">                              <span class="n">weight_hh_reverse</span><span class="p">,</span>
</span><span id="__span-40-4">                              <span class="n">bias_ih_reverse</span><span class="p">,</span>
</span><span id="__span-40-5">                              <span class="n">bias_hh_reveerse</span><span class="p">,</span>
</span><span id="__span-40-6">                              <span class="n">h_prev_reverse</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># backward layer</span>
</span></code></pre></div></td></tr></table></div>
<p>以上 得到了 <code>forward output</code>和<code>backward output</code></p>
<p>为什么 只保留了 <code>h_out</code>，没有保留<code>h prev</code>呢？</p>
<blockquote>
<p>因为在RNN中，<code>h prev</code>可以从 <code>h out</code>中得到，所以为了方便 只取了 <code>h out</code></p>
</blockquote>
<p><img alt="image-20241221210611796" src="../images/image-20241221210611796.png" /></p>
<p>接下来，把 <code>forward output</code> 和 <code>backward output</code> 填充到 <code>h out</code>中</p>
<p>首先 <code>h out</code>是三维的，并且最后一维 由 <code>forward 和 backward</code> 填充起来的，所以填充时，索引的写法：从<span class="arithmatex">\(0\)</span>到 <code>h_dim</code></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-41-1"><span class="n">h_out</span><span class="p">[:,:,:</span><span class="n">h_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">forward_output</span>
</span></code></pre></div></td></tr></table></div>
<p>从<code>h_dim:</code>到最后</p>
<p>前向的输出，填充到前一半中，后一半的维度，用<code>backward output</code>填充</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-42-1"><span class="n">h_out</span><span class="p">[:,:,</span><span class="n">h_dim</span><span class="p">:]</span> <span class="o">=</span> <span class="n">backward_output</span>
</span></code></pre></div></td></tr></table></div>
<p>把 <code>前向输出</code> 和 <code>后向输出</code> 拼起来，然后返回</p>
<p>同样按照<code>官方api</code>，返回两个数：</p>
<ul>
<li>第一个数是 <code>h_out</code></li>
<li>第二个数就是 <code>state finall</code></li>
</ul>
<p><img alt="image-20241221210921980" src="../images/image-20241221210921980.png" /></p>
<p><code>Sate finall</code>维度是 <span class="arithmatex">\(D*num\_layers\)</span> × N × <span class="arithmatex">\(H_{out}\)</span></p>
<ul>
<li>前面表示 双向 和 层数的乘积</li>
<li>中间是<code>batch size</code></li>
<li>后面是 <code>H_out</code></li>
</ul>
<p>怎么写呢？</p>
<p>首先 要取出  <code>h out</code>的最后一个时刻，因为时刻是在中间那个维度，所以用 <code>-1</code>索引</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-43-1"><span class="k">return</span> <span class="n">h_out</span><span class="p">,</span><span class="n">h_out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(())</span>
</span></code></pre></div></td></tr></table></div>
<p>先取出 最后一个时刻，最后一个时刻的状态向量，形状 <span class="arithmatex">\(batch \_size×2倍的h\_dim\)</span>，先<code>reshape</code>，把2单独拎出来，然后reshape：</p>
<ul>
<li>Batch size不变</li>
<li>2单独拎出来</li>
<li>h dim就写成 h dim</li>
</ul>
<p>首先把二维张量 变成三维张量</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-44-1"><span class="k">return</span> <span class="n">h_out</span><span class="p">,</span><span class="n">h_out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">h_dim</span><span class="p">))</span>
</span></code></pre></div></td></tr></table></div>
<p>然后 把2提到前面，根据官方api：</p>
<ul>
<li>2 在前面</li>
</ul>
<p><img alt="image-20241221211444198" src="../images/image-20241221211444198.png" /></p>
<ul>
<li>batch size在中间</li>
</ul>
<p>所以把2 提到前面，调用一下转置函数，就是把 <code>第0维度</code> 和 <code>第1维度</code> 交换一下：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-45-1"><span class="k">return</span> <span class="n">h_out</span><span class="p">,</span><span class="n">h_out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">h_dim</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>以上双向自定义RNN 函数的实现</p>
<h4 id="rnn_2">自定义双向 RNN代码<a class="headerlink" href="#rnn_2" title="Permanent link">&para;</a></h4>
<p><img alt="image-20241221211612136" src="../images/image-20241221211612136.png" /></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-46-1"><span class="c1"># step3 手写一个 bidirectional_rnn_forward函数，实现双向RNN的计算原理</span>
</span><span id="__span-46-2"><span class="k">def</span><span class="w"> </span><span class="nf">bidirectional_rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
</span><span id="__span-46-3">                              <span class="n">weight_ih</span><span class="p">,</span>
</span><span id="__span-46-4">                              <span class="n">weight_hh</span><span class="p">,</span>
</span><span id="__span-46-5">                              <span class="n">bias_ih</span><span class="p">,</span>
</span><span id="__span-46-6">                              <span class="n">bias_hh</span><span class="p">,</span>
</span><span id="__span-46-7">                              <span class="n">h_prev</span><span class="p">,</span>
</span><span id="__span-46-8">                              <span class="n">weight_ih_reverse</span><span class="p">,</span>
</span><span id="__span-46-9">                              <span class="n">weight_hh_reverse</span><span class="p">,</span>
</span><span id="__span-46-10">                              <span class="n">bias_ih_reverse</span><span class="p">,</span>
</span><span id="__span-46-11">                              <span class="n">bias_hh_reverse</span><span class="p">,</span>
</span><span id="__span-46-12">                              <span class="n">h_prev_reverse</span><span class="p">):</span>
</span><span id="__span-46-13">    <span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-46-14">    <span class="n">h_dim</span> <span class="o">=</span> <span class="n">weight_ih</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-46-15">    <span class="n">h_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">h_dim</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 初始化一个输出（状态）矩阵，注意双向是两倍的特征大小</span>
</span><span id="__span-46-16">
</span><span id="__span-46-17">    <span class="n">forward_output</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
</span><span id="__span-46-18">                                 <span class="n">weight_ih</span><span class="p">,</span>
</span><span id="__span-46-19">                                 <span class="n">weight_hh</span><span class="p">,</span>
</span><span id="__span-46-20">                                 <span class="n">bias_ih</span><span class="p">,</span>
</span><span id="__span-46-21">                                 <span class="n">bias_hh</span><span class="p">,</span>
</span><span id="__span-46-22">                                 <span class="n">h_prev</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># forward layer</span>
</span><span id="__span-46-23">    <span class="n">backward_output</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="nb">input</span><span class="p">,[</span><span class="mi">1</span><span class="p">]),</span>
</span><span id="__span-46-24">                                  <span class="n">weight_ih_reverse</span><span class="p">,</span>
</span><span id="__span-46-25">                                  <span class="n">weight_hh_reverse</span><span class="p">,</span>
</span><span id="__span-46-26">                                  <span class="n">bias_ih_reverse</span><span class="p">,</span> 
</span><span id="__span-46-27">                                  <span class="n">bias_hh_reverse</span><span class="p">,</span>
</span><span id="__span-46-28">                                  <span class="n">h_prev_reverse</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># backward layer</span>
</span><span id="__span-46-29">
</span><span id="__span-46-30">    <span class="c1"># 将input按照时间的顺序翻转</span>
</span><span id="__span-46-31">    <span class="n">h_out</span><span class="p">[:,:,:</span><span class="n">h_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">forward_output</span>
</span><span id="__span-46-32">    <span class="n">h_out</span><span class="p">[:,:,</span><span class="n">h_dim</span><span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">backward_output</span><span class="p">,[</span><span class="mi">1</span><span class="p">])</span> <span class="c1">#需要再翻转一下 才能和forward output拼接</span>
</span><span id="__span-46-33">
</span><span id="__span-46-34">
</span><span id="__span-46-35">    <span class="n">h_n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">h_dim</span><span class="p">)</span>  <span class="c1"># 要最后的状态连接</span>
</span><span id="__span-46-36">
</span><span id="__span-46-37">    <span class="n">h_n</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">forward_output</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
</span><span id="__span-46-38">    <span class="n">h_n</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">backward_output</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
</span><span id="__span-46-39">
</span><span id="__span-46-40">    <span class="n">h_n</span> <span class="o">=</span> <span class="n">h_n</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-46-41">
</span><span id="__span-46-42">    <span class="k">return</span> <span class="n">h_out</span><span class="p">,</span><span class="n">h_n</span>
</span><span id="__span-46-43">    <span class="c1"># return h_out,h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)</span>
</span><span id="__span-46-44">
</span><span id="__span-46-45"><span class="c1"># 验证一下 bidirectional_rnn_forward的正确性</span>
</span><span id="__span-46-46"><span class="n">bi_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span>
</span><span id="__span-46-47">                <span class="n">hidden_size</span><span class="p">,</span>
</span><span id="__span-46-48">                <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-46-49">                <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-46-50"><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="n">bs</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">))</span>
</span><span id="__span-46-51"><span class="n">bi_rnn_output</span><span class="p">,</span><span class="n">bi_state_finall</span> <span class="o">=</span> <span class="n">bi_rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">h_prev</span><span class="p">)</span>
</span><span id="__span-46-52">
</span><span id="__span-46-53"><span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">bi_rnn</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
</span><span id="__span-46-54">    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>代码思路：</p>
<ol>
<li>首先把 <code>input</code>传入到 <code>forward layer</code>中</li>
<li>然后再把<code>input</code> 按照 时间的顺序 翻转一下，再传入<code>backwardward layer</code>中</li>
<li>再把 <code>forward output</code>和<code>backward output</code>拼起来，形成整体的<code>h out</code></li>
<li>最后返回序列 整体的隐含状态和 最后一个时刻的状态</li>
</ol>
<p>现在验证  双向 rnn  forward 正确性</p>
<p><strong>首先 实例化双向RNN 层</strong></p>
<p>复制下来，并设置 bidirection=True</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-47-1"><span class="c1"># 验证一下 bidirectional_rnn_forward的正确性</span>
</span><span id="__span-47-2"><span class="n">bi_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>同样定义一个<code>h_prev</code></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-48-1"><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">()</span>
</span></code></pre></div></td></tr></table></div>
<p>大小是 <code>2× batch size× hidden size</code></p>
<p><img alt="image-20241221212306336" src="../images/image-20241221212306336.png" /></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-49-1"><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">bs</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>调用RNN，传入<code>input</code>和<code>h_prev</code>，得到双向RNN的<code>output</code>和双向<code>state finall</code></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-50-1"><span class="n">bi_rnn_output</span><span class="p">,</span><span class="n">bi_state_finall</span> <span class="o">=</span> <span class="n">bi_rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">h_prev</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>得到官方api的结果</p>
<p><img alt="image-20241221212448882" src="../images/image-20241221212448882.png" /></p>
<p>对于RNN 查看一下 参数的名字，然后把这些参数代入到自定义的双向RNN函数中去</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-51-1"><span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">bi_rnn</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
</span><span id="__span-51-2">    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p><img alt="image-20241221212548749" src="../images/image-20241221212548749.png" /></p>
<p>可以看到在pytorch双向RNN 中的参数：</p>
<ol>
<li>weight ih l0</li>
<li>weight hh l0</li>
<li>bias ih l0</li>
<li>bias hh l0</li>
<li>weight ih l0 reverse</li>
<li>weight hh l0 reverse</li>
<li>bias ih l0</li>
<li>bias hh l0 reverse</li>
</ol>
<p>一共有8个参数，这是因为 <code>forward layer</code>有4个参数，<code>reverse layer</code>也有4个参数</p>
<p>有了这8个参数，就可以把这8个参数传入到双向RNN中</p>
<p>首先把 签名 copy下来：</p>
<p><img alt="image-20241221212723011" src="../images/image-20241221212723011.png" /></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-52-1"><span class="n">bidirectional_rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
</span><span id="__span-52-2">                          <span class="n">weight_ih</span><span class="p">,</span>
</span><span id="__span-52-3">                          <span class="n">weight_hh</span><span class="p">,</span>
</span><span id="__span-52-4">                          <span class="n">bias_ih</span><span class="p">,</span>
</span><span id="__span-52-5">                          <span class="n">bias_hh</span><span class="p">,</span>
</span><span id="__span-52-6">                          <span class="n">h_prev</span><span class="p">,</span>
</span><span id="__span-52-7">                          <span class="n">weight_ih_reverse</span><span class="p">,</span>
</span><span id="__span-52-8">                          <span class="n">weight_hh_reverse</span><span class="p">,</span>
</span><span id="__span-52-9">                          <span class="n">bias_ih_reverse</span><span class="p">,</span>
</span><span id="__span-52-10">                          <span class="n">bias_hh_reverse</span><span class="p">,</span>
</span><span id="__span-52-11">                          <span class="n">h_prev_reverse</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li><code>input</code>不变</li>
<li><code>weight ih</code>改成<code>weight ih l0</code></li>
<li><code>weight hh</code>，同样<code>weight hh l0</code></li>
</ul>
<p>还要加上<code>bi_rnn.</code>，也就是说把实例化的RNN层传进来</p>
<p><code>bi_rnn.bias ih l0</code> </p>
<p><code>bi_rnn.bias hh l0</code></p>
<p><img alt="image-20241221212936350" src="../images/image-20241221212936350.png" /></p>
<p><code>h prev</code>需要注意：是三维的</p>
<p>前面有个 2 ，只需要传入第一个就好了 <code>h prev[0]</code></p>
<p>反向的也是类似的</p>
<p><code>bi_rnn.weight ih l0 reverse</code></p>
<p>后面也是一样 <code>bi_rnn.weight hh l0 reverse</code></p>
<p><code>bi_rnn.bias ih l0 reverse</code> </p>
<p><code>bi_rnn.bias hh l0 reverse</code></p>
<p><code>h prev reverse</code>，用<code>h prev [1]</code></p>
<p><img alt="image-20241221213102881" src="../images/image-20241221213102881.png" /></p>
<p>定义  <code>custom_bi_rnn_output,custom_bi_state_finall</code>接收输出</p>
<p>接下来分别打印api的结果 和 自己写的函数的结果：</p>
<p><img alt="image-20241221213142401" src="../images/image-20241221213142401.png" /></p>
<blockquote>
<p>这个 结果有问题，（后面改了 就是各种翻转 )
</p>
</blockquote>
<p>由于是双向的 <code>hidden size=3</code>，但是输出状态长度是6，这是因为双向的有拼接</p>
<h2 id="_2">汇总所有代码<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-53-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-53-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-54-1"><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span>  <span class="c1"># 批大小，输入序列长度</span>
</span><span id="__span-54-2"><span class="n">input_size</span><span class="p">,</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="mi">3</span> <span class="c1"># 输入特征大小，隐含层特征大小</span>
</span><span id="__span-54-3"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">input_size</span><span class="p">)</span>  <span class="c1"># 随机初始化一个输入特征序列</span>
</span><span id="__span-54-4"><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">)</span> <span class="c1"># 初始隐含状态</span>
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-55-1"><span class="c1"># step1 调用pytorch RNN API</span>
</span><span id="__span-55-2"><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-55-3"><span class="n">rnn_output</span><span class="p">,</span><span class="n">state_finall</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">h_prev</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-55-4">
</span><span id="__span-55-5"><span class="nb">print</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">)</span>
</span><span id="__span-55-6"><span class="nb">print</span><span class="p">(</span><span class="n">state_finall</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>输出：</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span>
<span class="normal">9</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-56-1">tensor([[[-0.7709,  0.7301, -0.9299],
</span><span id="__span-56-2">         [-0.6976, -0.8241, -0.1903],
</span><span id="__span-56-3">         [-0.6485, -0.2633, -0.1093]],
</span><span id="__span-56-4">
</span><span id="__span-56-5">        [[-0.2035,  0.7439, -0.1369],
</span><span id="__span-56-6">         [-0.4805, -0.5790,  0.1787],
</span><span id="__span-56-7">         [-0.6185,  0.4854, -0.4907]]], grad_fn=&lt;TransposeBackward1&gt;)
</span><span id="__span-56-8">tensor([[[-0.6485, -0.2633, -0.1093],
</span><span id="__span-56-9">         [-0.6185,  0.4854, -0.4907]]], grad_fn=&lt;StackBackward0&gt;)
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-57-1"><span class="c1"># step2 手写 rnn_forward函数，实现RNN的计算原理</span>
</span><span id="__span-57-2"><span class="k">def</span><span class="w"> </span><span class="nf">rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">weight_ih</span><span class="p">,</span><span class="n">weight_hh</span><span class="p">,</span><span class="n">bias_ih</span><span class="p">,</span><span class="n">bias_hh</span><span class="p">,</span><span class="n">h_prev</span><span class="p">):</span>
</span><span id="__span-57-3">    <span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-57-4">    <span class="n">h_dim</span> <span class="o">=</span> <span class="n">weight_ih</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-57-5">    <span class="n">h_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">h_dim</span><span class="p">)</span> <span class="c1"># 初始化一个输出（状态）矩阵</span>
</span><span id="__span-57-6">
</span><span id="__span-57-7">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
</span><span id="__span-57-8">        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span><span class="n">t</span><span class="p">,:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 获取当前时刻的输入特征，bs*input_size*1</span>
</span><span id="__span-57-9">        <span class="n">w_ih_batch</span> <span class="o">=</span> <span class="n">weight_ih</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># bs * h_dim * input_size</span>
</span><span id="__span-57-10">        <span class="n">w_hh_batch</span> <span class="o">=</span> <span class="n">weight_hh</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="c1"># bs * h_dim * h_dim</span>
</span><span id="__span-57-11">
</span><span id="__span-57-12">        <span class="n">w_times_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">w_ih_batch</span><span class="p">,</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># bs*h_dim</span>
</span><span id="__span-57-13">        <span class="n">w_times_h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">w_hh_batch</span><span class="p">,</span><span class="n">h_prev</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># bs*h_him</span>
</span><span id="__span-57-14">        <span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">w_times_x</span> <span class="o">+</span> <span class="n">bias_ih</span> <span class="o">+</span> <span class="n">w_times_h</span> <span class="o">+</span> <span class="n">bias_hh</span><span class="p">)</span>
</span><span id="__span-57-15">
</span><span id="__span-57-16">        <span class="n">h_out</span><span class="p">[:,</span><span class="n">t</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">h_prev</span>
</span><span id="__span-57-17">
</span><span id="__span-57-18">    <span class="k">return</span> <span class="n">h_out</span><span class="p">,</span><span class="n">h_prev</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span>
<span class="normal">9</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-58-1"><span class="c1"># 验证结果</span>
</span><span id="__span-58-2"><span class="n">custom_rnn_output</span><span class="p">,</span><span class="n">custom_state_finall</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
</span><span id="__span-58-3">                                                    <span class="n">rnn</span><span class="o">.</span><span class="n">weight_ih_l0</span><span class="p">,</span>
</span><span id="__span-58-4">                                                    <span class="n">rnn</span><span class="o">.</span><span class="n">weight_hh_l0</span><span class="p">,</span>
</span><span id="__span-58-5">                                                    <span class="n">rnn</span><span class="o">.</span><span class="n">bias_ih_l0</span><span class="p">,</span>
</span><span id="__span-58-6">                                                    <span class="n">rnn</span><span class="o">.</span><span class="n">bias_hh_l0</span><span class="p">,</span>
</span><span id="__span-58-7">                                                    <span class="n">h_prev</span><span class="p">)</span>
</span><span id="__span-58-8"><span class="nb">print</span><span class="p">(</span><span class="n">custom_rnn_output</span><span class="p">)</span>
</span><span id="__span-58-9"><span class="nb">print</span><span class="p">(</span><span class="n">custom_state_finall</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>输出：</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span>
<span class="normal">9</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-59-1">tensor([[[-0.7709,  0.7301, -0.9299],
</span><span id="__span-59-2">         [-0.6976, -0.8241, -0.1903],
</span><span id="__span-59-3">         [-0.6485, -0.2633, -0.1093]],
</span><span id="__span-59-4">
</span><span id="__span-59-5">        [[-0.2035,  0.7439, -0.1369],
</span><span id="__span-59-6">         [-0.4805, -0.5790,  0.1787],
</span><span id="__span-59-7">         [-0.6185,  0.4854, -0.4907]]], grad_fn=&lt;CopySlices&gt;)
</span><span id="__span-59-8">tensor([[[-0.6485, -0.2633, -0.1093],
</span><span id="__span-59-9">         [-0.6185,  0.4854, -0.4907]]], grad_fn=&lt;UnsqueezeBackward0&gt;)
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-60-1"><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span><span class="n">custom_rnn_output</span><span class="p">))</span>
</span><span id="__span-60-2"><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">state_finall</span><span class="p">,</span><span class="n">custom_state_finall</span><span class="p">))</span>
</span></code></pre></div></td></tr></table></div>
<p>输出：True、True</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-61-1"><span class="c1"># step3 手写一个 bidirectional_rnn_forward函数，实现双向RNN的计算原理</span>
</span><span id="__span-61-2"><span class="k">def</span><span class="w"> </span><span class="nf">bidirectional_rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
</span><span id="__span-61-3">                              <span class="n">weight_ih</span><span class="p">,</span>
</span><span id="__span-61-4">                              <span class="n">weight_hh</span><span class="p">,</span>
</span><span id="__span-61-5">                              <span class="n">bias_ih</span><span class="p">,</span>
</span><span id="__span-61-6">                              <span class="n">bias_hh</span><span class="p">,</span>
</span><span id="__span-61-7">                              <span class="n">h_prev</span><span class="p">,</span>
</span><span id="__span-61-8">                              <span class="n">weight_ih_reverse</span><span class="p">,</span>
</span><span id="__span-61-9">                              <span class="n">weight_hh_reverse</span><span class="p">,</span>
</span><span id="__span-61-10">                              <span class="n">bias_ih_reverse</span><span class="p">,</span>
</span><span id="__span-61-11">                              <span class="n">bias_hh_reverse</span><span class="p">,</span>
</span><span id="__span-61-12">                              <span class="n">h_prev_reverse</span><span class="p">):</span>
</span><span id="__span-61-13">    <span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-61-14">    <span class="n">h_dim</span> <span class="o">=</span> <span class="n">weight_ih</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-61-15">    <span class="n">h_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">h_dim</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 初始化一个输出（状态）矩阵，注意双向是两倍的特征大小</span>
</span><span id="__span-61-16">
</span><span id="__span-61-17">    <span class="n">forward_output</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
</span><span id="__span-61-18">                                 <span class="n">weight_ih</span><span class="p">,</span>
</span><span id="__span-61-19">                                 <span class="n">weight_hh</span><span class="p">,</span>
</span><span id="__span-61-20">                                 <span class="n">bias_ih</span><span class="p">,</span>
</span><span id="__span-61-21">                                 <span class="n">bias_hh</span><span class="p">,</span>
</span><span id="__span-61-22">                                 <span class="n">h_prev</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># forward layer</span>
</span><span id="__span-61-23">    <span class="n">backward_output</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="nb">input</span><span class="p">,[</span><span class="mi">1</span><span class="p">]),</span>
</span><span id="__span-61-24">                                  <span class="n">weight_ih_reverse</span><span class="p">,</span>
</span><span id="__span-61-25">                                  <span class="n">weight_hh_reverse</span><span class="p">,</span>
</span><span id="__span-61-26">                                  <span class="n">bias_ih_reverse</span><span class="p">,</span> 
</span><span id="__span-61-27">                                  <span class="n">bias_hh_reverse</span><span class="p">,</span>
</span><span id="__span-61-28">                                  <span class="n">h_prev_reverse</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># backward layer</span>
</span><span id="__span-61-29">
</span><span id="__span-61-30">    <span class="c1"># 将input按照时间的顺序翻转</span>
</span><span id="__span-61-31">    <span class="n">h_out</span><span class="p">[:,:,:</span><span class="n">h_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">forward_output</span>
</span><span id="__span-61-32">    <span class="n">h_out</span><span class="p">[:,:,</span><span class="n">h_dim</span><span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">backward_output</span><span class="p">,[</span><span class="mi">1</span><span class="p">])</span> <span class="c1">#需要再翻转一下 才能和forward output拼接</span>
</span><span id="__span-61-33">
</span><span id="__span-61-34">
</span><span id="__span-61-35">    <span class="n">h_n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">h_dim</span><span class="p">)</span>  <span class="c1"># 要最后的状态连接</span>
</span><span id="__span-61-36">
</span><span id="__span-61-37">    <span class="n">h_n</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">forward_output</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
</span><span id="__span-61-38">    <span class="n">h_n</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">backward_output</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
</span><span id="__span-61-39">
</span><span id="__span-61-40">    <span class="n">h_n</span> <span class="o">=</span> <span class="n">h_n</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-61-41">
</span><span id="__span-61-42">    <span class="k">return</span> <span class="n">h_out</span><span class="p">,</span><span class="n">h_n</span>
</span><span id="__span-61-43">    <span class="c1"># return h_out,h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)</span>
</span><span id="__span-61-44">
</span><span id="__span-61-45"><span class="c1"># 验证一下 bidirectional_rnn_forward的正确性</span>
</span><span id="__span-61-46"><span class="n">bi_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-61-47"><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="n">bs</span><span class="p">,</span><span class="n">hidden_size</span><span class="p">))</span>
</span><span id="__span-61-48"><span class="n">bi_rnn_output</span><span class="p">,</span><span class="n">bi_state_finall</span> <span class="o">=</span> <span class="n">bi_rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">h_prev</span><span class="p">)</span>
</span><span id="__span-61-49">
</span><span id="__span-61-50"><span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">bi_rnn</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
</span><span id="__span-61-51">    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>输出</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-62-1">weight_ih_l0 Parameter containing:
</span><span id="__span-62-2">tensor([[ 0.5458,  0.5512],
</span><span id="__span-62-3">        [-0.5077, -0.0750],
</span><span id="__span-62-4">        [ 0.3572,  0.1419]], requires_grad=True)
</span><span id="__span-62-5">weight_hh_l0 Parameter containing:
</span><span id="__span-62-6">tensor([[-0.4093,  0.2012,  0.0746],
</span><span id="__span-62-7">        [-0.5619, -0.3820, -0.4060],
</span><span id="__span-62-8">        [-0.4412,  0.2706, -0.2816]], requires_grad=True)
</span><span id="__span-62-9">bias_ih_l0 Parameter containing:
</span><span id="__span-62-10">tensor([-0.5063, -0.1391, -0.0587], requires_grad=True)
</span><span id="__span-62-11">bias_hh_l0 Parameter containing:
</span><span id="__span-62-12">tensor([ 0.0343, -0.2352,  0.3234], requires_grad=True)
</span><span id="__span-62-13">weight_ih_l0_reverse Parameter containing:
</span><span id="__span-62-14">tensor([[ 0.1298,  0.5538],
</span><span id="__span-62-15">        [ 0.4151,  0.2533],
</span><span id="__span-62-16">        [-0.4401,  0.5322]], requires_grad=True)
</span><span id="__span-62-17">weight_hh_l0_reverse Parameter containing:
</span><span id="__span-62-18">tensor([[-0.4232,  0.2246,  0.4265],
</span><span id="__span-62-19">        [ 0.3016, -0.4142, -0.3064],
</span><span id="__span-62-20">        [-0.1960,  0.2845,  0.3770]], requires_grad=True)
</span><span id="__span-62-21">bias_ih_l0_reverse Parameter containing:
</span><span id="__span-62-22">tensor([-0.4372, -0.2452,  0.4506], requires_grad=True)
</span><span id="__span-62-23">bias_hh_l0_reverse Parameter containing:
</span><span id="__span-62-24">tensor([ 0.3957, -0.4655, -0.2143], requires_grad=True)
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-63-1"><span class="n">custom_bi_rnn_output</span><span class="p">,</span><span class="n">custom_bi_state_finall</span> <span class="o">=</span> <span class="n">bidirectional_rnn_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
</span><span id="__span-63-2">                                                                        <span class="n">bi_rnn</span><span class="o">.</span><span class="n">weight_ih_l0</span><span class="p">,</span>
</span><span id="__span-63-3">                                                                        <span class="n">bi_rnn</span><span class="o">.</span><span class="n">weight_hh_l0</span><span class="p">,</span>
</span><span id="__span-63-4">                                                                        <span class="n">bi_rnn</span><span class="o">.</span><span class="n">bias_ih_l0</span><span class="p">,</span>
</span><span id="__span-63-5">                                                                        <span class="n">bi_rnn</span><span class="o">.</span><span class="n">bias_hh_l0</span><span class="p">,</span>
</span><span id="__span-63-6">                                                                        <span class="n">h_prev</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="__span-63-7">                                                                        <span class="n">bi_rnn</span><span class="o">.</span><span class="n">weight_ih_l0_reverse</span><span class="p">,</span>
</span><span id="__span-63-8">                                                                        <span class="n">bi_rnn</span><span class="o">.</span><span class="n">weight_hh_l0_reverse</span><span class="p">,</span>
</span><span id="__span-63-9">                                                                        <span class="n">bi_rnn</span><span class="o">.</span><span class="n">bias_ih_l0_reverse</span><span class="p">,</span>
</span><span id="__span-63-10">                                                                        <span class="n">bi_rnn</span><span class="o">.</span><span class="n">bias_hh_l0_reverse</span><span class="p">,</span>
</span><span id="__span-63-11">                                                                        <span class="n">h_prev</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span>
<span class="normal">9</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-64-1"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pytorch API output&quot;</span><span class="p">)</span>
</span><span id="__span-64-2"><span class="nb">print</span><span class="p">(</span><span class="n">bi_rnn_output</span><span class="p">)</span>
</span><span id="__span-64-3"><span class="nb">print</span><span class="p">(</span><span class="n">bi_state_finall</span><span class="p">)</span>
</span><span id="__span-64-4">
</span><span id="__span-64-5"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> custom bidirectional_rnn_forward function output:&quot;</span><span class="p">)</span>
</span><span id="__span-64-6"><span class="nb">print</span><span class="p">(</span><span class="n">custom_bi_rnn_output</span><span class="p">)</span>
</span><span id="__span-64-7"><span class="nb">print</span><span class="p">(</span><span class="n">custom_bi_state_finall</span><span class="p">)</span>
</span><span id="__span-64-8"><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">bi_rnn_output</span><span class="p">,</span><span class="n">custom_bi_rnn_output</span><span class="p">))</span>
</span><span id="__span-64-9"><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">bi_state_finall</span><span class="p">,</span><span class="n">custom_bi_state_finall</span><span class="p">))</span>
</span></code></pre></div></td></tr></table></div>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-65-1">True
</span><span id="__span-65-2">True
</span></code></pre></div></td></tr></table></div>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="2024年12月21日 14:57:34"><span class="timeago" datetime="2024-12-21T14:57:34+00:00" locale="zh"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="2024年12月21日 14:57:34">2024-12-21</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="2024年12月20日 14:49:27"><span class="timeago" datetime="2024-12-20T14:49:27+00:00" locale="zh"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="2024年12月20日 14:49:27">2024-12-20</span>
  </span>

    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["toc.follow", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../js/timeago.min.js"></script>
      
        <script src="../../js/timeago_mkdocs_material.js"></script>
      
        <script src="../../mkdocs/javascripts/katex.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>