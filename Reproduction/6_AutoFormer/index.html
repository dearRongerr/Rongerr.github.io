
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="https://mydomain.org/mysite/Reproduction/6_AutoFormer/" rel="canonical"/>
<link href="../5_SegRNN_v3/" rel="prev"/>
<link href="../6_AutoFormer_v1/" rel="next"/>
<link href="../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.9" name="generator"/>
<title>Autoformer - 溶err</title>
<link href="../../assets/stylesheets/main.4af4bdda.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../css/timeago.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" rel="stylesheet"/>
<link href="../../mkdocs/css/no-footer.css" rel="stylesheet"/>
<link href="../../mkdocs/css/unordered-list-symbols.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="light-blue" data-md-color-primary="light-blue" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#autoformer">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="溶err" class="md-header__button md-logo" data-md-component="logo" href="../.." title="溶err">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            溶err
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Autoformer
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="light-blue" data-md-color-media="" data-md-color-primary="light-blue" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="" data-md-color-primary="indigo" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
<div class="md-search__suggest" data-md-component="search-suggest"></div>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
</nav>
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../sticks/mkdocs_learn/">
          
  
    
  
  便签

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../bagu/questions/1_questions/">
          
  
    
  
  面试

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../Error/github/">
          
  
    
  
  捉个虫

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../learning/0_pdfNotes/">
          
  
    
  
  笔记

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../literature/">
          
  
    
  
  文献

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../">
          
  
    
  
  复现&amp;代码

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../Statistics/">
          
  
    
  
  统计学

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../logs/">
          
  
    
  
  杂

        </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="溶err" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="溶err">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>
</a>
    溶err
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
<span class="md-ellipsis">
    便签
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_1_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_1">
<span class="md-nav__icon md-icon"></span>
            便签
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/mkdocs_learn/">
<span class="md-ellipsis">
    MkDocs
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/markdwon_learn/">
<span class="md-ellipsis">
    markdown
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/latex/">
<span class="md-ellipsis">
    LaTex
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/GitHub/">
<span class="md-ellipsis">
    GitHub
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/MacOS/">
<span class="md-ellipsis">
    MacOS
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/shell/">
<span class="md-ellipsis">
    Shell
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/linux/">
<span class="md-ellipsis">
    linux
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/screen/">
<span class="md-ellipsis">
    screen
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/docker/">
<span class="md-ellipsis">
    Docker
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/writting/">
<span class="md-ellipsis">
    写作
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/1_github_v1/">
<span class="md-ellipsis">
    github v1.0
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/2_python/">
<span class="md-ellipsis">
    python
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../sticks/3_vscode/">
<span class="md-ellipsis">
    VSCode
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
<span class="md-ellipsis">
    面试
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
            面试
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_2_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
<span class="md-ellipsis">
    题目
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_1">
<span class="md-nav__icon md-icon"></span>
            题目
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/questions/1_questions/">
<span class="md-ellipsis">
    面试问题
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_2_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../bagu/leetcode/">
<span class="md-ellipsis">
    力扣
    
  </span>
</a>
<label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_2_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_2">
<span class="md-nav__icon md-icon"></span>
            力扣
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/leetcode/1/">
<span class="md-ellipsis">
    1 两数之和
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/leetcode/2/">
<span class="md-ellipsis">
    2 两数相加
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_2_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../bagu/deeplearning/">
<span class="md-ellipsis">
    深度学习
    
  </span>
</a>
<label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_2_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_3">
<span class="md-nav__icon md-icon"></span>
            深度学习
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/deeplearning/transformer/">
<span class="md-ellipsis">
    手撕Transformer代码
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/deeplearning/former1/">
<span class="md-ellipsis">
    空
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/deeplearning/former2/">
<span class="md-ellipsis">
    空
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/deeplearning/pytorch_shape_function/">
<span class="md-ellipsis">
    pytorch的维度变换函数
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/deeplearning/1/">
<span class="md-ellipsis">
    visionTransformer代码
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_2_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
<span class="md-ellipsis">
    机器学习
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_4">
<span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/machinelearning/kmeans/">
<span class="md-ellipsis">
    手撕kmeans
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../bagu/machinelearning/2/">
<span class="md-ellipsis">
    手撕反向传播
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
<span class="md-ellipsis">
    捉个虫
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            捉个虫
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../Error/github/">
<span class="md-ellipsis">
    github
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Error/latex/">
<span class="md-ellipsis">
    Latex
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Error/python/">
<span class="md-ellipsis">
    python
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Error/macos/">
<span class="md-ellipsis">
    macOS
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Error/docker/">
<span class="md-ellipsis">
    docker
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
<span class="md-ellipsis">
    笔记
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
            笔记
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/0_pdfNotes/">
<span class="md-ellipsis">
    📒
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/3_ViT/">
<span class="md-ellipsis">
    ViT
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/1_clip/">
<span class="md-ellipsis">
    CLIP
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/2_MOCO/">
<span class="md-ellipsis">
    MOCO
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/2/">
<span class="md-ellipsis">
    图解LayerNorm &amp; BatchNorm
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/1/">
<span class="md-ellipsis">
    5种归一化方法
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/vit/">
<span class="md-ellipsis">
    vision Transformer的原理与难点源码实现
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/swintransformer/">
<span class="md-ellipsis">
    SwinTransformer 学习笔记
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/pe/">
<span class="md-ellipsis">
    4种位置编码
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/convs/">
<span class="md-ellipsis">
    卷积
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/3/">
<span class="md-ellipsis">
    李沐 目标检测部分
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/4_GAN/">
<span class="md-ellipsis">
    GAN
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/5_Bert/">
<span class="md-ellipsis">
    BERT从零详细解读
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/6_Diffusion/">
<span class="md-ellipsis">
    DDPM
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/6_Diffusion1/">
<span class="md-ellipsis">
    VDM
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/7_Clip/">
<span class="md-ellipsis">
    空
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/8_WeightNorm/">
<span class="md-ellipsis">
    WeightNorm
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/9_cGAN/">
<span class="md-ellipsis">
    GAN 变体
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/10_ResNet/">
<span class="md-ellipsis">
    项目实战：ResNet果蔬分类
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/11_excelcsvtensor/">
<span class="md-ellipsis">
    基础：excel\csv文件→tensor
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/12_KLdivergence/">
<span class="md-ellipsis">
    KL divergence
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/13_RNN/">
<span class="md-ellipsis">
    RNN
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/14_LSTM/">
<span class="md-ellipsis">
    LSTM
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/15_ContrastiveLearning/">
<span class="md-ellipsis">
    对比学习
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/16_YOLO/">
<span class="md-ellipsis">
    YOLO
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/17_DETR/">
<span class="md-ellipsis">
    DETR
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/18_DINO/">
<span class="md-ellipsis">
    DINO
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/19_GPT/">
<span class="md-ellipsis">
    GPT
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/20_distill/">
<span class="md-ellipsis">
    知识蒸馏
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../learning/21_FastRCNN/">
<span class="md-ellipsis">
    21 FastRCNN
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_5" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../literature/">
<span class="md-ellipsis">
    文献
    
  </span>
</a>
<label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_5_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
            文献
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_5_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../literature/TSP/">
<span class="md-ellipsis">
    时间序列预测
    
  </span>
</a>
<label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_5_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_2">
<span class="md-nav__icon md-icon"></span>
            时间序列预测
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/TSP/0_note/">
<span class="md-ellipsis">
    NOTE
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/TSP/1_SegRNN/">
<span class="md-ellipsis">
    2023、SegRNN
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/TSP/2_DLinear/">
<span class="md-ellipsis">
    2022、DLinear
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/TSP/3_TimesNet/">
<span class="md-ellipsis">
    2023、TimesNet
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/TSP/4_Informer/">
<span class="md-ellipsis">
    2021、 Informer
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/TSP/5_Autoformer/">
<span class="md-ellipsis">
    2021、Autoformer
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_5_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../literature/ObejectCounting/">
<span class="md-ellipsis">
    目标计数
    
  </span>
</a>
<label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_5_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_3">
<span class="md-nav__icon md-icon"></span>
            目标计数
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank1%20CountGD/">
<span class="md-ellipsis">
    rank1 CountGD
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank2%20GeCo/">
<span class="md-ellipsis">
    rank2 GeCo
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank3%20DAVE/">
<span class="md-ellipsis">
    rank3 DAVE
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank4%20CACViT/">
<span class="md-ellipsis">
    rank4 CACViT
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank5%20SSD/">
<span class="md-ellipsis">
    rank5 SSD
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank6%20LOCA/">
<span class="md-ellipsis">
    rank6 LOCA
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank7%20SemAug_CountTR/">
<span class="md-ellipsis">
    rank7 SemAug CountTR
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank8%20CounTR/">
<span class="md-ellipsis">
    rank8 CounTR
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank9%20SemAug_SAFECount/">
<span class="md-ellipsis">
    rank9 SemAug SAFECount
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank10%20SPDCN/">
<span class="md-ellipsis">
    rank10 SPDCN
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank11%20GCA_SUN/">
<span class="md-ellipsis">
    rank11 GCA SUN
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank12%20SAFECount/">
<span class="md-ellipsis">
    rank12 SAFECount
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank13%20BMNet/">
<span class="md-ellipsis">
    rank13 BMNet
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank14%20LaoNet/">
<span class="md-ellipsis">
    rank14 LaoNet
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank15%20CounTX/">
<span class="md-ellipsis">
    rank15 CounTX
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank16%20Counting_DETR/">
<span class="md-ellipsis">
    rank16 Counting DETR
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank17%20RCC/">
<span class="md-ellipsis">
    rank17 RCC
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank18%20Omnicount/">
<span class="md-ellipsis">
    rank18 Omnicount
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObejectCounting/rank19%20FamNet/">
<span class="md-ellipsis">
    rank19 FamNet
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_5_4" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../literature/ObjectDetection/">
<span class="md-ellipsis">
    目标检测
    
  </span>
</a>
<label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_5_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_4">
<span class="md-nav__icon md-icon"></span>
            目标检测
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObjectDetection/2/">
<span class="md-ellipsis">
    目标检测基础知识
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObjectDetection/1/">
<span class="md-ellipsis">
    DETR论文系列
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObjectDetection/3/">
<span class="md-ellipsis">
    （DETR）End-to-End Object Detection with Transformer
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/ObjectDetection/4/">
<span class="md-ellipsis">
    4
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_5_5" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../literature/MultiModal/">
<span class="md-ellipsis">
    多模态
    
  </span>
</a>
<label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_5_5_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_5">
<span class="md-nav__icon md-icon"></span>
            多模态
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../literature/MultiModal/1/">
<span class="md-ellipsis">
    1
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_6" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../">
<span class="md-ellipsis">
    复现&amp;代码
    
  </span>
</a>
<label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_6_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_6">
<span class="md-nav__icon md-icon"></span>
            复现&amp;代码
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../7_summary/">
<span class="md-ellipsis">
    汇总复现调用图
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../DAVE/">
<span class="md-ellipsis">
    DAVE复现
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../5_SegRNN_index/">
<span class="md-ellipsis">
    SegRNN
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../5_SegRNN_v1/">
<span class="md-ellipsis">
    复现SegRNN
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../5_SegRNN_v2/">
<span class="md-ellipsis">
    （补充）复现 SegRNN
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../5_SegRNN_v3/">
<span class="md-ellipsis">
    (手写笔记)SegRNN
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Autoformer
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    Autoformer
    
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#github">
<span class="md-ellipsis">
      github 源码主页
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_1">
<span class="md-ellipsis">
      准备
    </span>
</a>
<nav aria-label="准备" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#git-clone">
<span class="md-ellipsis">
      git clone
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#readme">
<span class="md-ellipsis">
      readme
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_2">
<span class="md-ellipsis">
      调试配置
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#python">
<span class="md-ellipsis">
      新建 python 虚拟环境
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#requirements">
<span class="md-ellipsis">
      requirements
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_3">
<span class="md-ellipsis">
      开始调试
    </span>
</a>
<nav aria-label="开始调试" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder-embedding">
<span class="md-ellipsis">
      encoder embedding
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_4">
<span class="md-ellipsis">
      模型定义
    </span>
</a>
<nav aria-label="模型定义" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_5">
<span class="md-ellipsis">
      编码器 解码器部分
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_6">
<span class="md-ellipsis">
      训练过程，形状变换
    </span>
</a>
<nav aria-label="训练过程，形状变换" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#autoformer-forward">
<span class="md-ellipsis">
      Autoformer forward
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_7">
<span class="md-ellipsis">
      序列分解
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#model-inputs">
<span class="md-ellipsis">
      model inputs
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder">
<span class="md-ellipsis">
      Encoder
    </span>
</a>
<nav aria-label="Encoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_8">
<span class="md-ellipsis">
      论文
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder-embedding_1">
<span class="md-ellipsis">
      Encoder Embedding
    </span>
</a>
<nav aria-label="Encoder Embedding" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_9">
<span class="md-ellipsis">
      类图
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder-forward">
<span class="md-ellipsis">
      Encoder forward
    </span>
</a>
<nav aria-label="Encoder forward" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_10">
<span class="md-ellipsis">
      类图
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoderlayer">
<span class="md-ellipsis">
      EncoderLayer
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#decoder">
<span class="md-ellipsis">
      Decoder
    </span>
</a>
<nav aria-label="Decoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_11">
<span class="md-ellipsis">
      类图
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_12">
<span class="md-ellipsis">
      论文
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_13">
<span class="md-ellipsis">
      代码
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#decoderlayer-forward">
<span class="md-ellipsis">
      DecoderLayer forward
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_14">
<span class="md-ellipsis">
      数据流动图
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#autoformer-forward_1">
<span class="md-ellipsis">
      汇总 Autoformer forward
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_15">
<span class="md-ellipsis">
      附录
    </span>
</a>
<nav aria-label="附录" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#conv2">
<span class="md-ellipsis">
      疑问一 为什么Conv2之后没有进行激活函数的应用
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#2-conv1d-nnlinear">
<span class="md-ellipsis">
      疑问 2 为什么是 conv1d，而不是 nn.Linear
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_16">
<span class="md-ellipsis">
      序列分解的现实意义
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_17">
<span class="md-ellipsis">
      理解多头注意力机制
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#k-v-q">
<span class="md-ellipsis">
      编码器的输出作为 K 和 V，解码器的输入作为 Q
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_18">
<span class="md-ellipsis">
      循环填充
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_19">
<span class="md-ellipsis">
      为什么季节成分和趋势成分不同的还原维度方法
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../6_AutoFormer_v1/">
<span class="md-ellipsis">
    (续) Autoformer
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_6_10" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../CodeRepo/">
<span class="md-ellipsis">
    一些代码
    
  </span>
</a>
<label class="md-nav__link" for="__nav_6_10" id="__nav_6_10_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_6_10_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_6_10">
<span class="md-nav__icon md-icon"></span>
            一些代码
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../1/">
<span class="md-ellipsis">
    一些模块
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2/">
<span class="md-ellipsis">
    特征融合方式
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../3/">
<span class="md-ellipsis">
    一些感悟
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../4/">
<span class="md-ellipsis">
    预训练权重
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../CodeRepo/1_MultiHeadAttention/">
<span class="md-ellipsis">
    多头注意力机制形状变化
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../CodeRepo/2_transformer/">
<span class="md-ellipsis">
    从现实生活的角度看 Transformer
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../CodeRepo/1_0_Autoformer/">
<span class="md-ellipsis">
    Autoformer
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_7" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../Statistics/">
<span class="md-ellipsis">
    统计学
    
  </span>
</a>
<label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_7_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_7">
<span class="md-nav__icon md-icon"></span>
            统计学
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../Statistics/1_FFT/">
<span class="md-ellipsis">
    Fourier级数
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Statistics/2_FFT/">
<span class="md-ellipsis">
    Fourier基础知识
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Statistics/1_0_fourier/">
<span class="md-ellipsis">
    复平面旋转&amp;DFT
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Statistics/1_1_fourier/">
<span class="md-ellipsis">
    直观理解傅里叶变换
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Statistics/1_2_signal/">
<span class="md-ellipsis">
    信号的合成与分解
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../Statistics/1_3_complexExp/">
<span class="md-ellipsis">
    复指数的性质
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_8" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../logs/">
<span class="md-ellipsis">
    杂
    
  </span>
</a>
<label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_8_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_8">
<span class="md-nav__icon md-icon"></span>
            杂
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../logs/diary/">
<span class="md-ellipsis">
    乐观 &amp; 坚强
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../logs/1_date/">
<span class="md-ellipsis">
    一些日期
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#github">
<span class="md-ellipsis">
      github 源码主页
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_1">
<span class="md-ellipsis">
      准备
    </span>
</a>
<nav aria-label="准备" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#git-clone">
<span class="md-ellipsis">
      git clone
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#readme">
<span class="md-ellipsis">
      readme
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_2">
<span class="md-ellipsis">
      调试配置
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#python">
<span class="md-ellipsis">
      新建 python 虚拟环境
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#requirements">
<span class="md-ellipsis">
      requirements
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_3">
<span class="md-ellipsis">
      开始调试
    </span>
</a>
<nav aria-label="开始调试" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder-embedding">
<span class="md-ellipsis">
      encoder embedding
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_4">
<span class="md-ellipsis">
      模型定义
    </span>
</a>
<nav aria-label="模型定义" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_5">
<span class="md-ellipsis">
      编码器 解码器部分
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_6">
<span class="md-ellipsis">
      训练过程，形状变换
    </span>
</a>
<nav aria-label="训练过程，形状变换" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#autoformer-forward">
<span class="md-ellipsis">
      Autoformer forward
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_7">
<span class="md-ellipsis">
      序列分解
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#model-inputs">
<span class="md-ellipsis">
      model inputs
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder">
<span class="md-ellipsis">
      Encoder
    </span>
</a>
<nav aria-label="Encoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_8">
<span class="md-ellipsis">
      论文
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder-embedding_1">
<span class="md-ellipsis">
      Encoder Embedding
    </span>
</a>
<nav aria-label="Encoder Embedding" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_9">
<span class="md-ellipsis">
      类图
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder-forward">
<span class="md-ellipsis">
      Encoder forward
    </span>
</a>
<nav aria-label="Encoder forward" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_10">
<span class="md-ellipsis">
      类图
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoderlayer">
<span class="md-ellipsis">
      EncoderLayer
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#decoder">
<span class="md-ellipsis">
      Decoder
    </span>
</a>
<nav aria-label="Decoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_11">
<span class="md-ellipsis">
      类图
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_12">
<span class="md-ellipsis">
      论文
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_13">
<span class="md-ellipsis">
      代码
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#decoderlayer-forward">
<span class="md-ellipsis">
      DecoderLayer forward
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_14">
<span class="md-ellipsis">
      数据流动图
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#autoformer-forward_1">
<span class="md-ellipsis">
      汇总 Autoformer forward
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_15">
<span class="md-ellipsis">
      附录
    </span>
</a>
<nav aria-label="附录" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#conv2">
<span class="md-ellipsis">
      疑问一 为什么Conv2之后没有进行激活函数的应用
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#2-conv1d-nnlinear">
<span class="md-ellipsis">
      疑问 2 为什么是 conv1d，而不是 nn.Linear
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_16">
<span class="md-ellipsis">
      序列分解的现实意义
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_17">
<span class="md-ellipsis">
      理解多头注意力机制
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#k-v-q">
<span class="md-ellipsis">
      编码器的输出作为 K 和 V，解码器的输入作为 Q
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_18">
<span class="md-ellipsis">
      循环填充
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_19">
<span class="md-ellipsis">
      为什么季节成分和趋势成分不同的还原维度方法
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="autoformer">Autoformer<a class="headerlink" href="#autoformer" title="Permanent link">¶</a></h1>
<h2 id="github">github 源码主页<a class="headerlink" href="#github" title="Permanent link">¶</a></h2>
<p>Autoformer (NeurIPS 2021) 自动成型机 (NeurIPS 2021)</p>
<p>Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting
Autoformer：用于长期序列预测的具有自相关的分解变压器</p>
<p>Time series forecasting is a critical demand for real applications. Enlighted by the classic time series analysis and stochastic process theory, we propose the Autoformer as a general series forecasting model [<a href="https://arxiv.org/abs/2106.13008">paper</a>]. <strong>Autoformer goes beyond the Transformer family and achieves the series-wise connection for the first time.</strong>
时间序列预测是实际应用的关键需求。受经典时间序列分析和随机过程理论的启发，我们提出了 Autoformer 作为通用序列预测模型 [<a href="https://arxiv.org/abs/2106.13008">论文</a>]。Autoformer**超越了 Transformer 家族，首次实现了序列连接。**</p>
<p>In long-term forecasting, Autoformer achieves SOTA, with a <strong>38% relative improvement</strong> on six benchmarks, covering five practical applications: <strong>energy, traffic, economics, weather and disease</strong>.
在长期预测中，Autoformer 实现了 SOTA，在六个基准上**相对提升了 38%** ，涵盖了**能源、交通、经济、天气和疾病**五个实际应用。</p>
<p><strong>News</strong> (2023.08) Autoformer has been included in <a href="https://huggingface.co/models?search=autoformer">Hugging Face</a>. See <a href="https://huggingface.co/blog/autoformer">blog</a>.
🚩<strong>新闻</strong>(2023.08) Autoformer 已包含在<a href="https://huggingface.co/models?search=autoformer">Hugging Face</a>中。查看<a href="https://huggingface.co/blog/autoformer">博客</a>。</p>
<p>🚩<strong>News</strong> (2023.06) The extension version of Autoformer (<a href="https://www.nature.com/articles/s42256-023-00667-9">Interpretable weather forecasting for worldwide stations with a unified deep model</a>) has been published in Nature Machine Intelligence as the <a href="https://www.nature.com/natmachintell/volumes/5/issues/6">Cover Article</a>.
🚩<strong>新闻</strong>(2023.06) Autoformer 的扩展版本 (<a href="https://www.nature.com/articles/s42256-023-00667-9">使用统一深度模型为全球站点提供可解释的天气预报</a>) 在《自然机器智能》杂志上作为<a href="https://www.nature.com/natmachintell/volumes/5/issues/6">封面文章</a>发表。</p>
<p>🚩<strong>News</strong> (2023.02) Autoformer has been included in our [<a href="https://github.com/thuml/Time-Series-Library">Time-Series-Library]</a>, which covers long- and short-term forecasting, imputation, anomaly detection, and classification.
🚩<strong>新闻</strong>(2023.02) Autoformer 已包含在我们的[<a href="https://github.com/thuml/Time-Series-Library">时间序列库]</a>中，它涵盖长期和短期预测、归纳、异常检测和分类。</p>
<p>🚩<strong>News</strong> (2022.02-2022.03) Autoformer has been deployed in <a href="https://en.wikipedia.org/wiki/2022_Winter_Olympics">2022 Winter Olympics</a> to provide weather forecasting for competition venues, including wind speed and temperature.
🚩<strong>新闻</strong>（2022.02-2022.03）Autoformer 已部署在<a href="https://en.wikipedia.org/wiki/2022_Winter_Olympics">2022 年冬奥会，</a>为比赛场馆提供天气预报，包括风速、温度等。</p>
<h2 id="_1">准备<a class="headerlink" href="#_1" title="Permanent link">¶</a></h2>
<h3 id="git-clone">git clone<a class="headerlink" href="#git-clone" title="Permanent link">¶</a></h3>
<p><img alt="image-20250317144505215" src="../images/image-20250317144505215.png"/> </p>
<p>克隆远程仓库的方法：</p>
<p>（1）HTTPS，在把本地仓库的代码 push 到远程仓库的时候，需要验证用户名和密码</p>
<p>（2）SSH，git 开头的是 SSH 协议，这种方式在推送的时候，不需要验证用户名和密码，但是需要在 github 上添加SSH公钥的配置（推荐）</p>
<p>（3）zip download</p>
<p>我这里使用了 SSH 配置：</p>
<p><img alt="image-20250317144903028" src="../images/image-20250317144903028.png"/> </p>
<p>服务器直接 git clone 是很慢。所以本地 git clone，然后再上传服务器。</p>
<p><img alt="image-20250317145242243" src="../images/image-20250317145242243.png"/> </p>
<p>本地下载好以后，使用 FileZilla上传到远程服务器</p>
<p><img alt="image-20250317145427044" src="../images/image-20250317145427044.png"/> </p>
<p>down到本地以后，删除 .git文件，取消连接着远程仓库</p>
<p><img alt="image-20250317145705700" src="../images/image-20250317145705700.png"/> </p>
<p><img alt="image-20250317145752968" src="../images/image-20250317145752968.png"/> </p>
<h3 id="readme">readme<a class="headerlink" href="#readme" title="Permanent link">¶</a></h3>
<p>下载数据集</p>
<p>设置数据集路径</p>
<p><img alt="image-20250317150739551" src="../images/image-20250317150739551.png"/> </p>
<h3 id="_2">调试配置<a class="headerlink" href="#_2" title="Permanent link">¶</a></h3>
<p>新建配置文件</p>
<p><img alt="image-20250317150852423" src="../images/image-20250317150852423.png"/> </p>
<p>修改配置文件</p>
<p><img alt="image-20250317151048416" src="../images/image-20250317151048416.png"/> </p>
<p>修改配置文件</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a href="#__codelineno-0-1" id="__codelineno-0-1" name="__codelineno-0-1"></a>        {
</span><span id="__span-0-2"><a href="#__codelineno-0-2" id="__codelineno-0-2" name="__codelineno-0-2"></a>            "name": "Autoformer",
</span><span id="__span-0-3"><a href="#__codelineno-0-3" id="__codelineno-0-3" name="__codelineno-0-3"></a>            "type": "python",
</span><span id="__span-0-4"><a href="#__codelineno-0-4" id="__codelineno-0-4" name="__codelineno-0-4"></a>            "request": "attach",
</span><span id="__span-0-5"><a href="#__codelineno-0-5" id="__codelineno-0-5" name="__codelineno-0-5"></a>            "connect": {
</span><span id="__span-0-6"><a href="#__codelineno-0-6" id="__codelineno-0-6" name="__codelineno-0-6"></a>                "host": "localhost",
</span><span id="__span-0-7"><a href="#__codelineno-0-7" id="__codelineno-0-7" name="__codelineno-0-7"></a>                "port": 5997
</span><span id="__span-0-8"><a href="#__codelineno-0-8" id="__codelineno-0-8" name="__codelineno-0-8"></a>            }
</span><span id="__span-0-9"><a href="#__codelineno-0-9" id="__codelineno-0-9" name="__codelineno-0-9"></a>        },
</span></code></pre></div>
<p>修改 sh 文件</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a href="#__codelineno-1-1" id="__codelineno-1-1" name="__codelineno-1-1"></a>python -m debugpy --listen 5997 --wait-for-client run.py \
</span></code></pre></div>
<h3 id="python">新建 python 虚拟环境<a class="headerlink" href="#python" title="Permanent link">¶</a></h3>
<p>本实验所需要的实验环境</p>
<blockquote>
<p>Install Python 3.6, PyTorch 1.9.0.</p>
</blockquote>
<p>参考命令</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-2-1"><a href="#__codelineno-2-1" id="__codelineno-2-1" name="__codelineno-2-1"></a>conda create -n dave python==3.8
</span><span id="__span-2-2"><a href="#__codelineno-2-2" id="__codelineno-2-2" name="__codelineno-2-2"></a>conda activate dave
</span><span id="__span-2-3"><a href="#__codelineno-2-3" id="__codelineno-2-3" name="__codelineno-2-3"></a>conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia
</span><span id="__span-2-4"><a href="#__codelineno-2-4" id="__codelineno-2-4" name="__codelineno-2-4"></a>conda install numpy
</span><span id="__span-2-5"><a href="#__codelineno-2-5" id="__codelineno-2-5" name="__codelineno-2-5"></a>conda install scikit-image
</span><span id="__span-2-6"><a href="#__codelineno-2-6" id="__codelineno-2-6" name="__codelineno-2-6"></a>conda install scikit-learn
</span><span id="__span-2-7"><a href="#__codelineno-2-7" id="__codelineno-2-7" name="__codelineno-2-7"></a>conda install tqdm
</span><span id="__span-2-8"><a href="#__codelineno-2-8" id="__codelineno-2-8" name="__codelineno-2-8"></a>conda install pycocotools
</span></code></pre></div>
<p>激活、退出：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a href="#__codelineno-3-1" id="__codelineno-3-1" name="__codelineno-3-1"></a># To activate this environment, use               
</span><span id="__span-3-2"><a href="#__codelineno-3-2" id="__codelineno-3-2" name="__codelineno-3-2"></a>#     $ conda activate Autoformer
</span><span id="__span-3-3"><a href="#__codelineno-3-3" id="__codelineno-3-3" name="__codelineno-3-3"></a>#
</span><span id="__span-3-4"><a href="#__codelineno-3-4" id="__codelineno-3-4" name="__codelineno-3-4"></a># To deactivate an active environment, use
</span><span id="__span-3-5"><a href="#__codelineno-3-5" id="__codelineno-3-5" name="__codelineno-3-5"></a>#
</span><span id="__span-3-6"><a href="#__codelineno-3-6" id="__codelineno-3-6" name="__codelineno-3-6"></a>#     $ conda deactivate
</span></code></pre></div>
<p>用 requirements.txt 安装需要的库</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-4-1"><a href="#__codelineno-4-1" id="__codelineno-4-1" name="__codelineno-4-1"></a>conda create -n SegRNN python=3.8
</span><span id="__span-4-2"><a href="#__codelineno-4-2" id="__codelineno-4-2" name="__codelineno-4-2"></a>conda activate SegRNN
</span><span id="__span-4-3"><a href="#__codelineno-4-3" id="__codelineno-4-3" name="__codelineno-4-3"></a>pip install -r requirements.txt
</span></code></pre></div>
<p>启动 sh 文件：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-5-1"><a href="#__codelineno-5-1" id="__codelineno-5-1" name="__codelineno-5-1"></a>sh run_main.sh
</span></code></pre></div>
<p><strong>适用于本实验的所有命令 :</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a href="#__codelineno-6-1" id="__codelineno-6-1" name="__codelineno-6-1"></a><span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="n">Autoformer</span> <span class="n">python</span><span class="o">=</span><span class="mf">3.6</span>
</span><span id="__span-6-2"><a href="#__codelineno-6-2" id="__codelineno-6-2" name="__codelineno-6-2"></a><span class="n">conda</span> <span class="n">activate</span> <span class="n">Autoformer</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a href="#__codelineno-7-1" id="__codelineno-7-1" name="__codelineno-7-1"></a><span class="n">conda</span> <span class="n">env</span> <span class="nb">list</span>
</span><span id="__span-7-2"><a href="#__codelineno-7-2" id="__codelineno-7-2" name="__codelineno-7-2"></a><span class="n">conda</span> <span class="n">actiavte</span> <span class="n">环境名</span>
</span><span id="__span-7-3"><a href="#__codelineno-7-3" id="__codelineno-7-3" name="__codelineno-7-3"></a><span class="n">conda</span> <span class="n">deactivate</span>
</span></code></pre></div>
<p><a href="https://pytorch.org/">pytorch 官网</a>查看所需命令</p>
<p><img alt="image-20250317154037815" src="../images/image-20250317154037815.png"/> </p>
<p><img alt="image-20250317153952283" src="../images/image-20250317153952283.png"/></p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-8-1"><a href="#__codelineno-8-1" id="__codelineno-8-1" name="__codelineno-8-1"></a>conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=10.2 -c pytorch
</span></code></pre></div>
<h3 id="requirements">requirements<a class="headerlink" href="#requirements" title="Permanent link">¶</a></h3>
<div class="language-text highlight"><pre><span></span><code><span id="__span-9-1"><a href="#__codelineno-9-1" id="__codelineno-9-1" name="__codelineno-9-1"></a>pip install -r requirements.txt
</span></code></pre></div>
<p>或者：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-10-1"><a href="#__codelineno-10-1" id="__codelineno-10-1" name="__codelineno-10-1"></a>conda create -n Autoformer python=3.6
</span><span id="__span-10-2"><a href="#__codelineno-10-2" id="__codelineno-10-2" name="__codelineno-10-2"></a>conda activate Autoformer
</span><span id="__span-10-3"><a href="#__codelineno-10-3" id="__codelineno-10-3" name="__codelineno-10-3"></a>conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=10.2 -c pytorch
</span><span id="__span-10-4"><a href="#__codelineno-10-4" id="__codelineno-10-4" name="__codelineno-10-4"></a>conda install pandas
</span><span id="__span-10-5"><a href="#__codelineno-10-5" id="__codelineno-10-5" name="__codelineno-10-5"></a>conda install scikit-learn
</span><span id="__span-10-6"><a href="#__codelineno-10-6" id="__codelineno-10-6" name="__codelineno-10-6"></a>conda install debugpy
</span><span id="__span-10-7"><a href="#__codelineno-10-7" id="__codelineno-10-7" name="__codelineno-10-7"></a>conda install matplotlib
</span><span id="__span-10-8"><a href="#__codelineno-10-8" id="__codelineno-10-8" name="__codelineno-10-8"></a>conda install reformer_pytorch
</span></code></pre></div>
<p>配置好以后，成功进入调试：</p>
<p><img alt="image-20250317155629524" src="../images/image-20250317155629524.png"/> </p>
<h2 id="_3">开始调试<a class="headerlink" href="#_3" title="Permanent link">¶</a></h2>
<p>代码相似度极高。</p>
<p><strong>Autoformer init：36（18）-》24</strong></p>
<p><img alt="image-20250317160059801" src="../images/image-20250317160059801.png"/></p>
<p>setting:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-11-1"><a href="#__codelineno-11-1" id="__codelineno-11-1" name="__codelineno-11-1"></a>ili_36_24_Autoformer_custom_ftM_sl36_ll18_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0
</span></code></pre></div>
<p>model_id  36 预测 24 步长（label=18）、AutoFormer 模型，自定义数据集，预测多变量，输入序列 36，标签序列 18，预测序列 24，嵌入维度 512，注意力头数 8，2层编码层，1 层解码层，</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-12-1"><a href="#__codelineno-12-1" id="__codelineno-12-1" name="__codelineno-12-1"></a>df2048_fc3_ebtimeF_dtTrue_Exp_0
</span><span id="__span-12-2"><a href="#__codelineno-12-2" id="__codelineno-12-2" name="__codelineno-12-2"></a>                args.d_ff,
</span><span id="__span-12-3"><a href="#__codelineno-12-3" id="__codelineno-12-3" name="__codelineno-12-3"></a>                args.factor,
</span><span id="__span-12-4"><a href="#__codelineno-12-4" id="__codelineno-12-4" name="__codelineno-12-4"></a>                args.embed,
</span><span id="__span-12-5"><a href="#__codelineno-12-5" id="__codelineno-12-5" name="__codelineno-12-5"></a>                args.distil,
</span><span id="__span-12-6"><a href="#__codelineno-12-6" id="__codelineno-12-6" name="__codelineno-12-6"></a>                args.des, ii)
</span></code></pre></div>
<p><strong>Autoformer model</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><a href="#__codelineno-13-1" id="__codelineno-13-1" name="__codelineno-13-1"></a><span class="n">Model</span><span class="p">(</span>
</span><span id="__span-13-2"><a href="#__codelineno-13-2" id="__codelineno-13-2" name="__codelineno-13-2"></a>  <span class="p">(</span><span class="n">decomp</span><span class="p">):</span> <span class="n">series_decomp</span><span class="p">(</span>
</span><span id="__span-13-3"><a href="#__codelineno-13-3" id="__codelineno-13-3" name="__codelineno-13-3"></a>    <span class="p">(</span><span class="n">moving_avg</span><span class="p">):</span> <span class="n">moving_avg</span><span class="p">(</span>
</span><span id="__span-13-4"><a href="#__codelineno-13-4" id="__codelineno-13-4" name="__codelineno-13-4"></a>      <span class="p">(</span><span class="n">avg</span><span class="p">):</span> <span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="__span-13-5"><a href="#__codelineno-13-5" id="__codelineno-13-5" name="__codelineno-13-5"></a>    <span class="p">)</span>
</span><span id="__span-13-6"><a href="#__codelineno-13-6" id="__codelineno-13-6" name="__codelineno-13-6"></a>  <span class="p">)</span>
</span><span id="__span-13-7"><a href="#__codelineno-13-7" id="__codelineno-13-7" name="__codelineno-13-7"></a>  <span class="p">(</span><span class="n">enc_embedding</span><span class="p">):</span> <span class="n">DataEmbedding_wo_pos</span><span class="p">(</span>
</span><span id="__span-13-8"><a href="#__codelineno-13-8" id="__codelineno-13-8" name="__codelineno-13-8"></a>    <span class="p">(</span><span class="n">value_embedding</span><span class="p">):</span> <span class="n">TokenEmbedding</span><span class="p">(</span>
</span><span id="__span-13-9"><a href="#__codelineno-13-9" id="__codelineno-13-9" name="__codelineno-13-9"></a>      <span class="p">(</span><span class="n">tokenConv</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="n">circular</span><span class="p">)</span>
</span><span id="__span-13-10"><a href="#__codelineno-13-10" id="__codelineno-13-10" name="__codelineno-13-10"></a>    <span class="p">)</span>
</span><span id="__span-13-11"><a href="#__codelineno-13-11" id="__codelineno-13-11" name="__codelineno-13-11"></a>    <span class="p">(</span><span class="n">position_embedding</span><span class="p">):</span> <span class="n">PositionalEmbedding</span><span class="p">()</span>
</span><span id="__span-13-12"><a href="#__codelineno-13-12" id="__codelineno-13-12" name="__codelineno-13-12"></a>    <span class="p">(</span><span class="n">temporal_embedding</span><span class="p">):</span> <span class="n">TimeFeatureEmbedding</span><span class="p">(</span>
</span><span id="__span-13-13"><a href="#__codelineno-13-13" id="__codelineno-13-13" name="__codelineno-13-13"></a>      <span class="p">(</span><span class="n">embed</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-14"><a href="#__codelineno-13-14" id="__codelineno-13-14" name="__codelineno-13-14"></a>    <span class="p">)</span>
</span><span id="__span-13-15"><a href="#__codelineno-13-15" id="__codelineno-13-15" name="__codelineno-13-15"></a>    <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-16"><a href="#__codelineno-13-16" id="__codelineno-13-16" name="__codelineno-13-16"></a>  <span class="p">)</span>
</span><span id="__span-13-17"><a href="#__codelineno-13-17" id="__codelineno-13-17" name="__codelineno-13-17"></a>  <span class="p">(</span><span class="n">dec_embedding</span><span class="p">):</span> <span class="n">DataEmbedding_wo_pos</span><span class="p">(</span>
</span><span id="__span-13-18"><a href="#__codelineno-13-18" id="__codelineno-13-18" name="__codelineno-13-18"></a>    <span class="p">(</span><span class="n">value_embedding</span><span class="p">):</span> <span class="n">TokenEmbedding</span><span class="p">(</span>
</span><span id="__span-13-19"><a href="#__codelineno-13-19" id="__codelineno-13-19" name="__codelineno-13-19"></a>      <span class="p">(</span><span class="n">tokenConv</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="n">circular</span><span class="p">)</span>
</span><span id="__span-13-20"><a href="#__codelineno-13-20" id="__codelineno-13-20" name="__codelineno-13-20"></a>    <span class="p">)</span>
</span><span id="__span-13-21"><a href="#__codelineno-13-21" id="__codelineno-13-21" name="__codelineno-13-21"></a>    <span class="p">(</span><span class="n">position_embedding</span><span class="p">):</span> <span class="n">PositionalEmbedding</span><span class="p">()</span>
</span><span id="__span-13-22"><a href="#__codelineno-13-22" id="__codelineno-13-22" name="__codelineno-13-22"></a>    <span class="p">(</span><span class="n">temporal_embedding</span><span class="p">):</span> <span class="n">TimeFeatureEmbedding</span><span class="p">(</span>
</span><span id="__span-13-23"><a href="#__codelineno-13-23" id="__codelineno-13-23" name="__codelineno-13-23"></a>      <span class="p">(</span><span class="n">embed</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-24"><a href="#__codelineno-13-24" id="__codelineno-13-24" name="__codelineno-13-24"></a>    <span class="p">)</span>
</span><span id="__span-13-25"><a href="#__codelineno-13-25" id="__codelineno-13-25" name="__codelineno-13-25"></a>    <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-26"><a href="#__codelineno-13-26" id="__codelineno-13-26" name="__codelineno-13-26"></a>  <span class="p">)</span>
</span><span id="__span-13-27"><a href="#__codelineno-13-27" id="__codelineno-13-27" name="__codelineno-13-27"></a>  <span class="p">(</span><span class="n">encoder</span><span class="p">):</span> <span class="n">Encoder</span><span class="p">(</span>
</span><span id="__span-13-28"><a href="#__codelineno-13-28" id="__codelineno-13-28" name="__codelineno-13-28"></a>    <span class="p">(</span><span class="n">attn_layers</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
</span><span id="__span-13-29"><a href="#__codelineno-13-29" id="__codelineno-13-29" name="__codelineno-13-29"></a>      <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">EncoderLayer</span><span class="p">(</span>
</span><span id="__span-13-30"><a href="#__codelineno-13-30" id="__codelineno-13-30" name="__codelineno-13-30"></a>        <span class="p">(</span><span class="n">attention</span><span class="p">):</span> <span class="n">AutoCorrelationLayer</span><span class="p">(</span>
</span><span id="__span-13-31"><a href="#__codelineno-13-31" id="__codelineno-13-31" name="__codelineno-13-31"></a>          <span class="p">(</span><span class="n">inner_correlation</span><span class="p">):</span> <span class="n">AutoCorrelation</span><span class="p">(</span>
</span><span id="__span-13-32"><a href="#__codelineno-13-32" id="__codelineno-13-32" name="__codelineno-13-32"></a>            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-33"><a href="#__codelineno-13-33" id="__codelineno-13-33" name="__codelineno-13-33"></a>          <span class="p">)</span>
</span><span id="__span-13-34"><a href="#__codelineno-13-34" id="__codelineno-13-34" name="__codelineno-13-34"></a>          <span class="p">(</span><span class="n">query_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-35"><a href="#__codelineno-13-35" id="__codelineno-13-35" name="__codelineno-13-35"></a>          <span class="p">(</span><span class="n">key_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-36"><a href="#__codelineno-13-36" id="__codelineno-13-36" name="__codelineno-13-36"></a>          <span class="p">(</span><span class="n">value_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-37"><a href="#__codelineno-13-37" id="__codelineno-13-37" name="__codelineno-13-37"></a>          <span class="p">(</span><span class="n">out_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-38"><a href="#__codelineno-13-38" id="__codelineno-13-38" name="__codelineno-13-38"></a>        <span class="p">)</span>
</span><span id="__span-13-39"><a href="#__codelineno-13-39" id="__codelineno-13-39" name="__codelineno-13-39"></a>        <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-40"><a href="#__codelineno-13-40" id="__codelineno-13-40" name="__codelineno-13-40"></a>        <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-41"><a href="#__codelineno-13-41" id="__codelineno-13-41" name="__codelineno-13-41"></a>        <span class="p">(</span><span class="n">decomp1</span><span class="p">):</span> <span class="n">series_decomp</span><span class="p">(</span>
</span><span id="__span-13-42"><a href="#__codelineno-13-42" id="__codelineno-13-42" name="__codelineno-13-42"></a>          <span class="p">(</span><span class="n">moving_avg</span><span class="p">):</span> <span class="n">moving_avg</span><span class="p">(</span>
</span><span id="__span-13-43"><a href="#__codelineno-13-43" id="__codelineno-13-43" name="__codelineno-13-43"></a>            <span class="p">(</span><span class="n">avg</span><span class="p">):</span> <span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="__span-13-44"><a href="#__codelineno-13-44" id="__codelineno-13-44" name="__codelineno-13-44"></a>          <span class="p">)</span>
</span><span id="__span-13-45"><a href="#__codelineno-13-45" id="__codelineno-13-45" name="__codelineno-13-45"></a>        <span class="p">)</span>
</span><span id="__span-13-46"><a href="#__codelineno-13-46" id="__codelineno-13-46" name="__codelineno-13-46"></a>        <span class="p">(</span><span class="n">decomp2</span><span class="p">):</span> <span class="n">series_decomp</span><span class="p">(</span>
</span><span id="__span-13-47"><a href="#__codelineno-13-47" id="__codelineno-13-47" name="__codelineno-13-47"></a>          <span class="p">(</span><span class="n">moving_avg</span><span class="p">):</span> <span class="n">moving_avg</span><span class="p">(</span>
</span><span id="__span-13-48"><a href="#__codelineno-13-48" id="__codelineno-13-48" name="__codelineno-13-48"></a>            <span class="p">(</span><span class="n">avg</span><span class="p">):</span> <span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="__span-13-49"><a href="#__codelineno-13-49" id="__codelineno-13-49" name="__codelineno-13-49"></a>          <span class="p">)</span>
</span><span id="__span-13-50"><a href="#__codelineno-13-50" id="__codelineno-13-50" name="__codelineno-13-50"></a>        <span class="p">)</span>
</span><span id="__span-13-51"><a href="#__codelineno-13-51" id="__codelineno-13-51" name="__codelineno-13-51"></a>        <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-52"><a href="#__codelineno-13-52" id="__codelineno-13-52" name="__codelineno-13-52"></a>      <span class="p">)</span>
</span><span id="__span-13-53"><a href="#__codelineno-13-53" id="__codelineno-13-53" name="__codelineno-13-53"></a>      <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">EncoderLayer</span><span class="p">(</span>
</span><span id="__span-13-54"><a href="#__codelineno-13-54" id="__codelineno-13-54" name="__codelineno-13-54"></a>        <span class="p">(</span><span class="n">attention</span><span class="p">):</span> <span class="n">AutoCorrelationLayer</span><span class="p">(</span>
</span><span id="__span-13-55"><a href="#__codelineno-13-55" id="__codelineno-13-55" name="__codelineno-13-55"></a>          <span class="p">(</span><span class="n">inner_correlation</span><span class="p">):</span> <span class="n">AutoCorrelation</span><span class="p">(</span>
</span><span id="__span-13-56"><a href="#__codelineno-13-56" id="__codelineno-13-56" name="__codelineno-13-56"></a>            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-57"><a href="#__codelineno-13-57" id="__codelineno-13-57" name="__codelineno-13-57"></a>          <span class="p">)</span>
</span><span id="__span-13-58"><a href="#__codelineno-13-58" id="__codelineno-13-58" name="__codelineno-13-58"></a>          <span class="p">(</span><span class="n">query_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-59"><a href="#__codelineno-13-59" id="__codelineno-13-59" name="__codelineno-13-59"></a>          <span class="p">(</span><span class="n">key_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-60"><a href="#__codelineno-13-60" id="__codelineno-13-60" name="__codelineno-13-60"></a>          <span class="p">(</span><span class="n">value_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-61"><a href="#__codelineno-13-61" id="__codelineno-13-61" name="__codelineno-13-61"></a>          <span class="p">(</span><span class="n">out_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-62"><a href="#__codelineno-13-62" id="__codelineno-13-62" name="__codelineno-13-62"></a>        <span class="p">)</span>
</span><span id="__span-13-63"><a href="#__codelineno-13-63" id="__codelineno-13-63" name="__codelineno-13-63"></a>        <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-64"><a href="#__codelineno-13-64" id="__codelineno-13-64" name="__codelineno-13-64"></a>        <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-65"><a href="#__codelineno-13-65" id="__codelineno-13-65" name="__codelineno-13-65"></a>        <span class="p">(</span><span class="n">decomp1</span><span class="p">):</span> <span class="n">series_decomp</span><span class="p">(</span>
</span><span id="__span-13-66"><a href="#__codelineno-13-66" id="__codelineno-13-66" name="__codelineno-13-66"></a>          <span class="p">(</span><span class="n">moving_avg</span><span class="p">):</span> <span class="n">moving_avg</span><span class="p">(</span>
</span><span id="__span-13-67"><a href="#__codelineno-13-67" id="__codelineno-13-67" name="__codelineno-13-67"></a>            <span class="p">(</span><span class="n">avg</span><span class="p">):</span> <span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="__span-13-68"><a href="#__codelineno-13-68" id="__codelineno-13-68" name="__codelineno-13-68"></a>          <span class="p">)</span>
</span><span id="__span-13-69"><a href="#__codelineno-13-69" id="__codelineno-13-69" name="__codelineno-13-69"></a>        <span class="p">)</span>
</span><span id="__span-13-70"><a href="#__codelineno-13-70" id="__codelineno-13-70" name="__codelineno-13-70"></a>        <span class="p">(</span><span class="n">decomp2</span><span class="p">):</span> <span class="n">series_decomp</span><span class="p">(</span>
</span><span id="__span-13-71"><a href="#__codelineno-13-71" id="__codelineno-13-71" name="__codelineno-13-71"></a>          <span class="p">(</span><span class="n">moving_avg</span><span class="p">):</span> <span class="n">moving_avg</span><span class="p">(</span>
</span><span id="__span-13-72"><a href="#__codelineno-13-72" id="__codelineno-13-72" name="__codelineno-13-72"></a>            <span class="p">(</span><span class="n">avg</span><span class="p">):</span> <span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="__span-13-73"><a href="#__codelineno-13-73" id="__codelineno-13-73" name="__codelineno-13-73"></a>          <span class="p">)</span>
</span><span id="__span-13-74"><a href="#__codelineno-13-74" id="__codelineno-13-74" name="__codelineno-13-74"></a>        <span class="p">)</span>
</span><span id="__span-13-75"><a href="#__codelineno-13-75" id="__codelineno-13-75" name="__codelineno-13-75"></a>        <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-76"><a href="#__codelineno-13-76" id="__codelineno-13-76" name="__codelineno-13-76"></a>      <span class="p">)</span>
</span><span id="__span-13-77"><a href="#__codelineno-13-77" id="__codelineno-13-77" name="__codelineno-13-77"></a>    <span class="p">)</span>
</span><span id="__span-13-78"><a href="#__codelineno-13-78" id="__codelineno-13-78" name="__codelineno-13-78"></a>    <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">my_Layernorm</span><span class="p">(</span>
</span><span id="__span-13-79"><a href="#__codelineno-13-79" id="__codelineno-13-79" name="__codelineno-13-79"></a>      <span class="p">(</span><span class="n">layernorm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="mi">512</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-80"><a href="#__codelineno-13-80" id="__codelineno-13-80" name="__codelineno-13-80"></a>    <span class="p">)</span>
</span><span id="__span-13-81"><a href="#__codelineno-13-81" id="__codelineno-13-81" name="__codelineno-13-81"></a>  <span class="p">)</span>
</span><span id="__span-13-82"><a href="#__codelineno-13-82" id="__codelineno-13-82" name="__codelineno-13-82"></a>  <span class="p">(</span><span class="n">decoder</span><span class="p">):</span> <span class="n">Decoder</span><span class="p">(</span>
</span><span id="__span-13-83"><a href="#__codelineno-13-83" id="__codelineno-13-83" name="__codelineno-13-83"></a>    <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
</span><span id="__span-13-84"><a href="#__codelineno-13-84" id="__codelineno-13-84" name="__codelineno-13-84"></a>      <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">DecoderLayer</span><span class="p">(</span>
</span><span id="__span-13-85"><a href="#__codelineno-13-85" id="__codelineno-13-85" name="__codelineno-13-85"></a>        <span class="p">(</span><span class="n">self_attention</span><span class="p">):</span> <span class="n">AutoCorrelationLayer</span><span class="p">(</span>
</span><span id="__span-13-86"><a href="#__codelineno-13-86" id="__codelineno-13-86" name="__codelineno-13-86"></a>          <span class="p">(</span><span class="n">inner_correlation</span><span class="p">):</span> <span class="n">AutoCorrelation</span><span class="p">(</span>
</span><span id="__span-13-87"><a href="#__codelineno-13-87" id="__codelineno-13-87" name="__codelineno-13-87"></a>            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-88"><a href="#__codelineno-13-88" id="__codelineno-13-88" name="__codelineno-13-88"></a>          <span class="p">)</span>
</span><span id="__span-13-89"><a href="#__codelineno-13-89" id="__codelineno-13-89" name="__codelineno-13-89"></a>          <span class="p">(</span><span class="n">query_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-90"><a href="#__codelineno-13-90" id="__codelineno-13-90" name="__codelineno-13-90"></a>          <span class="p">(</span><span class="n">key_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-91"><a href="#__codelineno-13-91" id="__codelineno-13-91" name="__codelineno-13-91"></a>          <span class="p">(</span><span class="n">value_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-92"><a href="#__codelineno-13-92" id="__codelineno-13-92" name="__codelineno-13-92"></a>          <span class="p">(</span><span class="n">out_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-93"><a href="#__codelineno-13-93" id="__codelineno-13-93" name="__codelineno-13-93"></a>        <span class="p">)</span>
</span><span id="__span-13-94"><a href="#__codelineno-13-94" id="__codelineno-13-94" name="__codelineno-13-94"></a>        <span class="p">(</span><span class="n">cross_attention</span><span class="p">):</span> <span class="n">AutoCorrelationLayer</span><span class="p">(</span>
</span><span id="__span-13-95"><a href="#__codelineno-13-95" id="__codelineno-13-95" name="__codelineno-13-95"></a>          <span class="p">(</span><span class="n">inner_correlation</span><span class="p">):</span> <span class="n">AutoCorrelation</span><span class="p">(</span>
</span><span id="__span-13-96"><a href="#__codelineno-13-96" id="__codelineno-13-96" name="__codelineno-13-96"></a>            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-97"><a href="#__codelineno-13-97" id="__codelineno-13-97" name="__codelineno-13-97"></a>          <span class="p">)</span>
</span><span id="__span-13-98"><a href="#__codelineno-13-98" id="__codelineno-13-98" name="__codelineno-13-98"></a>          <span class="p">(</span><span class="n">query_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-99"><a href="#__codelineno-13-99" id="__codelineno-13-99" name="__codelineno-13-99"></a>          <span class="p">(</span><span class="n">key_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-100"><a href="#__codelineno-13-100" id="__codelineno-13-100" name="__codelineno-13-100"></a>          <span class="p">(</span><span class="n">value_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-101"><a href="#__codelineno-13-101" id="__codelineno-13-101" name="__codelineno-13-101"></a>          <span class="p">(</span><span class="n">out_projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-102"><a href="#__codelineno-13-102" id="__codelineno-13-102" name="__codelineno-13-102"></a>        <span class="p">)</span>
</span><span id="__span-13-103"><a href="#__codelineno-13-103" id="__codelineno-13-103" name="__codelineno-13-103"></a>        <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-104"><a href="#__codelineno-13-104" id="__codelineno-13-104" name="__codelineno-13-104"></a>        <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-105"><a href="#__codelineno-13-105" id="__codelineno-13-105" name="__codelineno-13-105"></a>        <span class="p">(</span><span class="n">decomp1</span><span class="p">):</span> <span class="n">series_decomp</span><span class="p">(</span>
</span><span id="__span-13-106"><a href="#__codelineno-13-106" id="__codelineno-13-106" name="__codelineno-13-106"></a>          <span class="p">(</span><span class="n">moving_avg</span><span class="p">):</span> <span class="n">moving_avg</span><span class="p">(</span>
</span><span id="__span-13-107"><a href="#__codelineno-13-107" id="__codelineno-13-107" name="__codelineno-13-107"></a>            <span class="p">(</span><span class="n">avg</span><span class="p">):</span> <span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="__span-13-108"><a href="#__codelineno-13-108" id="__codelineno-13-108" name="__codelineno-13-108"></a>          <span class="p">)</span>
</span><span id="__span-13-109"><a href="#__codelineno-13-109" id="__codelineno-13-109" name="__codelineno-13-109"></a>        <span class="p">)</span>
</span><span id="__span-13-110"><a href="#__codelineno-13-110" id="__codelineno-13-110" name="__codelineno-13-110"></a>        <span class="p">(</span><span class="n">decomp2</span><span class="p">):</span> <span class="n">series_decomp</span><span class="p">(</span>
</span><span id="__span-13-111"><a href="#__codelineno-13-111" id="__codelineno-13-111" name="__codelineno-13-111"></a>          <span class="p">(</span><span class="n">moving_avg</span><span class="p">):</span> <span class="n">moving_avg</span><span class="p">(</span>
</span><span id="__span-13-112"><a href="#__codelineno-13-112" id="__codelineno-13-112" name="__codelineno-13-112"></a>            <span class="p">(</span><span class="n">avg</span><span class="p">):</span> <span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="__span-13-113"><a href="#__codelineno-13-113" id="__codelineno-13-113" name="__codelineno-13-113"></a>          <span class="p">)</span>
</span><span id="__span-13-114"><a href="#__codelineno-13-114" id="__codelineno-13-114" name="__codelineno-13-114"></a>        <span class="p">)</span>
</span><span id="__span-13-115"><a href="#__codelineno-13-115" id="__codelineno-13-115" name="__codelineno-13-115"></a>        <span class="p">(</span><span class="n">decomp3</span><span class="p">):</span> <span class="n">series_decomp</span><span class="p">(</span>
</span><span id="__span-13-116"><a href="#__codelineno-13-116" id="__codelineno-13-116" name="__codelineno-13-116"></a>          <span class="p">(</span><span class="n">moving_avg</span><span class="p">):</span> <span class="n">moving_avg</span><span class="p">(</span>
</span><span id="__span-13-117"><a href="#__codelineno-13-117" id="__codelineno-13-117" name="__codelineno-13-117"></a>            <span class="p">(</span><span class="n">avg</span><span class="p">):</span> <span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="__span-13-118"><a href="#__codelineno-13-118" id="__codelineno-13-118" name="__codelineno-13-118"></a>          <span class="p">)</span>
</span><span id="__span-13-119"><a href="#__codelineno-13-119" id="__codelineno-13-119" name="__codelineno-13-119"></a>        <span class="p">)</span>
</span><span id="__span-13-120"><a href="#__codelineno-13-120" id="__codelineno-13-120" name="__codelineno-13-120"></a>        <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-13-121"><a href="#__codelineno-13-121" id="__codelineno-13-121" name="__codelineno-13-121"></a>        <span class="p">(</span><span class="n">projection</span><span class="p">):</span> <span class="n">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="n">circular</span><span class="p">)</span>
</span><span id="__span-13-122"><a href="#__codelineno-13-122" id="__codelineno-13-122" name="__codelineno-13-122"></a>      <span class="p">)</span>
</span><span id="__span-13-123"><a href="#__codelineno-13-123" id="__codelineno-13-123" name="__codelineno-13-123"></a>    <span class="p">)</span>
</span><span id="__span-13-124"><a href="#__codelineno-13-124" id="__codelineno-13-124" name="__codelineno-13-124"></a>    <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">my_Layernorm</span><span class="p">(</span>
</span><span id="__span-13-125"><a href="#__codelineno-13-125" id="__codelineno-13-125" name="__codelineno-13-125"></a>      <span class="p">(</span><span class="n">layernorm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="mi">512</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-126"><a href="#__codelineno-13-126" id="__codelineno-13-126" name="__codelineno-13-126"></a>    <span class="p">)</span>
</span><span id="__span-13-127"><a href="#__codelineno-13-127" id="__codelineno-13-127" name="__codelineno-13-127"></a>    <span class="p">(</span><span class="n">projection</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-13-128"><a href="#__codelineno-13-128" id="__codelineno-13-128" name="__codelineno-13-128"></a>  <span class="p">)</span>
</span><span id="__span-13-129"><a href="#__codelineno-13-129" id="__codelineno-13-129" name="__codelineno-13-129"></a><span class="p">)</span>
</span></code></pre></div>
<p>数据集的加载是完全一样的。</p>
<h3 id="encoder-embedding">encoder embedding<a class="headerlink" href="#encoder-embedding" title="Permanent link">¶</a></h3>
<p>目的：结合时间特征，将 数据特征嵌入到指定维度</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-14-1"><a href="#__codelineno-14-1" id="__codelineno-14-1" name="__codelineno-14-1"></a><span class="n">enc_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_embedding</span><span class="p">(</span><span class="n">x_enc</span><span class="p">,</span> <span class="n">x_mark_enc</span><span class="p">)</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a href="#__codelineno-15-1" id="__codelineno-15-1" name="__codelineno-15-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">enc_embedding</span> <span class="o">=</span> <span class="n">DataEmbedding_wo_pos</span><span class="p">(</span><span class="n">configs</span><span class="o">.</span><span class="n">enc_in</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">embed</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">freq</span><span class="p">,</span><span class="n">configs</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
</span></code></pre></div>
<p><img alt="image-20250317204752276" src="../images/image-20250317204752276.png"/></p>
<p><img alt="image-20250317205258626" src="../images/image-20250317205258626.png"/></p>
<p><strong>流程图</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><a href="#__codelineno-16-1" id="__codelineno-16-1" name="__codelineno-16-1"></a><span class="n">输入</span><span class="p">:</span>
</span><span id="__span-16-2"><a href="#__codelineno-16-2" id="__codelineno-16-2" name="__codelineno-16-2"></a><span class="n">x_enc</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">]</span>        <span class="n">x_mark_enc</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">time_features</span><span class="p">]</span>
</span><span id="__span-16-3"><a href="#__codelineno-16-3" id="__codelineno-16-3" name="__codelineno-16-3"></a>    <span class="o">|</span>                        <span class="o">|</span>
</span><span id="__span-16-4"><a href="#__codelineno-16-4" id="__codelineno-16-4" name="__codelineno-16-4"></a>    <span class="n">v</span>                        <span class="n">v</span>
</span><span id="__span-16-5"><a href="#__codelineno-16-5" id="__codelineno-16-5" name="__codelineno-16-5"></a><span class="o">+-----------------------------------------------+</span>
</span><span id="__span-16-6"><a href="#__codelineno-16-6" id="__codelineno-16-6" name="__codelineno-16-6"></a><span class="o">|</span>           <span class="n">Model</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span><span class="n">调用</span>                  <span class="o">|</span>
</span><span id="__span-16-7"><a href="#__codelineno-16-7" id="__codelineno-16-7" name="__codelineno-16-7"></a><span class="o">|</span>      <span class="bp">self</span><span class="o">.</span><span class="n">enc_embedding</span><span class="p">(</span><span class="n">x_enc</span><span class="p">,</span> <span class="n">x_mark_enc</span><span class="p">)</span>    <span class="o">|</span>
</span><span id="__span-16-8"><a href="#__codelineno-16-8" id="__codelineno-16-8" name="__codelineno-16-8"></a><span class="o">+-----------------------------------------------+</span>
</span><span id="__span-16-9"><a href="#__codelineno-16-9" id="__codelineno-16-9" name="__codelineno-16-9"></a>            <span class="o">|</span>                <span class="o">|</span>
</span><span id="__span-16-10"><a href="#__codelineno-16-10" id="__codelineno-16-10" name="__codelineno-16-10"></a>            <span class="n">v</span>                <span class="n">v</span>
</span><span id="__span-16-11"><a href="#__codelineno-16-11" id="__codelineno-16-11" name="__codelineno-16-11"></a><span class="o">+------------------------+</span>  <span class="o">+---------------------------+</span>
</span><span id="__span-16-12"><a href="#__codelineno-16-12" id="__codelineno-16-12" name="__codelineno-16-12"></a><span class="o">|</span> <span class="n">TokenEmbedding</span> <span class="p">(</span><span class="n">值嵌入</span><span class="p">)</span> <span class="o">|</span>  <span class="o">|</span> <span class="n">TemporalEmbedding</span> <span class="p">(</span><span class="n">时间嵌入</span><span class="p">)</span><span class="o">|</span>
</span><span id="__span-16-13"><a href="#__codelineno-16-13" id="__codelineno-16-13" name="__codelineno-16-13"></a><span class="o">+------------------------+</span>  <span class="o">+---------------------------+</span>
</span><span id="__span-16-14"><a href="#__codelineno-16-14" id="__codelineno-16-14" name="__codelineno-16-14"></a><span class="o">|</span> <span class="n">输入</span><span class="p">:</span> <span class="n">x</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">]</span>      <span class="o">|</span>  <span class="o">|</span> <span class="n">输入</span><span class="p">:</span> <span class="n">x_mark</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">time_f</span><span class="p">]</span><span class="o">|</span>
</span><span id="__span-16-15"><a href="#__codelineno-16-15" id="__codelineno-16-15" name="__codelineno-16-15"></a><span class="o">|</span>                        <span class="o">|</span>  <span class="o">|</span>                           <span class="o">|</span>
</span><span id="__span-16-16"><a href="#__codelineno-16-16" id="__codelineno-16-16" name="__codelineno-16-16"></a><span class="o">|</span> <span class="n">操作</span><span class="p">:</span>                  <span class="o">|</span>  <span class="o">|</span> <span class="n">操作</span><span class="p">:</span>                     <span class="o">|</span>
</span><span id="__span-16-17"><a href="#__codelineno-16-17" id="__codelineno-16-17" name="__codelineno-16-17"></a><span class="o">|</span> <span class="mf">1.</span><span class="n">转置</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">L</span><span class="p">]</span>      <span class="o">|</span>  <span class="o">|</span> <span class="mf">1.</span><span class="n">转换为long类型</span>          <span class="o">|</span>
</span><span id="__span-16-18"><a href="#__codelineno-16-18" id="__codelineno-16-18" name="__codelineno-16-18"></a><span class="o">|</span> <span class="mf">2.1</span><span class="n">D卷积</span><span class="p">:</span> <span class="n">D</span> <span class="o">-&gt;</span> <span class="n">d_model</span> <span class="o">|</span>  <span class="o">|</span> <span class="mf">2.</span><span class="n">提取时间特征</span><span class="p">:</span>           <span class="o">|</span>
</span><span id="__span-16-19"><a href="#__codelineno-16-19" id="__codelineno-16-19" name="__codelineno-16-19"></a><span class="o">|</span> <span class="mf">3.</span><span class="n">转置回</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d_model</span><span class="p">]</span><span class="o">|</span>  <span class="o">|</span>   <span class="o">-</span> <span class="n">month_x</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">])</span>   <span class="o">|</span>
</span><span id="__span-16-20"><a href="#__codelineno-16-20" id="__codelineno-16-20" name="__codelineno-16-20"></a><span class="o">|</span>                        <span class="o">|</span>  <span class="o">|</span>   <span class="o">-</span> <span class="n">day_x</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">])</span>      <span class="o">|</span>
</span><span id="__span-16-21"><a href="#__codelineno-16-21" id="__codelineno-16-21" name="__codelineno-16-21"></a><span class="o">|</span> <span class="n">输出</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d_model</span><span class="p">]</span>  <span class="o">|</span>  <span class="o">|</span>   <span class="o">-</span> <span class="n">weekday_x</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,:,</span><span class="mi">2</span><span class="p">])</span>  <span class="o">|</span>
</span><span id="__span-16-22"><a href="#__codelineno-16-22" id="__codelineno-16-22" name="__codelineno-16-22"></a><span class="o">|</span>                        <span class="o">|</span>  <span class="o">|</span>   <span class="o">-</span> <span class="n">hour_x</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,:,</span><span class="mi">3</span><span class="p">])</span>     <span class="o">|</span>
</span><span id="__span-16-23"><a href="#__codelineno-16-23" id="__codelineno-16-23" name="__codelineno-16-23"></a><span class="o">+------------------------+</span>  <span class="o">|</span>   <span class="o">-</span> <span class="n">minute_x</span> <span class="p">(</span><span class="n">可选</span><span class="p">)</span>       <span class="o">|</span>
</span><span id="__span-16-24"><a href="#__codelineno-16-24" id="__codelineno-16-24" name="__codelineno-16-24"></a>            <span class="o">|</span>               <span class="o">|</span>                           <span class="o">|</span>
</span><span id="__span-16-25"><a href="#__codelineno-16-25" id="__codelineno-16-25" name="__codelineno-16-25"></a>            <span class="o">|</span>               <span class="o">|</span> <span class="mf">3.</span><span class="n">查表获取各时间特征的嵌入</span>  <span class="o">|</span>
</span><span id="__span-16-26"><a href="#__codelineno-16-26" id="__codelineno-16-26" name="__codelineno-16-26"></a>            <span class="o">|</span>               <span class="o">|</span> <span class="mf">4.</span><span class="n">将所有时间嵌入相加</span>       <span class="o">|</span>
</span><span id="__span-16-27"><a href="#__codelineno-16-27" id="__codelineno-16-27" name="__codelineno-16-27"></a>            <span class="o">|</span>               <span class="o">|</span>                           <span class="o">|</span>
</span><span id="__span-16-28"><a href="#__codelineno-16-28" id="__codelineno-16-28" name="__codelineno-16-28"></a>            <span class="o">|</span>               <span class="o">|</span> <span class="n">输出</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d_model</span><span class="p">]</span>     <span class="o">|</span>
</span><span id="__span-16-29"><a href="#__codelineno-16-29" id="__codelineno-16-29" name="__codelineno-16-29"></a>            <span class="o">|</span>               <span class="o">+---------------------------+</span>
</span><span id="__span-16-30"><a href="#__codelineno-16-30" id="__codelineno-16-30" name="__codelineno-16-30"></a>            <span class="o">|</span>                        <span class="o">|</span>
</span><span id="__span-16-31"><a href="#__codelineno-16-31" id="__codelineno-16-31" name="__codelineno-16-31"></a>            <span class="o">+------------+------------+</span>
</span><span id="__span-16-32"><a href="#__codelineno-16-32" id="__codelineno-16-32" name="__codelineno-16-32"></a>                         <span class="n">v</span>
</span><span id="__span-16-33"><a href="#__codelineno-16-33" id="__codelineno-16-33" name="__codelineno-16-33"></a>            <span class="o">+---------------------------+</span>
</span><span id="__span-16-34"><a href="#__codelineno-16-34" id="__codelineno-16-34" name="__codelineno-16-34"></a>            <span class="o">|</span> <span class="n">相加并应用Dropout</span>         <span class="o">|</span>
</span><span id="__span-16-35"><a href="#__codelineno-16-35" id="__codelineno-16-35" name="__codelineno-16-35"></a>            <span class="o">|</span> <span class="n">value_emb</span> <span class="o">+</span> <span class="n">temporal_emb</span> <span class="o">|</span>
</span><span id="__span-16-36"><a href="#__codelineno-16-36" id="__codelineno-16-36" name="__codelineno-16-36"></a>            <span class="o">+---------------------------+</span>
</span><span id="__span-16-37"><a href="#__codelineno-16-37" id="__codelineno-16-37" name="__codelineno-16-37"></a>                         <span class="o">|</span>
</span><span id="__span-16-38"><a href="#__codelineno-16-38" id="__codelineno-16-38" name="__codelineno-16-38"></a>                         <span class="n">v</span>
</span><span id="__span-16-39"><a href="#__codelineno-16-39" id="__codelineno-16-39" name="__codelineno-16-39"></a>                  <span class="n">输出</span><span class="p">:</span> <span class="n">enc_out</span>
</span><span id="__span-16-40"><a href="#__codelineno-16-40" id="__codelineno-16-40" name="__codelineno-16-40"></a>                 <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d_model</span><span class="p">]</span>
</span></code></pre></div>
<ol>
<li><strong>值嵌入 (TokenEmbedding)</strong>:</li>
<li>通过卷积操作将原始特征 [B, L, D] 映射到更高维度表示 [B, L, d_model]</li>
<li>使用循环填充的1D卷积捕获局部特征模式</li>
<li><strong>时间嵌入 (TemporalEmbedding)</strong>:</li>
<li>将时间标记 [B, L, time_features] 转换为 [B, L, d_model] 的嵌入向量</li>
<li>分别为月、日、星期、小时等时间特征查表获取嵌入，然后相加</li>
<li>时间嵌入帮助模型识别时间模式(季节性、每日/每周周期等)</li>
<li><strong>组合嵌入</strong>:</li>
<li>将值嵌入和时间嵌入相加，形成最终编码器输入 [B, L, d_model]</li>
<li>注意此版本不包含位置嵌入(DataEmbedding_wo_pos)</li>
</ol>
<p>这种多重嵌入方式使模型能同时利用时间序列的值信息和时间特征信息，为后续的注意力机制和时间序列建模提供丰富的上下文。</p>
<h2 id="_4">模型定义<a class="headerlink" href="#_4" title="Permanent link">¶</a></h2>
<h3 id="_5">编码器 解码器部分<a class="headerlink" href="#_5" title="Permanent link">¶</a></h3>
<pre class="mermaid"><code>classDiagram
    class Model {
        +DataEmbedding_wo_pos enc_embedding
        +DataEmbedding_wo_pos dec_embedding
        +Encoder encoder
        +Decoder decoder
        +series_decomp decomp
        +forward(x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)
    }

    class Encoder {
        +List~EncoderLayer~ layers
        +my_Layernorm norm_layer
        +forward(x, attn_mask)
    }

    class EncoderLayer {
        +AutoCorrelationLayer attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +Dropout dropout
        +activation
        +forward(x, attn_mask)
    }

    class AutoCorrelationLayer {
        +AutoCorrelation attention
        +Linear query_projection
        +Linear key_projection
        +Linear value_projection
        +Linear out_projection
        +forward(queries, keys, values, attn_mask)
    }

    class AutoCorrelation {
        +bool mask_flag
        +int factor
        +float scale
        +Dropout dropout
        +bool output_attention
        +time_delay_agg_training(values, corr)
        +time_delay_agg_inference(values, corr)
        +forward(queries, keys, values, attn_mask)
    }

    class Decoder {
        +List~DecoderLayer~ layers
        +my_Layernorm norm_layer
        +Linear projection
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }

    class DecoderLayer {
        +AutoCorrelationLayer self_attention
        +AutoCorrelationLayer cross_attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +Dropout dropout
        +activation
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }

    Model --&gt; Encoder
    Model --&gt; Decoder
    Encoder --&gt; EncoderLayer
    EncoderLayer --&gt; AutoCorrelationLayer
    EncoderLayer --&gt; Conv1d
    EncoderLayer --&gt; series_decomp
    AutoCorrelationLayer --&gt; AutoCorrelation
    Decoder --&gt; DecoderLayer
    DecoderLayer --&gt; AutoCorrelationLayer
    DecoderLayer --&gt; Conv1d
    DecoderLayer --&gt; series_decomp
</code></pre>
<h2 id="_6">训练过程，形状变换<a class="headerlink" href="#_6" title="Permanent link">¶</a></h2>
<p>（1）</p>
<p>代码：</p>
<p><img alt="image-20250319202142537" src="../images/image-20250319202142537.png"/> </p>
<p>逐字讲解：</p>
<p>model 训练从 exp_main.py的 train 函数开始，epoch 表示整个训练集迭代几次，for batchx、batchy、batch x mark、batch y mark 一个批次一个批次的训练，第一个 for 训练的 epoch 是我们自己可以设置的，第二个 for 训练的 iteration 迭代次数是 <code>数据集长度 ➗ batch size</code></p>
<p>接下来，调用 <code>self._predict</code> 方法进行预测，这里 predict 函数需要的参数 batchx、batchy、batch x mark、batch y mark 形状分别是 <code>batch_x = [32,36,7], batch_y = [32,42(18+24),7],batch_x_mark=[32,36,4],batch_y_mark = [32,42,4]</code></p>
<p>32 表示 一个 batch 样本的个数；</p>
<p>36 表示每个样本的时间步，也可以说是回溯窗口的大小，或者叫输入序列的长度</p>
<p>7 表示 illness 数据集的特征数</p>
<p>batchy 的 42 表示 18 的 label length，是取的 原始输入序列的 二分之一，这个在论文中有说</p>
<p><img alt="image-20250319202958076" src="../images/image-20250319202958076.png"/> </p>
<p>编码器的输入 是 <code>I times d</code> <span class="arithmatex">\(I\)</span> 表示 输入序列长度，在这里例子就是 36，<span class="arithmatex">\(d\)</span> 是特征数，这里的特征数，都去掉了时间戳，也就是 7</p>
<p>解码器的输入是 <code>二分之 I + O</code>，<code>二分之 I</code>表示 输入序列长度的一半，<code>O</code> 表示预测步长，也就是输出序列的长度</p>
<p>batch x mark，batch y mark 就是处理的时间戳特征了，包含一天的第几个小时，一个月的第几天，一周的第几天，一个月的第几天，就是我们之前讲过的 SegRNN，这里处理还涉及了 归一化 和中心化，不再重复啦。</p>
<hr/>
<p><strong>好了，接下来进入 预测部分，<mark>步进</mark>，也就是 predict 函数</strong> </p>
<p>首先，构造完整的解码器输入，具体的操作是，切片 batch y 中的预测步长，填充 0，并与 之前的 label length 进行拼接。也就是这两行代码</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-17-1"><a href="#__codelineno-17-1" id="__codelineno-17-1" name="__codelineno-17-1"></a><span class="c1"># decoder input </span>
</span><span id="__span-17-2"><a href="#__codelineno-17-2" id="__codelineno-17-2" name="__codelineno-17-2"></a><span class="c1"># 创建解码器输入的零张量部分，用于预测未来时间步</span>
</span><span id="__span-17-3"><a href="#__codelineno-17-3" id="__codelineno-17-3" name="__codelineno-17-3"></a><span class="c1"># batch_y[B, label_len+pred_len, D] -&gt; 切片 -&gt; [B, pred_len, D] -&gt; 创建相同形状全零张量 -&gt; dec_inp[B, pred_len, D]</span>
</span><span id="__span-17-4"><a href="#__codelineno-17-4" id="__codelineno-17-4" name="__codelineno-17-4"></a><span class="n">dec_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">batch_y</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">pred_len</span><span class="p">:,</span> <span class="p">:])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="__span-17-5"><a href="#__codelineno-17-5" id="__codelineno-17-5" name="__codelineno-17-5"></a>
</span><span id="__span-17-6"><a href="#__codelineno-17-6" id="__codelineno-17-6" name="__codelineno-17-6"></a><span class="c1"># 将历史数据(标签序列)与零张量连接，形成完整的解码器输入，并移动到指定设备</span>
</span><span id="__span-17-7"><a href="#__codelineno-17-7" id="__codelineno-17-7" name="__codelineno-17-7"></a><span class="c1"># [B, label_len, D] + [B, pred_len, D] -&gt; torch.cat沿维度1拼接 -&gt; [B, label_len+pred_len, D] -&gt; to(device) -&gt; 在GPU上的dec_inp</span>
</span><span id="__span-17-8"><a href="#__codelineno-17-8" id="__codelineno-17-8" name="__codelineno-17-8"></a><span class="n">dec_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">batch_y</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">label_len</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dec_inp</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></code></pre></div>
<p>构造的完整解码器的输入，形状还是 32,42,7。</p>
<p>（这里的代码并不是那么重要，所以就不粘贴了，占地方）接下来是一个内部方法 run model，类似 forward，但因为不是一个具体的模型，所以就叫 run model了，类内调用了这个函数，才会执行，这里没有调用，进入下一步，判断是否采用了自动精度训练，我也不明白，大概是模型加速把，总之是 false，执行 else。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-18-1"><a href="#__codelineno-18-1" id="__codelineno-18-1" name="__codelineno-18-1"></a><span class="k">else</span><span class="p">:</span>
</span><span id="__span-18-2"><a href="#__codelineno-18-2" id="__codelineno-18-2" name="__codelineno-18-2"></a>    <span class="c1"># 使用普通精度执行模型计算</span>
</span><span id="__span-18-3"><a href="#__codelineno-18-3" id="__codelineno-18-3" name="__codelineno-18-3"></a>    <span class="c1"># _run_model() -&gt; outputs[B, label_len+pred_len, D]</span>
</span><span id="__span-18-4"><a href="#__codelineno-18-4" id="__codelineno-18-4" name="__codelineno-18-4"></a>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">_run_model</span><span class="p">()</span>
</span></code></pre></div>
<p>调用的内部方法 <code>_run_model()</code>，步进，进入到 run model 内部。</p>
<p><img alt="image-20250319211512141" src="../images/image-20250319211512141.png"/></p>
<p>首先，这里的 self.model 是 <code>Exp_Basic</code>中的 <code>build_model</code> 定义来的，而且<code>exp_main</code> ， <code>Basic</code> 的子类 重写了 父类的方法，并通过字典，键是字符串，值的类，索引进行类的初始化，这个也是 SegRNN 中介绍过的。总之，这里的 <code>self.model</code> 是 <code>Autoformer</code> </p>
<p><img alt="image-20250319211736243" src="../images/image-20250319211736243.png"/></p>
<p><strong>点击步进，进入 Autoformer 的 forward 中。一个 batch 中样本的处理</strong> </p>
<hr/>
<h3 id="autoformer-forward">Autoformer  forward<a class="headerlink" href="#autoformer-forward" title="Permanent link">¶</a></h3>
<p>首先，这里Autoformer  forward 接收的参数：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><a href="#__codelineno-19-1" id="__codelineno-19-1" name="__codelineno-19-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_enc</span><span class="p">,</span> <span class="n">x_mark_enc</span><span class="p">,</span> <span class="n">x_dec</span><span class="p">,</span> <span class="n">x_mark_dec</span><span class="p">,</span>
</span><span id="__span-19-2"><a href="#__codelineno-19-2" id="__codelineno-19-2" name="__codelineno-19-2"></a>            <span class="n">enc_self_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="__span-19-3"><a href="#__codelineno-19-3" id="__codelineno-19-3" name="__codelineno-19-3"></a>            <span class="n">dec_self_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="__span-19-4"><a href="#__codelineno-19-4" id="__codelineno-19-4" name="__codelineno-19-4"></a>            <span class="n">dec_enc_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></code></pre></div>
<p>必须传入的参数 是  <code>x_enc, x_mark_enc, x_dec, x_mark_dec</code> 我们这里就是 <code>batch x，batch y，batch x mark，batch y mark</code>，且形状分别是 <code>[32,36,7]、[32,42,7]、[32,36,4]、[32,42,4]</code></p>
<p>可选参数是 Transformer 中的 3 个 mask，默认是 None。解释一下 Transformer 中的三个 mask 分别是什么：</p>
<blockquote>
<p>三个mask机制，分别指的是</p>
<ul>
<li>第一个 编码端输⼊ 由于padding字符的mask，为了⼀个batchsize中，所有长度不相同的样本，能构成⼀个矩阵，所以有pad字符，但是在后⾯进⾏inputencoder的⾃注意⼒计算时，pad字符不能影响计算结果，所以需要mask；</li>
<li>第⼆个mask是解码端的mask，这个mask是涉及到因果的mask，因为Transformer是⼀个⾃回归模型，在进⾏运算时，为了并⾏计算，我们是把inputs和outputs⼀起喂给模型的，inputs直接给模型没事，但是outputs在得到最后的输出时，不能借助未来信息，只能是当前时刻及其之前时刻的输出，所以需要⼀个mask机制，这个mask是⼀个上三角矩阵，保证在预测当前输出时，不会借助未来信息。</li>
<li>第三个mask，是编码器和解码器的交互注意⼒，编码器的输出作为key和value，解码器的输出作为query，因为⽬标序列 每个样本的长度是不⼀样的，同时原序列的样本长度也是不⼀样的，⽽且⼀对之间 长度也是不⼀样的，所以需要⼀个mask 将原序列中某个单词某个位置 跟 ⽬标序列中 某个位置 如果它们之间 有⼀个pad的话 说明是⽆效字符，得到这样的掩码矩阵。</li>
</ul>
<p>编码器以及 编码器和解码器的 mask 是为了保证长度的对齐，解码器的 mask 是为了在预测时 避免看到未来的信息</p>
</blockquote>
<p>回到 Autoformer 这里，看这个模型是怎么处理，输入数据和输出数据，以及模型的创新是怎么实现的。</p>
<p>首先，看到下面这几行代码。</p>
<p><img alt="image-20250319214129515" src="../images/image-20250319214129515.png"/></p>
<p>这几行代码的目的是为了解码器的输入的初始化，编码器阶段是用不到。</p>
<hr/>
<h3 id="_7">序列分解<a class="headerlink" href="#_7" title="Permanent link">¶</a></h3>
<p><strong>看论文 输入序列的趋势序列和季节趋势是怎么提取的。</strong> </p>
<p>本文将时间序列分解为 趋势序列和季节向量</p>
<p><img alt="image-20250319214344038" src="../images/image-20250319214344038.png"/></p>
<p>趋势向量反映了数据的长期变化趋势和季节趋势。并且论文中提到 对于未来序列进行分解是不现实的，因为未来的所有序列都是不知道的。因此，为了解决这个问题，原文提出了 序列分解模块，思想是 从预测的中间隐藏变量中 逐步提取 长期稳定的趋势 。</p>
<p>具体的做法，使用移动平均来平滑周期性波动来突出长期趋势。</p>
<p>文中也给出了公式：</p>
<p><img alt="image-20250319220946468" src="../images/image-20250319220946468.png"/></p>
<p>公式的解释：对于长度 为 L 的输入序列 X ，形状是 L×d，使用平均池化进行移动平均，并且使用填充操作保持序列长度不变。后面用一个 SeriesDecomp(X)来表示 上面的过程，简化一下记号。</p>
<p><strong>论文中的模型结构图也有画出这部分</strong></p>
<p><img alt="image-20250319221443646" src="../images/image-20250319221443646.png"/></p>
<p>首先 箭头指的地方时 直观地显示了 输入序列 趋势序列 和 季节序列是怎么来的。输入序列 的 趋势序列 是对 输入序列 去均值；季节信息，也就是周期波动信息是 输入序列 - 均值 ，这个周期波动信息 是围绕 0 进行波动的。基于对输入序列的分解的认识，对于解码器 趋势序列 和 季节序列的 初始化也是很有道理的。</p>
<p>图片的下半部分，是解码器的输入，显示了 预测序列 趋势序列和季节序列的初始化，其中趋势序列使用输入序列的均值进行初始化，季节波动信息用 0 来初始化</p>
<hr/>
<p><strong>接下来，看代码中，对预测序列 的 趋势序列 和 季节序列的提取。</strong></p>
<p>首先 有 历史数据 x_enc [B, L, D]的，预测和标签数据 x_dec [B, L+P, D]，接着进行时间序列分解 将历史序列分解为季节性和趋势两个成分</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-20-1"><a href="#__codelineno-20-1" id="__codelineno-20-1" name="__codelineno-20-1"></a><span class="n">seasonal_init</span><span class="p">,</span> <span class="n">trend_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp</span><span class="p">(</span><span class="n">x_enc</span><span class="p">)</span>
</span></code></pre></div>
<p>得到 趋势初始值：历史序列均值，季节性初始值：全零张量</p>
<p>基于 输入序列的 序列分解结果，构造 解码器的输入，具体来说：</p>
<ul>
<li>
<p>输出序列 趋势输入= 历史趋势末尾 + 趋势初始值</p>
</li>
<li>
<p>输出序列 季节性输入 = 历史季节性末尾 + 季节性初始值(零)</p>
</li>
</ul>
<p>也就是源码中的这几行：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-21-1"><a href="#__codelineno-21-1" id="__codelineno-21-1" name="__codelineno-21-1"></a><span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_enc</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-21-2"><a href="#__codelineno-21-2" id="__codelineno-21-2" name="__codelineno-21-2"></a><span class="n">zeros</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">x_dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_len</span><span class="p">,</span> <span class="n">x_dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">x_enc</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> 
</span><span id="__span-21-3"><a href="#__codelineno-21-3" id="__codelineno-21-3" name="__codelineno-21-3"></a><span class="n">trend_init</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">trend_init</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">label_len</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">mean</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-21-4"><a href="#__codelineno-21-4" id="__codelineno-21-4" name="__codelineno-21-4"></a><span class="n">seasonal_init</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">seasonal_init</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">label_len</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">zeros</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></div>
<p>这个模型的结构简单来说是 利用 **编码器**处理历史数据，**解码器**利用编码器输出和组装的初始输入生成预测，就是一个很标准的 Transformer 处理数据的架构。我们得到的最终输出是 趋势和季节性预测相加，因为有 label length，所以对于输出 是 提取末尾 pred_len 长度作为最终预测结果</p>
<p>Autoformer 的核心思想就是 将时间序列分解为不同频率成分并分别建模，再组合生成最终预测。</p>
<p><strong>先有个大体的印象，后面看到代码 详细的讲解。</strong></p>
<hr/>
<p>在进行后面的Encoder 和 Decoder之前，<strong>先看 趋势项 和 季节项 的具体实现方法。</strong> 有点复杂，但是一步步来。</p>
<p>▶️ 首先是调用 的   self.decomp</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-22-1"><a href="#__codelineno-22-1" id="__codelineno-22-1" name="__codelineno-22-1"></a><span class="n">seasonal_init</span><span class="p">,</span> <span class="n">trend_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp</span><span class="p">(</span><span class="n">x_enc</span><span class="p">)</span>
</span></code></pre></div>
<p>▶️ 而 self.decomp 又是 初始化 series_decomp 类</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-23-1"><a href="#__codelineno-23-1" id="__codelineno-23-1" name="__codelineno-23-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">decomp</span> <span class="o">=</span> <span class="n">series_decomp</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
</span></code></pre></div>
<p>▶️ 看到 series_decomp 类的定义</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-24-1"><a href="#__codelineno-24-1" id="__codelineno-24-1" name="__codelineno-24-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">series_decomp</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></code></pre></div>
<p>🟢 类的定义</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-25-1"><a href="#__codelineno-25-1" id="__codelineno-25-1" name="__codelineno-25-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">series_decomp</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-25-2"><a href="#__codelineno-25-2" id="__codelineno-25-2" name="__codelineno-25-2"></a><span class="w">    </span><span class="sd">"""</span>
</span><span id="__span-25-3"><a href="#__codelineno-25-3" id="__codelineno-25-3" name="__codelineno-25-3"></a><span class="sd">    Series decomposition block</span>
</span><span id="__span-25-4"><a href="#__codelineno-25-4" id="__codelineno-25-4" name="__codelineno-25-4"></a><span class="sd">    """</span>
</span><span id="__span-25-5"><a href="#__codelineno-25-5" id="__codelineno-25-5" name="__codelineno-25-5"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">):</span>
</span><span id="__span-25-6"><a href="#__codelineno-25-6" id="__codelineno-25-6" name="__codelineno-25-6"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">series_decomp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-25-7"><a href="#__codelineno-25-7" id="__codelineno-25-7" name="__codelineno-25-7"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moving_avg</span> <span class="o">=</span> <span class="n">moving_avg</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-25-8"><a href="#__codelineno-25-8" id="__codelineno-25-8" name="__codelineno-25-8"></a>
</span><span id="__span-25-9"><a href="#__codelineno-25-9" id="__codelineno-25-9" name="__codelineno-25-9"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-25-10"><a href="#__codelineno-25-10" id="__codelineno-25-10" name="__codelineno-25-10"></a>
</span><span id="__span-25-11"><a href="#__codelineno-25-11" id="__codelineno-25-11" name="__codelineno-25-11"></a>        <span class="c1"># 计算移动平均，提取序列趋势分量</span>
</span><span id="__span-25-12"><a href="#__codelineno-25-12" id="__codelineno-25-12" name="__codelineno-25-12"></a>        <span class="c1"># x 形状[B, L, D] -&gt; moving_mean形状[B, L, D]</span>
</span><span id="__span-25-13"><a href="#__codelineno-25-13" id="__codelineno-25-13" name="__codelineno-25-13"></a>        <span class="c1">#  moving_avg内部会进行填充，保证输出形状与输入相同</span>
</span><span id="__span-25-14"><a href="#__codelineno-25-14" id="__codelineno-25-14" name="__codelineno-25-14"></a>        <span class="n">moving_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-25-15"><a href="#__codelineno-25-15" id="__codelineno-25-15" name="__codelineno-25-15"></a>
</span><span id="__span-25-16"><a href="#__codelineno-25-16" id="__codelineno-25-16" name="__codelineno-25-16"></a>        <span class="c1"># 通过原始序列减去趋势分量，得到残差(季节性分量)，逐元素减法操作</span>
</span><span id="__span-25-17"><a href="#__codelineno-25-17" id="__codelineno-25-17" name="__codelineno-25-17"></a>        <span class="c1"># x形状[B, L, D] - moving_mean形状[B, L, D] -&gt; res形状[B, L, D]</span>
</span><span id="__span-25-18"><a href="#__codelineno-25-18" id="__codelineno-25-18" name="__codelineno-25-18"></a>        <span class="n">res</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">moving_mean</span>
</span><span id="__span-25-19"><a href="#__codelineno-25-19" id="__codelineno-25-19" name="__codelineno-25-19"></a>
</span><span id="__span-25-20"><a href="#__codelineno-25-20" id="__codelineno-25-20" name="__codelineno-25-20"></a>        <span class="c1"># 返回季节性分量和趋势分量，均保持原始形状[B, L, D]</span>
</span><span id="__span-25-21"><a href="#__codelineno-25-21" id="__codelineno-25-21" name="__codelineno-25-21"></a>        <span class="c1"># 第一个返回值res是季节性分量，第二个返回值moving_mean是趋势分量</span>
</span><span id="__span-25-22"><a href="#__codelineno-25-22" id="__codelineno-25-22" name="__codelineno-25-22"></a>        <span class="k">return</span> <span class="n">res</span><span class="p">,</span> <span class="n">moving_mean</span>
</span></code></pre></div>
<p>▶️ 类内 调用 <code>moving_avg</code> </p>
<p><img alt="image-20250317202431440" src="../images/image-20250317202431440.png"/></p>
<p>▶️ 看到 <code>moving_avg</code> 类的定义</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-26-1"><a href="#__codelineno-26-1" id="__codelineno-26-1" name="__codelineno-26-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">moving_avg</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></code></pre></div>
<p>🟢 <code>moving_avg</code> 定义</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-27-1"><a href="#__codelineno-27-1" id="__codelineno-27-1" name="__codelineno-27-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">moving_avg</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-27-2"><a href="#__codelineno-27-2" id="__codelineno-27-2" name="__codelineno-27-2"></a><span class="w">    </span><span class="sd">"""</span>
</span><span id="__span-27-3"><a href="#__codelineno-27-3" id="__codelineno-27-3" name="__codelineno-27-3"></a><span class="sd">    Moving average block to highlight the trend of time series</span>
</span><span id="__span-27-4"><a href="#__codelineno-27-4" id="__codelineno-27-4" name="__codelineno-27-4"></a><span class="sd">    """</span>
</span><span id="__span-27-5"><a href="#__codelineno-27-5" id="__codelineno-27-5" name="__codelineno-27-5"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
</span><span id="__span-27-6"><a href="#__codelineno-27-6" id="__codelineno-27-6" name="__codelineno-27-6"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-27-7"><a href="#__codelineno-27-7" id="__codelineno-27-7" name="__codelineno-27-7"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
</span><span id="__span-27-8"><a href="#__codelineno-27-8" id="__codelineno-27-8" name="__codelineno-27-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">avg</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-27-9"><a href="#__codelineno-27-9" id="__codelineno-27-9" name="__codelineno-27-9"></a>
</span><span id="__span-27-10"><a href="#__codelineno-27-10" id="__codelineno-27-10" name="__codelineno-27-10"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-27-11"><a href="#__codelineno-27-11" id="__codelineno-27-11" name="__codelineno-27-11"></a>        <span class="c1"># padding on the both ends of time series</span>
</span><span id="__span-27-12"><a href="#__codelineno-27-12" id="__codelineno-27-12" name="__codelineno-27-12"></a>
</span><span id="__span-27-13"><a href="#__codelineno-27-13" id="__codelineno-27-13" name="__codelineno-27-13"></a>        <span class="c1"># 提取第一个时间步并重复，用于前端填充</span>
</span><span id="__span-27-14"><a href="#__codelineno-27-14" id="__codelineno-27-14" name="__codelineno-27-14"></a>        <span class="c1">#  [B, L, D] -&gt; [B, 1, D] -&gt; [B, (kernel_size-1)//2, D]</span>
</span><span id="__span-27-15"><a href="#__codelineno-27-15" id="__codelineno-27-15" name="__codelineno-27-15"></a>        <span class="n">front</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
</span><span id="__span-27-16"><a href="#__codelineno-27-16" id="__codelineno-27-16" name="__codelineno-27-16"></a>
</span><span id="__span-27-17"><a href="#__codelineno-27-17" id="__codelineno-27-17" name="__codelineno-27-17"></a>        <span class="c1"># 提取最后一个时间步并重复，用于后端填充</span>
</span><span id="__span-27-18"><a href="#__codelineno-27-18" id="__codelineno-27-18" name="__codelineno-27-18"></a>        <span class="c1"># [B, L, D] -&gt; [B, 1, D] -&gt; [B, (kernel_size-1)//2, D]</span>
</span><span id="__span-27-19"><a href="#__codelineno-27-19" id="__codelineno-27-19" name="__codelineno-27-19"></a>        <span class="n">end</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-27-20"><a href="#__codelineno-27-20" id="__codelineno-27-20" name="__codelineno-27-20"></a>
</span><span id="__span-27-21"><a href="#__codelineno-27-21" id="__codelineno-27-21" name="__codelineno-27-21"></a>        <span class="c1"># 连接填充部分与原序列</span>
</span><span id="__span-27-22"><a href="#__codelineno-27-22" id="__codelineno-27-22" name="__codelineno-27-22"></a>        <span class="c1"># [B, (k-1)//2, D] + [B, L, D] + [B, (k-1)//2, D] -&gt; [B, L+(k-1), D]</span>
</span><span id="__span-27-23"><a href="#__codelineno-27-23" id="__codelineno-27-23" name="__codelineno-27-23"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">front</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-27-24"><a href="#__codelineno-27-24" id="__codelineno-27-24" name="__codelineno-27-24"></a>
</span><span id="__span-27-25"><a href="#__codelineno-27-25" id="__codelineno-27-25" name="__codelineno-27-25"></a>        <span class="c1"># 转置并应用一维平均池化</span>
</span><span id="__span-27-26"><a href="#__codelineno-27-26" id="__codelineno-27-26" name="__codelineno-27-26"></a>        <span class="c1"># [B, L+(k-1), D] -&gt; [B, D, L+(k-1)] -&gt; [B, D, L]</span>
</span><span id="__span-27-27"><a href="#__codelineno-27-27" id="__codelineno-27-27" name="__codelineno-27-27"></a>        <span class="c1"># 池化窗口大小为kernel_size，步长为1，输出长度为(L+(k-1)-k+1)=L （length + 2P - K + 1）</span>
</span><span id="__span-27-28"><a href="#__codelineno-27-28" id="__codelineno-27-28" name="__codelineno-27-28"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="__span-27-29"><a href="#__codelineno-27-29" id="__codelineno-27-29" name="__codelineno-27-29"></a>
</span><span id="__span-27-30"><a href="#__codelineno-27-30" id="__codelineno-27-30" name="__codelineno-27-30"></a>        <span class="c1"># 转置回原始维度顺序 [B, D, L] -&gt; [B, L, D]</span>
</span><span id="__span-27-31"><a href="#__codelineno-27-31" id="__codelineno-27-31" name="__codelineno-27-31"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-27-32"><a href="#__codelineno-27-32" id="__codelineno-27-32" name="__codelineno-27-32"></a>        <span class="k">return</span> <span class="n">x</span>
</span></code></pre></div>
<p>总结：就是 3 次调用：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-28-1"><a href="#__codelineno-28-1" id="__codelineno-28-1" name="__codelineno-28-1"></a><span class="n">seasonal_init</span><span class="p">,</span> <span class="n">trend_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp</span><span class="p">(</span><span class="n">x_enc</span><span class="p">)</span>
</span><span id="__span-28-2"><a href="#__codelineno-28-2" id="__codelineno-28-2" name="__codelineno-28-2"></a>
</span><span id="__span-28-3"><a href="#__codelineno-28-3" id="__codelineno-28-3" name="__codelineno-28-3"></a><span class="bp">self</span><span class="o">.</span><span class="n">decomp</span> <span class="o">=</span> <span class="n">series_decomp</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
</span><span id="__span-28-4"><a href="#__codelineno-28-4" id="__codelineno-28-4" name="__codelineno-28-4"></a>
</span><span id="__span-28-5"><a href="#__codelineno-28-5" id="__codelineno-28-5" name="__codelineno-28-5"></a><span class="k">class</span><span class="w"> </span><span class="nc">series_decomp</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-28-6"><a href="#__codelineno-28-6" id="__codelineno-28-6" name="__codelineno-28-6"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">):</span>
</span><span id="__span-28-7"><a href="#__codelineno-28-7" id="__codelineno-28-7" name="__codelineno-28-7"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">series_decomp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-28-8"><a href="#__codelineno-28-8" id="__codelineno-28-8" name="__codelineno-28-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moving_avg</span> <span class="o">=</span> <span class="n">moving_avg</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-28-9"><a href="#__codelineno-28-9" id="__codelineno-28-9" name="__codelineno-28-9"></a>
</span><span id="__span-28-10"><a href="#__codelineno-28-10" id="__codelineno-28-10" name="__codelineno-28-10"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-28-11"><a href="#__codelineno-28-11" id="__codelineno-28-11" name="__codelineno-28-11"></a>        <span class="n">moving_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-28-12"><a href="#__codelineno-28-12" id="__codelineno-28-12" name="__codelineno-28-12"></a>
</span><span id="__span-28-13"><a href="#__codelineno-28-13" id="__codelineno-28-13" name="__codelineno-28-13"></a><span class="k">class</span><span class="w"> </span><span class="nc">moving_avg</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-28-14"><a href="#__codelineno-28-14" id="__codelineno-28-14" name="__codelineno-28-14"></a>     <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-28-15"><a href="#__codelineno-28-15" id="__codelineno-28-15" name="__codelineno-28-15"></a>        <span class="n">front</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
</span><span id="__span-28-16"><a href="#__codelineno-28-16" id="__codelineno-28-16" name="__codelineno-28-16"></a>        <span class="n">end</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-28-17"><a href="#__codelineno-28-17" id="__codelineno-28-17" name="__codelineno-28-17"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">front</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-28-18"><a href="#__codelineno-28-18" id="__codelineno-28-18" name="__codelineno-28-18"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="__span-28-19"><a href="#__codelineno-28-19" id="__codelineno-28-19" name="__codelineno-28-19"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-28-20"><a href="#__codelineno-28-20" id="__codelineno-28-20" name="__codelineno-28-20"></a>        <span class="k">return</span> <span class="n">x</span>    
</span></code></pre></div>
<p>用一张图表示 Autoformer 序列分解的的过程，这个分解过程将原始序列 x_enc 分解为两个相同形状 [B,L,D] 的张量：趋势成分和季节性成分：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-29-1"><a href="#__codelineno-29-1" id="__codelineno-29-1" name="__codelineno-29-1"></a>                    输入: x_enc [B, L, D]
</span><span id="__span-29-2"><a href="#__codelineno-29-2" id="__codelineno-29-2" name="__codelineno-29-2"></a>                          |
</span><span id="__span-29-3"><a href="#__codelineno-29-3" id="__codelineno-29-3" name="__codelineno-29-3"></a>                          v
</span><span id="__span-29-4"><a href="#__codelineno-29-4" id="__codelineno-29-4" name="__codelineno-29-4"></a>            +---------------------------+
</span><span id="__span-29-5"><a href="#__codelineno-29-5" id="__codelineno-29-5" name="__codelineno-29-5"></a>            | Model.forward()           |
</span><span id="__span-29-6"><a href="#__codelineno-29-6" id="__codelineno-29-6" name="__codelineno-29-6"></a>            | 调用: self.decomp(x_enc)  |
</span><span id="__span-29-7"><a href="#__codelineno-29-7" id="__codelineno-29-7" name="__codelineno-29-7"></a>            +---------------------------+
</span><span id="__span-29-8"><a href="#__codelineno-29-8" id="__codelineno-29-8" name="__codelineno-29-8"></a>                          |
</span><span id="__span-29-9"><a href="#__codelineno-29-9" id="__codelineno-29-9" name="__codelineno-29-9"></a>                          v
</span><span id="__span-29-10"><a href="#__codelineno-29-10" id="__codelineno-29-10" name="__codelineno-29-10"></a>            +---------------------------+
</span><span id="__span-29-11"><a href="#__codelineno-29-11" id="__codelineno-29-11" name="__codelineno-29-11"></a>            | series_decomp(kernel_size)|
</span><span id="__span-29-12"><a href="#__codelineno-29-12" id="__codelineno-29-12" name="__codelineno-29-12"></a>            | self.decomp实例           |
</span><span id="__span-29-13"><a href="#__codelineno-29-13" id="__codelineno-29-13" name="__codelineno-29-13"></a>            +---------------------------+
</span><span id="__span-29-14"><a href="#__codelineno-29-14" id="__codelineno-29-14" name="__codelineno-29-14"></a>                          |
</span><span id="__span-29-15"><a href="#__codelineno-29-15" id="__codelineno-29-15" name="__codelineno-29-15"></a>                          v
</span><span id="__span-29-16"><a href="#__codelineno-29-16" id="__codelineno-29-16" name="__codelineno-29-16"></a>            +---------------------------+
</span><span id="__span-29-17"><a href="#__codelineno-29-17" id="__codelineno-29-17" name="__codelineno-29-17"></a>            | series_decomp.forward(x)  |
</span><span id="__span-29-18"><a href="#__codelineno-29-18" id="__codelineno-29-18" name="__codelineno-29-18"></a>            | 1. 调用移动平均计算趋势   |
</span><span id="__span-29-19"><a href="#__codelineno-29-19" id="__codelineno-29-19" name="__codelineno-29-19"></a>            | 2. 原序列减去趋势得到季节性|
</span><span id="__span-29-20"><a href="#__codelineno-29-20" id="__codelineno-29-20" name="__codelineno-29-20"></a>            +---------------------------+
</span><span id="__span-29-21"><a href="#__codelineno-29-21" id="__codelineno-29-21" name="__codelineno-29-21"></a>                          |
</span><span id="__span-29-22"><a href="#__codelineno-29-22" id="__codelineno-29-22" name="__codelineno-29-22"></a>                  +-------+-------+
</span><span id="__span-29-23"><a href="#__codelineno-29-23" id="__codelineno-29-23" name="__codelineno-29-23"></a>                  |               |
</span><span id="__span-29-24"><a href="#__codelineno-29-24" id="__codelineno-29-24" name="__codelineno-29-24"></a>                  v               v
</span><span id="__span-29-25"><a href="#__codelineno-29-25" id="__codelineno-29-25" name="__codelineno-29-25"></a>    +---------------------------+  +---------------------------+
</span><span id="__span-29-26"><a href="#__codelineno-29-26" id="__codelineno-29-26" name="__codelineno-29-26"></a>    | moving_avg.forward(x)     |  | 季节性计算                |
</span><span id="__span-29-27"><a href="#__codelineno-29-27" id="__codelineno-29-27" name="__codelineno-29-27"></a>    | 步骤:                     |  | res = x - moving_mean     |
</span><span id="__span-29-28"><a href="#__codelineno-29-28" id="__codelineno-29-28" name="__codelineno-29-28"></a>    | 1.前后填充序列           |  |                           |
</span><span id="__span-29-29"><a href="#__codelineno-29-29" id="__codelineno-29-29" name="__codelineno-29-29"></a>    | 2.应用平均池化           |  |                           |
</span><span id="__span-29-30"><a href="#__codelineno-29-30" id="__codelineno-29-30" name="__codelineno-29-30"></a>    | 3.返回趋势分量           |  |                           |
</span><span id="__span-29-31"><a href="#__codelineno-29-31" id="__codelineno-29-31" name="__codelineno-29-31"></a>    +---------------------------+  +---------------------------+
</span><span id="__span-29-32"><a href="#__codelineno-29-32" id="__codelineno-29-32" name="__codelineno-29-32"></a>                  |               |
</span><span id="__span-29-33"><a href="#__codelineno-29-33" id="__codelineno-29-33" name="__codelineno-29-33"></a>                  v               v
</span><span id="__span-29-34"><a href="#__codelineno-29-34" id="__codelineno-29-34" name="__codelineno-29-34"></a>             趋势分量        季节性分量
</span><span id="__span-29-35"><a href="#__codelineno-29-35" id="__codelineno-29-35" name="__codelineno-29-35"></a>          trend_init [B,L,D]  seasonal_init [B,L,D]
</span><span id="__span-29-36"><a href="#__codelineno-29-36" id="__codelineno-29-36" name="__codelineno-29-36"></a>                  |               |
</span><span id="__span-29-37"><a href="#__codelineno-29-37" id="__codelineno-29-37" name="__codelineno-29-37"></a>                  +       +       +
</span><span id="__span-29-38"><a href="#__codelineno-29-38" id="__codelineno-29-38" name="__codelineno-29-38"></a>                          |
</span><span id="__span-29-39"><a href="#__codelineno-29-39" id="__codelineno-29-39" name="__codelineno-29-39"></a>                          v
</span><span id="__span-29-40"><a href="#__codelineno-29-40" id="__codelineno-29-40" name="__codelineno-29-40"></a>                返回到Model.forward()
</span><span id="__span-29-41"><a href="#__codelineno-29-41" id="__codelineno-29-41" name="__codelineno-29-41"></a>                进行后续处理
</span></code></pre></div>
<p>讲图 逐字稿：</p>
<p>（1）<strong>Model.forward()</strong> 调用 self.decomp(x_enc)进行序列分解</p>
<p>（2）<strong>series_decomp.forward(x)</strong></p>
<blockquote>
<p>包含两个主要步骤:</p>
<ul>
<li>调用 self.moving_avg(x)计算移动平均，得到趋势分量</li>
<li>计算原序列与趋势分量的差值，得到季节性分量</li>
</ul>
</blockquote>
<p>（3）<strong>moving_avg.forward(x)</strong></p>
<blockquote>
<p>执行移动平均计算:</p>
<ul>
<li>通过重复首尾元素进行序列填充</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-30-1"><a href="#__codelineno-30-1" id="__codelineno-30-1" name="__codelineno-30-1"></a><span class="n">front</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
</span><span id="__span-30-2"><a href="#__codelineno-30-2" id="__codelineno-30-2" name="__codelineno-30-2"></a><span class="n">end</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-30-3"><a href="#__codelineno-30-3" id="__codelineno-30-3" name="__codelineno-30-3"></a> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">front</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></div>
<ul>
<li>应用一维平均池化操作</li>
</ul>
<div class="language-text highlight"><pre><span></span><code><span id="__span-31-1"><a href="#__codelineno-31-1" id="__codelineno-31-1" name="__codelineno-31-1"></a>x = self.avg(x.permute(0, 2, 1))
</span></code></pre></div>
<p><strong>说明 为什么填充，是为了 保证序列在平均池化后 长度不变</strong></p>
<ul>
<li>返回平滑后的趋势分量</li>
</ul>
<p>这部分的形状变化： </p>
<p><img alt="image-20250319224010427" src="../images/image-20250319224010427.png"/> </p>
</blockquote>
<p>现在开始 返回 <strong>moving_avg.forward(x)</strong> 是利用 1D 平均池化 得到 趋势序列，将结果返回给 <strong>series_decomp</strong> ，也就是这句代码 <code>moving_mean = self.moving_avg(x)</code>，得到趋势序列以后，永远序列减趋势序列 <code>res = x - moving_mean</code> ，得到季节分量，也就是周期性信息。具体的代码：</p>
<p><img alt="image-20250319224315222" src="../images/image-20250319224315222.png"/></p>
<p>最终 将结果 返回给 Autoformer forward 中的 seasonal_init, trend_init</p>
<p><img alt="image-20250319224700908" src="../images/image-20250319224700908.png"/></p>
<p>并且 用这两个 init 初始化 解码器的输入。</p>
<p>这里得注意一下，对于 标签序列，也就是 输入序列的趋势信息的提取用的是 1D平均池化，而对预测 predict length 的趋势信息初始化 就直接用的 输入序列的均值</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-32-1"><a href="#__codelineno-32-1" id="__codelineno-32-1" name="__codelineno-32-1"></a><span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_enc</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></code></pre></div>
<p>周期性趋势也是，label length 的季节趋势是 残差，也就是 原始序列 减去 趋势序列，而 predict length 的 季节趋势就是直接初始化为 0 了。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-33-1"><a href="#__codelineno-33-1" id="__codelineno-33-1" name="__codelineno-33-1"></a><span class="n">zeros</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">x_dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_len</span><span class="p">,</span> <span class="n">x_dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">x_enc</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> 
</span></code></pre></div>
<p>这里是 小小的区别，小小的注意。</p>
<p>好了 这部分，序列分解说完了，代码讲了，原文讲了，公式对应上了，图也说了。原文 <code>Series decomposition block</code>  就过啦</p>
<p><img alt="image-20250319225341745" src="../images/image-20250319225341745.png"/> </p>
<p>序列分解 over</p>
<hr/>
<h3 id="model-inputs">model inputs<a class="headerlink" href="#model-inputs" title="Permanent link">¶</a></h3>
<p>下面开始 模型的输入，先从论文开始讲解：</p>
<p><img alt="image-20250319225433558" src="../images/image-20250319225433558.png"/> </p>
<p>模型的输入部分，模型的输入包括编码器的输入和解码器的输入。具体来说，</p>
<p>编码器的输入是过去 <span class="arithmatex">\(I\)</span> 个时间步，文中给出的符号表示 <span class="arithmatex">\(\mathcal{X}^{I \times d}\)</span> ，<span class="arithmatex">\(I\)</span> 表示时间步长，<span class="arithmatex">\(d\)</span> 表示每个时间步的特征数。</p>
<p>解码器的输入包括了 季节性序列 和 趋势性序列，具体的符号表示分别是 <span class="arithmatex">\(\mathcal{X}_{des}\)</span>  和 <span class="arithmatex">\(\mathcal{X}_{det}\)</span>    形状是一样的：<span class="arithmatex">\((\frac{I}{2}+O)\)</span>  、<span class="arithmatex">\(\frac{I}{2}\)</span> 是 label length 的长度，取的是原始输入序列长度的 一半。O 是 预测步长 predict length。d 同样是每个时间步的特征数。接下来，我们来看公式是怎么表示的：</p>
<p><img alt="image-20250320085551155" src="../images/image-20250320085551155.png"/></p>
<p><span class="arithmatex">\(\mathcal{X}_{ens}、\mathcal{X}_{ent}\)</span>  分别表示 从 原始 输入序列 <span class="arithmatex">\(\mathcal{X}_{en}\)</span> 分解出的季节成分和趋势成分，截取出后半部分 <span class="arithmatex">\(\frac{I}{2}:I\)</span> 作为 label length，与长度为 predict  length 的时间步进行拼接，用 0 填充的长度为 predict length的向量记作 <span class="arithmatex">\(\mathcal{X}_0\)</span> ，用输入时间序列时间步均值填充的长度为 predict length 的向量记作 <span class="arithmatex">\(\mathcal{X}_{mean}\)</span></p>
<p>然后，<span class="arithmatex">\(\mathcal{X}_{ens}\)</span> 与 <span class="arithmatex">\(\mathcal{X}_0\)</span> 进行 concat 得到 解码器季节成分的初始值  <span class="arithmatex">\(\mathcal{X}_{des}\)</span></p>
<p>对应着的 <span class="arithmatex">\(\mathcal{X}_{ent}\)</span> 与 <span class="arithmatex">\(\mathcal{X}_{mean}\)</span> concat 得到解码器趋势成分的初始值 <span class="arithmatex">\(\mathcal{X}_{mean}\)</span></p>
<p><strong>再强调一下，这里所涉及的向量的记号和形状：</strong> </p>
<ul>
<li>编码器的输入是 过去 <span class="arithmatex">\(I\)</span> 个时间步，表示 <span class="arithmatex">\(\mathcal{X}^{I \times d}\)</span> ，<span class="arithmatex">\(I\)</span> 表示时间步长，<span class="arithmatex">\(d\)</span> 表示每个时间步的特征数。</li>
<li>解码器季节成分的输入是 <span class="arithmatex">\(\mathcal{X}_{des} ^{(\frac{I}{2}+O)\times d}\)</span> 、解码器趋势成分的输入是 <span class="arithmatex">\(\mathcal{X}_{det} ^{(\frac{I}{2}+O)\times d}\)</span> </li>
<li>涉及到的中间变量，<span class="arithmatex">\(\mathcal{X}^{\frac{I}{2} \times d}_{ens}\)</span> ，<span class="arithmatex">\(\mathcal{X}^{\frac{I}{2} \times d}_{ent}\)</span> 可以理解为标签序列的季节成分和趋势成分，就是从输入序列分解的季节成分和趋势成分中截取的后半段。</li>
<li>预测序列季节成分的初始值是 <span class="arithmatex">\(\mathcal{X}_0 ^{O \times d}\)</span> ，趋势成分初始值是 <span class="arithmatex">\(\mathcal{X}^{O \times d} _{Mean}\)</span></li>
</ul>
<p>也就是论文中模型结构图的：</p>
<p><img alt="image-20250320091412914" src="../images/image-20250320091412914.png"/></p>
<p>具体到代码，就是 autoformer forward的前 5 行，其中 self.decomp是我们刚刚仔细讲过的 序列分解模块 Series decomposition block：</p>
<p><img alt="image-20250320091921116" src="../images/image-20250320091921116.png"/></p>
<p>这部分代码比较好理解，就这样，以上部分完成了对原文 model inputs 部分的讲解，代码，论文，图，公式都讲了。</p>
<p><img alt="image-20250320092158923" src="../images/image-20250320092158923.png"/></p>
<p><img alt="image-20250320092214513" src="../images/image-20250320092214513.png"/></p>
<h3 id="encoder">Encoder<a class="headerlink" href="#encoder" title="Permanent link">¶</a></h3>
<p><strong>接下来进入论文的 Encoder 部分</strong> </p>
<p>会同样按照，论文、图、公式、代码一一对应的逻辑进行讲解</p>
<p><strong>首先，Autoformer 遵循原始 Transformer 的结构，</strong> </p>
<p><img alt="image-20250320092445230" src="../images/image-20250320092445230.png"/></p>
<p>编码器，解码器，编码器接收的 input 是 word embedding + positional embedding，然后通过自注意力机制和前馈神经网络。解码器接收的 输入是 output，预测部分，同样是 word embedding+positional embedding，然后分别经过解码器输入的 自注意力机制，以及和编码器输出 的 交叉注意力，最后经过 全连接层，得到最终的输出。</p>
<p>首先强调一下关于Transformer 为什么是注意力机制和全连接层的设计？</p>
<blockquote>
<p>首先，Transformer 在 NLP中接收的数据格式 是 [B,L,D]，batch size，一个 batch 中有多少个句子，一个句子中有几个词 L，每个词的嵌入D，也就是每个词用长度为多少的向量表示</p>
<p>最直观的讲解，就是 注意力机制进行 L 层面的交互，前馈神经网络进行 D 层面的交互。</p>
<p><strong>L 层面也就是注意到了 词与词之间的交互，D 层面就是词与词之间特征的交互</strong> </p>
<blockquote>
<p>在L层面（单词层面）进行交互，计算每个单词对其他单词的注意力权重，捕捉词与词之间的关系；</p>
<p>在D层面（即单词嵌入的特征层面）进行交互，对每个单词的嵌入向量进行非线性变换，捕捉词与词之间的特征交互 </p>
</blockquote>
<p><strong>对应到时间序列中</strong></p>
<p>1️⃣ 标准 <mark>输入</mark> 格式也是 BLD，具体的解释： </p>
<blockquote>
<p>B = 32 (批量大小，32个时间序列样本)
L = 36 (每个样本有36个时间步，如过去36天的数据)
D = 7 (每个时间步有7个特征，如对于股票可能包括开盘价、收盘价、最高价、最低价、交易量等)</p>
</blockquote>
<p>2️⃣ <mark>处理</mark>   注意力机制</p>
<p>编码器中，注意力在所有36个时间步之间建立连接
解码器中，注意力既在预测序列内部建立连接，也与编码器输出建立连接</p>
<p>时间步之间的建模 可以 发现股票价格每周五可能下跌，或者每月初可能上涨的模式</p>
<p>3️⃣ <mark>处理</mark>  前馈全连接层</p>
<p>处理每个时间步内7个特征之间的关系</p>
<p>例如，交易量与价格变动的关系，或开盘价与收盘价的关系</p>
</blockquote>
<p>诶，说起这个，关于用现实例子理解这些模型，</p>
<p><strong>首先，卷积是什么意思？</strong> </p>
<p>假如我们要认识一个人A，B 是 A 的直接朋友，形成了B 对 A 的第一次认识，B 就相当于卷积核了，那直接认识 A的肯定不止一个人，还有B1，B2，B3...等，每个人对形成了对 A 的第一次认识，父母认识 A更关注生活层面，学校中直接认识的 A 更关于为人处事部分，工作中直接认识的 A 更关于 A 的生产性。这里直接认识 A 的B1，B2，B3...就是每一层中 卷积核的个数。除了直接认识 A 的，还有通过直接认识 A 的人B 认识 A，这波人叫 C，那还有通过 C 认识 A 的，那 C 又认识 D，D 又通过 C 认识 A。除了别人认识 A，A 自己也有对自己的认识。</p>
<p><strong>Transformer是什么意思？</strong></p>
<p>除了刚刚说的 注意力机制和前馈全连接层的理解，还有 Encoder 、Decoder 、多头注意力机制的理解。</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> Encoder&amp;Decoder 的交互怎么理解？</li>
</ul>
<p>首先，整体上的这个图：</p>
<p><img alt="image-20250320095403477" src="../images/image-20250320095403477.png"/></p>
<p>编码器相当于甲方，解码器相当于乙方，甲方有需求，自己公司内部一级一级沟通，从最开始的想法最终形成方法交给最后一个人，这个人去和乙公司沟通，乙公司又有很多个部分，每个部分分别完成甲公司提出的方案的一部分，这一个过程中需要不断的与甲公司手拿最终方案的人不断沟通，最终乙公司完成方案。</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 多头注意力机制怎么理解</li>
</ul>
<p>对于 BLD 的序列，首先明白的是，那个维度分多头了，是 D 维度分成 num head维度和 head dim，其中 num head × head dim = embedding dim（D），相当于什么意思，一个人学知识（B =1），L 是要学的几本书，D 是每本书有几个章节，一般是一个老师教我们学一整本书，但多头注意力机制的意思是，一本书的几个章节，分开，比如第一个老师教第一章和第二章，第二个老师教第三章和第四章，最后两张第三个老师教，这样学习的时候，同样是一个学期，一个老师只需要关注两章的内容，对于课程节奏的把握知识理解的更透彻，效果会比一个老师教一整本书的内容要好一些。</p>
<p>B=3，就是班里的 3 个人，每个人这学期都要上这几本课，同样的 LD。</p>
<blockquote>
<p>最后一个 linear 层，应该是为了还原原始维度的。</p>
</blockquote>
<h4 id="_8">论文<a class="headerlink" href="#_8" title="Permanent link">¶</a></h4>
<p><strong>好了，扩展的远了，回到论文中Encoder 部分</strong> </p>
<p><img alt="image-20250320092249326" src="../images/image-20250320092249326.png"/></p>
<p>原文中说，首先编码器更专注季节部分的建模，编码器的输出包含过去的季节性信息，并将作为交叉信息帮助解码器细化预测结果，假设有 N 个编码器，则第 i 层编码器的总体方程可以表示为 <span class="arithmatex">\(\mathcal{X}_{en}^l = Encoder(\mathcal{X}_{en}^{l-1})\)</span> ，就是说 第 <span class="arithmatex">\(l\)</span> 层编码器接收 第 <span class="arithmatex">\(l-1\)</span> 层编码器的输出作为输入，具体的细节是原文的公式(3)</p>
<p><strong>下面对 公式 3 进行讲解</strong></p>
<p>首先，等号左边，下划线表示忽略掉季节成分，只关注季节成分。</p>
<p><span class="arithmatex">\(\mathcal{X}_{en}^l = \mathcal{S}_{en}^{l,2},l \in {1,...,N}\)</span>  表示 第 <span class="arithmatex">\(l\)</span> 层编码器的输出。</p>
<ul class="task-list">
<li>
<p>初始值，也就是编码器的输入是 <span class="arithmatex">\(\mathcal{X}_{en}^0\)</span> 是 输入时间序列的 <span class="arithmatex">\(\mathcal{X}_{en}\)</span> 的 word embedding</p>
</li>
<li class="task-list-item">
<p><label class="task-list-control"><input disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> <span class="arithmatex">\(\mathcal{S}_{en}^{l,i},i \in {1,2}\)</span> 表示 第 <span class="arithmatex">\(l\)</span>层中 第 i 个序列分解模块之后的季节性成分，然后公式中的 Auto-correlation 后面再说，这是本文的一个创新点。（ps，后面要重点看这个是什么意思。）</p>
</li>
</ul>
<blockquote>
<p>（我最开始看见这里的疑问，不用讲，忽略掉即可）先看公式等号的左边， <span class="arithmatex">\(\mathcal{S}_{en}^{l,1}\)</span> 首先，下标 <span class="arithmatex">\(en\)</span> 就是表示 编码器，<span class="arithmatex">\(l\)</span> 表示第几个编码器，那这个 <span class="arithmatex">\(1\)</span>是什么意思？</p>
</blockquote>
<p><strong>原文和公式说了，接下来来看代码，Encoder 是怎么实现的。</strong></p>
<h4 id="encoder-embedding_1">Encoder Embedding<a class="headerlink" href="#encoder-embedding_1" title="Permanent link">¶</a></h4>
<p><mark>首先，构造 Encoder 的输入</mark> ，编码器嵌入。</p>
<p><img alt="image-20250320143146767" src="../images/image-20250320143146767.png"/></p>
<p>具体怎么做的看autoformer 的 init 部分：</p>
<p><img alt="image-20250320143228968" src="../images/image-20250320143228968.png"/></p>
<p>看到这边调用的 <code>DataEmbedding_wo_pos</code> 这个类，其中具体地 valueEmbedding 和TemporaryEmbedding 又分别在 init 中显示调用了 <code>TokenEmbedding</code> 类和 <code>TemporalEmbedding</code> 类</p>
<p><img alt="image-20250320143256379" src="../images/image-20250320143256379.png"/></p>
<h5 id="_9">类图<a class="headerlink" href="#_9" title="Permanent link">¶</a></h5>
<p>嵌入部分的调用关系用流程图来表示：</p>
<pre class="mermaid"><code>classDiagram
    class DataEmbedding_wo_pos {
        +TokenEmbedding value_embedding
        +PositionalEmbedding position_embedding
        +TemporalEmbedding temporal_embedding
        +Dropout dropout
        +forward(x, x_mark)
    }

    class TokenEmbedding {
        +Conv1d tokenConv
        +forward(x)
    }

    class PositionalEmbedding {
        +Tensor pe
        +forward(x)
    }

    class TemporalEmbedding {
        +Embedding minute_embed
        +Embedding hour_embed
        +Embedding weekday_embed
        +Embedding day_embed
        +Embedding month_embed
        +forward(x)
    }

    class TimeFeatureEmbedding {
        +Linear embed
        +forward(x)
    }

    DataEmbedding_wo_pos --&gt; TokenEmbedding
    DataEmbedding_wo_pos --&gt; PositionalEmbedding
    DataEmbedding_wo_pos --&gt; TemporalEmbedding
    TemporalEmbedding --&gt; TimeFeatureEmbedding
</code></pre>
<p>首先，跟大家说这个图怎么画，首先在调试的过程中，看到调用相关的代码，就粘贴给 gpt，然后让 gpt 画。这个图就是 gpt 给我画的，它用的 mermaid ，生成代码，然后我粘贴到我的 markdown 文档中，我用的 markdown 编辑器是 Typora，可以解析 mermaid，用在线mermaid 也可以显示出图。直接搜 在线 mermaid。或者跟 gpt 说，用简单的流程图画，不用 mermaid，都能帮你把自己的代码理清楚。</p>
<p>mermaid 画出的类调用图，一个类用三行表示，第一行 类名、第二行，init 部分的定义、第三行类中方法的定义</p>
<p><strong>好了，现在开始讲图，</strong> </p>
<p>可以看到 <code>DataEmbedding_wo_pos</code> 类 的 init 分别调用了 <code>TokenEmbedding</code>类、<code>PositionalEmbedding</code>类和 <code>TemporalEmbedding</code>类，同时还定义了一个 dropout 层。</p>
<p>🔵 调用 <code>tokenEmbedding</code>类，init 部分是使用一个 <code>nn.Conv1d</code> 初始化了一个卷积层，传给 <code>self.tokenConv</code> ，后面在 这个类中的 forward 方法中用。</p>
<p><img alt="image-20250320144456851" src="../images/image-20250320144456851.png"/></p>
<p>通俗点说，这里的 tokenEmbedding 就是通过一个1D 卷积实现的，具体的形状变化注释中也给出了。</p>
<blockquote>
<p>怎么生成注释？</p>
<p>首先把代码粘给 gpt，然后，跟它说：<code>为每行代码 添加 两行注释，一行说明这行代码的目的，一行说明 形状的变化和操作 形状-&gt;操作-&gt;形状的格式，操作的格式类似 DecoderLayer.forward 显示出调用的什么类名.方法</code></p>
</blockquote>
<p>🔵 接下来看 位置编码 Positional Embedding，由于这里没有用，就不说了。</p>
<p>🔵 最后，时间戳编码，</p>
<p><img alt="image-20250320145727249" src="../images/image-20250320145727249.png"/></p>
<p>注意这里的时间戳编码是有一个判断的，经过调试，我们这里调用的是 <code>TimeFeatureEmbedding</code> 类。</p>
<p>也就是说什么意思，这个图画的有问题，不过意思也是对的，就不深究了。</p>
<p>接下来，我们跳到 <code>TimeFeatureEmbedding</code> 这个类的定义。</p>
<p><img alt="image-20250320150012753" src="../images/image-20250320150012753.png"/> </p>
<p>就是通过一个线性层，将 时间戳特征嵌入到指定维度。</p>
<p>首先，嵌入到指定维度是因为高维向量表示特征更精细。</p>
<p>其次，我们这里使用的是疾病数据集，是小时的，所以维度 4，表示的的是，小时-天，天-周，天-月，天-年。这一部分也说过很多次了，再说一次，加深印象。</p>
<p>具体来说输入的 <code>x_mark.shape=32,36,4 → nn.Linear → 32,36,512</code></p>
<p>接下来，总结一下这里的嵌入。首先 本文用到的所有嵌入都定义在了  <code>Embed.py</code>文件中</p>
<p><img alt="image-20250320150552474" src="../images/image-20250320150552474.png"/></p>
<p>而这个文件中，又定义了所有的嵌入类，又有 8 个。</p>
<blockquote>
<p>题外话，这个怎么看，是 vscode 的大纲视图，找出来，就能看到了</p>
<p><img alt="image-20250320150811010" src="../images/image-20250320150811010.png"/></p>
<p>大纲视图中，立方体表示定义的函数，小树杈的东西是类，类中有小立方体，是类中定义的函数，类中定义的函数，也就是小立方体中，折叠的部分是 使用这个函数或者类所需要的初始化参数。方括号+小立方体包括的部分是 类中调用的类的对象名，比如这里：</p>
<p><img alt="image-20250320151133019" src="../images/image-20250320151133019.png"/></p>
<p>以这个 TemporalEmbedding 类为例， 这个TemporalEmbedding 类中有两个方法方法，分别是 init 和 forward。</p>
<p>init 折叠的部分是 初始化这个类所需要的参数， forward 折叠的部分是调用这个时所需要的参数，其中 init 部分还实例化了 5 个对象，对象名分别是 mintue_embed、hour_embed、weekday_embed、day_embed、month_embed，但是这里具体实例化的哪个类。这里是没有显示的，得点进去自己看，可以看到这个对象其实都是实例化的Embed 这个类，很明显是一个自定义的，想看还得步进看具体实例化的哪个类。</p>
</blockquote>
<p>以上完成了 Encoder Input 的 Embedding 部分，分别进行了 token Embedding 和 TemporaryEmbedding来对历史时间步特征进行嵌入和时间特征进行嵌入。</p>
<p>汇总这里的维度形状变化：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-34-1"><a href="#__codelineno-34-1" id="__codelineno-34-1" name="__codelineno-34-1"></a><span class="c1"># x [B, L, D] → permute → [B, D, L] → 卷积 → [B, d_model, L] → transpose → [B, L, d_model]</span>
</span><span id="__span-34-2"><a href="#__codelineno-34-2" id="__codelineno-34-2" name="__codelineno-34-2"></a><span class="c1"># x_mark [B, L, d_inp] → 线性层变换(时间特征整体映射) → [B, L, d_model]</span>
</span><span id="__span-34-3"><a href="#__codelineno-34-3" id="__codelineno-34-3" name="__codelineno-34-3"></a><span class="c1"># [B, L, d_model] + [B, L, d_model] → [B, L, d_model]</span>
</span></code></pre></div>
<p><strong>接下来想给大家说的是， 1D 卷积怎么进行的 tokenEmbedding：</strong> </p>
<p>小小的点，小小的注意。</p>
<p>接收的标准输入是 BLD</p>
<ul>
<li>首先进行的是 permute，将想要嵌入的维度<code>D</code> 移到中间，然后进行 1D 卷积，嵌入到 <code>d_model</code>  （ <code>Embedding dim</code>），对应到 1D 卷积中，就是输入通道是 D，输出通道是 <code>d_model</code></li>
</ul>
<p><img alt="image-20250320152707837" src="../images/image-20250320152707837.png"/></p>
<ul>
<li>为什么这么做？因为卷积最开始主要用于图像，图像的标准格式是 BCHW，图像中的 HW 就表示图像的特征，只不过是用 2D的矩阵 表示的，而且这个 2D 矩阵保存了位置信息，不能随意展平。那此时，C 也就可以理解为每个像素的特征数。比如每个像素用彩色的 RGB 三个元素表示。</li>
<li>所以我们这里的时间序列中的 1D 卷积，也仿照图像中卷积的定义，每个时间步的特征数放到中间，表示输入通道数，然后将每个时间步的特征，映射到输出维度大小，这里表示为 <code>Embedding dim</code>，也就是 <code>d_model</code>。</li>
</ul>
<p>用一张图来表示，(这里其实很像 SegRNN 的视角转换)：</p>
<p><img alt="image-20250320154515642" src="../images/image-20250320154515642.png"/></p>
<p>而 时间戳特征的 nn.Linear就是直接对最后一个维度进行嵌入了</p>
<p><code>x_mark [B, L, d_inp] → 线性层变换(时间特征整体映射) → [B, L, d_model]</code></p>
<p>pytorch中常用的是维度变换函数 permute、transpose、view 都是直接写维度变换。</p>
<p>带着走一遍代码。</p>
<h4 id="encoder-forward">Encoder forward<a class="headerlink" href="#encoder-forward" title="Permanent link">¶</a></h4>
<p><strong>行了，嵌入讲完了，接下来，进入正式的 Encoder 的部分的数据流动。</strong></p>
<p>（终于）</p>
<p><img alt="image-20250320160444391" src="../images/image-20250320160444391.png"/></p>
<p>从 Autoformer forward 的 self.encoder 进入。</p>
<p>在步进之前看一眼怎么初始化的。非常复杂：</p>
<p><img alt="image-20250320160615516" src="../images/image-20250320160615516.png"/></p>
<p>来直接看图吧，具体怎么复杂。</p>
<h5 id="_10">类图<a class="headerlink" href="#_10" title="Permanent link">¶</a></h5>
<pre class="mermaid"><code>classDiagram
    class Model {
        +Encoder encoder
    }

    class Encoder {
        +List~EncoderLayer~ layers
        +my_Layernorm norm_layer
        +forward(x, attn_mask)
    }

    class EncoderLayer {
        +AutoCorrelationLayer attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +Dropout dropout
        +activation
        +forward(x, attn_mask)
    }

    class AutoCorrelationLayer {
        +AutoCorrelation attention
        +Linear query_projection
        +Linear key_projection
        +Linear value_projection
        +Linear out_projection
        +forward(queries, keys, values, attn_mask)
    }

    class AutoCorrelation {
        +bool mask_flag
        +int factor
        +float scale
        +Dropout dropout
        +bool output_attention
        +time_delay_agg_training(values, corr)
        +time_delay_agg_inference(values, corr)
        +forward(queries, keys, values, attn_mask)
    }

    Model --&gt; Encoder
    Encoder --&gt; EncoderLayer
    EncoderLayer --&gt; AutoCorrelationLayer
    EncoderLayer --&gt; Conv1d
    EncoderLayer --&gt; series_decomp
    AutoCorrelationLayer --&gt; AutoCorrelation
</code></pre>
<p>首先图中清楚的展示了，就图来说：</p>
<p><img alt="image-20250320161544932" src="../images/image-20250320161544932.png"/> </p>
<p>通过 Autoformer 定义的 model 中，定义了这个 Encoder 类，传给了 self.encoder 。</p>
<p>接着看这个 Encoder 类的定义，这个 Encoder 的初始化调用了EncoderLayer类，传给了 layers（就是EncoderLayer类的实例化对象）。</p>
<p>对应到代码：</p>
<p><img alt="image-20250320161505407" src="../images/image-20250320161505407.png"/></p>
<p>接着，Encoder 类调用了 EncoderLayer 类，那接下来就看 EncoderLayer 的定义：</p>
<p><img alt="image-20250320161704846" src="../images/image-20250320161704846.png"/></p>
<p>看图以及代码：</p>
<p><img alt="image-20250320162010963" src="../images/image-20250320162010963.png"/></p>
<p>（看代码）：红框是上面讲的，<code>Encoder</code> 的初始化需要调用 <code>EncoderLayer</code> 类</p>
<p>第一个红框表示，<code>EncoderLayer</code> 中的 <code>init</code> 初始化中又调用了 <code>AutoCorrelationLayer</code> 类，并且传入了  <code>EncoderLayer</code> 初始化过程中所需要的参数 ，去 <code>EncoderLayer</code> 这个类。</p>
<p><img alt="image-20250320162631428" src="../images/image-20250320162631428.png"/></p>
<p>结合 图 和 代码，这部分就可以理解了。</p>
<p><img alt="image-20250320163036643" src="../images/image-20250320163036643.png"/> </p>
<p>主要的复杂点就是初始化一个类的同时又需要初始化另一个类，初始化另一个类又需要初始化类。好好看看是可以理解的。</p>
<p>后面还有 AutoCorrelationLayer 的初始化，又要调用 AutoCorrelation 类</p>
<p><img alt="image-20250320163322154" src="../images/image-20250320163322154.png"/></p>
<p>理解的逻辑是一样的。</p>
<p>首先代码 Autoformer forward Encoder的初始化过程中，给出了每个类初始化所需要的传入参数</p>
<p><img alt="image-20250320163438704" src="../images/image-20250320163438704.png"/></p>
<p>而上面 mermaid 画的图，展示了每个类中 init 中具体调用的类和实例化的对象名。以及除了 init 方法外，还有类中可以调用的方法，比如 AutoCorrelation中，除了 init、forward 以外，还有 <code>time_delay_agg_training</code>  和 <code>time_delay_agg_inference</code> </p>
<p><img alt="image-20250320163849697" src="../images/image-20250320163849697.png"/></p>
<p>这部分调用关系希望我讲明白了，再用一张图说明一下：</p>
<p><img alt="img" src="https://i-blog.csdnimg.cn/direct/f98d7865ca134201a76ece97f66b7e51.png"/></p>
<p>就是说在我们在 Encoder forward 中传入到的x，会传入到 EncoderLayer 中的 forward 中进行处理，而   EncoderLayer forward 中又调用了 AutoCorrelation Layer 中的 forward，然后呢，AutoCorrelation Layer 中的 forward又调用了 AutoCorrelation的 forward，最后AutoCorrelation又调用了自己 AutoCorrelation time delay agg tranning 或者 inference。</p>
<p>（谁懂啊。当初逐步调试的时候，步进一个又步进一个，都找不到头 T_T），为什么说这部分调用不好理解，是因为按理说，forward 中有调用，去 init 中找。</p>
<p>具体来说，Autoformer 的 forward 中调用了 <code>self.encoder</code> ，在 init 中找到了</p>
<p><img alt="image-20250320164728075" src="../images/image-20250320164728075.png"/></p>
<p>那按住 command，跳进Encoder 的定义，不管是 init 还是 forward 中都没有显示的说明 attn_layer是调用的什么。</p>
<p><img alt="image-20250320164939255" src="../images/image-20250320164939255.png"/></p>
<p>所以最开始步进这里的时候，就很晕。其实这里所有的初始化以及调用都在最最开始的  Autoformer 的 self.encoder的初始化中给了。在后面定义的类中，按住 command 不能跳到类的定义。得从头开始。当然了，步进就不用管这些了。自己就跳来跳去了。</p>
<p><strong>好了 以上完成了 self.encoder的初始化，刚刚是对 编码器所接收的输入进行嵌入，这里是模型的定义</strong></p>
<p>基于以上的认识，接下来进入 forward 中，看数据的流动过程。</p>
<h4 id="encoderlayer">EncoderLayer<a class="headerlink" href="#encoderlayer" title="Permanent link">¶</a></h4>
<p>首先，从 ⬇️ 开始步进。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-35-1"><a href="#__codelineno-35-1" id="__codelineno-35-1" name="__codelineno-35-1"></a> <span class="n">enc_out</span><span class="p">,</span> <span class="n">attns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc_out</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">enc_self_mask</span><span class="p">)</span>
</span></code></pre></div>
<p>如我们所料，步进到了 Encoder 类的 forward：</p>
<p><img alt="image-20250320165637015" src="../images/image-20250320165637015.png"/></p>
<p>继续步进，一步步执行：</p>
<p><img alt="image-20250320165724095" src="../images/image-20250320165724095.png"/></p>
<p>条件判断执行 else，跳到</p>
<blockquote>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 每一步的现实意义在做什么</li>
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 形状变化、调用关系</li>
</ul>
</blockquote>
<p><img alt="image-20250320211248706" src="../images/image-20250320211248706.png"/></p>
<p>EncoderLayer 的 forward 中调用了自相关机制（自相关机制=AutoCorrelation Layer + AutoCorrelation），这是本文的创新点，后面再说。论文中也是说了后面再说，作者设计了这个模块就就是代替了原始 Transformer 的 self attention 的计算，你看这个命名，虽然调用的是AutoCorrelation Layer，但是变量命名时，仍然是 self.attention 也就可以理解为作者是改进了原始 Transformer 中的自注意力机制。</p>
<p>因此我们这里在调试 Encoder 的具体过程时，暂时不步进到 self.attention的具体的执行过程中。</p>
<p>单个 Encoder 的执行就是 执行一次 EncoderLayer，有几个 Encoder就执行几次 EncoderLayer。</p>
<p>现在说明这个 EncoderLayer 的执行过程：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-36-1"><a href="#__codelineno-36-1" id="__codelineno-36-1" name="__codelineno-36-1"></a>def forward(self, x, attn_mask=None)
</span></code></pre></div>
<p>首先 EncoderLayer 接收的输入是 x，mask 是可选参数。</p>
<p>输入<code>x</code>形状为[B, L, d_model]</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-37-1"><a href="#__codelineno-37-1" id="__codelineno-37-1" name="__codelineno-37-1"></a>       new_x, attn = self.attention(
</span><span id="__span-37-2"><a href="#__codelineno-37-2" id="__codelineno-37-2" name="__codelineno-37-2"></a>            x, x, x,
</span><span id="__span-37-3"><a href="#__codelineno-37-3" id="__codelineno-37-3" name="__codelineno-37-3"></a>            attn_mask=attn_mask
</span><span id="__span-37-4"><a href="#__codelineno-37-4" id="__codelineno-37-4" name="__codelineno-37-4"></a>        )
</span></code></pre></div>
<p>接下来进行 Encoder 部分的自注意力机制的计算，这里实际调用的是作者的创新模块，自相关层。后面不再区分叫法。</p>
<p>自注意力机制==接收==的参数 qkv都等于 x，因为是自注意力机制，形状是一样的 [B, L, d_model]。</p>
<p>这里的 <code>attn_mask=None</code></p>
<p>自注意力机制返回的 变量有 <code>new_x</code>, <code>attn</code>，形状分别是</p>
<p>new_x<code>[B, L, d_model]</code>和注意力权重attn<code>[B, n_heads, L, L]</code></p>
<p>这里的 new_x 意思是 有了对其他时间步权重的 x，attn 存的是两两时间步之间的注意力得分。（当然了，这是原始 自注意力机制 <span class="arithmatex">\(softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span> 的计算，论文实现的这里面是啥再说）</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-38-1"><a href="#__codelineno-38-1" id="__codelineno-38-1" name="__codelineno-38-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>
</span></code></pre></div>
<p>接下来，单个 Encoder 的输出 是 <code>dropout(new_x)</code> ，再经过残差连接。这里就是<span id="原始 Transformer 架构">原始 Transformer 架构</span>【见<a href="#附录">附录</a>】中的东西，也就是经过自注意力机制以后进行 残差连接和归一化。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-39-1"><a href="#__codelineno-39-1" id="__codelineno-39-1" name="__codelineno-39-1"></a><span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></code></pre></div>
<p>接下来，是序列分解</p>
<p><img alt="image-20250320214539531" src="../images/image-20250320214539531.png"/></p>
<p>序列分解的第一个返回值是季节成分，第二个返回值是使用移动平均得到趋势性成分</p>
<p>在 Encoder 中使用的序列分解时，不要趋势成分，只留下季节成分</p>
<p>接收的参数 x 是要分解的原始序列，形状是 [B, L, d_model]</p>
<p>返回的参数 x 是分解以后的季节成分，形状依然是 [B, L, d_model]</p>
<p><img alt="image-20250320215159566" src="../images/image-20250320215159566.png"/> </p>
<p>这部分代码就是原文公式 3 的第一行公式。</p>
<p>把代码贴到这里再体会一下：</p>
<p><img alt="image-20250320215324397" src="../images/image-20250320215324397.png"/></p>
<p>对第 <code>l-1</code> 层 Encoder 的输出进行自相关机制的计算，得到的输出与原始的 x 进行残差连接，最后对残差连接以后的输出进行趋势分解，同时只保留 季节成分，忽略掉趋势成分。</p>
<p><strong>接下来，进行公式 3 的第二行公式所对应的代码讲解：</strong> </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-40-1"><a href="#__codelineno-40-1" id="__codelineno-40-1" name="__codelineno-40-1"></a><span class="n">y</span> <span class="o">=</span> <span class="n">x</span>
</span></code></pre></div>
<p>下一句，新建 x 的副本，保存成 y，后面都对 x 的副本 也就是这个 y 操作，x 先放着不用，后面用于 FFN 以后的 残差连接。</p>
<p>那具体这里的 FFN 是怎么做的呢？</p>
<p>看到代码</p>
<p><img alt="image-20250320215804289" src="../images/image-20250320215804289.png"/></p>
<p>不看dropout 和激活层，因为这两个操作并不会改变输入张量的形状，接下来看到FFN 是由两个 1d 卷积也就是conv1d 实现的。</p>
<p><img alt="image-20250320215914272" src="../images/image-20250320215914272.png"/></p>
<p>接下来，再看 Encoder 的 init 部分，这两个 1d 卷积是怎么定义的，诶，就是一个先升维后降维的操作，就完成了 FFN。</p>
<p>就是说 self.conv1是将单个时间步嵌入维度 <code>d_model</code>升维到 <code>d_ff</code>；接下来 self.conv2就是将 <code>d_ff</code> 又恢复成原始形状 <code>d_model</code>。多说一句，好像没意义，其实有意义，第二次恢复的 <code>d_model</code> 其实是精细化的学习了 特征与特征之间的相关关系，并返回给了 <code>d_model</code> 中进行保存。</p>
<p>看到代码：</p>
<p><img alt="image-20250320220835532" src="../images/image-20250320220835532.png"/></p>
<p>关于这里，我有以下几点想说：</p>
<p>第一点，升维的操作，经过 conv1d 以后，相当于进行了全连接，也就是相当于nn.Linear的作用，那写过代码的都是到，nn.Linear以后进行激活，目的是为了增加模型的非线性表达能力，这点李沐的书上也提到过。这里就仿照着 conv1d 的输出也加上了激活函数。加的所有 dropout 都是为了防止过拟合，随机失活一些节点。</p>
<blockquote>
<p>标准前馈神经网络的设计模式：线性变换→非线性激活→线性变换；</p>
<p>在Transformer原始设计中，FFN部分表示为：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-41-1"><a href="#__codelineno-41-1" id="__codelineno-41-1" name="__codelineno-41-1"></a>FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
</span></code></pre></div>
<p>ReLU 激活函数和两个 nn.Linear</p>
</blockquote>
<p>第二点，前面已经说过，1d 卷积升维降维指的是通道维度，也就是 dim=1，所以这里进行 1d 卷积之前也是进行了 transpose。</p>
<p><del>第三点，为什么这里是 conv1d，而不是 nn.Linear，因为时间序列要保持时间步的前后关系，所以用 conv1d，类似滑动窗口，沿着时间步的前后顺序逐步滑动，每次移动 stride 个步长。</del> （我理解的不对）功能上就是一样的，只是数据的组织形式不同，不用刻意的区分。</p>
<p>第四点，这里的 d_ff 前馈神经网络的嵌入是 2048。</p>
<blockquote>
<p>d_ff通常设置为d_model的4倍，对于d_model=512的情况，d_ff就是2048。这也是Transformer原始论文中的设置。增大中间层维度可以提高模型的表达能力。</p>
</blockquote>
<p>第五点，self.conv2d的输出，在进行形状变换<code>transpose</code>的目的是为了恢复成时间序列的标准数据格式。</p>
<blockquote>
<p>将数据格式从卷积友好的<code>[B, C, L]</code>转回到Transformer架构通用的<code>[B, L, D]</code>格式</p>
</blockquote>
<p>再次直观地说明这一部分就是这样的：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-42-1"><a href="#__codelineno-42-1" id="__codelineno-42-1" name="__codelineno-42-1"></a># 第一步：转置使特征维度成为通道维度
</span><span id="__span-42-2"><a href="#__codelineno-42-2" id="__codelineno-42-2" name="__codelineno-42-2"></a>y.transpose(-1, 1)  # [B, L, d_model] -&gt; [B, d_model, L]
</span><span id="__span-42-3"><a href="#__codelineno-42-3" id="__codelineno-42-3" name="__codelineno-42-3"></a>
</span><span id="__span-42-4"><a href="#__codelineno-42-4" id="__codelineno-42-4" name="__codelineno-42-4"></a># 第二步：应用第一个卷积扩展通道维度
</span><span id="__span-42-5"><a href="#__codelineno-42-5" id="__codelineno-42-5" name="__codelineno-42-5"></a>self.conv1(...)  # [B, d_model, L] -&gt; [B, d_ff, L]
</span><span id="__span-42-6"><a href="#__codelineno-42-6" id="__codelineno-42-6" name="__codelineno-42-6"></a>
</span><span id="__span-42-7"><a href="#__codelineno-42-7" id="__codelineno-42-7" name="__codelineno-42-7"></a># 第三步：应用第二个卷积恢复原始通道维度
</span><span id="__span-42-8"><a href="#__codelineno-42-8" id="__codelineno-42-8" name="__codelineno-42-8"></a>self.conv2(...)  # [B, d_ff, L] -&gt; [B, d_model, L]
</span><span id="__span-42-9"><a href="#__codelineno-42-9" id="__codelineno-42-9" name="__codelineno-42-9"></a>
</span><span id="__span-42-10"><a href="#__codelineno-42-10" id="__codelineno-42-10" name="__codelineno-42-10"></a># 第四步：转置回原始序列格式
</span><span id="__span-42-11"><a href="#__codelineno-42-11" id="__codelineno-42-11" name="__codelineno-42-11"></a>(...).transpose(-1, 1)  # [B, d_model, L] -&gt; [B, L, d_model]
</span></code></pre></div>
<p><strong>后记：</strong> </p>
<blockquote>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> <span id="为什么Conv2之后没有进行激活函数的应用">产生的疑问</span>：<a href="#附录">为什么Conv2之后没有进行激活函数的应用</a> </li>
</ul>
<p>直观的理解我的疑问：</p>
<p><strong>现有设计 (第二层无激活)：</strong></p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-43-1"><a href="#__codelineno-43-1" id="__codelineno-43-1" name="__codelineno-43-1"></a>y = Conv1 -&gt; ReLU -&gt; Dropout -&gt; Conv2 -&gt; Dropout
</span></code></pre></div>
<p><strong>替代设计 (两层都有激活)</strong> </p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-44-1"><a href="#__codelineno-44-1" id="__codelineno-44-1" name="__codelineno-44-1"></a>y = Conv1 -&gt; ReLU -&gt; Dropout -&gt; Conv2 -&gt; ReLU -&gt; Dropout
</span></code></pre></div>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 为什么是 conv1d，而不是 nn.Linear</li>
</ul>
<p>没啥必要了，主要是在数据组织和计算效率方面有差异，Autoformer选择Conv1D而非Linear，是基于架构一致性、计算效率和未来扩展性（设置 kernel size 以后可以捕捉时间步之间的局部相关性）的考虑，功能上是完全一样的。</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 打印了单个 EncoderLayer的模型参数</li>
</ul>
<p><img alt="image-20250321163630773" src="../images/image-20250321163630773.png"/> </p>
</blockquote>
<p>最后还有一句，将原始特征x与变换后的特征y相加，再次应用序列分解提取季节性部分</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-45-1"><a href="#__codelineno-45-1" id="__codelineno-45-1" name="__codelineno-45-1"></a><span class="c1"># 将原始特征x与变换后的特征y相加，再次应用序列分解提取季节性部分</span>
</span><span id="__span-45-2"><a href="#__codelineno-45-2" id="__codelineno-45-2" name="__codelineno-45-2"></a><span class="c1"># (x + y)[B, L, d_model] -&gt; 序列分解 -&gt; 输出res[B, L, d_model]和未使用的趋势分量_[B, L, d_model]</span>
</span><span id="__span-45-3"><a href="#__codelineno-45-3" id="__codelineno-45-3" name="__codelineno-45-3"></a><span class="n">res</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>到了这一步，其实代码该讲的也讲完了，更重要的在于开始思考，为什么这样设计？</strong></p>
<ul>
<li>粘贴自 gpt 的答案贴在附录中了，解释得挺好的，还有和传统 Transformer 的对比</li>
<li>（我理解的部分自己复述）使得编码器在逐层编码的过程中，更关注高频的季节成分，所以每次传进编码器的是，抛弃了趋势成分的季节成分。</li>
</ul>
<p>编码器中关于季节成分、趋势成分的理解：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-46-1"><a href="#__codelineno-46-1" id="__codelineno-46-1" name="__codelineno-46-1"></a>输入 x [B,L,D] (混合季节性和趋势)
</span><span id="__span-46-2"><a href="#__codelineno-46-2" id="__codelineno-46-2" name="__codelineno-46-2"></a>  ↓
</span><span id="__span-46-3"><a href="#__codelineno-46-3" id="__codelineno-46-3" name="__codelineno-46-3"></a>自注意力处理
</span><span id="__span-46-4"><a href="#__codelineno-46-4" id="__codelineno-46-4" name="__codelineno-46-4"></a>  ↓
</span><span id="__span-46-5"><a href="#__codelineno-46-5" id="__codelineno-46-5" name="__codelineno-46-5"></a>残差连接 x + dropout(new_x) (仍含混合成分)
</span><span id="__span-46-6"><a href="#__codelineno-46-6" id="__codelineno-46-6" name="__codelineno-46-6"></a>  ↓
</span><span id="__span-46-7"><a href="#__codelineno-46-7" id="__codelineno-46-7" name="__codelineno-46-7"></a>序列分解1 
</span><span id="__span-46-8"><a href="#__codelineno-46-8" id="__codelineno-46-8" name="__codelineno-46-8"></a>  ↓          ↘
</span><span id="__span-46-9"><a href="#__codelineno-46-9" id="__codelineno-46-9" name="__codelineno-46-9"></a>季节性分量x    趋势分量 (丢弃)
</span><span id="__span-46-10"><a href="#__codelineno-46-10" id="__codelineno-46-10" name="__codelineno-46-10"></a>  ↓
</span><span id="__span-46-11"><a href="#__codelineno-46-11" id="__codelineno-46-11" name="__codelineno-46-11"></a>前馈网络处理 (Conv1D → ReLU → Conv1D)
</span><span id="__span-46-12"><a href="#__codelineno-46-12" id="__codelineno-46-12" name="__codelineno-46-12"></a>  ↓
</span><span id="__span-46-13"><a href="#__codelineno-46-13" id="__codelineno-46-13" name="__codelineno-46-13"></a>前馈输出 y (季节性特征)
</span><span id="__span-46-14"><a href="#__codelineno-46-14" id="__codelineno-46-14" name="__codelineno-46-14"></a>  ↓
</span><span id="__span-46-15"><a href="#__codelineno-46-15" id="__codelineno-46-15" name="__codelineno-46-15"></a>季节性分量x + 前馈输出y (混合季节性)
</span><span id="__span-46-16"><a href="#__codelineno-46-16" id="__codelineno-46-16" name="__codelineno-46-16"></a>  ↓
</span><span id="__span-46-17"><a href="#__codelineno-46-17" id="__codelineno-46-17" name="__codelineno-46-17"></a>序列分解2
</span><span id="__span-46-18"><a href="#__codelineno-46-18" id="__codelineno-46-18" name="__codelineno-46-18"></a>  ↓          ↘
</span><span id="__span-46-19"><a href="#__codelineno-46-19" id="__codelineno-46-19" name="__codelineno-46-19"></a>季节性分量res  趋势分量 (丢弃)
</span><span id="__span-46-20"><a href="#__codelineno-46-20" id="__codelineno-46-20" name="__codelineno-46-20"></a>  ↓
</span><span id="__span-46-21"><a href="#__codelineno-46-21" id="__codelineno-46-21" name="__codelineno-46-21"></a>输出到下一层 res (纯季节性)
</span></code></pre></div>
<p>最后记下两句话把：</p>
<ul>
<li>频率域分析：从频率角度看，趋势对应低频成分，季节性对应高频成分，分开处理有助于提取各自的特点。</li>
<li>编码器：专注于捕获周期性和季节性模式（高频成分）</li>
<li>虽然在编码器层内部丢弃了趋势信息，但Autoformer并没有完全忽略趋势。</li>
<li>通过在每一层都丢弃趋势成分，模型能够在多层堆叠过程中持续关注季节性变化，而不被趋势变化干扰。</li>
<li>解码器中单独累积趋势，避免趋势预测对季节性预测的干扰</li>
<li>渐进式分解架构</li>
<li>每层都应用序列分解，逐步提炼季节性特征，多层堆叠可以捕获不同尺度的季节性模式</li>
</ul>
<p>行了，上面关于编码器的部分讲的差不多了</p>
<p><strong>公式</strong> </p>
<p><img alt="image-20250321103256921" src="../images/image-20250321103256921.png"/></p>
<p>最开始上标的 1 和 2 不明白，现在也明白了，可以理解为 1 是自注意力机制的输出，只保留的季节成分；2 是经过前馈网的输出，只保留了季节成分。</p>
<p>这些自注意力机制 和 前馈网 后面都有残差连接。这中间还有 dropout、激活等。</p>
<p><strong>代码</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-47-1"><a href="#__codelineno-47-1" id="__codelineno-47-1" name="__codelineno-47-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="__span-47-2"><a href="#__codelineno-47-2" id="__codelineno-47-2" name="__codelineno-47-2"></a>
</span><span id="__span-47-3"><a href="#__codelineno-47-3" id="__codelineno-47-3" name="__codelineno-47-3"></a>    <span class="n">new_x</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
</span><span id="__span-47-4"><a href="#__codelineno-47-4" id="__codelineno-47-4" name="__codelineno-47-4"></a>        <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
</span><span id="__span-47-5"><a href="#__codelineno-47-5" id="__codelineno-47-5" name="__codelineno-47-5"></a>        <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span>
</span><span id="__span-47-6"><a href="#__codelineno-47-6" id="__codelineno-47-6" name="__codelineno-47-6"></a>    <span class="p">)</span>
</span><span id="__span-47-7"><a href="#__codelineno-47-7" id="__codelineno-47-7" name="__codelineno-47-7"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>
</span><span id="__span-47-8"><a href="#__codelineno-47-8" id="__codelineno-47-8" name="__codelineno-47-8"></a>    <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-47-9"><a href="#__codelineno-47-9" id="__codelineno-47-9" name="__codelineno-47-9"></a>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="__span-47-10"><a href="#__codelineno-47-10" id="__codelineno-47-10" name="__codelineno-47-10"></a>
</span><span id="__span-47-11"><a href="#__codelineno-47-11" id="__codelineno-47-11" name="__codelineno-47-11"></a>
</span><span id="__span-47-12"><a href="#__codelineno-47-12" id="__codelineno-47-12" name="__codelineno-47-12"></a>    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
</span><span id="__span-47-13"><a href="#__codelineno-47-13" id="__codelineno-47-13" name="__codelineno-47-13"></a>    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="__span-47-14"><a href="#__codelineno-47-14" id="__codelineno-47-14" name="__codelineno-47-14"></a>    <span class="n">res</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</span><span id="__span-47-15"><a href="#__codelineno-47-15" id="__codelineno-47-15" name="__codelineno-47-15"></a>
</span><span id="__span-47-16"><a href="#__codelineno-47-16" id="__codelineno-47-16" name="__codelineno-47-16"></a>    <span class="k">return</span> <span class="n">res</span><span class="p">,</span> <span class="n">attn</span>
</span></code></pre></div>
<p>再来看一眼**文中模型结构图**：</p>
<p><img alt="image-20250321103610423" src="../images/image-20250321103610423.png"/> </p>
<p>可以看到 都是一一对应的。</p>
<p>对，还有一个，原文堆叠 Encoder 的层数=2。</p>
<hr/>
<h3 id="decoder">Decoder<a class="headerlink" href="#decoder" title="Permanent link">¶</a></h3>
<p><strong>下面进入解码器部分，禁用所有断点，只保留调用解码的部分：</strong> </p>
<p><img alt="image-20250321104519139" src="../images/image-20250321104519139.png"/></p>
<p>启动命令：</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-48-1"><a href="#__codelineno-48-1" id="__codelineno-48-1" name="__codelineno-48-1"></a><span class="o">(</span>base<span class="o">)</span><span class="w"> </span>$<span class="w"> </span>conda<span class="w"> </span>env<span class="w"> </span>list
</span><span id="__span-48-2"><a href="#__codelineno-48-2" id="__codelineno-48-2" name="__codelineno-48-2"></a>
</span><span id="__span-48-3"><a href="#__codelineno-48-3" id="__codelineno-48-3" name="__codelineno-48-3"></a><span class="c1"># conda environments:</span>
</span><span id="__span-48-4"><a href="#__codelineno-48-4" id="__codelineno-48-4" name="__codelineno-48-4"></a><span class="c1">#</span>
</span><span id="__span-48-5"><a href="#__codelineno-48-5" id="__codelineno-48-5" name="__codelineno-48-5"></a>base<span class="w">                 </span>*<span class="w"> </span>/home/student2023/xiehr2023/miniconda3
</span><span id="__span-48-6"><a href="#__codelineno-48-6" id="__codelineno-48-6" name="__codelineno-48-6"></a>Autoformer<span class="w">             </span>/home/student2023/xiehr2023/miniconda3/envs/Autoformer
</span><span id="__span-48-7"><a href="#__codelineno-48-7" id="__codelineno-48-7" name="__codelineno-48-7"></a>SegRNN<span class="w">                 </span>/home/student2023/xiehr2023/miniconda3/envs/SegRNN
</span><span id="__span-48-8"><a href="#__codelineno-48-8" id="__codelineno-48-8" name="__codelineno-48-8"></a>timesNet<span class="w">               </span>/home/student2023/xiehr2023/miniconda3/envs/timesNet
</span><span id="__span-48-9"><a href="#__codelineno-48-9" id="__codelineno-48-9" name="__codelineno-48-9"></a>
</span><span id="__span-48-10"><a href="#__codelineno-48-10" id="__codelineno-48-10" name="__codelineno-48-10"></a><span class="o">(</span>base<span class="o">)</span><span class="w"> </span>$<span class="w"> </span>conda<span class="w"> </span>activate<span class="w"> </span>Autoformer
</span><span id="__span-48-11"><a href="#__codelineno-48-11" id="__codelineno-48-11" name="__codelineno-48-11"></a><span class="o">(</span>Autoformer<span class="o">)</span><span class="w"> </span>$<span class="w"> </span>sh<span class="w"> </span>scripts/ILI_script/Autoformer.sh
</span></code></pre></div>
<p>在步进代码之前，先看mermaid 画的类图，看清解码器的调用流程：  </p>
<h4 id="_11">类图<a class="headerlink" href="#_11" title="Permanent link">¶</a></h4>
<pre class="mermaid"><code>
classDiagram
    class Model {
        +int seq_len
        +int label_len
        +int pred_len
        +bool output_attention
        +series_decomp decomp
        +DataEmbedding_wo_pos enc_embedding
        +DataEmbedding_wo_pos dec_embedding
        +Encoder encoder
        +Decoder decoder
        +forward(x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)
    }

    class series_decomp {
        +moving_avg moving_avg
        +forward(x) res, moving_mean
    }
    class moving_avg {
        +int kernel_size
        +AvgPool1d avg
        +forward(x)
    }

    class DataEmbedding_wo_pos {
        +TokenEmbedding value_embedding
        +PositionalEmbedding position_embedding
        +TemporalEmbedding temporal_embedding
        +Dropout dropout
        +forward(x, x_mark)
    }

    class Encoder {
        +List~EncoderLayer~ layers
        +my_Layernorm norm_layer
        +forward(x, attn_mask)
    }

    class EncoderLayer {
        +AutoCorrelationLayer attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +Dropout dropout
        +activation
        +forward(x, attn_mask)
    }

    class AutoCorrelationLayer {
        +AutoCorrelation attention
        +Linear query_projection
        +Linear key_projection
        +Linear value_projection
        +Linear out_projection
        +forward(queries, keys, values, attn_mask)
    }

    class AutoCorrelation {
        +bool mask_flag
        +int factor
        +float scale
        +Dropout dropout
        +bool output_attention
        +time_delay_agg_training(values, corr)
        +time_delay_agg_inference(values, corr)
        +forward(queries, keys, values, attn_mask)
    }

    class Decoder {
        +List~DecoderLayer~ layers
        +my_Layernorm norm_layer
        +Linear projection
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }

    class DecoderLayer {
        +AutoCorrelationLayer self_attention
        +AutoCorrelationLayer cross_attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +series_decomp decomp3
        +Dropout dropout
        +activation
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }

    Model --&gt; series_decomp
    Model --&gt; DataEmbedding_wo_pos
    Model --&gt; Encoder
    Model --&gt; Decoder
    Encoder --&gt; EncoderLayer
    EncoderLayer --&gt; AutoCorrelationLayer
    EncoderLayer --&gt; Conv1d
    EncoderLayer --&gt; series_decomp
    AutoCorrelationLayer --&gt; AutoCorrelation
    Decoder --&gt; DecoderLayer
    DecoderLayer --&gt; AutoCorrelationLayer
    DecoderLayer --&gt; Conv1d
    DecoderLayer --&gt; series_decomp   
    series_decomp --&gt; moving_avg
    moving_avg --&gt; AvgPool1d
</code></pre>
<p>下面开始讲图。</p>
<p><strong>Mode init</strong> </p>
<p>首先，从第一框开始，model 就是指的 Autoformer，首先 Autoformer 的 init 部分有 输入序列长度，sequence length；标签序列的长度 label length，标签序列用于指导预测，本文截取输入序列长度的一半；预测序列长度 predict length，因为本文设置的标签序列，所以解码器实际的输出时间步是 label length+predict length，所以预测的部分，还要把 label length 截掉。attention 暂时不看，我也不明白有啥用，大概就是一个初始化吧。</p>
<p>然后就是序列分解函数，分解季节成分和趋势成分。趋势成分是低频成分，表示时间序列长期稳定的趋势，用的是移动平均，在代码实现中，具体用的是 1D 平均池化。季节成分是高频成分，表示周期性信息，分解中的做法是 原始序列减去季节成分。res = 原始序列 <code>x</code> - 趋势成分 <code>move_average</code></p>
<p>下面是两个嵌入层，目的是进行 token Embedding 和 temporal Embedding，将原始时间步特征嵌入到指定维度，更精细的表示特征。举个例子：时间步特征[32,36,7] → [32,36,512]   [32,42,7] →[32,42,512] ；（时间步）时间特征[32,36,4] → [32,36,512]   [32,42,4] →[32,42,512] </p>
<p>接下来就是 Encoder 和 Decoder，其中 Encoder 会堆叠 EncoderLayer，Decoder 堆叠 DecoderLayer，</p>
<p>其中 EncoderLayer 堆叠了 2 层，结合图和代码，看单层 EncoderLayer 和 DecoderLayer 的 相同与不同。</p>
<p>（1）Decoder 比 Encoder 多了一个线性层 <code>nn.Linear</code> （遵循了标准 Transformer 架构）</p>
<p>（2）DecoderLayer 调用了两次 自相关层，这是因为 Decoder 中进行两种注意力机制的运算，自注意力机制和交叉注意机制，并且再次说明，自注意力层、交叉注意力层、FFN 以后，会进行残差连接。</p>
<p>（3）DecoderLayer 进行了三次序列分解，猜测，分别是在自注意机制、交叉注意力机制以及 FFN 以后，分别进行分解</p>
<p>（4）同时在 DecoderLayer 中，应该尤其注意 趋势成分是怎么处理的，因为 EncoderLayer 中，忽略掉了趋势成分。</p>
<p>（5）接下来共同调用的 自相关层，单个自相关层、以及序列分解是完全一样的。</p>
<p><img alt="image-20250321160449357" src="../images/image-20250321160449357.png"/></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-49-1"><a href="#__codelineno-49-1" id="__codelineno-49-1" name="__codelineno-49-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span>
</span><span id="__span-49-2"><a href="#__codelineno-49-2" id="__codelineno-49-2" name="__codelineno-49-2"></a>    <span class="p">[</span>
</span><span id="__span-49-3"><a href="#__codelineno-49-3" id="__codelineno-49-3" name="__codelineno-49-3"></a>        <span class="n">EncoderLayer</span><span class="p">(</span>
</span><span id="__span-49-4"><a href="#__codelineno-49-4" id="__codelineno-49-4" name="__codelineno-49-4"></a>            <span class="n">AutoCorrelationLayer</span><span class="p">(</span>
</span><span id="__span-49-5"><a href="#__codelineno-49-5" id="__codelineno-49-5" name="__codelineno-49-5"></a>                <span class="n">AutoCorrelation</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">factor</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
</span><span id="__span-49-6"><a href="#__codelineno-49-6" id="__codelineno-49-6" name="__codelineno-49-6"></a>                                <span class="n">output_attention</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">output_attention</span><span class="p">),</span>
</span><span id="__span-49-7"><a href="#__codelineno-49-7" id="__codelineno-49-7" name="__codelineno-49-7"></a>                <span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span>
</span><span id="__span-49-8"><a href="#__codelineno-49-8" id="__codelineno-49-8" name="__codelineno-49-8"></a>            <span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
</span><span id="__span-49-9"><a href="#__codelineno-49-9" id="__codelineno-49-9" name="__codelineno-49-9"></a>            <span class="n">configs</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span>
</span><span id="__span-49-10"><a href="#__codelineno-49-10" id="__codelineno-49-10" name="__codelineno-49-10"></a>            <span class="n">moving_avg</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">moving_avg</span><span class="p">,</span>
</span><span id="__span-49-11"><a href="#__codelineno-49-11" id="__codelineno-49-11" name="__codelineno-49-11"></a>            <span class="n">dropout</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
</span><span id="__span-49-12"><a href="#__codelineno-49-12" id="__codelineno-49-12" name="__codelineno-49-12"></a>            <span class="n">activation</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">activation</span>
</span><span id="__span-49-13"><a href="#__codelineno-49-13" id="__codelineno-49-13" name="__codelineno-49-13"></a>        <span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">configs</span><span class="o">.</span><span class="n">e_layers</span><span class="p">)</span>
</span><span id="__span-49-14"><a href="#__codelineno-49-14" id="__codelineno-49-14" name="__codelineno-49-14"></a>    <span class="p">],</span>
</span><span id="__span-49-15"><a href="#__codelineno-49-15" id="__codelineno-49-15" name="__codelineno-49-15"></a>    <span class="n">norm_layer</span><span class="o">=</span><span class="n">my_Layernorm</span><span class="p">(</span><span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span><span id="__span-49-16"><a href="#__codelineno-49-16" id="__codelineno-49-16" name="__codelineno-49-16"></a><span class="p">)</span>
</span><span id="__span-49-17"><a href="#__codelineno-49-17" id="__codelineno-49-17" name="__codelineno-49-17"></a><span class="c1"># Decoder</span>
</span><span id="__span-49-18"><a href="#__codelineno-49-18" id="__codelineno-49-18" name="__codelineno-49-18"></a><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span>
</span><span id="__span-49-19"><a href="#__codelineno-49-19" id="__codelineno-49-19" name="__codelineno-49-19"></a>    <span class="p">[</span>
</span><span id="__span-49-20"><a href="#__codelineno-49-20" id="__codelineno-49-20" name="__codelineno-49-20"></a>        <span class="n">DecoderLayer</span><span class="p">(</span>
</span><span id="__span-49-21"><a href="#__codelineno-49-21" id="__codelineno-49-21" name="__codelineno-49-21"></a>            <span class="n">AutoCorrelationLayer</span><span class="p">(</span>
</span><span id="__span-49-22"><a href="#__codelineno-49-22" id="__codelineno-49-22" name="__codelineno-49-22"></a>                <span class="n">AutoCorrelation</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">factor</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
</span><span id="__span-49-23"><a href="#__codelineno-49-23" id="__codelineno-49-23" name="__codelineno-49-23"></a>                                <span class="n">output_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-49-24"><a href="#__codelineno-49-24" id="__codelineno-49-24" name="__codelineno-49-24"></a>                <span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span>
</span><span id="__span-49-25"><a href="#__codelineno-49-25" id="__codelineno-49-25" name="__codelineno-49-25"></a>            <span class="n">AutoCorrelationLayer</span><span class="p">(</span>
</span><span id="__span-49-26"><a href="#__codelineno-49-26" id="__codelineno-49-26" name="__codelineno-49-26"></a>                <span class="n">AutoCorrelation</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">factor</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
</span><span id="__span-49-27"><a href="#__codelineno-49-27" id="__codelineno-49-27" name="__codelineno-49-27"></a>                                <span class="n">output_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-49-28"><a href="#__codelineno-49-28" id="__codelineno-49-28" name="__codelineno-49-28"></a>                <span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span>
</span><span id="__span-49-29"><a href="#__codelineno-49-29" id="__codelineno-49-29" name="__codelineno-49-29"></a>            <span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
</span><span id="__span-49-30"><a href="#__codelineno-49-30" id="__codelineno-49-30" name="__codelineno-49-30"></a>            <span class="n">configs</span><span class="o">.</span><span class="n">c_out</span><span class="p">,</span>
</span><span id="__span-49-31"><a href="#__codelineno-49-31" id="__codelineno-49-31" name="__codelineno-49-31"></a>            <span class="n">configs</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span>
</span><span id="__span-49-32"><a href="#__codelineno-49-32" id="__codelineno-49-32" name="__codelineno-49-32"></a>            <span class="n">moving_avg</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">moving_avg</span><span class="p">,</span>
</span><span id="__span-49-33"><a href="#__codelineno-49-33" id="__codelineno-49-33" name="__codelineno-49-33"></a>            <span class="n">dropout</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
</span><span id="__span-49-34"><a href="#__codelineno-49-34" id="__codelineno-49-34" name="__codelineno-49-34"></a>            <span class="n">activation</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span>
</span><span id="__span-49-35"><a href="#__codelineno-49-35" id="__codelineno-49-35" name="__codelineno-49-35"></a>        <span class="p">)</span>
</span><span id="__span-49-36"><a href="#__codelineno-49-36" id="__codelineno-49-36" name="__codelineno-49-36"></a>        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">configs</span><span class="o">.</span><span class="n">d_layers</span><span class="p">)</span>
</span><span id="__span-49-37"><a href="#__codelineno-49-37" id="__codelineno-49-37" name="__codelineno-49-37"></a>    <span class="p">],</span>
</span><span id="__span-49-38"><a href="#__codelineno-49-38" id="__codelineno-49-38" name="__codelineno-49-38"></a>    <span class="n">norm_layer</span><span class="o">=</span><span class="n">my_Layernorm</span><span class="p">(</span><span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
</span><span id="__span-49-39"><a href="#__codelineno-49-39" id="__codelineno-49-39" name="__codelineno-49-39"></a>    <span class="n">projection</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">configs</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">configs</span><span class="o">.</span><span class="n">c_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-49-40"><a href="#__codelineno-49-40" id="__codelineno-49-40" name="__codelineno-49-40"></a><span class="p">)</span>
</span></code></pre></div>
<p>说着说着就远了</p>
<h4 id="_12">论文<a class="headerlink" href="#_12" title="Permanent link">¶</a></h4>
<p>继续按照论文、公式、代码、图的逻辑进行讲解</p>
<p>看论文把。</p>
<p><img alt="image-20250321163503744" src="../images/image-20250321163503744.png"/></p>
<p><img alt="image-20250321163514281" src="../images/image-20250321163514281.png"/></p>
<p>一句一句看吧：</p>
<p>🟢   </p>
<blockquote>
<p><img alt="image-20250321164634097" src="../images/image-20250321164634097.png"/></p>
<p>解码器</p>
<p>解码器包含两个部分，表示趋势成分的累积结构 和 季节成分堆叠的自相关机制</p>
<p>每个的DecoderLayer 包含了内部自相关机制(自注意力机制)和编码器和解码器相关机制(交叉注意力机制)，目的是为了  ① 细化预测 并且 ② 充分利用历史的季节信息</p>
</blockquote>
<p>🟢 </p>
<p><img alt="image-20250321164717926" src="../images/image-20250321164717926.png"/> </p>
<blockquote>
<p>注意，模型提取潜在的趋势信息，在解码器中，通过中间的隐藏变量。</p>
<p>（注意，在解码器中，模型 通过中间的隐藏变量 提取潜在的趋势信息）</p>
<p>这种机制 使得Autoformer 逐步 优化 趋势预测 并且 消除 干扰信息</p>
<p>以便在自相关机制中 发现 基于周期的依赖关系。</p>
<blockquote>
<p>我：（就是说 隐藏的中间变量预测趋势信息，在自相关机制中 预测季节信息）</p>
</blockquote>
<p>假设 解码器 有 M 层，结合来自编码器的潜在变量 <span class="arithmatex">\(\mathcal{X}_{en}^N\)</span> （我：应该是编码器的最后一层输出），第 l 层解码器的方程可以表示为 $ \mathcal{X}<em de="de">{de}^{l} = Decoder(\mathcal{X}</em><sup>{l-1},\mathcal{X}_{en}</sup>N)$ （我：第 <span class="arithmatex">\(l\)</span> 层解码器的输入，接收来自==上一层解码器的输出==  和 <mark>最后一层编码器的输出</mark> 作为输入）</p>
</blockquote>
<p>🟢 单个 DecoderLayer 的过程，可以概括如下：</p>
<p><img alt="image-20250321165833556" src="../images/image-20250321165833556.png"/> </p>
<p>符号解释</p>
<p><img alt="image-20250321171255673" src="../images/image-20250321171255673.png"/> </p>
<ul>
<li><span class="arithmatex">\(\mathcal{X}_{de}^l = \mathcal{S}_{de}^{l,3},l \in \{1,...,M\}\)</span> 表示第 <span class="arithmatex">\(l\)</span> 层解码层的输出，M 是 DecoderLayer 的层数</li>
</ul>
<blockquote>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 3 是啥？</li>
</ul>
</blockquote>
<ul>
<li><span class="arithmatex">\(\mathcal{X}_{de}^0\)</span> 是对原始解码器输入 <span class="arithmatex">\(\mathcal{X}_{des}\)</span>  的嵌入 （ <span class="arithmatex">\(\mathcal{X}_{des}\)</span> → 嵌入层 →  <span class="arithmatex">\(\mathcal{X}_{de}^0\)</span>） </li>
</ul>
<blockquote>
<p>(outputs)（就是原始数据表示、Transformer 结构右侧解码器的输入（真服了，为了怕自己忘，用着这么多解释T.T））</p>
</blockquote>
<ul>
<li><span class="arithmatex">\(\mathcal{T}_{de}^0 = \mathcal{X}_{det}\)</span> 用于累积==（所以就是趋势分量是累加来的）==    </li>
</ul>
<blockquote>
<p>真晕啊，都啥符号啊。真服了。翻把。毕竟符号是统一的。</p>
<p>找到了！</p>
<p><img alt="image-20250321171042677" src="../images/image-20250321171042677.png"/> </p>
<ul>
<li>de 解码器 、s 季节分量、t 趋势分量</li>
<li><span class="arithmatex">\(\mathcal{X}\)</span> 原始数据，未经嵌入</li>
<li>上标 0 表示初始值</li>
<li>s 成分（季节成分）的初始化是 0 填充的、t 成分（趋势成分）的初始化是 均值填充的</li>
<li><span class="arithmatex">\(\mathcal{T}\)</span> 表示趋势分量，（Trend）</li>
<li><span class="arithmatex">\(\mathcal{S}\)</span> 表示 季节分量，（Season）</li>
</ul>
<blockquote>
<p>不错，命名还挺讲究的。来自我的超高评价，害，研究这个的，哪有几个精神美丽的，干活了。</p>
</blockquote>
</blockquote>
<ul>
<li><span class="arithmatex">\(\mathcal{S}_{de}^{l,i}\)</span> 、<span class="arithmatex">\(\mathcal{T}_{de}^{l,i}\)</span> $   i  \in {1,2,3}$ 分别表示 第<span class="arithmatex">\(l\)</span> 层的DecoderLayer 中，第 <span class="arithmatex">\(i\)</span> 次序列分解快之后得到的季节成分和趋势成分。(<span class="arithmatex">\(l\)</span> 表示 <span class="arithmatex">\(l\)</span> 层，编码器解码器中同理)</li>
<li><span class="arithmatex">\(\mathcal{W}_{l,i} , i\in \{1,2,3\}\)</span>  表示 对 第 <span class="arithmatex">\(i\)</span> 个提取的趋势分量 <span class="arithmatex">\(\mathcal{T}_{de}^{l,i}\)</span> 的 线性投影。（W 是什么的缩写？）</li>
</ul>
<p>🟢</p>
<p><img alt="image-20250321172443831" src="../images/image-20250321172443831.png"/> </p>
<p>最终的预测结果 是对两个分解成分的加和，公式表示为 <span class="arithmatex">\(\mathcal{W}_\mathcal{S} * \mathcal{X}_{de}^M + \mathcal{T}_{de}^M\)</span> </p>
<ul>
<li>这里的 <span class="arithmatex">\(\mathcal{W}\)</span> 都表示线性投影。</li>
<li><span class="arithmatex">\(\mathcal{W}_\mathcal{S}\)</span> 将 最后一层 解码器的输出 投影到目标维度</li>
</ul>
<blockquote>
<p><img alt="image-20250321173400693" src="../images/image-20250321173400693.png"/> </p>
</blockquote>
<p>我：这个公式  <span class="arithmatex">\(\mathcal{X}_{de}^{l} = \mathcal{S}_{de}^{l,3}\)</span> 也就是表示了 解码器中数据的的流动也是季节成分。</p>
<h4 id="_13">代码<a class="headerlink" href="#_13" title="Permanent link">¶</a></h4>
<p><strong>好了，论文看完了，现在看代码。</strong></p>
<ul>
<li>从 Autoformer forward 出发</li>
</ul>
<p><img alt="image-20250321173752060" src="../images/image-20250321173752060.png"/></p>
<ul>
<li>进入 Decoder forward</li>
</ul>
<p><img alt="image-20250321173851471" src="../images/image-20250321173851471.png"/> </p>
<ul>
<li>Decoder 的初始化</li>
</ul>
<p><img alt="image-20250321173954060" src="../images/image-20250321173954060.png"/></p>
<p>其中 <code>d_layers = 1</code> （全局搜索）</p>
<p>包括了一层 DecoderLayer、自注意力机制、交叉注意力机制，最后的投影层是为了把嵌入维度 还原为 原始维度。</p>
<h4 id="decoderlayer-forward">DecoderLayer  forward<a class="headerlink" href="#decoderlayer-forward" title="Permanent link">¶</a></h4>
<ul>
<li>步进，果然进入了 DecoderLayer 的 forward</li>
</ul>
<p><img alt="image-20250321174221181" src="../images/image-20250321174221181.png"/> </p>
<ul>
<li>逐步步进执行，果然进入了 AutoCorrelationLayer 的 forward</li>
</ul>
<p><img alt="image-20250321174253332" src="../images/image-20250321174253332.png"/> </p>
<p>接下来就是 qkv 的计算，这行关键，会步进到具体的 AutoCorrelation的计算。看维度，可以看到这里还用的<span id="返回理解多头注意力机制"><a href="#跳到理解多头注意力机制">多头注意力机制</a></span>。  </p>
<p><img alt="image-20250321174359890" src="../images/image-20250321174359890.png"/> </p>
<ul>
<li>步进，果然 跳到了 AutoCorrelation-forward</li>
</ul>
<p><img alt="image-20250321175357208" src="../images/image-20250321175357208.png"/> </p>
<p><strong>其实，跳进来没用，因为暂时不看自相关机制的计算，这是本文的创新。逐步跳出，看 DecoderLayer 的处理。</strong></p>
<p>两个注意力计算，三个趋势分解，一个还原维度的线性层。</p>
<p><strong>DecoderLayer代码执行的流程图</strong> </p>
<div class="language-markdown highlight"><pre><span></span><code><span id="__span-50-1"><a href="#__codelineno-50-1" id="__codelineno-50-1" name="__codelineno-50-1"></a>┌─────────────────────────┐
</span><span id="__span-50-2"><a href="#__codelineno-50-2" id="__codelineno-50-2" name="__codelineno-50-2"></a>│ 输入序列 x [B,L,d_model] │
</span><span id="__span-50-3"><a href="#__codelineno-50-3" id="__codelineno-50-3" name="__codelineno-50-3"></a>└──────────────┬──────────┘
</span><span id="__span-50-4"><a href="#__codelineno-50-4" id="__codelineno-50-4" name="__codelineno-50-4"></a>               ↓
</span><span id="__span-50-5"><a href="#__codelineno-50-5" id="__codelineno-50-5" name="__codelineno-50-5"></a>┌─────────────────────────┐
</span><span id="__span-50-6"><a href="#__codelineno-50-6" id="__codelineno-50-6" name="__codelineno-50-6"></a>│     自注意力 + 残差连接    │    x = x + dropout(self_attention(x,x,x))
</span><span id="__span-50-7"><a href="#__codelineno-50-7" id="__codelineno-50-7" name="__codelineno-50-7"></a>└──────────────┬──────────┘
</span><span id="__span-50-8"><a href="#__codelineno-50-8" id="__codelineno-50-8" name="__codelineno-50-8"></a>               ↓
</span><span id="__span-50-9"><a href="#__codelineno-50-9" id="__codelineno-50-9" name="__codelineno-50-9"></a>┌─────────────────────────┐
</span><span id="__span-50-10"><a href="#__codelineno-50-10" id="__codelineno-50-10" name="__codelineno-50-10"></a>│       序列分解 1         │    x, trend1 = decomp1(x)
</span><span id="__span-50-11"><a href="#__codelineno-50-11" id="__codelineno-50-11" name="__codelineno-50-11"></a>└──────┬──────────────────┘
</span><span id="__span-50-12"><a href="#__codelineno-50-12" id="__codelineno-50-12" name="__codelineno-50-12"></a>       │                  ↘
</span><span id="__span-50-13"><a href="#__codelineno-50-13" id="__codelineno-50-13" name="__codelineno-50-13"></a>       │                   trend1 [B,L,d_model] → 保存
</span><span id="__span-50-14"><a href="#__codelineno-50-14" id="__codelineno-50-14" name="__codelineno-50-14"></a>       ↓
</span><span id="__span-50-15"><a href="#__codelineno-50-15" id="__codelineno-50-15" name="__codelineno-50-15"></a>┌─────────────────────────┐
</span><span id="__span-50-16"><a href="#__codelineno-50-16" id="__codelineno-50-16" name="__codelineno-50-16"></a>│ 交叉注意力(编码器输出)     │    x = x + dropout(cross_attention(x,cross,cross))
</span><span id="__span-50-17"><a href="#__codelineno-50-17" id="__codelineno-50-17" name="__codelineno-50-17"></a>└──────────────┬──────────┘
</span><span id="__span-50-18"><a href="#__codelineno-50-18" id="__codelineno-50-18" name="__codelineno-50-18"></a>               ↓
</span><span id="__span-50-19"><a href="#__codelineno-50-19" id="__codelineno-50-19" name="__codelineno-50-19"></a>┌─────────────────────────┐
</span><span id="__span-50-20"><a href="#__codelineno-50-20" id="__codelineno-50-20" name="__codelineno-50-20"></a>│       序列分解 2         │    x, trend2 = decomp2(x)
</span><span id="__span-50-21"><a href="#__codelineno-50-21" id="__codelineno-50-21" name="__codelineno-50-21"></a>└──────┬──────────────────┘
</span><span id="__span-50-22"><a href="#__codelineno-50-22" id="__codelineno-50-22" name="__codelineno-50-22"></a>       │                  ↘
</span><span id="__span-50-23"><a href="#__codelineno-50-23" id="__codelineno-50-23" name="__codelineno-50-23"></a>       │                   trend2 [B,L,d_model] → 保存
</span><span id="__span-50-24"><a href="#__codelineno-50-24" id="__codelineno-50-24" name="__codelineno-50-24"></a>       ↓
</span><span id="__span-50-25"><a href="#__codelineno-50-25" id="__codelineno-50-25" name="__codelineno-50-25"></a>┌─────────────────────────┐
</span><span id="__span-50-26"><a href="#__codelineno-50-26" id="__codelineno-50-26" name="__codelineno-50-26"></a>│        前馈网络          │
</span><span id="__span-50-27"><a href="#__codelineno-50-27" id="__codelineno-50-27" name="__codelineno-50-27"></a>│  y=x (复制操作)          │
</span><span id="__span-50-28"><a href="#__codelineno-50-28" id="__codelineno-50-28" name="__codelineno-50-28"></a>│  卷积1 + 激活 + Dropout  │    y = dropout(activation(conv1(y.transpose)))
</span><span id="__span-50-29"><a href="#__codelineno-50-29" id="__codelineno-50-29" name="__codelineno-50-29"></a>│  卷积2 + Dropout        │    y = dropout(conv2(y).transpose)
</span><span id="__span-50-30"><a href="#__codelineno-50-30" id="__codelineno-50-30" name="__codelineno-50-30"></a>└──────────────┬──────────┘
</span><span id="__span-50-31"><a href="#__codelineno-50-31" id="__codelineno-50-31" name="__codelineno-50-31"></a>               ↓
</span><span id="__span-50-32"><a href="#__codelineno-50-32" id="__codelineno-50-32" name="__codelineno-50-32"></a>┌─────────────────────────┐
</span><span id="__span-50-33"><a href="#__codelineno-50-33" id="__codelineno-50-33" name="__codelineno-50-33"></a>│ 残差连接 (x + y)         │
</span><span id="__span-50-34"><a href="#__codelineno-50-34" id="__codelineno-50-34" name="__codelineno-50-34"></a>└──────────────┬──────────┘
</span><span id="__span-50-35"><a href="#__codelineno-50-35" id="__codelineno-50-35" name="__codelineno-50-35"></a>               ↓
</span><span id="__span-50-36"><a href="#__codelineno-50-36" id="__codelineno-50-36" name="__codelineno-50-36"></a>┌─────────────────────────┐
</span><span id="__span-50-37"><a href="#__codelineno-50-37" id="__codelineno-50-37" name="__codelineno-50-37"></a>│       序列分解 3         │    x, trend3 = decomp3(x + y)
</span><span id="__span-50-38"><a href="#__codelineno-50-38" id="__codelineno-50-38" name="__codelineno-50-38"></a>└──────┬──────────────────┘
</span><span id="__span-50-39"><a href="#__codelineno-50-39" id="__codelineno-50-39" name="__codelineno-50-39"></a>       │                  ↘
</span><span id="__span-50-40"><a href="#__codelineno-50-40" id="__codelineno-50-40" name="__codelineno-50-40"></a>       │                   trend3 [B,L,d_model] → 保存
</span><span id="__span-50-41"><a href="#__codelineno-50-41" id="__codelineno-50-41" name="__codelineno-50-41"></a>       ↓
</span><span id="__span-50-42"><a href="#__codelineno-50-42" id="__codelineno-50-42" name="__codelineno-50-42"></a>       │
</span><span id="__span-50-43"><a href="#__codelineno-50-43" id="__codelineno-50-43" name="__codelineno-50-43"></a>       │    ┌───────────────────────────────────┐
</span><span id="__span-50-44"><a href="#__codelineno-50-44" id="__codelineno-50-44" name="__codelineno-50-44"></a>       │    │   合并趋势: trend1 + trend2 + trend3 │
</span><span id="__span-50-45"><a href="#__codelineno-50-45" id="__codelineno-50-45" name="__codelineno-50-45"></a>       │    └───────────────┬───────────────────┘
</span><span id="__span-50-46"><a href="#__codelineno-50-46" id="__codelineno-50-46" name="__codelineno-50-46"></a>       │                    ↓
</span><span id="__span-50-47"><a href="#__codelineno-50-47" id="__codelineno-50-47" name="__codelineno-50-47"></a>       │    ┌───────────────────────────────────┐
</span><span id="__span-50-48"><a href="#__codelineno-50-48" id="__codelineno-50-48" name="__codelineno-50-48"></a>       │    │   趋势投影 (维度调整)               │    residual_trend = projection(residual_trend)
</span><span id="__span-50-49"><a href="#__codelineno-50-49" id="__codelineno-50-49" name="__codelineno-50-49"></a>       │    └───────────────┬───────────────────┘
</span><span id="__span-50-50"><a href="#__codelineno-50-50" id="__codelineno-50-50" name="__codelineno-50-50"></a>       │                    ↓
</span><span id="__span-50-51"><a href="#__codelineno-50-51" id="__codelineno-50-51" name="__codelineno-50-51"></a>       │                    │
</span><span id="__span-50-52"><a href="#__codelineno-50-52" id="__codelineno-50-52" name="__codelineno-50-52"></a>┌──────┴────────┐  ┌────────┴─────────┐
</span><span id="__span-50-53"><a href="#__codelineno-50-53" id="__codelineno-50-53" name="__codelineno-50-53"></a>│ 季节性输出 x   │  │ 趋势输出         │
</span><span id="__span-50-54"><a href="#__codelineno-50-54" id="__codelineno-50-54" name="__codelineno-50-54"></a>│ [B,L,d_model] │  │ [B,L,c_out]      │
</span><span id="__span-50-55"><a href="#__codelineno-50-55" id="__codelineno-50-55" name="__codelineno-50-55"></a>└───────────────┘  └──────────────────┘
</span></code></pre></div>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 为什么只对趋势成分进行投影？（答：都投影了，只是位置不同）</li>
</ul>
<blockquote>
<p>通过投影层将趋势维度从d_model调整为c_out(输出特征维度)</p>
<p>首先，需要明确，趋势成分和季节成分最后都需要 从 <code>d_model</code> 还原为 <code>feature dim</code> 也就是原始的数据维度，但是这里趋势成分和季节成分还原的位置不同，方式不同。</p>
<p><strong>第一个还原位置</strong></p>
<p>① 首先明确 Decoder 调用 DecoderLayer 进行单个解码器的处理</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-51-1"><a href="#__codelineno-51-1" id="__codelineno-51-1" name="__codelineno-51-1"></a><span class="c1"># 返回季节性成分(不进行投影)和投影后的趋势成分</span>
</span><span id="__span-51-2"><a href="#__codelineno-51-2" id="__codelineno-51-2" name="__codelineno-51-2"></a><span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">residual_trend</span>
</span><span id="__span-51-3"><a href="#__codelineno-51-3" id="__codelineno-51-3" name="__codelineno-51-3"></a><span class="c1"># x: [B, L, d_model], residual_trend: [B, L, c_out]</span>
</span></code></pre></div>
<p>单个 DecoderLayer 返回的季节成分和趋势成分的维度就是不一样的</p>
<p>② 季节成分在 Decoder 中还原维度，通过 if 判断</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-52-1"><a href="#__codelineno-52-1" id="__codelineno-52-1" name="__codelineno-52-1"></a>        <span class="c1"># 如果存在投影层，则对输出进行投影处理</span>
</span><span id="__span-52-2"><a href="#__codelineno-52-2" id="__codelineno-52-2" name="__codelineno-52-2"></a>        <span class="c1"># x[B, L, d_model] -&gt; Linear -&gt; x[B, L, c_out]</span>
</span><span id="__span-52-3"><a href="#__codelineno-52-3" id="__codelineno-52-3" name="__codelineno-52-3"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-52-4"><a href="#__codelineno-52-4" id="__codelineno-52-4" name="__codelineno-52-4"></a>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></code></pre></div>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 那具体什么时候触发呢？每次 DecoderLayer 执行完？还是所有 DecoderLayer 最终执行完？ 回答：后者，理由：</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-53-1"><a href="#__codelineno-53-1" id="__codelineno-53-1" name="__codelineno-53-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cross</span><span class="p">,</span> <span class="n">x_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cross_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trend</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="__span-53-2"><a href="#__codelineno-53-2" id="__codelineno-53-2" name="__codelineno-53-2"></a>    <span class="c1"># 遍历每个解码器层，处理输入序列x和交叉序列cross</span>
</span><span id="__span-53-3"><a href="#__codelineno-53-3" id="__codelineno-53-3" name="__codelineno-53-3"></a>    <span class="c1"># x[B, L, d_model] -&gt; DecoderLayer.forward -&gt; x[B, L, d_model], residual_trend[B, L, c_out]</span>
</span><span id="__span-53-4"><a href="#__codelineno-53-4" id="__codelineno-53-4" name="__codelineno-53-4"></a>    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span><span id="__span-53-5"><a href="#__codelineno-53-5" id="__codelineno-53-5" name="__codelineno-53-5"></a>        <span class="c1"># 调用解码器层的前向传播方法，更新x和残差趋势</span>
</span><span id="__span-53-6"><a href="#__codelineno-53-6" id="__codelineno-53-6" name="__codelineno-53-6"></a>        <span class="c1"># x[B, L, d_model], cross[B, L, d_model] -&gt; DecoderLayer.forward -&gt; x[B, L, d_model], residual_trend[B, L, c_out]</span>
</span><span id="__span-53-7"><a href="#__codelineno-53-7" id="__codelineno-53-7" name="__codelineno-53-7"></a>        <span class="n">x</span><span class="p">,</span> <span class="n">residual_trend</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cross</span><span class="p">,</span> <span class="n">x_mask</span><span class="o">=</span><span class="n">x_mask</span><span class="p">,</span> <span class="n">cross_mask</span><span class="o">=</span><span class="n">cross_mask</span><span class="p">)</span>
</span><span id="__span-53-8"><a href="#__codelineno-53-8" id="__codelineno-53-8" name="__codelineno-53-8"></a>        <span class="c1"># 更新趋势信息，将残差趋势添加到当前趋势</span>
</span><span id="__span-53-9"><a href="#__codelineno-53-9" id="__codelineno-53-9" name="__codelineno-53-9"></a>        <span class="c1"># trend[B, L, c_out] + residual_trend[B, L, c_out] -&gt; trend[B, L, c_out]</span>
</span><span id="__span-53-10"><a href="#__codelineno-53-10" id="__codelineno-53-10" name="__codelineno-53-10"></a>        <span class="n">trend</span> <span class="o">=</span> <span class="n">trend</span> <span class="o">+</span> <span class="n">residual_trend</span>
</span><span id="__span-53-11"><a href="#__codelineno-53-11" id="__codelineno-53-11" name="__codelineno-53-11"></a>
</span><span id="__span-53-12"><a href="#__codelineno-53-12" id="__codelineno-53-12" name="__codelineno-53-12"></a>    <span class="c1"># 如果存在归一化层，则对输出进行归一化处理</span>
</span><span id="__span-53-13"><a href="#__codelineno-53-13" id="__codelineno-53-13" name="__codelineno-53-13"></a>    <span class="c1"># x[B, L, d_model] -&gt; LayerNorm -&gt; x[B, L, d_model]</span>
</span><span id="__span-53-14"><a href="#__codelineno-53-14" id="__codelineno-53-14" name="__codelineno-53-14"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-53-15"><a href="#__codelineno-53-15" id="__codelineno-53-15" name="__codelineno-53-15"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-53-16"><a href="#__codelineno-53-16" id="__codelineno-53-16" name="__codelineno-53-16"></a>
</span><span id="__span-53-17"><a href="#__codelineno-53-17" id="__codelineno-53-17" name="__codelineno-53-17"></a>    <span class="c1"># 如果存在投影层，则对输出进行投影处理</span>
</span><span id="__span-53-18"><a href="#__codelineno-53-18" id="__codelineno-53-18" name="__codelineno-53-18"></a>    <span class="c1"># x[B, L, d_model] -&gt; Linear -&gt; x[B, L, c_out]</span>
</span><span id="__span-53-19"><a href="#__codelineno-53-19" id="__codelineno-53-19" name="__codelineno-53-19"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-53-20"><a href="#__codelineno-53-20" id="__codelineno-53-20" name="__codelineno-53-20"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></code></pre></div>
<p>for 循环是为循环 DecoderLayer，DecoderLayer 执行完了，就是最后的处理了。</p>
</blockquote>
<h4 id="_14">数据流动图<a class="headerlink" href="#_14" title="Permanent link">¶</a></h4>
<blockquote>
<p>③ 趋势成分 在每次 DecoderLayer  forward 的最后就会进行还原维度</p>
<p><img alt="image-20250322100529812" src="../images/image-20250322100529812.png"/></p>
<p><strong>画图直观理解 Decoder 和 DecoderLayer 中关于季节成分和趋势成分的理解</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-54-1"><a href="#__codelineno-54-1" id="__codelineno-54-1" name="__codelineno-54-1"></a><span class="err">┌─────────────────────────────────────────────────────────────────────┐</span>
</span><span id="__span-54-2"><a href="#__codelineno-54-2" id="__codelineno-54-2" name="__codelineno-54-2"></a><span class="err">│</span>                         <span class="n">Decoder</span><span class="o">.</span><span class="n">forward</span>                              <span class="err">│</span>
</span><span id="__span-54-3"><a href="#__codelineno-54-3" id="__codelineno-54-3" name="__codelineno-54-3"></a><span class="err">└────────────────────────────────┬────────────────────────────────────┘</span>
</span><span id="__span-54-4"><a href="#__codelineno-54-4" id="__codelineno-54-4" name="__codelineno-54-4"></a>                                 <span class="err">↓</span>
</span><span id="__span-54-5"><a href="#__codelineno-54-5" id="__codelineno-54-5" name="__codelineno-54-5"></a><span class="err">┌──────────────────────────────────────────────────────────────────────────────┐</span>
</span><span id="__span-54-6"><a href="#__codelineno-54-6" id="__codelineno-54-6" name="__codelineno-54-6"></a><span class="err">│</span>  <span class="n">输入</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">],</span> <span class="n">cross</span><span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">],</span> <span class="n">trend</span><span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">],</span> <span class="n">masks</span>            <span class="err">│</span>
</span><span id="__span-54-7"><a href="#__codelineno-54-7" id="__codelineno-54-7" name="__codelineno-54-7"></a><span class="err">└────────────────────────────────┬─────────────────────────────────────────────┘</span>
</span><span id="__span-54-8"><a href="#__codelineno-54-8" id="__codelineno-54-8" name="__codelineno-54-8"></a>                                 <span class="err">↓</span>
</span><span id="__span-54-9"><a href="#__codelineno-54-9" id="__codelineno-54-9" name="__codelineno-54-9"></a>
</span><span id="__span-54-10"><a href="#__codelineno-54-10" id="__codelineno-54-10" name="__codelineno-54-10"></a><span class="err">┌─────────────────────────────────────</span> <span class="n">DecoderLayer</span> <span class="mi">1</span> <span class="err">─────────────────────────────────────┐</span>
</span><span id="__span-54-11"><a href="#__codelineno-54-11" id="__codelineno-54-11" name="__codelineno-54-11"></a><span class="err">│</span>  <span class="err">┌─────────────────────────┐</span>                                                              <span class="err">│</span>
</span><span id="__span-54-12"><a href="#__codelineno-54-12" id="__codelineno-54-12" name="__codelineno-54-12"></a><span class="err">│</span>  <span class="err">│</span> <span class="n">输入序列</span> <span class="n">x</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span> <span class="err">│</span>                                                              <span class="err">│</span>
</span><span id="__span-54-13"><a href="#__codelineno-54-13" id="__codelineno-54-13" name="__codelineno-54-13"></a><span class="err">│</span>  <span class="err">└──────────────┬──────────┘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-14"><a href="#__codelineno-54-14" id="__codelineno-54-14" name="__codelineno-54-14"></a><span class="err">│</span>                 <span class="err">↓</span>                                                                         <span class="err">│</span>
</span><span id="__span-54-15"><a href="#__codelineno-54-15" id="__codelineno-54-15" name="__codelineno-54-15"></a><span class="err">│</span>  <span class="err">┌─────────────────────────┐</span>                                                              <span class="err">│</span>
</span><span id="__span-54-16"><a href="#__codelineno-54-16" id="__codelineno-54-16" name="__codelineno-54-16"></a><span class="err">│</span>  <span class="err">│</span>   <span class="n">自注意力</span> <span class="o">+</span> <span class="n">残差连接</span>     <span class="err">│</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">dropout</span><span class="p">(</span><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">))</span>                    <span class="err">│</span>
</span><span id="__span-54-17"><a href="#__codelineno-54-17" id="__codelineno-54-17" name="__codelineno-54-17"></a><span class="err">│</span>  <span class="err">└──────────────┬──────────┘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-18"><a href="#__codelineno-54-18" id="__codelineno-54-18" name="__codelineno-54-18"></a><span class="err">│</span>                 <span class="err">↓</span>                                                                         <span class="err">│</span>
</span><span id="__span-54-19"><a href="#__codelineno-54-19" id="__codelineno-54-19" name="__codelineno-54-19"></a><span class="err">│</span>  <span class="err">┌─────────────────────────┐</span>                                                              <span class="err">│</span>
</span><span id="__span-54-20"><a href="#__codelineno-54-20" id="__codelineno-54-20" name="__codelineno-54-20"></a><span class="err">│</span>  <span class="err">│</span>       <span class="n">序列分解</span> <span class="mi">1</span>         <span class="err">│</span>    <span class="n">x</span><span class="p">,</span> <span class="n">trend1</span> <span class="o">=</span> <span class="n">decomp1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                                    <span class="err">│</span>
</span><span id="__span-54-21"><a href="#__codelineno-54-21" id="__codelineno-54-21" name="__codelineno-54-21"></a><span class="err">│</span>  <span class="err">└──────┬──────────────────┘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-22"><a href="#__codelineno-54-22" id="__codelineno-54-22" name="__codelineno-54-22"></a><span class="err">│</span>         <span class="err">│</span>                  <span class="err">↘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-23"><a href="#__codelineno-54-23" id="__codelineno-54-23" name="__codelineno-54-23"></a><span class="err">│</span>         <span class="err">│</span>                   <span class="n">trend1</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span> <span class="err">→</span> <span class="n">保存</span>                                    <span class="err">│</span>
</span><span id="__span-54-24"><a href="#__codelineno-54-24" id="__codelineno-54-24" name="__codelineno-54-24"></a><span class="err">│</span>         <span class="err">↓</span>                                                                                 <span class="err">│</span>
</span><span id="__span-54-25"><a href="#__codelineno-54-25" id="__codelineno-54-25" name="__codelineno-54-25"></a><span class="err">│</span>  <span class="err">┌─────────────────────────┐</span>                                                              <span class="err">│</span>
</span><span id="__span-54-26"><a href="#__codelineno-54-26" id="__codelineno-54-26" name="__codelineno-54-26"></a><span class="err">│</span>  <span class="err">│</span> <span class="n">交叉注意力</span><span class="p">(</span><span class="n">编码器输出</span><span class="p">)</span>     <span class="err">│</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">dropout</span><span class="p">(</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">cross</span><span class="p">,</span><span class="n">cross</span><span class="p">))</span>          <span class="err">│</span>
</span><span id="__span-54-27"><a href="#__codelineno-54-27" id="__codelineno-54-27" name="__codelineno-54-27"></a><span class="err">│</span>  <span class="err">└──────────────┬──────────┘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-28"><a href="#__codelineno-54-28" id="__codelineno-54-28" name="__codelineno-54-28"></a><span class="err">│</span>                 <span class="err">↓</span>                                                                         <span class="err">│</span>
</span><span id="__span-54-29"><a href="#__codelineno-54-29" id="__codelineno-54-29" name="__codelineno-54-29"></a><span class="err">│</span>  <span class="err">┌─────────────────────────┐</span>                                                              <span class="err">│</span>
</span><span id="__span-54-30"><a href="#__codelineno-54-30" id="__codelineno-54-30" name="__codelineno-54-30"></a><span class="err">│</span>  <span class="err">│</span>       <span class="n">序列分解</span> <span class="mi">2</span>         <span class="err">│</span>    <span class="n">x</span><span class="p">,</span> <span class="n">trend2</span> <span class="o">=</span> <span class="n">decomp2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                                    <span class="err">│</span>
</span><span id="__span-54-31"><a href="#__codelineno-54-31" id="__codelineno-54-31" name="__codelineno-54-31"></a><span class="err">│</span>  <span class="err">└──────┬──────────────────┘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-32"><a href="#__codelineno-54-32" id="__codelineno-54-32" name="__codelineno-54-32"></a><span class="err">│</span>         <span class="err">│</span>                  <span class="err">↘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-33"><a href="#__codelineno-54-33" id="__codelineno-54-33" name="__codelineno-54-33"></a><span class="err">│</span>         <span class="err">│</span>                   <span class="n">trend2</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span> <span class="err">→</span> <span class="n">保存</span>                                    <span class="err">│</span>
</span><span id="__span-54-34"><a href="#__codelineno-54-34" id="__codelineno-54-34" name="__codelineno-54-34"></a><span class="err">│</span>         <span class="err">↓</span>                                                                                 <span class="err">│</span>
</span><span id="__span-54-35"><a href="#__codelineno-54-35" id="__codelineno-54-35" name="__codelineno-54-35"></a><span class="err">│</span>  <span class="err">┌─────────────────────────┐</span>                                                              <span class="err">│</span>
</span><span id="__span-54-36"><a href="#__codelineno-54-36" id="__codelineno-54-36" name="__codelineno-54-36"></a><span class="err">│</span>  <span class="err">│</span>        <span class="n">前馈网络</span>          <span class="err">│</span>                                                              <span class="err">│</span>
</span><span id="__span-54-37"><a href="#__codelineno-54-37" id="__codelineno-54-37" name="__codelineno-54-37"></a><span class="err">│</span>  <span class="err">│</span>  <span class="n">y</span><span class="o">=</span><span class="n">x</span> <span class="p">(</span><span class="n">复制操作</span><span class="p">)</span>          <span class="err">│</span>                                                              <span class="err">│</span>
</span><span id="__span-54-38"><a href="#__codelineno-54-38" id="__codelineno-54-38" name="__codelineno-54-38"></a><span class="err">│</span>  <span class="err">│</span>  <span class="n">卷积1</span> <span class="o">+</span> <span class="n">激活</span> <span class="o">+</span> <span class="n">Dropout</span>  <span class="err">│</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">activation</span><span class="p">(</span><span class="n">conv1</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">transpose</span><span class="p">)))</span>              <span class="err">│</span>
</span><span id="__span-54-39"><a href="#__codelineno-54-39" id="__codelineno-54-39" name="__codelineno-54-39"></a><span class="err">│</span>  <span class="err">│</span>  <span class="n">卷积2</span> <span class="o">+</span> <span class="n">Dropout</span>        <span class="err">│</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">conv2</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">)</span>                          <span class="err">│</span>
</span><span id="__span-54-40"><a href="#__codelineno-54-40" id="__codelineno-54-40" name="__codelineno-54-40"></a><span class="err">│</span>  <span class="err">└──────────────┬──────────┘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-41"><a href="#__codelineno-54-41" id="__codelineno-54-41" name="__codelineno-54-41"></a><span class="err">│</span>                 <span class="err">↓</span>                                                                         <span class="err">│</span>
</span><span id="__span-54-42"><a href="#__codelineno-54-42" id="__codelineno-54-42" name="__codelineno-54-42"></a><span class="err">│</span>  <span class="err">┌─────────────────────────┐</span>                                                              <span class="err">│</span>
</span><span id="__span-54-43"><a href="#__codelineno-54-43" id="__codelineno-54-43" name="__codelineno-54-43"></a><span class="err">│</span>  <span class="err">│</span> <span class="n">残差连接</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>         <span class="err">│</span>                                                              <span class="err">│</span>
</span><span id="__span-54-44"><a href="#__codelineno-54-44" id="__codelineno-54-44" name="__codelineno-54-44"></a><span class="err">│</span>  <span class="err">└──────────────┬──────────┘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-45"><a href="#__codelineno-54-45" id="__codelineno-54-45" name="__codelineno-54-45"></a><span class="err">│</span>                 <span class="err">↓</span>                                                                         <span class="err">│</span>
</span><span id="__span-54-46"><a href="#__codelineno-54-46" id="__codelineno-54-46" name="__codelineno-54-46"></a><span class="err">│</span>  <span class="err">┌─────────────────────────┐</span>                                                              <span class="err">│</span>
</span><span id="__span-54-47"><a href="#__codelineno-54-47" id="__codelineno-54-47" name="__codelineno-54-47"></a><span class="err">│</span>  <span class="err">│</span>       <span class="n">序列分解</span> <span class="mi">3</span>         <span class="err">│</span>    <span class="n">x</span><span class="p">,</span> <span class="n">trend3</span> <span class="o">=</span> <span class="n">decomp3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>                                <span class="err">│</span>
</span><span id="__span-54-48"><a href="#__codelineno-54-48" id="__codelineno-54-48" name="__codelineno-54-48"></a><span class="err">│</span>  <span class="err">└──────┬──────────────────┘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-49"><a href="#__codelineno-54-49" id="__codelineno-54-49" name="__codelineno-54-49"></a><span class="err">│</span>         <span class="err">│</span>                  <span class="err">↘</span>                                                              <span class="err">│</span>
</span><span id="__span-54-50"><a href="#__codelineno-54-50" id="__codelineno-54-50" name="__codelineno-54-50"></a><span class="err">│</span>         <span class="err">│</span>                   <span class="n">trend3</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span> <span class="err">→</span> <span class="n">保存</span>                                    <span class="err">│</span>
</span><span id="__span-54-51"><a href="#__codelineno-54-51" id="__codelineno-54-51" name="__codelineno-54-51"></a><span class="err">│</span>         <span class="err">│</span>                                                                                 <span class="err">│</span>
</span><span id="__span-54-52"><a href="#__codelineno-54-52" id="__codelineno-54-52" name="__codelineno-54-52"></a><span class="err">│</span>         <span class="err">│</span>    <span class="err">┌─────────────────────────────────────┐</span>                                      <span class="err">│</span>
</span><span id="__span-54-53"><a href="#__codelineno-54-53" id="__codelineno-54-53" name="__codelineno-54-53"></a><span class="err">│</span>         <span class="err">│</span>    <span class="err">│</span> <span class="n">累加趋势</span><span class="p">:</span> <span class="n">trend1</span> <span class="o">+</span> <span class="n">trend2</span> <span class="o">+</span> <span class="n">trend3</span>   <span class="err">│</span>     <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span>                    <span class="err">│</span>
</span><span id="__span-54-54"><a href="#__codelineno-54-54" id="__codelineno-54-54" name="__codelineno-54-54"></a><span class="err">│</span>         <span class="err">│</span>    <span class="err">└───────────────┬─────────────────────┘</span>                                      <span class="err">│</span>
</span><span id="__span-54-55"><a href="#__codelineno-54-55" id="__codelineno-54-55" name="__codelineno-54-55"></a><span class="err">│</span>         <span class="err">│</span>                    <span class="err">↓</span>                                                            <span class="err">│</span>
</span><span id="__span-54-56"><a href="#__codelineno-54-56" id="__codelineno-54-56" name="__codelineno-54-56"></a><span class="err">│</span>         <span class="err">│</span>    <span class="err">┌─────────────────────────────────────┐</span>                                      <span class="err">│</span>
</span><span id="__span-54-57"><a href="#__codelineno-54-57" id="__codelineno-54-57" name="__codelineno-54-57"></a><span class="err">│</span>         <span class="err">│</span>    <span class="err">│</span> <span class="n">趋势投影</span><span class="p">:</span> <span class="n">卷积</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>    <span class="err">│</span>     <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span> <span class="err">→</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>      <span class="err">│</span>
</span><span id="__span-54-58"><a href="#__codelineno-54-58" id="__codelineno-54-58" name="__codelineno-54-58"></a><span class="err">│</span>         <span class="err">│</span>    <span class="err">└───────────────┬─────────────────────┘</span>                                      <span class="err">│</span>
</span><span id="__span-54-59"><a href="#__codelineno-54-59" id="__codelineno-54-59" name="__codelineno-54-59"></a><span class="err">│</span>         <span class="err">│</span>                    <span class="err">↓</span>                                                            <span class="err">│</span>
</span><span id="__span-54-60"><a href="#__codelineno-54-60" id="__codelineno-54-60" name="__codelineno-54-60"></a><span class="err">│</span>  <span class="err">┌──────┴────────┐</span>    <span class="err">┌──────┴───────────┐</span>                                                <span class="err">│</span>
</span><span id="__span-54-61"><a href="#__codelineno-54-61" id="__codelineno-54-61" name="__codelineno-54-61"></a><span class="err">│</span>  <span class="err">│</span> <span class="n">季节性输出</span> <span class="n">x</span>   <span class="err">│</span>    <span class="err">│</span> <span class="n">趋势输出</span> <span class="n">residual_trend</span> <span class="err">│</span>                                         <span class="err">│</span>
</span><span id="__span-54-62"><a href="#__codelineno-54-62" id="__codelineno-54-62" name="__codelineno-54-62"></a><span class="err">│</span>  <span class="err">│</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span> <span class="err">│</span>    <span class="err">│</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>       <span class="err">│</span>                                                <span class="err">│</span>
</span><span id="__span-54-63"><a href="#__codelineno-54-63" id="__codelineno-54-63" name="__codelineno-54-63"></a><span class="err">│</span>  <span class="err">└───────┬───────┘</span>    <span class="err">└──────────┬────────┘</span>                                                <span class="err">│</span>
</span><span id="__span-54-64"><a href="#__codelineno-54-64" id="__codelineno-54-64" name="__codelineno-54-64"></a><span class="err">└──────────┼─────────────────────┬─┴────────────────────────────────────────────────────────┘</span>
</span><span id="__span-54-65"><a href="#__codelineno-54-65" id="__codelineno-54-65" name="__codelineno-54-65"></a>           <span class="err">│</span>                     <span class="err">↓</span>
</span><span id="__span-54-66"><a href="#__codelineno-54-66" id="__codelineno-54-66" name="__codelineno-54-66"></a>           <span class="err">│</span>             <span class="err">┌────────────────────┐</span>
</span><span id="__span-54-67"><a href="#__codelineno-54-67" id="__codelineno-54-67" name="__codelineno-54-67"></a>           <span class="err">│</span>             <span class="err">│</span> <span class="n">Decoder中累积趋势</span>   <span class="err">│</span>     <span class="n">trend</span> <span class="o">+=</span> <span class="n">residual_trend</span>
</span><span id="__span-54-68"><a href="#__codelineno-54-68" id="__codelineno-54-68" name="__codelineno-54-68"></a>           <span class="err">│</span>             <span class="err">│</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>        <span class="err">│</span>     <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span> <span class="o">+=</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>
</span><span id="__span-54-69"><a href="#__codelineno-54-69" id="__codelineno-54-69" name="__codelineno-54-69"></a>           <span class="err">│</span>             <span class="err">└─────────┬──────────┘</span>
</span><span id="__span-54-70"><a href="#__codelineno-54-70" id="__codelineno-54-70" name="__codelineno-54-70"></a>           <span class="err">↓</span>                       <span class="err">↓</span>
</span><span id="__span-54-71"><a href="#__codelineno-54-71" id="__codelineno-54-71" name="__codelineno-54-71"></a><span class="err">┌────────────────────┐</span>    <span class="err">┌────────────────────┐</span>
</span><span id="__span-54-72"><a href="#__codelineno-54-72" id="__codelineno-54-72" name="__codelineno-54-72"></a><span class="err">│</span>  <span class="n">x传入下一层</span>        <span class="err">│</span>    <span class="err">│</span>  <span class="n">trend传入下一层</span>    <span class="err">│</span>
</span><span id="__span-54-73"><a href="#__codelineno-54-73" id="__codelineno-54-73" name="__codelineno-54-73"></a><span class="err">│</span>  <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span>     <span class="err">│</span>    <span class="err">│</span>  <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>       <span class="err">│</span>
</span><span id="__span-54-74"><a href="#__codelineno-54-74" id="__codelineno-54-74" name="__codelineno-54-74"></a><span class="err">└────────┬───────────┘</span>    <span class="err">└──────────┬─────────┘</span>
</span><span id="__span-54-75"><a href="#__codelineno-54-75" id="__codelineno-54-75" name="__codelineno-54-75"></a>         <span class="err">↓</span>                           <span class="err">↓</span>
</span><span id="__span-54-76"><a href="#__codelineno-54-76" id="__codelineno-54-76" name="__codelineno-54-76"></a><span class="err">┌────────────────────────────────────────────────────────────────────┐</span>
</span><span id="__span-54-77"><a href="#__codelineno-54-77" id="__codelineno-54-77" name="__codelineno-54-77"></a><span class="err">│</span>                         <span class="n">DecoderLayer</span> <span class="mi">2</span>                             <span class="err">│</span>
</span><span id="__span-54-78"><a href="#__codelineno-54-78" id="__codelineno-54-78" name="__codelineno-54-78"></a><span class="err">│</span>                          <span class="p">(</span><span class="n">重复流程</span><span class="p">)</span>                                 <span class="err">│</span>
</span><span id="__span-54-79"><a href="#__codelineno-54-79" id="__codelineno-54-79" name="__codelineno-54-79"></a><span class="err">└────────┬───────────────────────────────────┬────────────────────────┘</span>
</span><span id="__span-54-80"><a href="#__codelineno-54-80" id="__codelineno-54-80" name="__codelineno-54-80"></a>         <span class="err">↓</span>                                   <span class="err">↓</span>
</span><span id="__span-54-81"><a href="#__codelineno-54-81" id="__codelineno-54-81" name="__codelineno-54-81"></a>         <span class="o">.</span>                                   <span class="o">.</span>
</span><span id="__span-54-82"><a href="#__codelineno-54-82" id="__codelineno-54-82" name="__codelineno-54-82"></a>         <span class="o">.</span>                                   <span class="o">.</span>
</span><span id="__span-54-83"><a href="#__codelineno-54-83" id="__codelineno-54-83" name="__codelineno-54-83"></a>         <span class="err">↓</span>                                   <span class="err">↓</span>
</span><span id="__span-54-84"><a href="#__codelineno-54-84" id="__codelineno-54-84" name="__codelineno-54-84"></a><span class="err">┌────────────────────────────────────────────────────────────────────┐</span>
</span><span id="__span-54-85"><a href="#__codelineno-54-85" id="__codelineno-54-85" name="__codelineno-54-85"></a><span class="err">│</span>                         <span class="n">DecoderLayer</span> <span class="n">N</span>                             <span class="err">│</span>
</span><span id="__span-54-86"><a href="#__codelineno-54-86" id="__codelineno-54-86" name="__codelineno-54-86"></a><span class="err">│</span>                          <span class="p">(</span><span class="n">重复流程</span><span class="p">)</span>                                 <span class="err">│</span>
</span><span id="__span-54-87"><a href="#__codelineno-54-87" id="__codelineno-54-87" name="__codelineno-54-87"></a><span class="err">└────────┬───────────────────────────────────┬────────────────────────┘</span>
</span><span id="__span-54-88"><a href="#__codelineno-54-88" id="__codelineno-54-88" name="__codelineno-54-88"></a>         <span class="err">↓</span>                                   <span class="err">↓</span>
</span><span id="__span-54-89"><a href="#__codelineno-54-89" id="__codelineno-54-89" name="__codelineno-54-89"></a><span class="err">┌────────────────────┐</span>             <span class="err">┌────────────────────┐</span>
</span><span id="__span-54-90"><a href="#__codelineno-54-90" id="__codelineno-54-90" name="__codelineno-54-90"></a><span class="err">│</span>  <span class="n">最终季节性</span> <span class="n">x</span>       <span class="err">│</span>             <span class="err">│</span>  <span class="n">累积趋势</span> <span class="n">trend</span>     <span class="err">│</span>
</span><span id="__span-54-91"><a href="#__codelineno-54-91" id="__codelineno-54-91" name="__codelineno-54-91"></a><span class="err">│</span>  <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span>     <span class="err">│</span>             <span class="err">│</span>  <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>       <span class="err">│</span>
</span><span id="__span-54-92"><a href="#__codelineno-54-92" id="__codelineno-54-92" name="__codelineno-54-92"></a><span class="err">└────────┬───────────┘</span>             <span class="err">└──────────┬─────────┘</span>
</span><span id="__span-54-93"><a href="#__codelineno-54-93" id="__codelineno-54-93" name="__codelineno-54-93"></a>         <span class="err">↓</span>                                    <span class="err">│</span>
</span><span id="__span-54-94"><a href="#__codelineno-54-94" id="__codelineno-54-94" name="__codelineno-54-94"></a><span class="err">┌────────────────────┐</span>                        <span class="err">│</span>
</span><span id="__span-54-95"><a href="#__codelineno-54-95" id="__codelineno-54-95" name="__codelineno-54-95"></a><span class="err">│</span> <span class="n">归一化</span><span class="p">(</span><span class="n">如果存在</span><span class="p">)</span>    <span class="err">│</span>                        <span class="err">│</span>
</span><span id="__span-54-96"><a href="#__codelineno-54-96" id="__codelineno-54-96" name="__codelineno-54-96"></a><span class="err">│</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span>      <span class="err">│</span>                        <span class="err">│</span>
</span><span id="__span-54-97"><a href="#__codelineno-54-97" id="__codelineno-54-97" name="__codelineno-54-97"></a><span class="err">└────────┬───────────┘</span>                        <span class="err">│</span>
</span><span id="__span-54-98"><a href="#__codelineno-54-98" id="__codelineno-54-98" name="__codelineno-54-98"></a>         <span class="err">↓</span>                                    <span class="err">│</span>
</span><span id="__span-54-99"><a href="#__codelineno-54-99" id="__codelineno-54-99" name="__codelineno-54-99"></a><span class="err">┌────────────────────┐</span>                        <span class="err">│</span>
</span><span id="__span-54-100"><a href="#__codelineno-54-100" id="__codelineno-54-100" name="__codelineno-54-100"></a><span class="err">│</span> <span class="n">投影</span><span class="p">(</span><span class="n">如果存在</span><span class="p">)</span>      <span class="err">│</span>     <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">d_model</span><span class="p">]</span> <span class="err">→</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>
</span><span id="__span-54-101"><a href="#__codelineno-54-101" id="__codelineno-54-101" name="__codelineno-54-101"></a><span class="err">│</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>        <span class="err">│</span>                        <span class="err">│</span>
</span><span id="__span-54-102"><a href="#__codelineno-54-102" id="__codelineno-54-102" name="__codelineno-54-102"></a><span class="err">└────────┬───────────┘</span>                        <span class="err">│</span>
</span><span id="__span-54-103"><a href="#__codelineno-54-103" id="__codelineno-54-103" name="__codelineno-54-103"></a>         <span class="err">↓</span>                                    <span class="err">↓</span>
</span><span id="__span-54-104"><a href="#__codelineno-54-104" id="__codelineno-54-104" name="__codelineno-54-104"></a><span class="err">┌────────────────────┐</span>             <span class="err">┌────────────────────┐</span>
</span><span id="__span-54-105"><a href="#__codelineno-54-105" id="__codelineno-54-105" name="__codelineno-54-105"></a><span class="err">│</span>  <span class="n">Decoder季节性输出</span>  <span class="err">│</span>             <span class="err">│</span>  <span class="n">Decoder趋势输出</span>   <span class="err">│</span>
</span><span id="__span-54-106"><a href="#__codelineno-54-106" id="__codelineno-54-106" name="__codelineno-54-106"></a><span class="err">│</span>  <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>       <span class="err">│</span>             <span class="err">│</span>  <span class="p">[</span><span class="n">B</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">c_out</span><span class="p">]</span>       <span class="err">│</span>
</span><span id="__span-54-107"><a href="#__codelineno-54-107" id="__codelineno-54-107" name="__codelineno-54-107"></a><span class="err">└────────────────────┘</span>             <span class="err">└────────────────────┘</span>
</span></code></pre></div>
<p><strong>第二个 还原操作</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-55-1"><a href="#__codelineno-55-1" id="__codelineno-55-1" name="__codelineno-55-1"></a><span class="c1"># 解码器层内的趋势投影（使用卷积）</span>
</span><span id="__span-55-2"><a href="#__codelineno-55-2" id="__codelineno-55-2" name="__codelineno-55-2"></a><span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">c_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
</span><span id="__span-55-3"><a href="#__codelineno-55-3" id="__codelineno-55-3" name="__codelineno-55-3"></a>                           <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">'circular'</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-55-4"><a href="#__codelineno-55-4" id="__codelineno-55-4" name="__codelineno-55-4"></a>
</span><span id="__span-55-5"><a href="#__codelineno-55-5" id="__codelineno-55-5" name="__codelineno-55-5"></a><span class="c1"># Decoder类中的季节性投影（通常是线性层）</span>
</span><span id="__span-55-6"><a href="#__codelineno-55-6" id="__codelineno-55-6" name="__codelineno-55-6"></a><span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c_out</span><span class="p">)</span> <span class="k">if</span> <span class="n">projection</span> <span class="k">else</span> <span class="kc">None</span>
</span></code></pre></div>
<p>趋势使用卷积投影有以下优势：（趋势的初始化本来使用的是平均池化，这里的 1D 卷积就相当于全连接，只是不同的数据组织方式）</p>
<ul>
<li>可以捕获局部时间依赖关系（kernel_size=3）</li>
<li>使用循环填充（circular padding）适合处理时间序列的周期性</li>
<li>可以平滑处理趋势变化，减少噪声</li>
</ul>
</blockquote>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 单个 DecoderLayer 返回的是 纯季节性成分 x[B,L,d_model] 和 转换后的趋势成分 residual_trend[B,L,c_out]</li>
</ul>
<blockquote>
<p>🟢 <strong>形状变化理解：</strong></p>
<p><strong>🔵 季节性成分:</strong></p>
<p>▶️ <strong>DecoderLayer内部:</strong></p>
<ul>
<li>始终保持形状 [B,L,d_model]</li>
<li>经过多个子模块处理(自注意力、交叉注意力、前馈网络)，但维度不变</li>
<li>输出到Decoder时仍为 [B,L,d_model]</li>
</ul>
<p><strong>▶️ Decoder最终处理:</strong> </p>
<ul>
<li>通过所有DecoderLayer后仍为 [B,L,d_model]</li>
<li>经过可选的归一化层，维度不变</li>
<li>经过可选的投影层后，变为 [B,L,c_out]</li>
</ul>
<p>🔵 <strong>趋势成分:</strong></p>
<p>▶️ <strong>DecoderLayer内部趋势提取:</strong></p>
<p>三个趋势分量(trend1/2/3)都是 [B,L,d_model]</p>
<p>三个趋势累加后仍为 [B,L,d_model]</p>
<p>经过投影层后，变为 [B,L,c_out]</p>
<p>▶️ <strong>Decoder中的趋势累积:</strong></p>
<p>Decoder输入的初始趋势trend为 [B,L,c_out]</p>
<p>每个DecoderLayer输出的residual_trend为 [B,L,c_out]</p>
<p>累积后的趋势维度保持 [B,L,c_out]</p>
<p><strong>🌈各层间的数据传递</strong> </p>
<p>🫧 <strong>季节性成分传递:</strong></p>
<p>季节性成分x在各DecoderLayer之间传递</p>
<p>每个DecoderLayer处理后的季节性成分作为下一层的输入</p>
<p>最后一层的季节性成分输出后经过归一化和投影</p>
<p>🫧 <strong>趋势成分传递:</strong> </p>
<p>DecoderLayer内部提取的三个趋势成分在层内累加和投影</p>
<p>每个DecoderLayer提供一个趋势贡献residual_trend</p>
<p><mark>Decoder维护一个累积趋势trend，垂直累积各层的贡献</mark></p>
<p>最终累积后的趋势不需要额外处理，直接作为输出</p>
<p>🐋 <strong>DecoderLayer的输入输出:</strong></p>
<p><strong>输入:</strong> 季节性成分<code>x[B,L,d_model]</code>和编码器输出<code>cross[B,L,d_model]</code></p>
<p><strong>输出:</strong> 处理后的季节性成分<code>x[B,L,d_model]</code>和趋势贡献<code>residual_trend[B,L,c_out]</code></p>
</blockquote>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> <a href="#编码器的输出作为 K 和 V，解码器的输入作为 Q"><span id="返回编码器的输出作为 K 和 V，解码器的输入作为 Q">编码器的输出作为 K 和 V，解码器的输入作为 Q</span></a></li>
</ul>
<p>目标序列生成查询，对照源序列</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 对比编码器的设计</li>
</ul>
<blockquote>
<p>与编码器的关键区别</p>
<ul>
<li>趋势处理方式: </li>
</ul>
<p>编码器: 丢弃趋势成分，只保留季节性</p>
<p>解码器: 保存并累积趋势成分，最终与季节性分开输出</p>
<ul>
<li>分解次数:</li>
</ul>
<p>编码器: 两次分解(注意力后和前馈网络后)</p>
<p>解码器: 三次分解(自注意力后、交叉注意力后和前馈网络后)</p>
<ul>
<li>趋势累积:</li>
</ul>
<p>编码器: 无趋势累积</p>
<p>解码器: 三个阶段的趋势相加，形成完整趋势表示</p>
<ul>
<li>输出维度:</li>
</ul>
<p>编码器: 输出保持d_model维度</p>
<p>解码器: 对趋势成分进行投影，调整为c_out维度
这种设计体现了Autoformer对时间序列分解的精心处理，通过在解码器中累积趋势信息，结合编码器提取的季节性特征，最终能够实现高质量的时间序列预测。</p>
</blockquote>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 问：为什么要这样设计？</li>
</ul>
<blockquote>
<p>问题描述：</p>
<p>趋势成分：在每个解码器层内部就进行投影，并且各层的投影结果被累积</p>
<p>季节性成分：保持原始维度通过所有解码器层，只在最后进行一次统一投影</p>
</blockquote>
<p>我的理解：季节是一个高频成分，保持高维表示能够更好的表示特征。趋势成分是一个低频成分，不管是卷积或者是 DecoderLayer 最后的 conv 都可以达到平滑噪声的作用，使得网络能够更加关注长期稳定的趋势</p>
<blockquote>
<p><strong>对哦~得到 gpt 的肯定。okay，go。它的补充：</strong> </p>
<p>Autoformer的设计理念确实围绕着对时间序列数据中不同频率成分的差异化处理</p>
<p><strong>🔵 季节性成分（高频）的处理策略</strong></p>
<p><strong>保持高维表示</strong></p>
<ul>
<li>季节性成分在DecoderLayer中始终保持d_model维度（通常较高）</li>
<li>只在Decoder的最后阶段才投影到c_out维度</li>
<li>这种设计允许模型使用丰富的特征空间来表达复杂的周期性模式</li>
</ul>
<p><strong>注意力机制的优势</strong></p>
<ul>
<li>自注意力和交叉注意力特别适合捕获不同时间点之间的周期性关系</li>
<li>季节性成分经过多次注意力处理，可以识别复杂的重复模式</li>
</ul>
<p><strong>多层渐进式处理</strong> </p>
<ul>
<li>每层DecoderLayer都对季节性进行进一步精炼</li>
<li>通过层层传递，允许模型学习到层次化的季节模式（如日内模式、周内模式、月内模式）</li>
</ul>
<p>🔵 <strong>趋势成分（低频）的处理策略</strong></p>
<p><strong>早期降维和平滑处理</strong></p>
<ul>
<li>趋势成分在每个DecoderLayer内就通过卷积（kernel_size=3）进行投影
  卷积的本质是一种平滑操作，能够过滤掉高频噪声</li>
<li>循环填充（padding_mode='circular'）适合处理周期性时间序列
  累积机制的意义</li>
</ul>
<p><strong>不同层捕获的趋势通过简单累加的方式组合</strong></p>
<ul>
<li>趋势使用较低的维度（c_out）就足以表达</li>
<li>趋势本质上是低频、平滑的成分，特征复杂度低于季节性</li>
</ul>
</blockquote>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> <a href="#跳到为什么季节成分和趋势成分">为什么季节成分和趋势成分</a><a id="返回为什么季节成分和趋势成分"></a>，一个用nn.Linear一个用 nn.conv1D？</li>
</ul>
<p>补充：趋势成分卷积时用到的<a href="#循环填充"><span id="返回循环填充">循环填充</span></a></p>
<p>自己的话：kernel size=3，平滑趋势，考虑局部上下文，过滤噪声；（会有一步形状变化，但因为卷积核参数共享，可以识别相同的周期变化）</p>
<p>季节成分使用 nn.Linear，保留高频成分和快速变化的部分。</p>
<ul>
<li>季节性成分（高频、复杂变化）：使用线性投影保留精细结构</li>
<li>趋势成分（低频、平滑变化）：使用卷积投影引入平滑效果和局部上下文</li>
</ul>
<p><strong>最后的一点关于查漏补缺</strong></p>
<p>Decoder 对应论文中的这里：</p>
<p><img alt="image-20250322153705300" src="../images/image-20250322153705300.png"/></p>
<p>公式中，</p>
<p><img alt="image-20250322153735389" src="../images/image-20250322153735389.png"/> </p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input checked="" disabled="" type="checkbox"><span class="task-list-indicator"></span></input></label> 这里的 <span class="arithmatex">\(\mathcal{W}_{l,}\)</span> 是什么意思？</li>
</ul>
<p>首先需要明确，各个的形状</p>
<p><span class="arithmatex">\(\mathcal{T}_{de}^{l}、\mathcal{T}_{de}^{l-1} \in \mathbb{R}^{B \times L \times c_{out}}\)</span></p>
<p><span class="arithmatex">\(\mathcal{T}_{de}^{l,1} 、\mathcal{T}_{de}^{l,2}、\mathcal{T}_{de}^{l,3}\in \mathbb{R}^{B \times L \times d_{model}}\)</span></p>
<p><strong>→</strong> <span class="arithmatex">\(\mathcal{W}_{l,1}、\mathcal{W}_{l,2}、\mathcal{W}_{l,3}\)</span> 分别是调整维度所用到的变换矩阵</p>
<h2 id="autoformer-forward_1">汇总 Autoformer forward<a class="headerlink" href="#autoformer-forward_1" title="Permanent link">¶</a></h2>
<p>1</p>
<pre class="mermaid"><code>classDiagram
    class Model {
        +int seq_len
        +int label_len
        +int pred_len
        +bool output_attention
        +series_decomp decomp
        +DataEmbedding_wo_pos enc_embedding
        +DataEmbedding_wo_pos dec_embedding
        +Encoder encoder
        +Decoder decoder
        +forward(x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)
    }

    class series_decomp {
        +moving_avg moving_avg
        +forward(x) res, moving_mean
    }

    class moving_avg {
        +int kernel_size
        +AvgPool1d avg
        +forward(x)
    }

    class DataEmbedding_wo_pos {
        +TokenEmbedding value_embedding
        +PositionalEmbedding position_embedding
        +TemporalEmbedding or TimeFeatureEmbedding temporal_embedding  
        +Dropout dropout
        +forward(x, x_mark)
    }

    class TokenEmbedding {
        +Conv1d tokenConv
        +forward(x)
    }

    class TemporalEmbedding {
        +Embedding minute_embed
        +Embedding hour_embed
        +Embedding weekday_embed
        +Embedding day_embed
        +Embedding month_embed
        +forward(x)
    }

    class TimeFeatureEmbedding {
        +Linear embed
        +forward(x)
    }

    class Encoder {
        +List~EncoderLayer~ layers
        +my_Layernorm norm_layer
        +forward(x, attn_mask)
    }

    class EncoderLayer {
        +AutoCorrelationLayer attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +Dropout dropout
        +activation
        +forward(x, attn_mask)
    }

    class AutoCorrelationLayer {
        +AutoCorrelation attention
        +Linear query_projection
        +Linear key_projection
        +Linear value_projection
        +Linear out_projection
        +forward(queries, keys, values, attn_mask)
    }

    class AutoCorrelation {
        +bool mask_flag
        +int factor
        +float scale
        +Dropout dropout
        +bool output_attention
        +time_delay_agg_training(values, corr)
        +time_delay_agg_inference(values, corr)
        +forward(queries, keys, values, attn_mask)
    }

    class Decoder {
        +List~DecoderLayer~ layers
        +my_Layernorm norm_layer
        +Linear projection
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }

    class DecoderLayer {
        +AutoCorrelationLayer self_attention
        +AutoCorrelationLayer cross_attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +series_decomp decomp3
        +Dropout dropout
        +activation
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }

    %% 核心组件关系
    Model --&gt; series_decomp
    Model --&gt; DataEmbedding_wo_pos
    Model --&gt; Encoder
    Model --&gt; Decoder

    %% 嵌入层关系 - 修正为条件关系
    DataEmbedding_wo_pos --&gt; TokenEmbedding
    DataEmbedding_wo_pos ..&gt; TemporalEmbedding : 当embed_type!='timeF'
    DataEmbedding_wo_pos ..&gt; TimeFeatureEmbedding : 当embed_type='timeF'

    %% 编码器组件关系
    Encoder --&gt; EncoderLayer
    EncoderLayer --&gt; AutoCorrelationLayer
    EncoderLayer --&gt; Conv1d
    EncoderLayer --&gt; series_decomp
    AutoCorrelationLayer --&gt; AutoCorrelation

    %% 解码器组件关系
    Decoder --&gt; DecoderLayer
    DecoderLayer --&gt; AutoCorrelationLayer
    DecoderLayer --&gt; Conv1d
    DecoderLayer --&gt; series_decomp

    %% 序列分解关系
    series_decomp --&gt; moving_avg
    moving_avg --&gt; AvgPool1d
</code></pre>
<p>2 </p>
<pre class="mermaid"><code>classDiagram
    class Model {
        +int seq_len
        +int label_len
        +int pred_len
        +bool output_attention
        +series_decomp decomp
        +DataEmbedding_wo_pos enc_embedding
        +DataEmbedding_wo_pos dec_embedding
        +Encoder encoder
        +Decoder decoder
        +forward(x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)
    }

    class series_decomp {
        +moving_avg moving_avg
        +forward(x) res, moving_mean
    }

    class moving_avg {
        +int kernel_size
        +AvgPool1d avg
        +forward(x)
    }

    class DataEmbedding_wo_pos {
        +TokenEmbedding value_embedding
        +TemporalEmbedding|TimeFeatureEmbedding temporal_embedding
        +Dropout dropout
        +forward(x, x_mark)
        +__init__(c_in, d_model, embed_type, freq, dropout)
    }

    class TokenEmbedding {
        +Conv1d tokenConv
        +forward(x)
    }

    class TemporalEmbedding {
        +Embedding minute_embed
        +Embedding hour_embed
        +Embedding weekday_embed
        +Embedding day_embed
        +Embedding month_embed
        +forward(x)
    }

    class TimeFeatureEmbedding {
        +Linear embed
        +forward(x)
    }

    class Encoder {
        +List~EncoderLayer~ layers
        +my_Layernorm norm_layer
        +forward(x, attn_mask)
    }

    class EncoderLayer {
        +AutoCorrelationLayer attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +Dropout dropout
        +activation
        +forward(x, attn_mask)
    }

    class AutoCorrelationLayer {
        +AutoCorrelation attention
        +Linear query_projection
        +Linear key_projection
        +Linear value_projection
        +Linear out_projection
        +forward(queries, keys, values, attn_mask)
    }

    class AutoCorrelation {
        +bool mask_flag
        +int factor
        +float scale
        +Dropout dropout
        +bool output_attention
        +time_delay_agg_training(values, corr)
        +time_delay_agg_inference(values, corr)
        +forward(queries, keys, values, attn_mask)
    }

    class Decoder {
        +List~DecoderLayer~ layers
        +my_Layernorm norm_layer
        +Linear projection
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }

    class DecoderLayer {
        +AutoCorrelationLayer self_attention
        +AutoCorrelationLayer cross_attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +series_decomp decomp3
        +Dropout dropout
        +activation
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }

    %% Model中的组件实例化关系
    Model *-- "1" series_decomp : 创建decomp
    Model *-- "1" DataEmbedding_wo_pos : 创建enc_embedding
    Model *-- "1" DataEmbedding_wo_pos : 创建dec_embedding
    Model *-- "1" Encoder : 创建encoder
    Model *-- "1" Decoder : 创建decoder

    %% DataEmbedding_wo_pos内部组件
    DataEmbedding_wo_pos *-- "1" TokenEmbedding : 创建value_embedding
    DataEmbedding_wo_pos *-- "1" TemporalEmbedding : 创建temporal_embedding(当embed_type!='timeF')
    DataEmbedding_wo_pos *-- "1" TimeFeatureEmbedding : 创建temporal_embedding(当embed_type='timeF')

    %% 其他组件关系
    series_decomp *-- "1" moving_avg
    Encoder *-- "e_layers" EncoderLayer
    EncoderLayer *-- "1" AutoCorrelationLayer
    EncoderLayer *-- "2" series_decomp : decomp1,decomp2
    AutoCorrelationLayer *-- "1" AutoCorrelation
    Decoder *-- "d_layers" DecoderLayer
    DecoderLayer *-- "2" AutoCorrelationLayer : self和cross注意力
    DecoderLayer *-- "3" series_decomp : decomp1,2,3
</code></pre>
<h2 id="_15"><span id="附录">附录</span><a class="headerlink" href="#_15" title="Permanent link">¶</a></h2>
<p>原始 Transformer 架构</p>
<p><a href="#原始 Transformer 架构">返回 正文位置：原始 Transformer 架构</a> </p>
<p><img alt="image-20250320213456149" src="../images/image-20250320213456149.png"/></p>
<ul>
<li><code>d_model</code> 是嵌入维度，也就是 Embedding dim，D 是单个时间步观察的特征数。</li>
<li>得到 Embeddingdim 的操作，NLP 中一般叫 word embedding，这里叫 tokenEmbedding也还挺合理的，单个时间步就是单个 token。</li>
</ul>
<h3 id="conv2">疑问一 <a href="#为什么Conv2之后没有进行激活函数的应用">为什么Conv2之后没有进行激活函数的应用</a><a class="headerlink" href="#conv2" title="Permanent link">¶</a></h3>
<p>（1）遵循原始Transformer的设计模式，在原始Transformer设计中，FFN的结构为：<code>FFN(x) = max(0, xW₁ + b₁)W₂ + b₂</code>，这相当于两个线性变换，中间有一个ReLU激活函数。第一个线性变换后应用激活函数，而第二个线性变换后不应用激活函数。Autoformer使用1D卷积替代线性变换，但保持了相同的激活函数模式。</p>
<p>（2）保持输出的线性特性，最后一层不使用激活函数可以保持输出的线性特性，这对于时间序列预测尤其重要。如果在第二个卷积层后应用激活函数：</p>
<ul>
<li>对于ReLU：会导致负值被截断为零，限制模型表达负向趋势的能力</li>
<li>对于其他激活函数：会引入非线性变换，可能限制模型对线性趋势的建模能力</li>
<li>在时间序列建模中，保持一定的线性特性很重要，因为许多时间序列包含强线性趋势。最后一层不使用激活函数，可以使模型更好地捕捉这些线性趋势。</li>
</ul>
<p>（3）总之，<strong>第一个卷积层和激活函数**负责捕捉非线性特征，**第二个卷积层没有激活函数</strong>，保持了一定的线性映射能力，这种设计在非线性表达能力和保持线性特性之间取得了平衡</p>
<h3 id="2-conv1d-nnlinear">疑问 2 为什么是 conv1d，而不是 nn.Linear<a class="headerlink" href="#2-conv1d-nnlinear" title="Permanent link">¶</a></h3>
<ul>
<li>首先，必须明确的是，使用nn.Linear <strong>不是</strong> 为每个时间步创建单独的全连接层，而是对所有时间步应用相同的权重（权重共享）。</li>
</ul>
<p><mark>例子：</mark> </p>
<p><u>方式1：应用于整个序列（批量处理所有时间步）</u> </p>
<p>假设输入x形状为[B, L, D]</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-56-1"><a href="#__codelineno-56-1" id="__codelineno-56-1" name="__codelineno-56-1"></a>linear_layer = nn.Linear(D, D_out)
</span><span id="__span-56-2"><a href="#__codelineno-56-2" id="__codelineno-56-2" name="__codelineno-56-2"></a>output = linear_layer(x)  # 输出形状为[B, L, D_out]
</span></code></pre></div>
<p><strong>同一个线性层会应用到所有时间步，权重是共享的。这与kernel_size=1的Conv1D非常相似</strong> </p>
<ul>
<li>kernel_size=1的Conv1D在功能上类似于独立应用于每个时间步的全连接层，但它有一个关键区别：权重共享。使用卷积意味着同一组权重应用于所有时间步</li>
</ul>
<p><u>方式2：循环应用于每个时间步</u> </p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-57-1"><a href="#__codelineno-57-1" id="__codelineno-57-1" name="__codelineno-57-1"></a># 假设输入x形状为[B, L, D]
</span><span id="__span-57-2"><a href="#__codelineno-57-2" id="__codelineno-57-2" name="__codelineno-57-2"></a>linear_layer = nn.Linear(D, D_out)
</span><span id="__span-57-3"><a href="#__codelineno-57-3" id="__codelineno-57-3" name="__codelineno-57-3"></a>outputs = []
</span><span id="__span-57-4"><a href="#__codelineno-57-4" id="__codelineno-57-4" name="__codelineno-57-4"></a>for i in range(L):
</span><span id="__span-57-5"><a href="#__codelineno-57-5" id="__codelineno-57-5" name="__codelineno-57-5"></a>    output_step = linear_layer(x[:, i, :])  # 输出形状为[B, D_out]
</span><span id="__span-57-6"><a href="#__codelineno-57-6" id="__codelineno-57-6" name="__codelineno-57-6"></a>    outputs.append(output_step)
</span><span id="__span-57-7"><a href="#__codelineno-57-7" id="__codelineno-57-7" name="__codelineno-57-7"></a>output = torch.stack(outputs, dim=1)  # 输出形状为[B, L, D_out]
</span></code></pre></div>
<p>这种方式也是使用同一个线性层，权重仍然是共享的。</p>
<p>其实这个疑问看回答其实也是大可不必的。看看就行了，想着看明白不大可能。</p>
<p><strong>在Autoformer中使用kernel_size=1的Conv1D时，它在功能上与方式1中的nn.Linear非常相似，主要区别在于：</strong> </p>
<ol>
<li><strong>维度顺序</strong>：</li>
<li>Conv1D期望输入形状为<code>[B, C, L]</code>（批次大小、通道数、序列长度）</li>
<li>Linear期望输入形状为<code>[B, L, D]</code>（批次大小、序列长度、特征维度）</li>
<li><strong>实现效率</strong>：</li>
<li>Conv1D在GPU上对于序列数据有更优化的实现</li>
<li>使用Conv1D可以避免reshape操作或循环处理时间步</li>
<li><strong>框架设计</strong>：</li>
<li>使用Conv1D与Autoformer的整体架构设计更一致</li>
<li>Conv1D提供了扩展到更大kernel_size的可能性</li>
</ol>
<p><img alt="image-20250320225707561" src="../images/image-20250320225707561.png"/> </p>
<p><img alt="image-20250320225721285" src="../images/image-20250320225721285.png"/></p>
<p><img alt="image-20250320225730620" src="../images/image-20250320225730620.png"/> </p>
<h3 id="_16">序列分解的现实意义<a class="headerlink" href="#_16" title="Permanent link">¶</a></h3>
<p><strong>Autoformer中序列分解的特殊性与意义</strong> </p>
<p>Autoformer中的序列分解（series_decomp）是该模型最具创新性的特点之一，它深刻体现了时间序列数据处理的特殊需求。让我详细解析这一设计的意义：</p>
<p>🟢 <strong><u>时间序列的基本组成部分</u></strong> </p>
<p>首先，时间序列数据通常可以分解为三个基本组成部分：
- <strong>趋势（Trend）</strong>：长期的变化方向，如经济增长的总体趋势
- <strong>季节性（Seasonality）</strong>：周期性的变化模式，如每年的季节性销售变化
- <strong>残差（Residual）</strong>：随机波动或噪声</p>
<p>传统时间序列分析方法（如ARIMA、指数平滑法）通常会先分解时间序列，再分别建模。</p>
<p><strong><u>🟢 Autoformer的序列分解机制</u></strong></p>
<p>Autoformer通过<code>series_decomp</code>将输入序列分解为季节性和趋势两个部分：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-58-1"><a href="#__codelineno-58-1" id="__codelineno-58-1" name="__codelineno-58-1"></a><span class="n">res</span><span class="p">,</span> <span class="n">moving_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></code></pre></div>
<p>这里：
- <code>res</code> 是季节性部分（短期周期性变化）
- <code>moving_mean</code> 是趋势部分（长期方向性变化）</p>
<p>关键点是，<strong>Autoformer在多个位置使用序列分解，并有选择地丢弃趋势部分</strong>。例如：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-59-1"><a href="#__codelineno-59-1" id="__codelineno-59-1" name="__codelineno-59-1"></a><span class="c1"># 将残差连接结果分解，仅保留季节性部分</span>
</span><span id="__span-59-2"><a href="#__codelineno-59-2" id="__codelineno-59-2" name="__codelineno-59-2"></a><span class="n">res</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div>
<p><u>🟢  <strong>这种设计的深层意义</strong></u> </p>
<p>（1）分离不同时间尺度的模式</p>
<p>趋势和季节性代表不同时间尺度的变化模式：
- 趋势反映长期变化（低频信号）
- 季节性反映短期周期变化（高频信号）</p>
<p>通过分离这两种模式，Autoformer可以：
- 让自注意力机制专注于捕获季节性模式，这更适合其擅长捕获的局部依赖关系
- 单独处理趋势部分，避免混合不同频率的信号导致的建模困难</p>
<p><strong>（2）渐进式分解架构</strong> </p>
<p>Autoformer采用"渐进式分解"（Progressive Decomposition）架构：
- 每一层都进行序列分解，逐步提取季节性特征
- 通过丢弃中间层的趋势信息，强制模型关注季节性模式
- 在解码器中，分别累积各层的趋势信息，形成最终趋势预测</p>
<p>这种架构解决了传统Transformer在长序列预测中的瓶颈：
- 传统Transformer在长期预测中往往存在"长期预测偏差积累"问题
- 分离趋势和季节性可以减轻这种偏差积累</p>
<p><strong>（3）增强季节性和周期性模式的学习</strong> </p>
<p>时间序列中的季节性模式（如每日、每周、每月或每年的周期性）是预测的重要依据。通过保留季节性部分：
- 模型可以更好地捕捉这些重复出现的模式
- 增强了对周期性行为的识别能力
- 降低了趋势变化对季节性模式识别的干扰</p>
<p>🟢 <u><strong>工程实现的巧妙之处</strong></u> </p>
<p>从代码实现看，Autoformer的序列分解采用简单而有效的移动平均方法：
<div class="language-python highlight"><pre><span></span><code><span id="__span-60-1"><a href="#__codelineno-60-1" id="__codelineno-60-1" name="__codelineno-60-1"></a><span class="c1"># 计算移动平均，提取序列趋势分量</span>
</span><span id="__span-60-2"><a href="#__codelineno-60-2" id="__codelineno-60-2" name="__codelineno-60-2"></a><span class="n">moving_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-60-3"><a href="#__codelineno-60-3" id="__codelineno-60-3" name="__codelineno-60-3"></a><span class="c1"># 通过原始序列减去趋势分量，得到残差(季节性分量)</span>
</span><span id="__span-60-4"><a href="#__codelineno-60-4" id="__codelineno-60-4" name="__codelineno-60-4"></a><span class="n">res</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">moving_mean</span>
</span></code></pre></div></p>
<p>这一简单操作在计算效率与效果之间取得了很好的平衡：
- 计算开销低，适合处理长序列
- 效果显著，能有效分离不同频率的信号</p>
<p><u><strong>🟢 与传统Transformer的关键区别</strong></u> </p>
<p>在传统Transformer中，残差连接直接将原始输入和处理后的输出相加：
<div class="language-python highlight"><pre><span></span><code><span id="__span-61-1"><a href="#__codelineno-61-1" id="__codelineno-61-1" name="__codelineno-61-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>  <span class="c1"># 传统Transformer</span>
</span></code></pre></div></p>
<p>而Autoformer在相加后还进行了序列分解：
<div class="language-python highlight"><pre><span></span><code><span id="__span-62-1"><a href="#__codelineno-62-1" id="__codelineno-62-1" name="__codelineno-62-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>
</span><span id="__span-62-2"><a href="#__codelineno-62-2" id="__codelineno-62-2" name="__codelineno-62-2"></a><span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Autoformer的特殊处理</span>
</span></code></pre></div></p>
<p>以及在前馈网络后：
<div class="language-python highlight"><pre><span></span><code><span id="__span-63-1"><a href="#__codelineno-63-1" id="__codelineno-63-1" name="__codelineno-63-1"></a><span class="n">res</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decomp2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># 传统Transformer只有 x = x + y</span>
</span></code></pre></div></p>
<p>这一创新设计从根本上改变了信息在网络中的流动方式，使模型更适合时间序列预测任务。</p>
<p><strong>🟢 实际应用意义</strong> </p>
<p>在实际应用中，这种序列分解机制带来的好处包括：</p>
<ol>
<li><strong>提高长期预测准确性</strong>：通过分离趋势和季节性，减少长期预测中的误差积累</li>
<li><strong>增强可解释性</strong>：分解后的趋势和季节性组件更容易理解和解释</li>
<li><strong>更好的泛化能力</strong>：学习到的季节性模式可以更好地泛化到新的时间段</li>
<li><strong>减轻数据预处理负担</strong>：模型内置的分解机制减少了数据预处理的需求</li>
</ol>
<p><strong>🟢 结论</strong> </p>
<p>Autoformer中的序列分解不仅是一个技术细节，而是一个体现时间序列数据本质特性的关键设计。它使Autoformer能够有效区分和处理不同频率的信号，提高长期预测能力，并增强模型的可解释性。这一设计充分展示了如何将领域知识（时间序列分析）与深度学习架构（Transformer）有机结合，创造出更适合特定任务的模型架构。</p>
<h3 id="_17"><a href="#返回理解多头注意力机制"><span id="跳到理解多头注意力机制">理解多头注意力机制</span></a><a class="headerlink" href="#_17" title="Permanent link">¶</a></h3>
<p><img alt="image-20250321174539839" src="../images/image-20250321174539839.png"/> </p>
<h3 id="k-v-q"><a href="#返回编码器的输出作为 K 和 V，解码器的输入作为 Q"><span id="编码器的输出作为 K 和 V，解码器的输入作为 Q">编码器的输出作为 K 和 V，解码器的输入作为 Q</span></a><a class="headerlink" href="#k-v-q" title="Permanent link">¶</a></h3>
<p>🟢 注意力机制本质上是一种软检索系统：</p>
<ul>
<li>Query（查询）：表达当前解码位置需要的信息类型</li>
<li>Key（键）：表示源序列中各位置信息的"索引"</li>
<li>Value（值）：表示源序列中各位置的实际内容</li>
</ul>
<p>解码器通过其隐藏状态形成查询（Q），然后根据与编码器隐藏状态（K）的相关性，提取相应的编码器信息（V）。</p>
<p>🟢 在序列到序列学习中，交叉注意力实现了条件生成：</p>
<p>编码器：负责理解输入序列（如源语言文本或历史时间序列）</p>
<p>解码器：负责基于输入序列生成输出序列（如目标语言文本或未来时间序列）</p>
<p>解码器需要"有条件地"生成输出，这个条件就是编码器处理的输入序列，交叉注意力提供了这种条件机制。</p>
<p>🟢 在Autoformer这样的时间序列预测模型中，这种设计有其特殊意义：</p>
<p>编码器：处理历史时间序列，提取关键模式和特征</p>
<p>解码器：基于这些模式生成未来时间序列</p>
<p>交叉注意力让解码器能够在生成每个未来时间步时，选择性地关注历史序列中的相关模式，尤其是那些与当前预测位置相关的季节性或周期性模式。</p>
<h3 id="_18"><a href="#返回循环填充"><span id="循环填充">循环填充</span></a><a class="headerlink" href="#_18" title="Permanent link">¶</a></h3>
<p>适用：特别适合处理具有周期性特征的时间序列数据</p>
<p>本文使用循环填充的位置，对趋势成分还原原始维度的时候：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-64-1"><a href="#__codelineno-64-1" id="__codelineno-64-1" name="__codelineno-64-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> 
</span><span id="__span-64-2"><a href="#__codelineno-64-2" id="__codelineno-64-2" name="__codelineno-64-2"></a>                            <span class="n">out_channels</span><span class="o">=</span><span class="n">c_out</span><span class="p">,</span> 
</span><span id="__span-64-3"><a href="#__codelineno-64-3" id="__codelineno-64-3" name="__codelineno-64-3"></a>                            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
</span><span id="__span-64-4"><a href="#__codelineno-64-4" id="__codelineno-64-4" name="__codelineno-64-4"></a>                            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
</span><span id="__span-64-5"><a href="#__codelineno-64-5" id="__codelineno-64-5" name="__codelineno-64-5"></a>                            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
</span><span id="__span-64-6"><a href="#__codelineno-64-6" id="__codelineno-64-6" name="__codelineno-64-6"></a>                            <span class="n">padding_mode</span><span class="o">=</span><span class="s1">'circular'</span><span class="p">,</span> 
</span><span id="__span-64-7"><a href="#__codelineno-64-7" id="__codelineno-64-7" name="__codelineno-64-7"></a>                            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></code></pre></div>
<p>区分几种填充方式</p>
<p>【主要观察，<mark>a,b,c,d的左右</mark> 】</p>
<p>（1）0 填充：零填充（默认，'zeros'）：用0值填充序列边界</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-65-1"><a href="#__codelineno-65-1" id="__codelineno-65-1" name="__codelineno-65-1"></a>原序列: [a, b, c, d]
</span><span id="__span-65-2"><a href="#__codelineno-65-2" id="__codelineno-65-2" name="__codelineno-65-2"></a>填充后: [0, 0, a, b, c, d, 0, 0]
</span></code></pre></div>
<p>（2）重复填充：重复填充（'replicate'）：复制边界值</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-66-1"><a href="#__codelineno-66-1" id="__codelineno-66-1" name="__codelineno-66-1"></a>原序列: [a, b, c, d]
</span><span id="__span-66-2"><a href="#__codelineno-66-2" id="__codelineno-66-2" name="__codelineno-66-2"></a>填充后: [a, a, a, b, c, d, d, d]
</span></code></pre></div>
<p>(3)循环填充（'circular'）：将序列视为循环结构</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-67-1"><a href="#__codelineno-67-1" id="__codelineno-67-1" name="__codelineno-67-1"></a>原序列: [a, b, c, d]
</span><span id="__span-67-2"><a href="#__codelineno-67-2" id="__codelineno-67-2" name="__codelineno-67-2"></a>填充后: [c, d, a, b, c, d, a, b]
</span></code></pre></div>
<p><mark>为什么序列填充，使用时间序列</mark></p>
<p>将时间序列视为一个循环，序列末尾连接到序列开头，保持时间数据的周期性特性，不引入人为的值，只使用已有数据</p>
<p><strong>时间序列周期性的描述：</strong></p>
<ul>
<li>年度周期：每年同一时间点的数据可能有相似模式</li>
<li>月度周期：每月可能有相似的模式（如月初、月中、月末）</li>
<li>周度周期：每周的工作日和周末模式往往类似</li>
<li>日内周期：每天内的活动模式（如早高峰、午休时间）</li>
</ul>
<p>例子：</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-68-1"><a href="#__codelineno-68-1" id="__codelineno-68-1" name="__codelineno-68-1"></a>[周一, 周二, 周三, 周四, 周五, 周六, 周日]
</span></code></pre></div>
<p><strong>循环填充以后：</strong> </p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-69-1"><a href="#__codelineno-69-1" id="__codelineno-69-1" name="__codelineno-69-1"></a>边界处理后：[周六, 周日, 周一, 周二, ..., 周日, 周一, 周二]
</span></code></pre></div>
<p>PyTorch中的循环填充实现会将序列视为环形结构：</p>
<ul>
<li>对于需要在左侧填充的部分，从序列右端取值</li>
<li>对于需要在右侧填充的部分，从序列左端取值</li>
</ul>
<h3 id="_19"><a href="#返回为什么季节成分和趋势成分">为什么季节成分和趋势成分不同的还原维度方法</a> <a id="跳到为什么季节成分和趋势成分"></a><a class="headerlink" href="#_19" title="Permanent link">¶</a></h3>
<p><strong>首先，明确，季节成分 使用线性层投影</strong> </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-70-1"><a href="#__codelineno-70-1" id="__codelineno-70-1" name="__codelineno-70-1"></a><span class="c1"># Decoder类中的投影</span>
</span><span id="__span-70-2"><a href="#__codelineno-70-2" id="__codelineno-70-2" name="__codelineno-70-2"></a><span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">projection</span>  <span class="c1"># 通常是nn.Linear(d_model, c_out)</span>
</span><span id="__span-70-3"><a href="#__codelineno-70-3" id="__codelineno-70-3" name="__codelineno-70-3"></a>
</span><span id="__span-70-4"><a href="#__codelineno-70-4" id="__codelineno-70-4" name="__codelineno-70-4"></a><span class="c1"># 使用时</span>
</span><span id="__span-70-5"><a href="#__codelineno-70-5" id="__codelineno-70-5" name="__codelineno-70-5"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-70-6"><a href="#__codelineno-70-6" id="__codelineno-70-6" name="__codelineno-70-6"></a>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># x: [B, L, d_model] -&gt; [B, L, c_out]</span>
</span></code></pre></div>
<p><strong>趋势成分：使用卷积层（Conv1d）进行投影</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-71-1"><a href="#__codelineno-71-1" id="__codelineno-71-1" name="__codelineno-71-1"></a><span class="c1"># DecoderLayer类中的投影</span>
</span><span id="__span-71-2"><a href="#__codelineno-71-2" id="__codelineno-71-2" name="__codelineno-71-2"></a><span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">c_out</span><span class="p">,</span> 
</span><span id="__span-71-3"><a href="#__codelineno-71-3" id="__codelineno-71-3" name="__codelineno-71-3"></a>                           <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-71-4"><a href="#__codelineno-71-4" id="__codelineno-71-4" name="__codelineno-71-4"></a>                           <span class="n">padding_mode</span><span class="o">=</span><span class="s1">'circular'</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-71-5"><a href="#__codelineno-71-5" id="__codelineno-71-5" name="__codelineno-71-5"></a>
</span><span id="__span-71-6"><a href="#__codelineno-71-6" id="__codelineno-71-6" name="__codelineno-71-6"></a><span class="c1"># 使用时</span>
</span><span id="__span-71-7"><a href="#__codelineno-71-7" id="__codelineno-71-7" name="__codelineno-71-7"></a><span class="n">residual_trend</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">residual_trend</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></code></pre></div>
<p>🔵 解释这么设计的理由：</p>
<p>季节成分作为高频成分，<mark>不会平滑时间序列，保留了季节性的尖锐变化</mark></p>
<p>趋势成分作为低频成分，趋势本质上是==平滑==的</p>
<ul>
<li>设置 <strong>kernel size = 3</strong>，考虑了局部上下文有助于维持这种平滑性</li>
<li>卷积操作（特别是kernel_size=3）能够平滑趋势，过滤掉不必要的高频扰动</li>
<li>（参数共享）跨不同时间点的相同模式可以被相同的卷积核识别</li>
</ul>
<aside class="md-source-file">
<span class="md-source-file__fact">
<span class="md-icon" title="Last update">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="2025年3月23日 12:53:22"><span class="timeago" datetime="2025-03-23T12:53:22+00:00" locale="zh"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="2025年3月23日 12:53:22">2025-03-23</span>
</span>
<span class="md-source-file__fact">
<span class="md-icon" title="Created">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="2025年3月17日 12:13:52"><span class="timeago" datetime="2025-03-17T12:13:52+00:00" locale="zh"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="2025年3月17日 12:13:52">2025-03-17</span>
</span>
</aside>
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["toc.follow", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
<script src="../../js/timeago.min.js"></script>
<script src="../../js/timeago_mkdocs_material.js"></script>
<script src="../../mkdocs/javascripts/katex.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
<script type="module">import mermaid from "https://unpkg.com/mermaid@10.4.0/dist/mermaid.esm.min.mjs";
window.mermaidConfig = {default: {
    startOnLoad: false
}};</script></body>
</html>