{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<ol> <li>git &amp; \u7ec8\u7aef</li> <li>TODO</li> <li>mkdocs</li> <li>markdown</li> <li>leetcode\uff1a\u4e24\u6570\u4e4b\u548c</li> <li>leetcode\uff1a\u4e24\u6570\u76f8\u52a0</li> <li>\u624b\u6495Transformer\u4ee3\u7801</li> <li>pytorch\u7684\u7ef4\u5ea6\u53d8\u6362\u51fd\u6570</li> <li>\u624b\u6495\u4ee3\u7801 5\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5</li> <li>\u624b\u6495\u4ee3\u7801 kmeans</li> <li>\u624b\u6495\u4ee3\u7801 \u53cd\u5411\u4f20\u64ad</li> <li>\u56fe\u89e3LayerNorm &amp; BatchNorm</li> <li>5\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5</li> <li>ViT \u5b66\u4e60\u7b14\u8bb0</li> <li>SwinTransformer \u5b66\u4e60\u7b14\u8bb0</li> <li>4\u79cd\u4f4d\u7f6e\u7f16\u7801</li> <li>4\u79cd\u5377\u79ef</li> <li>\u674e\u6c90\u76ee\u6807\u68c0\u6d4b\u90e8\u5206</li> <li>GAN</li> <li>Bert</li> <li>Diffusion</li> <li>WeightNorm\u7684pytorch\u5b9e\u73b0</li> <li>cGAN&amp;LSGAN</li> <li>\u4fe1\u606f\u91cf | \u71b5 | \u4fe1\u606f\u71b5 | KL\u6563\u5ea6 | \u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570</li> <li>LSTM</li> <li>GRU</li> <li>RNN</li> <li>VAE</li> <li>\u5bf9\u6bd4\u5b66\u4e60</li> </ol> <p>\u6587\u732e\u9605\u8bfb\uff1a</p> <ol> <li>CountGD: Multi-Modal Open-World Counting</li> <li>A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation</li> <li>DAVE \u2013 A Detect-and-Verify Paradigm for Low-Shot Counting</li> <li></li> </ol> <p>\ud83d\udd17</p> <ul> <li>\u6570\u5b66\u5bb6\u662f\u6211\u7406\u60f3</li> <li>\u738b\u6728\u5934\u5b66\u79d1\u5b66</li> <li>wmathor</li> <li>Just for Life</li> <li>\u5927\u767d\u8bddAI</li> <li>Rayman\u5c0f\u4f55\u540c\u5b66|VAE</li> <li>deep_thoughts</li> <li>RethinkFun</li> <li></li> </ul>"},{"location":"Error/github/","title":"github","text":"<p>Git: fatal: unable to access 'https://github.com/dearRongerr/Rongerr.github.io.git/': Failure when receiving data from the peer</p> <p>\u7ebf\u4e0a\u548c\u672c\u5730\u4e0d\u540c\u6b65\u95ee\u9898</p> <p>\u5148\u5c06\u8fdc\u7a0b\u5206\u652f\u7684\u66f4\u6539\u5408\u5e76\u5230\u672c\u5730\u5206\u652f\uff0c\u7136\u540e\u518d\u63a8\u9001\u3002\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u64cd\u4f5c\uff1a</p> <ul> <li>\u62c9\u53d6\u8fdc\u7a0b\u5206\u652f\u7684\u66f4\u6539\u5e76\u5408\u5e76\u5230\u672c\u5730\u5206\u652f\uff1a</li> </ul> <pre><code>  git pull origin main --rebase\n</code></pre> <ul> <li>\u89e3\u51b3\u4efb\u4f55\u53ef\u80fd\u7684\u51b2\u7a81\u3002\u5982\u679c\u6709\u51b2\u7a81\uff0cGit \u4f1a\u63d0\u793a\u4f60\u89e3\u51b3\u51b2\u7a81\u3002\u89e3\u51b3\u51b2\u7a81\u540e\uff0c\u7ee7\u7eed\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</li> </ul> <pre><code>  git rebase --continue\n</code></pre> <ul> <li>\u6700\u540e\uff0c\u63a8\u9001\u672c\u5730\u5206\u652f\u5230\u8fdc\u7a0b\u4ed3\u5e93\uff1a</li> </ul> <pre><code>git push -u origin main\n</code></pre> <p>Git: fatal: unable to access 'https://github.com/dearRongerr/Rongerr.github.io.git/': Failed to connect to github.com port 443 after 75002 ms: Couldn't connect to server</p> <p>\u7f51\u7edc\u95ee\u9898\uff0c\u5173\u4ee3\u7406</p>"},{"location":"Error/latex/","title":"Latex","text":"<ul> <li> overleaf\u5168\u662f\u7ea2\u7ebf</li> </ul> <p>menu\u2014speak cheak\uff1aoff</p> <ul> <li> <code>% \u53bb\u6389thebibliography\u73af\u5883\u81ea\u5e26\u7684\u201c\u53c2\u8003\u6587\u732e\u201d\u6807\u9898</code></li> </ul> <p>\u95ee\u9898\u63cf\u8ff0\uff1a<code>.cls</code>\u6587\u4ef6\u4e2d\u7684\u58f0\u660e\uff0c\u5728\u6587\u6863\u6e32\u67d3\u7684\u65f6\u5019\uff0c\u81ea\u52a8\u51fa\u73b0\u201c\u53c2\u8003\u6587\u732e\u201d\u5b57\u6837\uff0c\u6ca1\u6709\u7f16\u53f7\u4e14\u4e0d\u5728\u76ee\u5f55\u4e2d\u7f16\u53f7</p> <pre><code>\\bibliographystyle{IEEEtran}\n</code></pre> <p>\u89e3\u51b3\uff1a</p> <pre><code>\\renewcommand{\\refname}{\\section{\u53c2\u8003\u6587\u732e}}\n\\bibliography{books}\n</code></pre> <ul> <li> section\u683c\u5f0f\u8bbe\u7f6e</li> </ul> <p></p> <pre><code>\\setcounter{page}{1}\n\n\\CTEXsetup[format={\\Large\\bfseries}]{section}\n\n\\begin{center}\n\\section*{\\textbf{\u5f00 \u9898 \u62a5 \u544a \u6b63 \u6587}}\n\\end{center}\n\n\u5b66\u4f4d\u8bba\u6587\u7814\u7a76\u8bfe\u9898\uff1a\n\n\\textbf{\u8bfe\u9898\u6765\u6e90\uff1a}1.\u7eb5\u5411\u8bfe\u9898\uff08  \uff09\uff1b2.\u6a2a\u5411\u8bfe\u9898\uff08  \uff09\uff1b3.\u81ea\u7531\u9009\u9898\uff08\u221a  \uff09\uff1b4.\u5176\u4ed6\uff08  \uff09\u3002\u8bf7\u6253\u201c\u221a\u201d\u3002\n\n\\section{\u7acb\u9898\u4f9d\u636e}\n\n\\subsection{\u7814\u7a76\u80cc\u666f\u53ca\u610f\u4e49}\n</code></pre> <p></p>"},{"location":"bagu/","title":"Index","text":"<pre><code>(base) ... bagu % tree\n.\n\u251c\u2500\u2500 deeplearning\n\u2502   \u251c\u2500\u2500 former\n\u2502   \u2502   \u251c\u2500\u2500 pe.md\n\u2502   \u2502   \u2514\u2500\u2500 transformer.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2514\u2500\u2500 pytorch\u5e38\u7528\u7684\u7ef4\u5ea6\u53d8\u6362\u51fd\u6570.md\n\u251c\u2500\u2500 leetcode\n\u2502   \u251c\u2500\u2500 1.md\n\u2502   \u251c\u2500\u2500 2.md\n\u2502   \u2514\u2500\u2500 index.md\n\u2514\u2500\u2500 machinelearning\n\n5 directories, 7 files\n</code></pre>"},{"location":"bagu/deeplearning/","title":"Index","text":"<ul> <li> Batchnorm &amp; layernorm</li> </ul>"},{"location":"bagu/deeplearning/1/","title":"visionTransformer\u4ee3\u7801","text":""},{"location":"bagu/deeplearning/1/#1-patch","title":"1 patch\u7684\u6784\u5efa","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# step1 convert image to embedding vector sequence\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    patch_embedding = patch @ weight\n    return patch_embedding\ndef image2emb_conv(image,kernel,stride):\n    conv_output = F.conv2d(image,kernel,stride=stride) # bs*oc*oh*ow\n    bs,oc,oh,ow = conv_output.shape\n    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n\n    return patch_embedding\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim) # model_dim\u662f\u8f93\u51fa\u901a\u9053\u6570\u76ee\uff0cpatch depth\u662f\u5377\u79ef\u6838\u7684\u9762\u79ef\u4e58\u4ee5\u8f93\u5165\u901a\u9053\u6570\n\n# \u5206\u5757\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)\nkernel = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size))\n\n# \u4e8c\u7ef4\u5377\u79ef\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_conv = image2emb_conv(image,kernel,patch_size)\n\n\nprint(patch_embedding_naive)\nprint(patch_embedding_conv)\n</code></pre> <p>\u6ce8\u91ca\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# step1 convert image to embedding vector sequence\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width = 1,3,8,8\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    # patchshape = torch.Size([1, 48, 4])   patch_size = 4\n    # 1\uff1abatchsize\n    # 48 = 3*4*4\uff08\u5377\u79ef\u8986\u76d6\u7684input region\uff09\n    # 4\uff1a1,3,8,8\u7684\u8f93\u5165\u56fe\u7247\u7528 1344\u7684\u5377\u79ef\u6838\u5377\u79ef\uff0c\u5f97\u52304\u4e2ainput region\n    # transpose(-1,-2) \u2192 1,4,48  \n    patch_embedding = patch @ weight\n    # 1,4,48 @ 48,8 = 1\u00d7 4 \u00d7 8\n    return patch_embedding\n\ndef image2emb_conv(image,kernel,stride):\n    # image = bs,ic,image_h,image_w = 1,3,8,8 \n    # kernel = 8 \u00d7 3 \u00d7 4 \u00d7 4\n    # stride = patch_size = 4 \n    conv_output = F.conv2d(image,kernel,stride=stride) # bs*oc*oh*ow\n    # (h-k+2p+s)/s = (8-4+4)/4  = 2\n    # conv_output = 8 \u00d7 1 \u00d7 2 \u00d7 2\n    bs,oc,oh,ow = conv_output.shape\n    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n    # conv_output = 1 \u00d7 8 \u00d7 2 \u00d7 2\n    # reshape \uff1a 1 \u00d7 8 \u00d7 4\n    # transpose(-1,-2)  1 \u00d7 4 \u00d7 8\n    #\uff08\u8f93\u5165\u56fe\u7247 \u5212\u5206\u6210 4\u4e2apatch\uff0c\u6bcf\u4e2apatch\u7531\u539f\u6765\u7684 48\u4e2a\u50cf\u7d20\u8868\u793a\uff0c\u964d\u7ef4\u62108\u7ef4\u8868\u793a\uff09\n    return patch_embedding\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim) # model_dim\u662f\u8f93\u51fa\u901a\u9053\u6570\u76ee\uff0cpatch depth\u662f\u5377\u79ef\u6838\u7684\u9762\u79ef\u4e58\u4ee5\u8f93\u5165\u901a\u9053\u6570\n\n# \u5206\u5757\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)\n\n# conv\u7248\u672c\uff1a\nkernel = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size))\n# weight = 48 \u00d7 8\n# transpose(0,1) : 8 \u00d7 48\n# reshape :8 \u00d7 3 \u00d7 4 \u00d7 4\n\n# \u4e8c\u7ef4\u5377\u79ef\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_conv = image2emb_conv(image,kernel,patch_size)\n# image = bs,ic,image_h,image_w = 1,3,8,8 \n# kernel = 8 \u00d7 3 \u00d7 4 \u00d7 4\n# patch_size = 4\n\nprint(patch_embedding_naive)\nprint(patch_embedding_conv)\n</code></pre>"},{"location":"bagu/deeplearning/1/#2-cls-token-embedding","title":"2 CLS token embedding","text":"<pre><code># step2 prepend CLS token embedding\n# patch_embedding_conv = 1 \u00d7 4 \u00d7 8\n# cls_token_embedding = 1 \u00d7 1 \u00d7 8\n\ncls_token_embedding = torch.randn(bs,1,model_dim,requires_grad=True)\n# token_embedding\n# \u7b2c\u4e00\u4e2a\u4f4d\u7f6e \u662f cls token\uff0ccls token\u7684\u5d4c\u5165\u7ef4\u5ea6\u662f 8\n# \u6240\u4ee5 dim = 1\ntoken_embedding = torch.cat([cls_token_embedding,patch_embedding_conv],dim=1)\n</code></pre>"},{"location":"bagu/deeplearning/1/#3-position-embedding","title":"3 Position embedding","text":"<pre><code># step3 add position embedding\npositon_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=True)\nseq_len = token_embedding.shape[1]\npositon_embedding = torch.tile(positon_embedding_table[:seq_len],[token_embedding.shape[0],1,1])\ntoken_embedding += positon_embedding\n</code></pre> <p>\u6ce8\u91ca\uff1a</p> <pre><code># step3 add position embedding\n# max_num_token = 16\n# model_dim = 8\n# positon_embedding_table = 16,8\npositon_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=True)\n\n# token_embedding = 1,5,8 (bs,5\u4e2a\u4f4d\u7f6e(1\u4e2acls token\u30014\u4e2a\u5355\u8bcd),model_dim = 8)\n# seq_len = 5\nseq_len = token_embedding.shape[1]\n\npositon_embedding = torch.tile(positon_embedding_table[:seq_len],[token_embedding.shape[0],1,1])\n# positon_embedding_table[:seq_len] = positon_embedding_table[:5] \u53d6\u524d5\u4e2a8\u7ef4\n# [:5] \u8868\u793a \u5bf9 \u7b2c\u4e00\u7ef4 \u7d22\u5f15\n# positon_embedding_table[:seq_len] = 5,8\n# [token_embedding.shape[0],1,1] = [1,1,1]\n# positon_embedding = 1,5,8\ntoken_embedding += positon_embedding\n# token_embedding = 1,5,8\n</code></pre>"},{"location":"bagu/deeplearning/1/#4-transformer-encoder","title":"4 Transformer Encoder","text":"<pre><code># step4 Pass embedding to Transformer Encoder\n# d_model = model_dim = 8\nencoder_layer = nn.TransformerEncoderLayer(d_model=model_dim,nhead=8)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\n# token_embedding = 1,5,8(\u53ef\u4ee5\u7406\u89e3\u4e3a 5\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd \u5d4c\u5165 8\u4e2a\u7ef4\u5ea6)\nencoder_output = transformer_encoder(token_embedding)\n</code></pre>"},{"location":"bagu/deeplearning/1/#5-classification-head","title":"5 classification head","text":"<pre><code># step5 do classification\ncls_token_output = encoder_output[:,0,:]\nlinear_layer = nn.Linear(model_dim,num_classes)\nlogits = linear_layer(cls_token_output)\nloss_fn = nn.CrossEntropyLoss()\nloss = loss_fn(logits,label)\nprint(loss)\n</code></pre>"},{"location":"bagu/deeplearning/1/#6","title":"6 \u5168\u90e8\u4ee3\u7801","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    patch_embedding = patch @ weight\n    return patch_embedding\ndef image2emb_conv(image,kernel,stride):\n    conv_output = F.conv2d(image,kernel,stride=stride) # bs*oc*oh*ow\n    bs,oc,oh,ow = conv_output.shape\n    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n\n    return patch_embedding\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\nmax_num_token = 16\nnum_classes = 10\nlabel = torch.randint(10,(bs,))\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim) # model_dim\u662f\u8f93\u51fa\u901a\u9053\u6570\u76ee\uff0cpatch depth\u662f\u5377\u79ef\u6838\u7684\u9762\u79ef\u4e58\u4ee5\u8f93\u5165\u901a\u9053\u6570\n\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)  # \u5206\u5757\u65b9\u6cd5\u5f97\u5230embedding\nkernel = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size))   # oc*ic*kh*kw\n\npatch_embedding_conv = image2emb_conv(image,kernel,patch_size) # \u4e8c\u7ef4\u5377\u79ef\u65b9\u6cd5\u5f97\u5230embedding\n\n# print(patch_embedding_naive)\n# print(patch_embedding_conv)\n\n# step2 prepend CLS token embedding\ncls_token_embedding = torch.randn(bs,1,model_dim,requires_grad=True)\ntoken_embedding = torch.cat([cls_token_embedding,patch_embedding_conv],dim=1)\n\n# step3 add position embedding\npositon_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=True)\nseq_len = token_embedding.shape[1]\npositon_embedding = torch.tile(positon_embedding_table[:seq_len],[token_embedding.shape[0],1,1])\ntoken_embedding += positon_embedding\n\n# step4 Pass embedding to Transformer Encoder\nencoder_layer = nn.TransformerEncoderLayer(d_model=model_dim,nhead=8)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\nencoder_output = transformer_encoder(token_embedding)\n\n# step5 do classification\ncls_token_output = encoder_output[:,0,:]\nlinear_layer = nn.Linear(model_dim,num_classes)\nlogits = linear_layer(cls_token_output)\nloss_fn = nn.CrossEntropyLoss()\nloss = loss_fn(logits,label)\nprint(loss)\n</code></pre>"},{"location":"bagu/deeplearning/pytorch_shape_function/","title":"pytorch\u7684\u7ef4\u5ea6\u53d8\u6362\u51fd\u6570","text":""},{"location":"bagu/deeplearning/pytorch_shape_function/#_1","title":"\u7ef4\u5ea6\u8f6c\u6362\u51fd\u6570","text":"<ol> <li><code>torch.unsqueeze(input, dim)</code>\uff1a\u5728\u6307\u5b9a\u7ef4\u5ea6 <code>dim</code> \u4e0a\u589e\u52a0\u4e00\u4e2a\u65b0\u7684\u7ef4\u5ea6\u3002\u5982\u679c <code>dim</code> \u5df2\u7ecf\u5b58\u5728\uff0c\u5219\u5728\u5176\u524d\u9762\u6dfb\u52a0\u65b0\u7684\u7ef4\u5ea6\u3002</li> <li><code>torch.squeeze(input, dim=None)</code>\uff1a\u79fb\u9664\u6240\u6709\u957f\u5ea6\u4e3a1\u7684\u7ef4\u5ea6\u3002\u5982\u679c\u6307\u5b9a\u4e86 <code>dim</code>\uff0c\u5219\u53ea\u79fb\u9664\u8be5\u7ef4\u5ea6\u3002</li> <li><code>torch.flatten(input, start_dim=0, end_dim=-1)</code>\uff1a\u5c06\u8f93\u5165\u5f20\u91cf\u4ece <code>start_dim</code> \u5230 <code>end_dim</code> \u7684\u6240\u6709\u7ef4\u5ea6\u5c55\u5e73\u3002</li> <li><code>torch.view(input, size)</code> \u6216 <code>input.view(size)</code>\uff1a\u91cd\u65b0\u8c03\u6574\u5f20\u91cf\u7684\u5f62\u72b6\uff0c\u4e0d\u6539\u53d8\u6570\u636e\u3002</li> <li><code>torch.reshape(input, shape)</code>\uff1a\u4e0e <code>view</code> \u7c7b\u4f3c\uff0c\u7528\u4e8e\u6539\u53d8\u5f20\u91cf\u7684\u5f62\u72b6\uff0c\u4f46 <code>reshape</code> \u53ef\u4ee5\u5904\u7406\u66f4\u590d\u6742\u7684\u7ef4\u5ea6\u53d8\u6362\uff0c\u5982\u589e\u52a0\u6216\u51cf\u5c11\u7ef4\u5ea6\u3002</li> <li><code>torch.permute(input, dims)</code>\uff1a\u91cd\u65b0\u6392\u5217\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6\uff0c<code>dims</code> \u662f\u4e00\u4e2a\u7ef4\u5ea6\u7d22\u5f15\u7684\u5143\u7ec4\u3002</li> <li><code>torch.transpose(input, dim0, dim1)</code>\uff1a\u4ea4\u6362\u8f93\u5165\u5f20\u91cf\u7684\u4e24\u4e2a\u7ef4\u5ea6\u3002</li> <li><code>torch.expand(input, size)</code>\uff1a\u5c06\u8f93\u5165\u5f20\u91cf\u6cbf\u6307\u5b9a\u7684\u7ef4\u5ea6\u590d\u5236\u6269\u5c55\u3002</li> <li><code>torch.cat(tensors, dim)</code>\uff1a\u6cbf\u6307\u5b9a\u7ef4\u5ea6 <code>dim</code> \u8fde\u63a5\u591a\u4e2a\u5f20\u91cf\u3002</li> <li><code>torch.stack(tensors, dim)</code>\uff1a\u6cbf\u65b0\u7684\u7ef4\u5ea6 <code>dim</code> \u5806\u53e0\u591a\u4e2a\u5f20\u91cf\uff0c\u4e0e <code>cat</code> \u4e0d\u540c\u7684\u662f\uff0c<code>stack</code> \u4f1a\u589e\u52a0\u4e00\u4e2a\u65b0\u7684\u7ef4\u5ea6\u3002</li> <li> <p><code>torch.reapeat</code></p> </li> <li> <p><code>torch.tile</code></p> </li> </ol> <pre><code>positon_embedding = torch.tile(positon_embedding_table[:seq_len],[token_embedding.shape[0],1,1])\n# positon_embedding_table[:seq_len] = positon_embedding_table[:5] \u53d6\u524d5\u4e2a8\u7ef4\n# [:5] \u8868\u793a \u5bf9 \u7b2c\u4e00\u7ef4 \u7d22\u5f15\n# positon_embedding_table[:seq_len] = 5,8\n# [token_embedding.shape[0],1,1] = [1,1,1]\n# positon_embedding = 1,5,8\n</code></pre>"},{"location":"bagu/deeplearning/pytorch_shape_function/#_2","title":"\u7406\u89e3\u5f20\u91cf","text":"<p>\u5047\u5982\u4f60\u6709\u4e00\u4e2a\u7bee\u5b50\uff0c\u91cc\u9762\u88c5\u6ee1\u4e86\u5404\u79cd\u989c\u8272\u7684\u5c0f\u7403\u3002\u6bcf\u4e2a\u5c0f\u7403\u4ee3\u8868\u4e00\u4e2a\u6570\u5b57\u3002\u73b0\u5728\uff0c\u5982\u679c\u6211\u4eec\u60f3\u628a\u8fd9\u4e9b\u5c0f\u7403\u6309\u7167\u4e00\u5b9a\u7684\u987a\u5e8f\u6392\u5217\uff0c\u6bd4\u5982\u4e00\u884c\u6216\u8005\u4e00\u5217\uff0c\u8fd9\u5c31\u662f\u4e00\u4e2a\u4e00\u7ef4\u6570\u7ec4\u3002\u5982\u679c\u4f60\u628a\u51e0\u884c\u8fd9\u6837\u7684\u5c0f\u7403\u6392\u5217\u8d77\u6765\uff0c\u5c31\u5f62\u6210\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u6570\u7ec4\uff0c\u5c31\u50cf\u4e00\u4e2a\u8868\u683c\u4e00\u6837\u3002\u5982\u679c\u4f60\u518d\u628a\u8fd9\u4e9b\u8868\u683c\u5806\u53e0\u8d77\u6765\uff0c\u5c31\u5f62\u6210\u4e86\u4e00\u4e2a\u4e09\u7ef4\u6570\u7ec4\u3002\u5728PyTorch\u4e2d\uff0c\u5f20\u91cf\u5c31\u662f\u4e00\u79cd\u7528\u6765\u8868\u793a\u8fd9\u4e9b\u4e0d\u540c\u7ef4\u5ea6\u6570\u7ec4\u7684\u6570\u636e\u7ed3\u6784\u3002</p> <p></p>"},{"location":"bagu/deeplearning/former/1/","title":"\u7a7a","text":""},{"location":"bagu/deeplearning/former/2/","title":"\u7a7a","text":""},{"location":"bagu/deeplearning/former/transformer/","title":"\u624b\u6495Transformer\u4ee3\u7801","text":""},{"location":"bagu/deeplearning/former/transformer/#_1","title":"\u81ea\u6ce8\u610f\u529b\u673a\u5236","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self,model_dim,dropout=0.1):\n\n        super().__init__()\n        self.model_dim = model_dim\n\n        self.q_proj = nn.Linear(model_dim,model_dim)\n        self.k_proj = nn.Linear(model_dim,model_dim)\n        self.v_proj = nn.Linear(model_dim,model_dim)\n\n        self.o_proj = nn.Linear(model_dim,model_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self,x,mask=None):\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        scores = torch.matmul(q,k.transpose(-1,-2))//math.sqrt(self.model_dim)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask==0,-1e09)\n\n        prob = F.softmax(scores,dim=-1)\n\n        prob = self.dropout(prob)\n\n        attn_weights = torch.matmul(prob,v)\n\n        output = self.o_proj(attn_weights)\n\n        return output\n\nmodel_dim = 512\nseq_len = 8\nbatch_size = 2\n# mask shape = seq_len * model_dim\nx = torch.randn(batch_size,seq_len,model_dim)\n\nsa = SelfAttention(model_dim)\n\nattn_weights = sa(x)\nprint(attn_weights.shape)  # \u8f93\u51fa\u7684\u5f62\u72b6\u5e94\u8be5\u662f(batch_size, seq_len, model_dim)\n</code></pre>"},{"location":"bagu/deeplearning/former/transformer/#_2","title":"\u624b\u6495\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self,model_dim,num_heads,dropout=0.1):\n        super().__init__()\n        self.model_dim = model_dim\n        self.num_heads = num_heads\n        self.head_dim = model_dim // num_heads\n\n        self.q_proj = nn.Linear(model_dim,model_dim)\n        self.k_proj = nn.Linear(model_dim,model_dim)\n        self.v_proj = nn.Linear(model_dim,model_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n        self.o_proj = nn.Linear(model_dim,model_dim)\n\n    def forward(self,q,k,v,mask=None):\n\n        batch_size,sequence_length,model_dim = q.shape\n\n        q = self.q_proj(q).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)\n        k = self.k_proj(k).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)\n        v = self.v_proj(v).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)\n\n        scores = torch.matmul(q,k.transpose(-1,-2))//math.sqrt(self.head_dim)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask==0,-1e09)\n\n        prob = F.softmax(scores,dim=-1)\n\n        prob = self.dropout(prob)\n\n        attn_weights = torch.matmul(prob,v).transpose(1,2).contiguous().view(batch_size,sequence_length,model_dim)\n\n        output = self.o_proj(attn_weights)\n        return output\n\nmodel_dim = 512 \nnum_heads = 8\nmha = MultiHeadAttention(model_dim=model_dim, num_heads=num_heads, dropout=0.1)\nbatch_size = 10\nsequence_length = 60\nq = torch.randn(batch_size, sequence_length, model_dim)\nk = torch.randn(batch_size, sequence_length, model_dim)\nv = torch.randn(batch_size, sequence_length, model_dim)\n\nmask = None\noutput = mha(q, k, v, mask)\nprint(output.shape)  # \u8f93\u51fa\u7684\u5f62\u72b6\u5e94\u8be5\u662f(batch_size, sequence_length, model_dim)\n</code></pre> <p>note\uff1a</p> <ul> <li> <p>\u5047\u8bbe\u8f93\u5165\u6570\u636eq, k, v\u7684\u5f62\u72b6\u662f(batch_size, sequence_length, model_dim)</p> </li> <li> <p>\u4f8b\u5982\uff0c\u4e00\u4e2a\u6279\u6b21\u5927\u5c0f\u4e3a10\uff0c\u5e8f\u5217\u957f\u5ea6\u4e3a60\uff0c\u6a21\u578b\u7ef4\u5ea6\u4e3a512\u7684\u8f93\u5165 </p> </li> <li> <p>\u8fd9\u662f\u56e0\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u7684\u76ee\u7684\u662f\u4e3a\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u8868\u793a\uff0c   \u800c\u4e0d\u662f\u751f\u6210\u4e00\u4e2a\u5e8f\u5217\u957f\u5ea6\u4e3a sequence_length \u7684\u5e8f\u5217\u3002</p> </li> </ul> <p>\u200b   \u6ce8\u610f\u529b\u673a\u5236\u4e3a\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u8868\u793a\uff0c\u8fd9\u4e2a\u8868\u793a\u7efc\u5408\u4e86\u5e8f\u5217\u4e2d\u6240\u6709\u5143\u7d20\u7684\u4fe1\u606f\uff0c</p> <p>\u200b   \u4f46\u8f93\u51fa\u7684\u5f62\u72b6\u4ecd\u7136\u662f (batch_size, sequence_length, model_dim)\uff0c</p> <p>\u200b   \u800c\u4e0d\u662f (batch_size, sequence_length, sequence_length)\u3002</p> <p>\u200b   \u8fd9\u662f\u56e0\u4e3a\u8f93\u51fa\u7684\u6bcf\u4e2a\u5143\u7d20\u662f\u5e8f\u5217\u4e2d\u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u5728\u6240\u6709\u5934\u4e2d\u7684\u52a0\u6743\u8868\u793a\uff0c \u200b   \u800c\u4e0d\u662f\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5143\u7d20\u5bf9\u5176\u4ed6\u5143\u7d20\u7684\u6ce8\u610f\u529b\u6743\u91cd\u77e9\u9635\u3002</p>"},{"location":"bagu/deeplearning/former/transformer/#_3","title":"\u624b\u6495\u4f4d\u7f6e\u7f16\u7801","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n</code></pre>"},{"location":"bagu/deeplearning/former/transformer/#_4","title":"\u4e00\u7ef4\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801","text":"<ul> <li>\u7c7b\u5199\u6cd5</li> <li>\u51fd\u6570\u5199\u6cd5</li> </ul> <pre><code>class SinCosPositionEmbedding(nn.Module):\n    def __init__(self, max_sequence_length,model_dim):\n        super().__init__()\n        self.max_sequence_length = max_sequence_length\n        self.model_dim = model_dim\n    def forward(self):\n        pe = torch.zeros(self.max_sequence_length,self.model_dim)\n        pos_mat = torch.arange(self.max_sequence_length).reshape(-1,1)\n        i_mat = torch.pow(10000,\n                          torch.arange(0,self.model_dim,2).reshape(1,-1)/self.model_dim\n                          )\n\n        pe[:,0::2] = torch.sin(pos_mat/i_mat)\n        pe[:,1::2] = torch.cos(pos_mat/i_mat)\n\n        return pe\nprint(SinCosPositionEmbedding(max_sequence_length=8,model_dim=4).forward())\n</code></pre> <p>\u51fd\u6570\u5199\u6cd5</p> <pre><code>def create_1d_absolute_sincos_embeddings(max_sequence_length,model_dim):\n    assert model_dim%2 == 0,\"wrong dimension\"\n    pe_table = torch.zeros(max_sequence_length,model_dim)\n    pos_mat = torch.arange(max_sequence_length).reshape(-1,1)\n    i_mat = torch.pow(\n        10000,\n        torch.arange(0,model_dim,2)/model_dim\n    )\n    pe_table[:,0::2]=torch.sin(pos_mat/i_mat)\n    pe_table[:,1::2]=torch.cos(pos_mat/i_mat)\n    return pe_table\n\n# Transformer\u8bba\u6587 \u4e00\u7ef4\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\nif __name__==\"__main__\":\n    max_sequence_length = 8\n    model_dim = 4\n    pe_table = create_1d_absolute_sincos_embeddings(max_sequence_length,model_dim)\n    print(pe_table)\n</code></pre>"},{"location":"bagu/deeplearning/former/transformer/#_5","title":"\u4e00\u7ef4\u53ef\u5b66\u4e60\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801","text":"<p>from vit</p> <p>\u7c7b\u5199\u6cd5\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nclass TrainablePositionEncoding(nn.Module):\n    def __init__(self, max_sequence_length, d_model):\n        super().__init__() \n        self.max_sequence_length = max_sequence_length\n        self.d_model = d_model\n\n    def forward(self):\n        pe = nn.Embedding(self.max_sequence_length, self.d_model)\n        nn.init.constant_(pe.weight, 0.)\n        return pe\nmax_sequence_length = 100\nd_model = 512\ntrainable_pe = TrainablePositionEncoding(max_sequence_length, d_model)\nposition_encodings = trainable_pe()\nprint(position_encodings.weight.shape)  # \u8f93\u51fa\uff1atorch.Size([100, 512])\n</code></pre> <p>\u51fd\u6570\u5199\u6cd5\uff1a</p> <pre><code>def create_1d_absolute_trainable_embeddings(max_sequence_length,model_dim):\n    pe = nn.Embedding(max_sequence_length,model_dim)\n    nn.init.constant_(pe.weight,0.)\n\n    return pe\n</code></pre>"},{"location":"bagu/deeplearning/former/transformer/#_6","title":"\u4e8c\u7ef4\u76f8\u5bf9\u53ef\u5b66\u4e60\u4f4d\u7f6e\u7f16\u7801","text":"<p>from SwinTransformer</p> <pre><code>def create_2d_relative_bias_trainable_embeddings(n_head,height,width,dim):\n    # width:5,[0,1,2,3,4],bias=[-width+1,width-1],2*width-1\n    # height:5,[0,1,2,3,4],bias=[-height+1,height-1],2*height-1\n\n    position_embedding = nn.Embedding((2*width-1)*(2*height-1),n_head)\n    nn.init.constant_(position_embedding.weight,0.)\n\n    def get_relative_position_index(height,width):\n        m1,m2 = torch.meshgrid(torch.arange(height),torch.arange(width))\n        coords = torch.stack(m1,m2) #[2,height,width]\n        coords_flatten = torch.flatten(coords,1) #[2,height*width]\n\n        # \u628a\u504f\u5dee\u53d8\u6210\u6b63\u6570\uff0c\u7136\u540e\u4eceposition_embedding\u4e2d\u6309\u7d22\u5f15\u53d6\u503c\n        relative_coords_bias = coords_flatten[:,:,None]-coords_flatten[:,None,:] # [2,height*width,height*width]\n\n        relative_coords_bias[0,:,:] += height-1\n        relative_coords_bias[1,:,:] += width-1\n\n        # A:2d,B:1d,B[[i*cols+j] = A[i,j]\n        relative_coords_bias[0,:,:] *= relative_coords_bias[1,:,:].max()+1\n\n        return relative_coords_bias.sum(0) # [height*width,height*width]\n    relative_position_bias = get_relative_position_index(height,width)\n    bias_embedding = position_embedding(torch.flatten(relative_position_bias)).reshape(height*width,height*width,n_head) #[height*width,height*width,n_head]\n\n    bias_embedding = position_embedding.permute(2,0,1).unsqueeze(0) # [1,n_head,height*width,height*width]\n\n    return bias_embedding\n</code></pre>"},{"location":"bagu/deeplearning/former/transformer/#_7","title":"\u4e8c\u7ef4\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801","text":"<p>from MAE</p> <pre><code># 4.2d absolute constant sincos embedding\n# Masked AutoEncoder \u8bba\u6587\ndef create_2d_absolute_sincos_embeddings(height,width,dim):\n    assert dim%4 ==0,\"wrong dimension!\"\n    position_embedding = torch.zeros(height*width,dim)\n    m1,m2 = torch.meshgrid(torch.arrange(height,dtype=torch.float),torch.arrange(width,dtype=torch.float))\n    coords = torch.stack(m1,m2)  # [2,height*width]\n\n    height_embedding = create_1d_absolute_sincos_embeddings(torch.flatten(coords[0]),dim//2)  # [height*width,dim//2]\n    width_embedding = create_1d_absolute_sincos_embeddings(torch.flatten(coords[1]),dim//2)  # [height*width,dim//2]\n\n    position_embedding[:,:dim//2] = height_embedding\n    position_embedding[:,:dim//2] = width_embedding\n\n    return position_embedding\n</code></pre>"},{"location":"bagu/machinelearning/2/","title":"\u624b\u6495\u53cd\u5411\u4f20\u64ad","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\ndef init_parameters(layers_dim):\n    L = len(layers_dim)\n    parameters ={}\n    for i in range(1,L):\n        parameters[\"w\"+str(i)] = np.random.random([layers_dim[i],layers_dim[i-1]])\n        parameters[\"b\"+str(i)] = np.zeros((layers_dim[i],1))\n    return parameters\n\ndef sigmoid(z):\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n        return sigmoid(z) * (1-sigmoid(z))\n\ndef forward(x,parameters):\n    a = []\n    z = []\n    caches = {}\n    a.append(x)\n    z.append(x)\n    layers = len(parameters)//2\n    for i in range(1,layers):\n        z_temp =parameters[\"w\"+str(i)].dot(x) + parameters[\"b\"+str(i)]\n        z.append(z_temp)\n        a.append(sigmoid(z_temp))\n    z_temp = parameters[\"w\"+str(layers)].dot(a[layers-1]) + parameters[\"b\"+str(layers)]\n    z.append(z_temp)\n    a.append(z_temp)\n\n    caches[\"z\"] = z\n    caches[\"a\"] = a    \n    return  caches,a[layers]\n\ndef backward(parameters,caches,al,y):\n    layers = len(parameters)//2\n    grades = {}\n    m = y.shape[1]\n    grades[\"dz\"+str(layers)] = al - y\n    grades[\"dw\"+str(layers)] = grades[\"dz\"+str(layers)].dot(caches[\"a\"][layers-1].T) /m\n    grades[\"db\"+str(layers)] = np.sum(grades[\"dz\"+str(layers)],axis = 1,keepdims = True) /m\n    for i in reversed(range(1,layers)):\n        grades[\"dz\"+str(i)] = parameters[\"w\"+str(i+1)].T.dot(grades[\"dz\"+str(i+1)]) * sigmoid_prime(caches[\"z\"][i])\n        grades[\"dw\"+str(i)] = grades[\"dz\"+str(i)].dot(caches[\"a\"][i-1].T)/m\n        grades[\"db\"+str(i)] = np.sum(grades[\"dz\"+str(i)],axis = 1,keepdims = True) /m\n    return grades   \n\ndef update_grades(parameters,grades,learning_rate):\n    layers = len(parameters)//2\n    for i in range(1,layers+1):\n        parameters[\"w\"+str(i)] -= learning_rate * grades[\"dw\"+str(i)]\n        parameters[\"b\"+str(i)] -= learning_rate * grades[\"db\"+str(i)]\n    return parameters\n\ndef compute_loss(al,y):\n    return np.mean(np.square(al-y))\n\ndef load_data():\n    x = np.arange(0.0,1.0,0.01)\n    y =20* np.sin(2*np.pi*x)\n    plt.scatter(x,y)\n    return x,y\n\nx,y = load_data()\nx = x.reshape(1,100)\ny = y.reshape(1,100)\nplt.scatter(x,y)\nparameters = init_parameters([1,25,1])\nal = 0\nfor i in range(4000):\n    caches,al = forward(x, parameters)\n    grades = backward(parameters, caches, al, y)\n    parameters = update_grades(parameters, grades, learning_rate= 0.3)\n    if i %100 ==0:\n        print(compute_loss(al, y))\nplt.scatter(x,al)\nplt.show()\n</code></pre> <p>\u6ce8\u91ca</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\ndef init_parameters(layers_dim):\n    # \u5b9a\u4e49\u4e00\u4e2a\u540d\u4e3a init_parameters \u7684\u51fd\u6570\uff0c\n    # \u5b83\u63a5\u6536\u4e00\u4e2a\u53c2\u6570 layers_dim\uff0c\u8fd9\u662f\u4e00\u4e2a\u5217\u8868\uff0c\u5305\u542b\u4e86\u6bcf\u4e00\u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\u3002\n    # print(layers_dim)  # [1, 25, 1] \n    # \u8f93\u5165\u5c42\u6709 1 \u4e2a\u795e\u7ecf\u5143\uff0c\u9690\u85cf\u5c42\u6709 25 \u4e2a\u795e\u7ecf\u5143\uff0c\u8f93\u51fa\u5c42\u6709 1 \u4e2a\u795e\u7ecf\u5143\n    L = len(layers_dim) # \u83b7\u53d6\u5c42\u7684\u6570\u91cf\n    parameters ={} # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8\u6bcf\u4e00\u5c42\u7684\u53c2\u6570\n    for i in range(1,L): # \u4ece\u7b2c1\u5c42\u5f00\u59cb\u904d\u5386\u5230\u5012\u6570\u7b2c\u4e8c\u5c42\n        # \u521d\u59cb\u5316\u6743\u91cd w1 w2  \u7b2c\u4e00\u5c42\u8f93\u5165\u5c42 \u4e0d\u521d\u59cb\u5316\n        '''\n        - parameters[\"w\"+str(i)] \u521d\u59cb\u5316\u6743\u91cd\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a [layers_dim[i], layers_dim[i-1]]\n          \u8fd9\u8868\u793a\u7b2c i \u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684\u5f62\u72b6\uff0c\u5176\u4e2d layers_dim[i] \u662f\u7b2c i \u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\uff0clayers_dim[i-1] \u662f\u524d\u4e00\u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\u3002\n        - parameters[\"b\"+str(i)] \u521d\u59cb\u5316\u504f\u7f6e\u9879\uff0c\u5f62\u72b6\u4e3a [layers_dim[i], 1]\n          \u8fd9\u8868\u793a\u7b2c i \u5c42\u7684\u504f\u7f6e\u9879\u7684\u5f62\u72b6\uff0c\u5176\u4e2d layers_dim[i] \u662f\u7b2c i \u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\u3002\n        '''\n        parameters[\"w\"+str(i)] = np.random.random([layers_dim[i],\n                                                   layers_dim[i-1]])\n        # \u521d\u59cb\u5316\u504f\u7f6e\u9879 b1 b2 \u7b2c0\u5c42\u8f93\u5165\u5c42 \u4e0d\u521d\u59cb\u5316\n        parameters[\"b\"+str(i)] = np.zeros((layers_dim[i],1))\n    # \u8fd4\u56de\u5305\u542b\u6240\u6709\u53c2\u6570\u7684\u5b57\u5178\n    # print(parameters['w1'].shape)  (25, 1)\n    '''\n        parameters['w1'].shape \u662f (25, 1)\uff1a\n        \u8868\u793a\u4ece\u8f93\u5165\u5c42\u5230\u7b2c 1 \u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684\u5f62\u72b6\u3002\n        \u8f93\u5165\u5c42\u6709 1 \u4e2a\u795e\u7ecf\u5143\uff0c\u7b2c 1 \u5c42\u6709 25 \u4e2a\u795e\u7ecf\u5143\uff0c\n        \u56e0\u6b64\u6743\u91cd\u77e9\u9635\u7684\u5f62\u72b6\u662f (25, 1)\u3002\n    '''\n    # print(parameters['w2'].shape)  (1, 25)\n    '''\n        parameters['w2'].shape \u662f (1, 25)\uff1a\n        \u8868\u793a\u4ece\u7b2c 1 \u5c42\u5230\u8f93\u51fa\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684\u5f62\u72b6\u3002\n        \u7b2c 1 \u5c42\u6709 25 \u4e2a\u795e\u7ecf\u5143\uff0c\u8f93\u51fa\u5c42\u6709 1 \u4e2a\u795e\u7ecf\u5143\uff0c\n        \u56e0\u6b64\u6743\u91cd\u77e9\u9635\u7684\u5f62\u72b6\u662f (1, 25)\u3002\n    '''\n    # print(parameters['b1'].shape)  (25, 1)\n    '''\n        parameters['b1'].shape \u662f (25, 1)\uff1a\n        \u8868\u793a\u7b2c 1 \u5c42\u7684\u504f\u7f6e\u9879\u7684\u5f62\u72b6\u3002\n        \u7b2c 1 \u5c42\u6709 25 \u4e2a\u795e\u7ecf\u5143\uff0c\n        \u56e0\u6b64\u504f\u7f6e\u9879\u7684\u5f62\u72b6\u662f (25, 1)\u3002\n    '''\n    # print(parameters['b2'].shape)  (1, 1)\n    '''\n        parameters['b2'].shape \u662f (1, 1)\uff1a\n        \u8868\u793a\u8f93\u51fa\u5c42\u7684\u504f\u7f6e\u9879\u7684\u5f62\u72b6\u3002\n        \u8f93\u51fa\u5c42\u6709 1 \u4e2a\u795e\u7ecf\u5143\uff0c\n        \u56e0\u6b64\u504f\u7f6e\u9879\u7684\u5f62\u72b6\u662f (1, 1)\u3002'''\n\n    '''\n        \u6743\u91cd\u77e9\u9635\u7684\u5f62\u72b6\u662f [\u5f53\u524d\u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf, \u524d\u4e00\u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf]\u3002\n        \u504f\u7f6e\u9879\u7684\u5f62\u72b6\u662f [\u5f53\u524d\u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf, 1]\u3002'''\n\n    return parameters\n\n\ndef sigmoid(z):\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n        return sigmoid(z) * (1-sigmoid(z))\n\ndef forward(x,parameters):\n    # forward \u51fd\u6570\u63a5\u6536\u4e24\u4e2a\u53c2\u6570\uff1a\u8f93\u5165\u6570\u636e x \u548c\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570 parameters\n    # a \u548c z \u662f\u4e24\u4e2a\u5217\u8868\uff0c\u5206\u522b\u7528\u4e8e\u5b58\u50a8\u6bcf\u4e00\u5c42\u7684\u6fc0\u6d3b\u503c\u548c\u7ebf\u6027\u53d8\u6362\u503c\n    # caches \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8 a \u548c z \u5217\u8868\uff0c\u4ee5\u4fbf\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u4f7f\u7528\n    # a.append(x) \u548c z.append(x) \u5c06\u8f93\u5165\u6570\u636e x \u6dfb\u52a0\u5230 a \u548c z \u5217\u8868\u4e2d\uff0c\u4f5c\u4e3a\u7b2c 0 \u5c42\u7684\u6fc0\u6d3b\u503c\u548c\u7ebf\u6027\u53d8\u6362\u503c\n    # layers \u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u6570\uff0c\u5047\u8bbe parameters \u5b57\u5178\u4e2d\u5305\u542b\u6bcf\u4e00\u5c42\u7684\u6743\u91cd\u548c\u504f\u7f6e\u9879\uff0c\u56e0\u6b64\u5c42\u6570\u4e3a len(parameters) // 2\n    a = []\n    z = []\n    caches = {}\n    a.append(x)\n    z.append(x)\n    layers = len(parameters)//2  \n    # \u56e0\u4e3a\u5373\u6709w\u53c8\u6709b\uff0c\u6240\u4ee5\u9664\u4ee52  len(parameters) = 4\n    # print(layers)  # 2\n    for i in range(1,layers):\n        # \u8fd9\u6bb5\u4ee3\u7801\u904d\u5386\u4ece\u7b2c 1 \u5c42\u5230\u5012\u6570\u7b2c\u4e8c\u5c42\u7684\u6240\u6709\u5c42\n        # \u7b2c0\u5c42\u8f93\u5165\u5c42 \u4e0d\u8fdb\u884c\u8ba1\u7b97\n        # \u5982\u679c\u662f3\u5c42\u7684\u8bdd\uff0c\u7b2c0\u5c42 input  \u7b2c1\u5c42 w1\u3001b1  \u7b2c2\u5c42 w2\u3001b2\n        # \u6240\u4ee5\u8fd9\u4e2a for\u5faa\u73af \u904d\u5386\u4e0d\u5230 \u8f93\u51fa\u5c42\n        z_temp =parameters[\"w\"+str(i)].dot(x) + parameters[\"b\"+str(i)]\n        # print(\"w\"+str(i),\"b\"+str(i))  # w1 b1\n        z.append(z_temp)\n        a.append(sigmoid(z_temp))\n    z_temp = parameters[\"w\"+str(layers)].dot(a[layers-1]) + parameters[\"b\"+str(layers)]\n    # \u8fd9\u8fb9 \u53ea\u67093\u5c42\uff0c\u53ef\u4ee5\u76f4\u63a5\u5199\uff0c\u9690\u542b\u5c42\u548c\u8f93\u5165\u5c42\u70b9\u4e58\uff1b\u6700\u540e\u4e00\u5c42\u548c\u524d\u4e00\u5c42\u70b9\u4e58\n    # \u5982\u679c\u518d\u589e\u52a0\u4e00\u5c42\uff0c\u8fd9\u4e2a\u4ee3\u7801\u662f\u6709\u70b9\u95ee\u9898\u7684\uff1aparameters[\"w\"+str(i)].dot(x)\n    # \u5982\u679c\u5f88\u591a\u5c42 \u901a\u7528\u7684\u8bdd \u5e94\u8be5\u662f.dot(a[layers-1])\n    z.append(z_temp)\n    a.append(z_temp) \n    # \u6700\u540e\u4e00\u5c42\u7684\u6fc0\u6d3b\u503c\u76f4\u63a5\u4f7f\u7528\u7ebf\u6027\u53d8\u6362\u503c z_temp\uff0c\u4e0d\u4f7f\u7528 sigmoid \u6fc0\u6d3b\u51fd\u6570\u3002\n    # \u5c06 z_temp \u6dfb\u52a0\u5230 a \u5217\u8868\u4e2d\u3002\n\n    caches[\"z\"] = z\n    caches[\"a\"] = a \n    # \u5c06 z \u548c a \u5217\u8868\u5b58\u50a8\u5728 caches \u5b57\u5178\u4e2d\uff0c\u952e\u5206\u522b\u4e3a \"z\" \u548c \"a\"\u3002\n    # \u8fd4\u56de caches \u5b57\u5178\u548c\u6700\u540e\u4e00\u5c42\u7684\u6fc0\u6d3b\u503c a[layers]\u3002\n    return  caches,a[layers]\n\ndef backward(parameters,caches,al,y):\n    # backward \u51fd\u6570\u63a5\u6536\u56db\u4e2a\u53c2\u6570\uff1a\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570 parameters\u3001\n    # \u524d\u5411\u4f20\u64ad\u7684\u7f13\u5b58 caches\u3001\n    # \u524d\u5411\u4f20\u64ad\u7684\u8f93\u51fa al \u548c\u771f\u5b9e\u6807\u7b7e y\u3002\n    layers = len(parameters)//2\n    # layers \u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u6570\uff0c\n    # \u5047\u8bbe parameters \u5b57\u5178\u4e2d\u5305\u542b\u6bcf\u4e00\u5c42\u7684\u6743\u91cd\u548c\u504f\u7f6e\u9879\uff0c\n    # \u56e0\u6b64\u5c42\u6570\u4e3a len(parameters) // 2\u3002\n    grades = {}\n    # grades \u662f\u4e00\u4e2a\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8\u6bcf\u4e00\u5c42\u7684\u68af\u5ea6\u3002\n    m = y.shape[1]\n    # print(y.shape) (1, 100)\n    # m \u662f\u6837\u672c\u6570\u91cf\uff0c\u5373 y \u7684\u5217\u6570\u3002\n    grades[\"dz\"+str(layers)] = al - y\n    # print(\"dz\"+str(layers))  dz2\n    # al \u662f\u524d\u5411\u4f20\u64ad\u5f97\u5230\u7684\u8f93\u51fa\u5c42\u7684\u6fc0\u6d3b\u503c\uff08\u9884\u6d4b\u503c\uff09\u3002\n    # y \u662f\u771f\u5b9e\u6807\u7b7e\u3002\n    # dz \u8868\u793a\u8f93\u51fa\u5c42\u7684\u8bef\u5dee\uff0c\u8ba1\u7b97\u516c\u5f0f\u4e3a dz = al - y\u3002\n    # \u8fd9\u4e2a\u516c\u5f0f\u8868\u793a\u9884\u6d4b\u503c\u4e0e\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5373\u8bef\u5dee\u3002\n    grades[\"dw\"+str(layers)] = grades[\"dz\"+str(layers)].dot(caches[\"a\"][layers-1].T) /m\n    # print(\"dw\"+str(layers)) dw2\n    # \u8ba1\u7b97\u8f93\u51fa\u5c42\u7684\u6743\u91cd\u68af\u5ea6 dw\uff1a\n    # grades[\"dz\" + str(layers)] \u662f\u8f93\u51fa\u5c42\u7684\u8bef\u5dee\u3002\n    # caches[\"a\"][layers - 1] \u662f\u524d\u4e00\u5c42\u7684\u6fc0\u6d3b\u503c\u3002\n    # m \u662f\u6837\u672c\u6570\u91cf\n    # dw \u8868\u793a\u8f93\u51fa\u5c42\u7684\u6743\u91cd\u68af\u5ea6\uff0c\u8ba1\u7b97\u516c\u5f0f\u4e3a dw = dz.dot(a_prev.T) / m\uff0c\n    # \u5176\u4e2d a_prev \u662f\u524d\u4e00\u5c42\u7684\u6fc0\u6d3b\u503c\u3002\n    # \u8fd9\u4e2a\u516c\u5f0f\u8868\u793a\u8bef\u5dee\u4e0e\u524d\u4e00\u5c42\u6fc0\u6d3b\u503c\u7684\u70b9\u79ef\uff0c\u7136\u540e\u9664\u4ee5\u6837\u672c\u6570\u91cf\uff0c\u5f97\u5230\u6743\u91cd\u7684\u5e73\u5747\u68af\u5ea6\u3002\n    grades[\"db\"+str(layers)] = np.sum(grades[\"dz\"+str(layers)],axis = 1,keepdims = True) /m\n    # print(\"db\"+str(layers))  db2\n    # \u8ba1\u7b97\u8f93\u51fa\u5c42\u7684\u504f\u7f6e\u68af\u5ea6 db\n    # \u53cd\u5411\u4f20\u64ad \u4ece\u8f93\u51fa\u5c42\u5f00\u59cb\n    # grades[\"dz\" + str(layers)] \u662f\u8f93\u51fa\u5c42\u7684\u8bef\u5dee\u3002\n    # np.sum(grades[\"dz\" + str(layers)], axis=1, keepdims=True) \n    # \u8ba1\u7b97\u8bef\u5dee\u5728\u6837\u672c\u7ef4\u5ea6\u4e0a\u7684\u603b\u548c\u3002\n    # m \u662f\u6837\u672c\u6570\u91cf\u3002\n    # db \u8868\u793a\u8f93\u51fa\u5c42\u7684\u504f\u7f6e\u68af\u5ea6\uff0c\n    # \u8ba1\u7b97\u516c\u5f0f\u4e3a db = np.sum(dz, axis=1, keepdims=True) / m\u3002\n    # \u8fd9\u4e2a\u516c\u5f0f\u8868\u793a\u8bef\u5dee\u5728\u6837\u672c\u7ef4\u5ea6\u4e0a\u7684\u5e73\u5747\u503c\uff0c\u5f97\u5230\u504f\u7f6e\u7684\u5e73\u5747\u68af\u5ea6\u3002\n    # dz \u8868\u793a\u8f93\u51fa\u5c42\u7684\u8bef\u5dee\uff0c\u8ba1\u7b97\u516c\u5f0f\u4e3a dz = al - y\u3002\n    # dw \u8868\u793a\u8f93\u51fa\u5c42\u7684\u6743\u91cd\u68af\u5ea6\uff0c\u8ba1\u7b97\u516c\u5f0f\u4e3a dw = dz.dot(a_prev.T) / m\u3002\n    # db \u8868\u793a\u8f93\u51fa\u5c42\u7684\u504f\u7f6e\u68af\u5ea6\uff0c\u8ba1\u7b97\u516c\u5f0f\u4e3a db = np.sum(dz, axis=1, keepdims=True) / m\u3002\n    # \u8fd9\u4e9b\u68af\u5ea6\u7528\u4e8e\u66f4\u65b0\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\uff0c\u4ee5\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\n    for i in reversed(range(1,layers)):\n        grades[\"dz\"+str(i)] = parameters[\"w\"+str(i+1)].T.dot(grades[\"dz\"+str(i+1)]) * sigmoid_prime(caches[\"z\"][i])\n        grades[\"dw\"+str(i)] = grades[\"dz\"+str(i)].dot(caches[\"a\"][i-1].T)/m\n        grades[\"db\"+str(i)] = np.sum(grades[\"dz\"+str(i)],axis = 1,keepdims = True) /m\n    # \u8fd4\u56de\u5305\u542b\u6240\u6709\u68af\u5ea6\u7684\u5b57\u5178 grades\n    return grades  \n\n\ndef update_grades(parameters,grades,learning_rate):\n    layers = len(parameters)//2\n    for i in range(1,layers+1):\n        parameters[\"w\"+str(i)] -= learning_rate * grades[\"dw\"+str(i)]\n        parameters[\"b\"+str(i)] -= learning_rate * grades[\"db\"+str(i)]\n    return parameters\n\ndef compute_loss(al,y):\n    return np.mean(np.square(al-y))\n\ndef load_data():\n    x = np.arange(0.0,1.0,0.01)\n    y =20* np.sin(2*np.pi*x)\n    plt.scatter(x,y)\n    return x,y\n\nx,y = load_data()\nx = x.reshape(1,100)\ny = y.reshape(1,100)\nplt.scatter(x,y)\nparameters = init_parameters([1,25,1])\nal = 0\n# for i in range(4000):\nfor i in range(1):\n    caches,al = forward(x, parameters)\n    grades = backward(parameters, caches, al, y)\n    parameters = update_grades(parameters, grades, learning_rate= 0.3)\n    # if i %100 ==0:\n    #     print(compute_loss(al, y))\n# plt.scatter(x,al)\n# plt.show()\n</code></pre>"},{"location":"bagu/machinelearning/kmeans/","title":"\u624b\u6495kmeans","text":"<pre><code>import numpy as np\ndef kmeans(data, k, thresh=1, max_iterations=100):\n  centers = data[\n     np.random.choice(data.shape[0], k, replace=False)\n     ]\n  for _ in range(max_iterations):\n    distances = np.linalg.norm(\n       data[:, None] - centers, \n       axis=2\n       ) # n,k,d\n    labels = np.argmin(distances, axis=1)\n    new_centers = np.array(\n       [data[labels == i].mean(axis=0) for i in range(k)]\n       )\n    if np.all(centers == new_centers):break\n    center_change = np.linalg.norm(new_centers - centers)\n    if center_change &lt; thresh:break\n    centers = new_centers\n  return labels, centers\ndata = np.random.rand(100, 2)  # 100\u4e2a\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u6709\u4e24\u4e2a\u7279\u5f81\nk = 3  # \u805a\u7c7b\u6570\u4e3a3\nlabels, centers = kmeans(data, k)\nprint(\"\u7c07\u6807\u7b7e:\", labels)\nprint(\"\u805a\u7c7b\u4e2d\u5fc3\u70b9:\", centers)\n</code></pre> <p>\u6ce8\u91ca\uff1a</p> <pre><code>import numpy as np\ndef kmeans(data, k, thresh=1, max_iterations=100):\n  # data \uff1a n_samples * feature_dim\n  # \u4ece data \u4e2d\u968f\u673a\u9009\u62e9 n_clusters \u4e2a\u4e0d\u540c\u7684\u6837\u672c\u7d22\u5f15\n  # centers : n_clusters * feature_dim\n  # replace=False\uff1a\u8868\u793a\u5728\u9009\u62e9\u6837\u672c\u65f6\u4e0d\u5141\u8bb8\u91cd\u590d\uff0c\u5373\u6bcf\u4e2a\u6837\u672c\u53ea\u80fd\u88ab\u9009\u62e9\u4e00\u6b21\u3002\n  centers = data[np.random.choice(data.shape[0], k, replace=False)]\n  for _ in range(max_iterations):\n    # data \uff1a n_smaples * feature_dim\n    # data[:, None]\uff1an_samples * 1 * feature_dim  \\ n 1 d\n    # centers \uff1a n_clusters * feature_dim \\ k d\n    # data[:, None] - centers : n_samples * n_clusters * feature_dim \\ n k d\n    # np.linalg.norm : n_samples * n_clusters\n    distances = np.linalg.norm(data[:, None] - centers, axis=2)\n    # labels : n_samples * 1\n    # distances : n_samples * n_clusters\n    # np.argmin \u51fd\u6570\u8fd4\u56de\u6307\u5b9a\u8f74\u4e0a\u6700\u5c0f\u503c\u7684\u7d22\u5f15\u3002\n    # axis=1 \u8868\u793a\u6cbf\u7740\u7b2c 1 \u8f74\uff08\u5373\u5217\uff09\u5bfb\u627e\u6700\u5c0f\u503c\u7684\u7d22\u5f15\u3002\n    labels = np.argmin(distances, axis=1)\n    '''\n        \u8ddd\u79bb\u77e9\u9635:\n        [[0.5 1.2 0.9]\n        [1.  0.8 1.5]\n        [0.3 0.4 0.2]\n        [1.1 0.7 0.6]\n        [0.9 1.3 0.4]]\n        \u7c07\u6807\u7b7e:\n        [0 1 2 2 2]\n        \u7c07\u6807\u7b7e\u7684\u5f62\u72b6: (5,)\n    '''\n    new_centers = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n\n    # np.all(centers == new_centers) \u8fd4\u56de True\uff0c\u8f93\u51fa\u7ed3\u679c\u4e3a \"\u4e2d\u5fc3\u70b9\u5df2\u6536\u655b\"\u3002\n    if np.all(centers == new_centers):\n      break\n    # \u8ba1\u7b97 new_centers \u548c centers \u4e4b\u95f4\u7684\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff08\u6216\u8303\u6570\uff09\u3002\n    center_change = np.linalg.norm(new_centers - centers)\n    if center_change &lt; thresh:\n        break\n    centers = new_centers\n  return labels, centers\n\ndata = np.random.rand(100, 2)  # 100\u4e2a\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u6709\u4e24\u4e2a\u7279\u5f81\nk = 3  # \u805a\u7c7b\u6570\u4e3a3\nlabels, centers = kmeans(data, k)\nprint(\"\u7c07\u6807\u7b7e:\", labels)\nprint(\"\u805a\u7c7b\u4e2d\u5fc3\u70b9:\", centers)\n</code></pre>"},{"location":"bagu/questions/","title":"Index","text":"<ul> <li>\u529b\u6263\u9898</li> <li>\u9762\u8bd5\u9898</li> </ul>"},{"location":"bagu/questions/1_questions/","title":"\u9762\u8bd5\u95ee\u9898","text":"<p>\uff081\uff09\u9762\u8bd5\u9898\uff1a\u8bf7\u539f\u7406\u4e0a\u89e3\u91ca\u4e3a\u4ec0\u4e48 Bert \u7684\u4e09\u4e2a Embedding \u53ef\u4ee5\u76f4\u63a5\u52a0\u5728\u4e00\u8d77</p> <p></p>"},{"location":"bagu/questions/leetcode/","title":"Index","text":"<ul> <li> 1</li> <li> 2</li> </ul>"},{"location":"bagu/questions/leetcode/1/","title":"1 \u4e24\u6570\u4e4b\u548c","text":"<p>\u529b\u62631 \u4e24\u6570\u4e4b\u548c </p> <pre><code>class Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n</code></pre> <p>\u793a\u4f8b 1\uff1a</p> <pre><code>\u8f93\u5165\uff1anums = [2,7,11,15], target = 9\n\u8f93\u51fa\uff1a[0,1]\n\u89e3\u91ca\uff1a\u56e0\u4e3a nums[0] + nums[1] == 9 \uff0c\u8fd4\u56de [0, 1] \u3002\n</code></pre> <pre><code>class Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        dict = {}\n\n        for i in range(len(nums)):\n            if target - nums[i] not in dict:\n                dict[nums[i]] = i\n            else:\n                return [dict[target-nums[i]],i]\n</code></pre> <p>\u9010\u5b57\u8be6\u89e3\uff1a\u9996\u5148\u8f93\u5165\u7684\u6570\u7ec4\u662fnums\uff0c\u5047\u8bbenums = [2,7,11,15]\uff0ci=0,1,2,3 \u5f53 i=0\u65f6\uff0c\u6b64\u65f6target=9\uff0ctarget-nums[0]=9-2=7,7\u4e0d\u5728\u5b57\u5178\u4e2d\uff0c\u56e0\u4e3a\u5b57\u5178\u662f\u7a7a\u7684\uff0c\u6240\u4ee5\u6267\u884cdict[num[i]] = i \u5373 dict[nums[0]]=0 \uff0cdict[2]=0\uff0ci=1,target-nums[i]=9-nums[1]=9-7=2,2\u5728\u5b57\u5178\u4e2d\uff0c\u6240\u4ee5\u6267\u884celse return[dict[2],1]=[0,1]\u8fd4\u56de\uff0c\u5f97\u5230\u7ed3\u679c</p> <p>\u518d\u6765\uff1a\u9996\u5148\u5b9a\u4e49\u7a7a\u5b57\u5178\uff0c\u7528\u6765\u5b58\u50a8\u5df2\u7ecf\u904d\u5386\u8fc7\u7684\u6570\u5b57\u53ca\u5176\u5bf9\u5e94\u7684\u7d22\u5f15\uff1b\u63a5\u4e0b\u6765 for loop\u904d\u5386\u6570\u7ec4nums,\u5176\u4e2di\u662f\u5f53\u524d\u904d\u5386\u5230\u7684\u6570\u5b57\u7684\u7d22\u5f15\uff1b\u5982\u679c\u4e0d\u5728\u5b57\u5178\u4e2d\uff0c\u8bf4\u660e\u4e4b\u524d\u6ca1\u6709\u9047\u5230\u8fc7\u4e0e\u5f53\u524d\u6570\u5b57\u76f8\u52a0\u7b49\u4e8etarget\u7684\u6570\u5b57\uff0c\u56e0\u6b64\u5c06\u5f53\u524d\u6570\u5b57\u53ca\u5176\u7d22\u5f15\u5b58\u5165\u5b57\u5178\u4e2d\u3002\u5982\u679c\u5728\u5b57\u5178\u4e2d\uff0c\u8bf4\u660e\u5df2\u7ecf\u627e\u5230\u4e86\u4e00\u5bf9\u6570\u5b57\uff0c\u548c\u7b49\u4e8etarget\uff0c\u4e8e\u662f\u8fd4\u56de\u8fd9\u4e24\u4e2a\u6570\u5b57\u7684\u7d22\u5f15\u3002\u8fd9\u4e24\u4e2a\u7d22\u5f15\u5206\u522b\u662fdict[target-nums[i]]\uff08\u5b58\u50a8\u5dee\u503c \u5bf9\u5e94\u7684\u7d22\u5f15\uff09\u548c i\uff08\u5f53\u524d\u6570\u5b57\u7684\u7d22\u5f15\uff09</p> <p>\u5206\u6790\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n)\uff0c\u5176\u4e2dn\u4e3a\u6570\u7ec4\u7684\u957f\u5ea6\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u5143\u7d20\u53ea\u88ab\u8bbf\u95ee\u4e00\u6b21\uff0c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3aO(n)\uff0c\u6700\u574f\u7684\u60c5\u51b5\u662f\u53ef\u80fd\u9700\u8981\u5b58\u50a8\u6240\u6709\u5143\u7d20\u7684\u7d22\u5f15</p> <p>\u8fd9\u4e2a\u65b9\u6cd5\u4f7f\u7528\u4e86\u54c8\u5e0c\u8868\uff08\u901a\u8fc7\u5b57\u5178\uff09\u5feb\u901f\u67e5\u627e\u76ee\u6807\u6570\u5b57\uff0c\u662f\u4e00\u79cd\u5178\u578b\u7684\u54c8\u5e0c\u8868\u5e94\u7528\u573a\u666f\uff0c\u63d0\u9ad8\u67e5\u627e\u6548\u7387</p>"},{"location":"bagu/questions/leetcode/2/","title":"2 \u4e24\u6570\u76f8\u52a0","text":""},{"location":"learning/","title":"Index","text":"<ul> <li> <p> vit\u91cd\u96be\u70b9\u6e90\u7801\u5b9e\u73b0</p> </li> <li> <p> vit\uff08\u9739\u96f3\u5427\u5566wz \u6e90\u7801\u8bb2\u89e3\uff09</p> </li> <li> <p> swintransformer\u7b14\u8bb0\uff08\u9739\u96f3\u5427\u5566wz\uff09</p> </li> <li> <p> swintransformer\u4ee3\u7801</p> </li> <li> <p> Diffusion</p> </li> <li> <p> 4\u79cd\u4f4d\u7f6e\u7f16\u7801</p> </li> <li> <p> 5\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5</p> </li> <li> <p> SwinTransformer</p> </li> <li> <p> 4\u79cd\u5377\u79ef</p> </li> <li> <p> GAN</p> </li> <li> <p> GAN \u53d8\u4f53</p> </li> <li> <p> Bert</p> </li> <li> <p> \u56fe\u89e3LayerNorm &amp; BatchNorm</p> </li> <li> <p> \u674e\u6c90\u76ee\u6807\u68c0\u6d4b</p> </li> <li> <p> DETR</p> </li> <li> <p> Transformer\u548c\u76ee\u6807\u68c0\u6d4b\u6cbf\u7740DETR\u7684\u5de5\u4f5c\uff1aDINO</p> </li> <li> <p> Clip</p> </li> <li> <p> \u591a\u6a21\u6001\u6cbf\u7740Clip\u7684\u5de5\u4f5c\uff1aBlip</p> </li> <li> <p> \u6cbf\u7740Transformer\u7684\u5de5\u4f5c</p> </li> <li> <p> WeightNorm</p> </li> <li> <p> ResNet\u5b9e\u6218\u9879\u76ee</p> </li> <li> <p> \u8ddf\u7740\u590d\u73b0\u9879\u76ee\uff0c\u505a\u4e00\u4e2a\u5b9e\u9645\u5e94\u7528</p> </li> <li> <p> VAE</p> </li> <li> <p> VDM</p> </li> <li> <p> RNN</p> </li> <li> <p> LSTM</p> </li> </ul> <p>\u662f\u4ec0\u4e48\u4e3a\u4ec0\u4e48\u600e\u4e48\u6837\u6570\u5b66\u5b9e\u4f8b</p>"},{"location":"learning/1/","title":"5\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5","text":"<p>45\u3001\u4e94\u79cd\u5f52\u4e00\u5316\u7684\u539f\u7406\u4e0ePyTorch\u9010\u884c\u624b\u5199\u5b9e\u73b0\u8bb2\u89e3(BatchNorm/LayerNorm/InsNorm/GroupNorm/WeightNorm)</p> <p>\u56fe\u6e90</p> <p></p> <p></p>"},{"location":"learning/1/#batchnorm","title":"BatchNorm","text":""},{"location":"learning/1/#_1","title":"\u56fe\u793a","text":"<p>\u6279\u5f52\u4e00\u5316\u3001\u901a\u9053\u7ea7\u522b\u7684\u5f52\u4e00\u5316</p>"},{"location":"learning/1/#apibatchnorm1d-2d","title":"\u5b98\u7f51api\uff0cBatchNorm1D &amp; 2D","text":"<p>BatchNorm1D\u7684\u8f93\u5165\uff1aNCL\uff0c\u7528\u4e8eNLP</p> <p></p> <p>BatchNorm2D\u7684\u8f93\u5165\u662f\u56db\u7ef4\u7684\uff0c\u7528\u4e8e\u56fe\u50cf</p> <p></p> <p>\u4e00\u4e2a\u662f\u4e09\u7ef4tensor\u4f5c\u4e3a\u8f93\u5165</p> <p>\u4e00\u4e2a\u662f\u56db\u7ef4tensor\u4f5c\u4e3a\u8f93\u5165</p>"},{"location":"learning/1/#batchnorm1d","title":"BatchNorm1D","text":"<ul> <li>\u9996\u5148\uff0c\u4f4d\u4e8etorch.nn\u6a21\u5757\u4e0b\uff0c\u662f\u4e00\u4e2aclass\uff0c\u6240\u4ee5\u8981\u7528\u7684\u8bdd\uff0c\u9700\u8981\u5b9e\u4f8b\u5316</li> <li>\u63a5\u4e0b\u6765\uff0c\u770b\u5b9e\u4f8b\u5316\u9700\u8981\u63a5\u6536\u7684\u53c2\u6570\uff1a</li> <li>num features\uff1a\u8f93\u5165\u5f20\u91cf\u7684\u7279\u5f81\u7ef4\u5ea6\uff0c\u6216\u8005\u901a\u9053\u7684\u6570\u76ee\uff0c\u6216\u8005embedding\u7684\u5927\u5c0f</li> <li>eps\uff1a5\u79cd\u5f52\u4e00\u5316\u90fd\u9700\u8981\u7684eps\uff0c\u5206\u6bcd\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u8ba9\u5206\u6bcd\u52a0\u4e0a\u4e00\u4e2a\u5fae\u5c0f\u7684\u91cf\uff0c\u4f7f\u5f97\u9664\u6cd5\u80fd\u591f\u6b63\u5e38\u8fdb\u884c\uff0c\u9ed8\u8ba41e-05</li> <li>momentum\uff1a\u52a8\u91cf<ul> <li>\u6279\u5f52\u4e00\u5316\u5728\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\u7684\u65f6\u5019\uff0cmomentum\u901a\u5e38\u9700\u8981\u8ddftrack_running_sate\u8054\u5408\u8d77\u6765\u7406\u89e3\uff0c\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u7684\u7edf\u8ba1\u91cf \u901a\u5e38\u662f\u901a\u8fc7\u6ed1\u52a8\u5e73\u5747\u8ba1\u7b97\u51fa\u6765\u4e86\uff0c\u800c\u4e0d\u662f\u5355\u4e00\u65f6\u523b\u7684mini batch\uff0c\u662f\u4e00\u4e2a\u7d2f\u8ba1\u7684\u8fc7\u7a0b\uff0c\u4e3a\u4e86\u63d0\u9ad8\u4f30\u8ba1\u7684\u51c6\u786e\u5ea6</li> </ul> </li> <li>affine\uff1a<ul> <li>\u4e5f\u5c31\u662f gamma &amp; beta\uff0c\u4e5f\u5c31\u662f\u518d\u505a\u5b8c\u5f52\u4e00\u5316\u4ee5\u540e\uff0c\u4e5f\u53ef\u4ee5\u52a0\u4e00\u4e2a\u6620\u5c04\uff0c\u5c06\u5176\u6620\u5c04\u5230\u4e00\u4e2a\u65b0\u7684\u5206\u5e03\u4e0a\uff0c\u505a\u4e00\u4e2arescale\u548crecenter</li> </ul> </li> </ul> <p>\u5b98\u7f51\u5b9a\u4e49\uff1a</p> <p></p> <p>\uff08\u89e3\u91ca\u5b98\u7f51\u5b9a\u4e49\uff09\u5747\u503c\u548c\u6807\u51c6\u5dee\u662f\u7ecf\u8fc7\u6574\u4e2amini_batch</p> <p>\u4e00\u53e5\u8bdd\u8bf4\u660e BatchNorm\uff1aper channel across mini-batch</p> <p>\u8d2f\u7a7f\u6574\u4e2amini batch\u8ba1\u7b97\u7edf\u8ba1\u91cf\uff0c\u6bcf\u4e2a\u901a\u9053\u5355\u72ec\u53bb\u7b97\u7684</p> <p>gamma \u548c beta \u662f\u53ef\u5b66\u4e60\u7684\u5411\u91cf\uff0c\u7ef4\u5ea6\u90fd\u662fC\uff0c\u9ed8\u8ba4\u7684\u60c5\u51b5\u4e0b $\\gamma = 1\u3001\\beta=0 $</p> <p>\u6807\u51c6\u5dee\u7528\u7684\u662f\u6709\u504f\u4f30\u8ba1\uff0c\u4e5f\u5c31\u662f\u8ba1\u7b97\u7684\u6807\u51c6\u5dee\u662f \\(\\frac{1}{n}\\)\uff0c\u5f3a\u8c03\u8fd9\u53e5\u8bdd\u7684\u76ee\u7684\u662f \uff0c\u5728\u8ba1\u7b97\u65b9\u5dee\u7684\u65f6\u5019\uff0c\u8981\u7528 \\(\\mathrm{unbiased=False}\\)\uff0c\u8fd9\u91cc\u7528\u5f97\u662f\u6709\u504f\u4f30\u8ba1</p> <p>\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5728\u8bad\u7ec3\u4e2d\uff0c\u4f1a\u4e0d\u65ad\u7684\u8bb0\u5f55\u5386\u53f2\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u5e76\u4e14\u4f7f\u75280.1\u7684\u52a8\u91cf\uff0c\u6765\u505a\u79fb\u52a8\u7684\u4f30\u8ba1\uff0c\u5f53\u8bad\u7ec3\u7ed3\u675f\u4ee5\u540e\uff0c\u7528\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u4f30\u8ba1\u91cf\u6765\u505a inference</p> <p>\u4e5f\u53ef\u4ee5\u8bbe\u7f6e track running states\u7b49\u4e8efalse\uff0c\u5c31\u662f\u4e0d\u8981\u8bb0\u5f55\u5386\u53f2\u7684\u79fb\u52a8\u7684\u503c</p> <p>\u4ee5\u4e0a\u662fapi\u7684\u4ecb\u7ecd</p> <p>\u63a5\u4e0b\u6765 \u81ea\u5df1\u5199\u4e00\u4e2aBatchNorm \u66f4\u597d\u7684\u7406\u89e3</p> <ul> <li> <p> NLP\u7684\u6807\u51c6\u6570\u636e\u683c\u5f0f\uff1ainputx = torch.randn(batch_size,times_steps,embedding_dim) # \\(N*L*C\\)</p> </li> <li> <p> \u5b9e\u4f8b\u5316\uff0c\u63a5\u6536\u7684\u8f93\u5165\uff08\u7279\u5f81\u7ef4\u5ea6\uff0c\u662f\u5426\u8fdb\u884c\u4eff\u5c04\u53d8\u6362\uff09\uff1abatch_norm_op = torch.nn.BatchNorm1d(embedding_dim,affine=False)</p> </li> <li> <p> batchnorm\u7684forward\u51fd\u6570 \u63a5\u6536\u7684\u6570\u636e\u96c6\u683c\u5f0f\u662f BDN  b\u8868\u793abatch size\uff1bD\u8868\u793amodel dim\uff1bN\u8868\u793a\u5e8f\u5217\u957f\u5ea6\uff08\u7b26\u53f7\u8868\u793a\u65b9\u6cd5\u7684\u4e0d\u540c</p> </li> </ul> <p><code>bn_y = batch_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)</code></p> <pre><code>import torch\n\nbatch_size = 2\ntimes_steps = 3\nembedding_dim = 4\n\ninputx = torch.randn(batch_size,times_steps,embedding_dim) # N*L*C\n\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm1d(embedding_dim,affine=False)\nbn_y = batch_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,1),keepdim=True)\nbn_std = inputx.std(dim=(0,1),unbiased=False,keepdim=True)\nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5)\nprint(bn_y)\nprint(verify_bn_y)\nprint(torch.allclose(bn_y,verify_bn_y))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[[-0.3771,  1.7863, -1.0572,  0.2856],\n         [-0.7956, -0.0363, -0.7429, -0.1670],\n         [ 2.0838,  0.7039,  1.1345,  0.7286]],\n\n        [[-0.5775, -0.3680, -1.1160, -1.3169],\n         [ 0.3298, -1.0699,  1.2153, -1.0909],\n         [-0.6634, -1.0160,  0.5663,  1.5606]]])\ntensor([[[-0.3771,  1.7863, -1.0572,  0.2856],\n         [-0.7956, -0.0363, -0.7429, -0.1670],\n         [ 2.0838,  0.7039,  1.1345,  0.7286]],\n\n        [[-0.5775, -0.3680, -1.1160, -1.3169],\n         [ 0.3298, -1.0699,  1.2153, -1.0909],\n         [-0.6634, -1.0160,  0.5663,  1.5606]]])\nTrue\n</code></pre> <p>\u89e3\u91ca\uff1a\u53bb\u770b\u56fe\u89e3BN&amp;LN</p> <p>\u9700\u8981\u5f3a\u8c03\uff1a</p> <ul> <li>\u5728batch\u548c\u957f\u5ea6\u8fd9\u4e00\u7ef4\u8ba1\u7b97\u7edf\u8ba1\u91cf</li> <li>\u8ba1\u7b97\u6807\u51c6\u5dee\u7684\u65f6\u5019\uff0c\u7528\u7684\u662f\u6709\u504f\u4f30\u8ba1\uff0c\u8bbe\u7f6eunbiased=false</li> </ul>"},{"location":"learning/1/#layernorm","title":"LayerNorm","text":"<p>\u5c42\u5f52\u4e00\u5316\u7684\u6982\u62ec\uff1aper sample\u3001per Layer\uff0c\u5bf9\u5355\u4e00\u6837\u672c\u3001\u6bcf\u4e00\u4e2a\u5c42 \u5355\u72ec\u8ba1\u7b97\uff0c\u4e0d\u9700\u8981\u8003\u8651minibatch</p> <p>LayerNorm\u6700\u5178\u578b\u7684\u4f7f\u7528\u573a\u666f\uff1aNLP</p> <p>\u628a\u7f51\u7edc\u4e2d \u6bcf\u4e00\u6b21 \u6bcf\u4e00\u4e2a\u65f6\u523b \u5f53\u6210\u4e00\u5c42</p> <p>\u6bcf\u4e2a\u65f6\u523b embedding dim\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee </p> <ul> <li> \u4e3a\u4ec0\u4e48nlp\u4e2d\u4f7f\u7528LN\uff1f</li> </ul> <p>\u5e94\u4e3anlp\u4e2d\uff0c\u4e0d\u540c\u53e5\u5b50\u957f\u5ea6\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4 \u5bf9\u4e8e\u6bcf\u4e2abatch\u4e2d\u7684 \u6216\u8005 \u53e5\u5b50\u4e2d \u8bcd\u6570\u662f\u4e0d\u4e00\u6837\u7684\uff1b\u6216\u8005\u6d4b\u8bd5\u65f6\uff0c\u53e5\u5b50\u7684\u957f\u5ea6\u53ef\u80fd\u8bad\u7ec3\u65f6\u4e5f\u6ca1\u89c1\u8fc7\uff0c\u800cBatchNorm\u662facross batch\u7684\uff0c\u6240\u4ee5\u6700\u597d\u4fdd\u8bc1batch\u5185\u90e8L\u662f\u56fa\u5b9a\u7684</p> <p>\u4e00\u53e5\u8bdd\uff1a\u53e5\u5b50\u4e2d\u8bcd\u7684\u6570\u91cf\u5e76\u4e0d\u4e00\u6837</p>"},{"location":"learning/1/#_2","title":"\u56fe\u793a","text":""},{"location":"learning/1/#_3","title":"\u8bed\u8a00\u63cf\u8ff0\u5b9e\u9645\u610f\u4e49","text":""},{"location":"learning/1/#for-nlp","title":"for nlp","text":"<ul> <li> LayerNorm\u5bf9 \u6bcf\u4e2a\u8bcd\u8fdb\u884c\u5f52\u4e00\u5316  bnd   dim=2\uff1f  \uff08\u6709\u51e0\u4e2a\u8bcd\u5c31\u6709\u5f97\u5230\u51e0\u4e2a\u5747\u503c\u548c\u65b9\u5dee\uff0c\u7136\u540e\u8fdb\u884c\u5f52\u4e00\u5316\uff09b\u00d7n\u00d71</li> </ul> <p>\u4e3e\u6570\u5b66\u4f8b\u5b50\uff1a2\u4e2a\u53e5\u5b50\u3001\u6bcf\u4e2a\u53e5\u5b503\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd\u7684\u7ef4\u5ea64\uff0c\u90a3\u4e48\u6211\u4eec\u5f97\u52306\u4e2a\u5747\u503c\u548c\u65b9\u5dee\uff0c\u6240\u4ee5\u5f52\u4e00\u5316\u540e\u7684\u7ef4\u5ea6  2\u00d73\u00d71</p> <p>\\(\\rightarrow\\)</p> <p>\\(\\rightarrow\\)</p> <p>\\(\\rightarrow\\)</p> <p>\\(\\rightarrow\\)</p> <p>\\(\\rightarrow\\)</p> <p>\\(\\rightarrow\\)</p> <ul> <li> BatchNorm \u5bf9\u8bcd\u7684\u7279\u5f81\u7ef4\u5f52\u4e00\u5316 bnd dim=0,1  1\u00d71\u00d7d\uff08\u6709\u51e0\u4e2a\u7279\u5f81\u7ef4\u5ea6\uff0c\u5c31\u4f1a\u5f97\u5230\u51e0\u4e2a\u5747\u503c\u548c\u65b9\u5dee\uff09  \\(\\downarrow\\) \\(\\downarrow\\) \\(\\downarrow\\) \\(\\downarrow\\) \\(\\downarrow\\) \\(\\downarrow\\)</li> </ul> <p>2\u00d73\u00d74 \u5f97\u5230 1\u00d71\u00d74 \u4e5f\u5c31\u662f4\u4e2a\u5747\u503c\uff0c\u7ec6\u8282\u4e5f\u4e0d\u7528\u6263\u8fd9\u4e48\u7ec6\uff0c\u76f4\u63a5keepdim=true</p>"},{"location":"learning/1/#for-cv","title":"for cv","text":"<ul> <li> <p> LayerNorm  bchw \u770b\u505a chw \u540c\u65f6\u5f52\u4e00\u5316\uff0c\u6709\u51e0\u4e2achw\u6709\u51e0\u4e2a\u5f52\u4e00\u5316\u5747\u503c\u548c\u65b9\u5dee\uff1bbnd\uff0c\u6709\u51e0\u4e2abn\u6709\u51e0\u4e2anlp\u5f52\u4e00\u5316\u7684\u5747\u503c\u548c\u65b9\u5dee \uff1bc \u4f9d\u7136\u662f\u72ec\u7acb\u7684\u8bcd\uff0c\u8868\u8fbe\u4e0d\u540c\u7684\u8bed\u4e49\uff1b</p> </li> <li> <p> BatchNorm \u628a \u901a\u9053 \u4f5cnlp\u4e2d\u7684\u72ec\u7acb \u8bcd\u3001token\uff0c\u5bf9\u4e8e\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u4ee5\u53ca\u7a7a\u95f4\u5206\u5e03 \u4e0d\u505a\u533a\u5206 \u53ef\u4ee5\u7406\u89e3\u4e3a bc(hw)\uff0c\u7c7b\u6bd4\u5230nlp\uff0c\u4e00\u4e2a\u901a\u9053\u7684\u4fe1\u606f\uff0c\u7531\u957f\u5ea6\u4e3ah*w\u7684\u5411\u91cf\u8868\u793a\uff0c\u4fdd\u7559\u901a\u9053\u4fe1\u606f\uff0c\u6cbf\u7740\u6837\u672c\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u5176\u5b9e\u4e5f\u662f\u5f15\u5165\u4e86\u5176\u4ed6\u6837\u672c\u7684\u566a\u58f0   bchw  $ \\rightarrow$  b\u00d7c\u00d7hw  dim=0,2,3</p> </li> </ul> <p>\u6570\u5b66\u5c0f\u4f8b\u5b50\uff1a4\u5f20\u56fe\uff0c3\u901a\u9053\uff0c2\u00d72\u7684\u56fe\uff0cBN\u4ee5\u540e\uff0c\u5f97\u5230 3\u4e2a\u5747\u503c\u548c\u65b9\u5dee </p> <p>4\u00d73\u00d72\u00d72 \\(\\rightarrow\\) 1\u00d73\u00d71\u00d71</p> <p>BatchNorm\u7684api\u4e2d\uff0c\u9700\u8981\u7684\u6570\u636e\u683c\u5f0f\u662fNCL\uff0c\u800c\u4e0d\u662f\u5e38\u7528\u7684NLC\uff0c\u53ef\u80fd\u539f\u56e0\u5c31\u662fbn\u901a\u5e38\u7528\u5728cv</p>"},{"location":"learning/1/#cv-nlp-bn-ln","title":"cv nlp bn ln","text":"<ul> <li> <p> \u5927\u6982\u662f \u7406\u89e3\u4e3a</p> </li> <li> <p>bn\uff08\u5f97\u5230\u5747\u503c\u5411\u91cf\uff09</p> </li> </ul> <p>4\u5f20\u56fe\uff0c\u6bcf\u5f20\u56fe\u7247 3\u4e2a\u901a\u9053\u7279\u5f81\uff0c\u6bcf\u4e2a\u901a\u9053 \u75314\u4e2a\u5143\u7d20\u8868\u8fbe\uff082\u00d72\uff09\uff08\u5f97\u52303\u4e2a\u5747\u503c\uff09\uff08cv&amp;bn\uff09</p> <p>2\u4e2a\u53e5\u5b50\uff0c\u6bcf\u4e2a\u53e5\u5b503\u4e2a\u8bcd\uff0c4\u7ef4\u5ea6\uff08\u5f97\u52304\u4e2a\u5747\u503c\uff09\uff08nlp&amp;bn\uff09</p> <ul> <li> <p>ln</p> </li> <li> <p> 4\u5f20\u56fe\uff0c3\u4e2a\u901a\u9053\u7279\u5f81\uff0c\u6bcf\u4e2a\u901a\u9053\u7531\u957f\u5ea6\u4e3a4\u7684\u5411\u91cf\u8868\u8fbe\uff082\u00d72\uff09\uff0c\u5f97\u52304\u4e2a\u5747\u503c\u548c\u65b9\u5dee\uff08per sample\u3001per layer\uff09\uff08cv  ln\uff09</p> </li> <li> 4\u4e2a\u53e5\u5b50\uff0c\u6bcf\u4e2a\u53e5\u5b503\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd\u5d4c\u51654\u4e2a\u7ef4\u5ea6\uff0c\u5f97\u523012\u4e2a\u5747\u503c\u548c\u65b9\u5dee\uff08ln\u3001nlp\uff09</li> </ul>"},{"location":"learning/1/#api","title":"\u5b98\u65b9api","text":"<p>LayerNorm\u53ea\u6709\u4e00\u4e2aapi</p> <p>\u540c\u6837\u4e5f\u662f\u4e00\u4e2aclass\uff0c\u5982\u679c\u8981\u53bb\u5b9e\u4f8b\u5316\u7684\u8bdd\uff0c\u53ea\u9700\u8981\u6307\u5b9a\u4e00\u4e0b \u88ab\u5f52\u4e00\u5316\u7684\u5f62\u72b6\uff0c\u4ee5\u53ca\u662f\u5426\u9700\u8981\u8fdb\u884c\u7f29\u653e\u53d8\u6362</p> <p>\u4e5f\u5c31\u662f\u8bf4\u63a5\u6536\u7684\u8f93\u5165\uff1a</p> <ul> <li>normalized_shape\uff1a\u88ab\u5f52\u4e00\u5316\u7684\u5f62\u72b6</li> <li>elementwise_affine\uff1a\u662f\u5426\u9700\u8981\u8fdb\u884c\u7f29\u653e\u53d8\u6362</li> </ul> <p>\u63a5\u4e0b\u6765\u770b\u5b9a\u4e49\uff1a</p> <p></p> <ul> <li>\u8fd9\u91cc\u7684\u5747\u503c\u548c\u65b9\u5dee\u662f\u5728\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8ba1\u7b97\u7684\uff08over the last dimension\uff09\u5bf9\u4e8e over across minibatch</li> <li>D\u5c31\u662f \u6211\u4eec\u8981\u4f20\u5165\u7684 normalized shape</li> <li>\u5728nlp\u4e2d \u901a\u5e38\u53ea\u9700\u8981\u4f20\u5165\u6807\u91cf\u5c31\u597d\u4e86\uff0c\u5c31\u662f\u8ba1\u7b97\u6700\u540e\u7684embedding\u7684\u7ef4\u5ea6</li> </ul>"},{"location":"learning/1/#_4","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<pre><code>import torch\n\nbatch_size = 2\ntimes_steps = 3\nembedding_dim = 4\n\ninputx = torch.randn(batch_size,times_steps,embedding_dim) # N*L*C\n# 2. \u5b9e\u73b0layer_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528 layer_norm API\nlayer_norm_op = torch.nn.LayerNorm(embedding_dim,elementwise_affine=False)\nln_y = layer_norm_op(inputx)\n\n## \u624b\u5199layer_norm\nln_mean = inputx.mean(dim=-1,keepdim=True)\nln_std = inputx.std(dim=-1,keepdim=True,unbiased=False)\nverify_bn_y = (inputx - ln_mean)/(ln_std + 1e-05)\n# print(ln_mean.shape) torch.Size([2, 3, 1])\n# print(ln_std.shape)  torch.Size([2, 3, 1])\n# print(ln_y.shape)   torch.Size([2, 3, 4])\n# print(verify_bn_y.shape)    torch.Size([2, 3, 4])\n# print(torch.allclose(ln_y,verify_bn_y)) True\n</code></pre> <p>bn\uff08\u5f97\u5230\u5747\u503c\u5411\u91cf\uff09</p> <p>4\u5f20\u56fe\uff0c\u6bcf\u5f20\u56fe\u7247 3\u4e2a\u901a\u9053\u7279\u5f81\uff0c\u6bcf\u4e2a\u901a\u9053 \u75314\u4e2a\u5143\u7d20\u8868\u8fbe\uff082\u00d72\uff09\uff08\u5f97\u52303\u4e2a\u5747\u503c\uff09\uff08cv&amp;bn\uff09</p> <p>4322\u21921311</p> <p>2\u4e2a\u53e5\u5b50\uff0c\u6bcf\u4e2a\u53e5\u5b503\u4e2a\u8bcd\uff0c4\u7ef4\u5ea6\uff08\u5f97\u52304\u4e2a\u5747\u503c\uff09\uff08nlp&amp;bn\uff09</p> <p>234\u2192114</p> <p>ln(\u5f97\u5230\u5747\u503c\u77e9\u9635)</p> <p>\u5bf9\u4e8eLN \u4e00\u5b9a\u8981\u660e\u767d\uff1aper sample\u3001per layer</p> <p>\u5bf9\u4e8eCV per sample\u5c31\u662f\u4e00\u5f20\u56fe\u7247</p> <p>\u5bf9\u4e8eNLP per layer \u5c31\u662f\u4e00\u4e2a\u8bcd</p> <p>4\u5f20\u56fe\uff0c\u6bcf\u5f20\u56fe\u7247 3\u4e2a\u901a\u9053\u7279\u5f81\uff0c\u6bcf\u4e2a\u901a\u9053 \u75314\u4e2a\u5143\u7d20\u8868\u8fbe\uff082\u00d72\uff09\uff08\u5f97\u52304\u4e2a\u5747\u503c per sample\uff09</p> <p>4322 \u21924111</p> <p>2\u4e2a\u53e5\u5b50\uff0c\u6bcf\u4e2a\u53e5\u5b503\u4e2a\u8bcd\uff0c4\u7ef4\u5ea6\uff08\u5f97\u52306\u4e2a\u5747\u503c\uff09\uff08nlp&amp;bn\uff09</p> <p>\uff08234\u2192231\uff09</p>"},{"location":"learning/1/#bnlnnlpcv","title":"\u4ee3\u7801\u5b9e\u73b0 BN\u3001LN&amp;NLP&amp;CV","text":"<p>nlp&amp;BN&amp;LN</p> <pre><code>import torch\n\nbatch_size = 2\ntimes_steps = 3\nembedding_dim = 4\n\ninputx = torch.randn(batch_size,times_steps,embedding_dim) # N*L*C\n\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm1d(embedding_dim,affine=False)\nbn_y = batch_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,1),keepdim=True)\nbn_std = inputx.std(dim=(0,1),unbiased=False,keepdim=True)\nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5)\n# print(bn_mean.shape) torch.Size([1, 1, 4])\n# print(bn_std.shape) torch.Size([1, 1, 4])\n# print(bn_y.shape)   torch.Size([2, 3, 4])\n# print(verify_bn_y.shape)    torch.Size([2, 3, 4])\n# print(torch.allclose(bn_y,verify_bn_y)) True\n\n# 2. \u5b9e\u73b0layer_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528 layer_norm API\nlayer_norm_op = torch.nn.LayerNorm(embedding_dim,elementwise_affine=False)\nln_y = layer_norm_op(inputx)\n\n## \u624b\u5199layer_norm\nln_mean = inputx.mean(dim=-1,keepdim=True)\nln_std = inputx.std(dim=-1,keepdim=True,unbiased=False)\nverify_bn_y = (inputx - ln_mean)/(ln_std + 1e-05)\n# print(ln_mean.shape) torch.Size([2, 3, 1])\n# print(ln_std.shape)  torch.Size([2, 3, 1])\n# print(ln_y.shape)   torch.Size([2, 3, 4])\n# print(verify_bn_y.shape)    torch.Size([2, 3, 4])\n# print(torch.allclose(ln_y,verify_bn_y)) True\n</code></pre> <p>CV&amp;BN&amp;LN</p> <pre><code>import torch\n\nbatch_size = 4\nchannels = 3\nh,w = 2,2\n\ninputx = torch.randn(batch_size,channels,h,w) # BCHW \u53ea\u8981\u7ef4\u5ea6\u662f\u6b63\u786e\u7684\uff0c\u6570\u5b57\u53ef\u4ee5\u968f\u4fbf\u751f\u6210\n\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm2d(channels,affine=False)\nbn_y = batch_norm_op(inputx) # torch.Size([4, 3, 2, 2])\n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,2,3),keepdim=True) # torch.Size([1, 3, 1, 1])\nbn_std = inputx.std(dim=(0,2,3),unbiased=False,keepdim=True) # torch.Size([1, 3, 1, 1])\nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5) # torch.Size([4, 3, 2, 2])\n\n# print(bn_mean.shape) \n# print(bn_std.shape) \n# print(bn_y.shape)   \n# print(verify_bn_y.shape)    \n# print(torch.allclose(bn_y,verify_bn_y))\n'''\n    torch.Size([1, 3, 1, 1])\n    torch.Size([1, 3, 1, 1])\n    torch.Size([4, 3, 2, 2])\n    torch.Size([4, 3, 2, 2])\n    True\n'''\n# 2. \u5b9e\u73b0layer_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528 layer_norm API\nlayer_norm_op = torch.nn.LayerNorm((channels,h,w),elementwise_affine=False)\nln_y = layer_norm_op(inputx)  # torch.Size([4, 3, 2, 2])\n\n## \u624b\u5199layer_norm\nln_mean = inputx.mean(dim=(1,2,3),keepdim=True)  # torch.Size([4, 1, 1, 1])\nln_std = inputx.std(dim=(1,2,3),keepdim=True,unbiased=False)  # torch.Size([4, 1, 1, 1])\nverify_bn_y = (inputx - ln_mean)/(ln_std + 1e-05)   # torch.Size([4, 3, 2, 2])\nprint(ln_mean.shape)\nprint(ln_std.shape)\nprint(ln_y.shape)\nprint(verify_bn_y.shape)\nprint(torch.allclose(ln_y,verify_bn_y))\n'''\n    torch.Size([4, 1, 1, 1])\n    torch.Size([4, 1, 1, 1])\n    torch.Size([4, 3, 2, 2])\n    torch.Size([4, 3, 2, 2])\n    True\n'''\n</code></pre> <p>\u7eaf\u4eab\u7248\uff1a</p> <p>for \u53e5\u5b50</p> <pre><code>import torch\n\nbatch_size = 2\ntimes_steps = 3\nembedding_dim = 4\n\ninputx = torch.randn(batch_size,times_steps,embedding_dim) # N*L*C\n\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm1d(embedding_dim,affine=False)\nbn_y = batch_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,1),keepdim=True)\nbn_std = inputx.std(dim=(0,1),unbiased=False,keepdim=True)\nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5)\nprint(torch.allclose(bn_y,verify_bn_y)) \n\n# 2. \u5b9e\u73b0layer_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528 layer_norm API\nlayer_norm_op = torch.nn.LayerNorm(embedding_dim,elementwise_affine=False)\nln_y = layer_norm_op(inputx)\n\n## \u624b\u5199layer_norm\nln_mean = inputx.mean(dim=-1,keepdim=True)\nln_std = inputx.std(dim=-1,keepdim=True,unbiased=False)\nverify_bn_y = (inputx - ln_mean)/(ln_std + 1e-05)\nprint(torch.allclose(ln_y,verify_bn_y)) \n</code></pre> <p>for  \u56fe\u7247</p> <pre><code>import torch\n\nbatch_size = 4\nchannels = 3\nh,w = 2,2\n\ninputx = torch.randn(batch_size,channels,h,w) # BCHW \u53ea\u8981\u7ef4\u5ea6\u662f\u6b63\u786e\u7684\uff0c\u6570\u5b57\u53ef\u4ee5\u968f\u4fbf\u751f\u6210\n\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm2d(channels,affine=False)\nbn_y = batch_norm_op(inputx) \n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,2,3),keepdim=True) \nbn_std = inputx.std(dim=(0,2,3),unbiased=False,keepdim=True) \nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5)\nprint(torch.allclose(bn_y,verify_bn_y))\n\n\n# 2. \u5b9e\u73b0layer_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528 layer_norm API\nlayer_norm_op = torch.nn.LayerNorm((channels,h,w),elementwise_affine=False)\nln_y = layer_norm_op(inputx) \n\n## \u624b\u5199layer_norm\nln_mean = inputx.mean(dim=(1,2,3),keepdim=True) \nln_std = inputx.std(dim=(1,2,3),keepdim=True,unbiased=False)  \nverify_bn_y = (inputx - ln_mean)/(ln_std + 1e-05)  \nprint(torch.allclose(ln_y,verify_bn_y))\n</code></pre>"},{"location":"learning/1/#instance-norm","title":"Instance Norm","text":"<p>\u5b9e\u4f8b\u5f52\u4e00\u5316\uff0c\u901a\u5e38\u7528\u5728 \u98ce\u683c\u8fc1\u79fb\u4e0a</p> <p>per sample\u3001per channel</p> <p>\u8fd9\u65f6\u8ba1\u7b97\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u65f6\u5019\uff0c\u662f\u5bf9\u6bcf\u4e00\u4e2a\u6837\u672c\u7684\u3001\u6bcf\u4e00\u4e2a\u7ef4\u5ea6</p> <p>\u5b98\u7f51api</p> <p></p> <ul> <li>\u662f\u4e00\u4e2aclass</li> <li><code>num_features</code>\uff1a\u8981\u5b9e\u73b0\u4e00\u4e2aInstance Norm\u7684\u8bdd\uff0c\u9700\u8981\u4f20\u5165\u7279\u5f81\u7ef4\u5ea6\u6216\u8005\u8bf4 \u901a\u9053\u7ef4\u5ea6\uff08model dim \u53ef\u4ee5\u7406\u89e3\u4e3a channel\uff09\uff0c\u56e0\u4e3a \u6211\u4eec\u8981\u9010\u6837\u672c\uff0c\u9010\u901a\u9053\u7684\u8fdb\u884c\u5f52\u4e00\u5316</li> <li><code>affine</code>\uff1a\u4e5f\u53ef\u4ee5\u8fdb\u884c\u4eff\u5c04\u53d8\u6362\uff0c\u4f46\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u662f\u8bbe\u7f6e\u4e3afalse\u7684\uff0c\u4e00\u822c\u662f\u76f4\u63a5\u5f52\u4e00\u5316\u5373\u53ef</li> </ul>"},{"location":"learning/1/#instance1d","title":"INSTANCE1D","text":"<p>\u4ee3\u7801\u5b9e\u73b0\u9700\u8981\u6ce8\u610f\uff0c\u63a5\u6536\u7684\u8f93\u5165\u6570\u636e\u683c\u5f0f\u662f\u4ec0\u4e48\u6837\u7684\uff0c\u8f93\u51fa\u7684\u6570\u636e\u683c\u5f0f\u53c8\u662f\u4ec0\u4e48\u6837\u7684\uff0c\u770b\u5b98\u7f51api</p> <p></p> <p>NCL</p> <p>for nlp\uff1a\u5d4c\u5165\u7ef4\u5ea6\u653e\u5230\u4e2d\u95f4\u3001\u8bcd\u6570\u6ede\u540e</p> <p>for cv\uff1aINSTANCE2D \uff1abchw \u4e0d\u53d8</p> <p>\u5c31\u5f88\u7c7b\u4f3cBatchNorm</p> <p>\u6240\u4ee5\u4f1a\u6709\u4e24\u6b21\u8f6c\u7f6e</p>"},{"location":"learning/1/#_5","title":"\u56fe\u793a","text":""},{"location":"learning/1/#for-nlp_1","title":"for NLP","text":""},{"location":"learning/1/#for-cv_1","title":"for CV","text":""},{"location":"learning/1/#_6","title":"\u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"learning/1/#for-nlp-instancenorm1d","title":"For nlp InstanceNorm1d","text":"<pre><code># 3. \u5b9e\u73b0instance_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528ins_norm\u5e76\u9a8c\u8bc1API\nins_norm_op = torch.nn.InstanceNorm1d(embedding_dim)\nin_y = ins_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199ins_norm\nin_mean = inputx.mean(dim=1,keepdim=True)\nin_std = inputx.std(dim=1,keepdim=True,unbiased=False)\nverify_in_y = (inputx - in_mean)/(in_std+1e-5)\nprint(torch.allclose(in_y,verify_in_y))\n</code></pre> <p>\u89e3\u91ca\uff1a</p> <ul> <li> <p> <code>in_y = ins_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)</code> \u5148\u8f6c\u7f6e\u518d\u8f6c\u7f6e\u56de\u6765</p> </li> <li> <p> inputx.shape = torch.Size([2, 3, 4])</p> </li> <li> in_y.shape = torch.Size([2, 3, 4])</li> <li> <p> in_mean.shape = torch.Size([2, 1, 4])</p> </li> <li> <p> \u89e3\u91ca\u4e3a\u4ec0\u4e48instanceNorm\u80fd\u591f\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\uff1f</p> </li> </ul> <p>\u8f93\u5165 batch size\u00d7sequence length\u00d7embedding dim\uff0c\u5982\u679c\u6211\u4eec\u53ea\u5bf9\u4e2d\u95f4\u8fd9\u4e00\u7ef4\u6c42\u5747\u503c\u7684\u8bdd\uff0c\u4e5f\u5c31\u662f\u8bf4 \u628a\u6240\u6709\u65f6\u523b\u7684embedding\u6c42\u4e00\u4e2a\u5747\u503c\uff0c\u76f8\u5f53\u4e8e\u5747\u503c\u662f\u5f53\u524d\u8fd9\u4e2a\u6837\u672c\u5728\u6240\u6709\u65f6\u523b \u4e0d\u53d8\u7684\u4e1c\u897f\uff0c\u90a3\u6211\u4eec\u628a\u4e0d\u53d8\u7684\u4e1c\u897f\u6d88\u6389\uff0c\u5c31\u662f\u8bf4 \u628a \u8fd9\u4e2a\u65f6\u5e8f\u6837\u672c\u5728 \u6240\u6709\u65f6\u523b\u4e2d \u90fd\u6709\u7684\u4e1c\u897f \u6d88\u6389\uff0c\u90a3\u4ec0\u4e48\u4e1c\u897f\u662f\u5728\u6240\u6709\u65f6\u523b\u90fd\u6709\u7684\u5462\uff1f\u5176\u5b9e\u5c31\u662f\u8fd9\u6837\u5427\u6837\u672c\u7684\u98ce\u683c\uff0c\u5982\u679c\u662f\u4e00\u53e5\u6587\u672c \u6216\u8005\u8bf4 \u4e00\u5f20\u56fe\u7247 \u6216\u8005\u8bf4\u4e00\u53e5\u8bdd \u4e00\u6bb5\u97f3\u9891\uff0c\u4e00\u5f20\u56fe\u7247\u5c31\u662f\u98ce\u683c\uff0c\u4e00\u53e5\u97f3\u9891\uff0c\u4e00\u53e5\u56e0\u4e3a\u4ec0\u4e48\u4e1c\u897f\u662f\u65f6\u4e0d\u53d8\u7684\uff1f\u5047\u5982\u8bf4 \u662f\u4e00\u4e2a\u4eba\u8bf4\u7684\u8bdd\u7684\u8bdd\uff0c\u90a3\u5c31\u662f\u8bf4 \u8fd9\u4e2a\u4eba\u7684\u8eab\u4efd\u662f\u65f6\u4e0d\u53d8\u7684\uff1b</p> <p>\u4e5f\u5c31\u662f\u8bf4 \u901a\u8fc7instanceNorm\uff0c\u5982\u679c\u662f\u56fe\u50cf\u7684\u8bdd\uff0c\u5c31\u53ef\u4ee5\u628a\u56fe\u50cf\u7684\u98ce\u683c\u7ed9\u6d88\u6389\uff0c\u90a3\u5982\u679c\u5728\u8bed\u97f3\u4e2d\uff0c\u5c31\u53ef\u4ee5\u628a\u8fd9\u4e2a\u4eba\u7684\u8eab\u4efd\u6d88\u6389\uff0c\u56e0\u4e3a\u6211\u4eec\u627e\u7684\u662f\u8de8\u65f6\u95f4\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\uff0c\u6211\u4eec\u505a\u7684\u5f52\u4e00\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6211\u4eec\u628a\u56fe\u7247\u4e2d\u7684\u98ce\u683c\u6d88\u6389\u4e86\uff0c\u628a\u8bed\u97f3\u4e2d\u8bf4\u8bdd\u4eba\u7684\u8eab\u4efd\u6d88\u6389\u4e86\uff0c\u5982\u679c\u662f\u6587\u672c\u7684\u8bdd\uff0c\u53ef\u80fd\u662f\u6587\u672c\u4e2d \u67d0\u4e00\u4e2a\u5171\u6709\u7684\u7279\u5f81\uff1b</p> <p>\u6240\u4ee5instanceNorm\u4e00\u822c\u7528\u5728\u98ce\u683c\u8fc1\u79fb\u4e2d\uff0c\u628a\u65f6\u4e0d\u53d8\u7684\u4e1c\u897f\u53bb\u6389\u4e86</p>"},{"location":"learning/1/#for-cv-instancenorm2d","title":"For cv InstanceNorm2d","text":"<pre><code># 3. \u5b9e\u73b0instance_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528ins_norm\u5e76\u9a8c\u8bc1API\nins_norm_op = torch.nn. (channels)\nin_y = ins_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n# print(inputx.shape) torch.Size([4, 3, 2, 2])\n## \u624b\u5199ins_norm\nin_mean = inputx.mean(dim=(2,3),keepdim=True)\n# dim=(2,3) print(in_mean.shape) torch.Size([4, 3, 1, 1])\n#dim=1  print(in_mean.shape) torch.Size([4, 1, 2, 2])\nin_std = inputx.std(dim=(2,3),keepdim=True,unbiased=False)\nverify_in_y = (inputx - in_mean)/(in_std+1e-5)\nprint(torch.allclose(in_y,verify_in_y))\n</code></pre> <ul> <li> dim=(2,3) || in_mean = inputx.mean(dim=(2,3),keepdim=True)</li> </ul>"},{"location":"learning/1/#groupnormalization","title":"GroupNormalization","text":"<p>\u5206\u7ec4\u5f52\u4e00\u5316\u3001\u7fa4\u5f52\u4e00\u5316</p> <p></p> <p>per sample\u3001per group</p> <p>\u8fd9\u4e2a\u8ddfLayerNorm\u662f\u6700\u50cf\u7684</p> <p>\u9700\u8981\u5c06channel\u5212\u5206\u6210group\uff0c\u7c7b\u4f3c\u5206\u7ec4\u5377\u79ef\u3001\u628a\u8f93\u5165\u901a\u9053\u5212\u5206\u6210\u4e0d\u540c\u7684group</p> <p>\u9996\u5148\u5bf9input tensor\u5212\u5206\u6210\u4e0d\u540c\u7684group\uff0c\u7136\u540e\u5bf9\u4e8e\u6bcf\u4e2a\u6837\u672c\uff0c\u6bcf\u4e2agroup\u8ba1\u7b97\u5f52\u4e00\u5316\u5373\u53ef</p> <p>\u8fd9\u91cc\u4ecd\u7136\u662f\u8ddfbatch size\u65e0\u5173\u7684\uff0cbatch Norm\u662f\u8ddfbatch size\u6709\u5173\u7684</p>"},{"location":"learning/1/#api_1","title":"\u5b98\u65b9api","text":"<p>\uff081\uff09\u662f\u4e00\u4e2aclass</p> <p>\uff082\uff09\u5b9e\u73b0\u7fa4\u5f52\u4e00\u5316\u7684\u8bdd\uff0c\u9700\u8981\u7684\u4f20\u5165\u53c2\u6570\uff1a</p> <ul> <li>num_groups\uff1agroup\u7684\u6570\u76ee\uff0c\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5982\u679cchannel=4\uff0c\u5212\u5206\u62102\u4e2agroup\uff0c\u90a3\u8fd9\u91cc\u6211\u4eec\u5c31\u53ef\u4ee5\u4f20\u51652</li> <li>num_channels\uff1a\u4f8b\u5b50\u4e2d num_channels=4</li> <li>affine\uff1a\u4e00\u822c\u9ed8\u8ba4=false</li> </ul>"},{"location":"learning/1/#nlpgn","title":"NLP&amp;GN","text":""},{"location":"learning/1/#_7","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<ul> <li>\u5b9e\u4f8b\u5316class</li> <li>\u8f93\u5165\u7684\u53c2\u6570\uff0c\u4ee5\u53ca\u8f93\u5165\u7684\u5f62\u72b6</li> </ul> <p>\u8f93\u5165\u901a\u9053\u6570 \u9700\u8981\u653e\u5728\u4e2d\u95f4\u7684\u7ef4\u5ea6\uff0c\u8c03\u7528groupNorm\u9700\u8981\u7c7b\u4f3cBatchNorm\uff0c\u505a\u4e00\u4e2a\u8f6c\u7f6e\uff0c\u4e5f\u662f\u8ddfinstanceNorm\u662f\u7c7b\u4f3c\u7684</p> <pre><code># 4. \u5b9e\u73b0group_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528group_norm\u5e76\u9a8c\u8bc1API\nnum_groups = 2\ngroup_norm_op = torch.nn.GroupNorm(num_groups,embedding_dim,affine=False)\ngn_y = group_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n</code></pre> <p>\u9996\u5148\u9700\u8981\u5c06inputx\u5212\u5206\u6210 num_groups\u7ec4\uff0c\u9700\u8981\u8c03\u7528\u5212\u5206\u7684api\uff0c\u4e5f\u5c31\u662fsplit\uff0c\u5207\u6210\u591a\u4e2atensor</p> <p>\u6307\u5b9a\u597d\u7ef4\u5ea6\u4ee5\u53ca\u5207\u5206\u7684\u5927\u5c0f</p> <p>\u5b98\u7f51api\uff0ctorch.split\u51fd\u6570\u600e\u4e48\u7528\uff0c\u9700\u8981\u4f20\u5165\u4ec0\u4e48\u53c2\u6570</p> <p></p> <p>\u9700\u8981\u4f20\u5165\u7684\u53c2\u6570\uff1a</p> <p></p> <ul> <li>tensor\uff1a\u5207\u8c01</li> <li>split_size_or_sections:\u5207\u6210\u591a\u5927\u7684</li> <li>dim\uff1a\u5207\u54ea\u4e2a\u7ef4\u5ea6</li> </ul> <pre><code>group_inputx = torch.split(inputx,split_size_or_sections=embedding_dim//num_groups,dim=-1)\n</code></pre> <p>\u5206\u7ec4\uff0c\u5206\u7ec4\u8fc7\u540e\u5728\u6bcf\u4e2a\u7ec4\u8ba1\u7b97</p> <p>per smaple\u3001per group </p> <pre><code># 4. \u5b9e\u73b0group_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528group_norm\u5e76\u9a8c\u8bc1API\nnum_groups = 2\ngroup_norm_op = torch.nn.GroupNorm(num_groups,embedding_dim,affine=False)\ngn_y = group_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199group_norm\ngroup_inputxs = torch.split(inputx,split_size_or_sections=embedding_dim//num_groups,dim=-1)\nresults = []\nfor g_inputx in group_inputxs:\n    gn_mean = g_inputx.mean(dim=(1,2),keepdim=True)\n    # print(gn_mean.shape) # torch.Size([2, 1, 1])\n    gn_std = g_inputx.std(dim=(1,2),keepdim=True,unbiased=False)\n    gn_result = (g_inputx - gn_mean)/(gn_std + 1e-5)\n    results.append(gn_result)\n\nverify_gn_y = torch.cat(results,dim=-1)\nprint(torch.allclose(gn_y,verify_gn_y))\n</code></pre>"},{"location":"learning/1/#_8","title":"\u56fe\u793a","text":""},{"location":"learning/1/#cvgn","title":"CV&amp;GN","text":"<p>\u5199\u4ee3\u7801\u65f6\u9700\u8981\u6ce8\u610f\uff1a</p> <p><code>ValueError: num_channels must be divisible by num_groups</code></p>"},{"location":"learning/1/#_9","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<pre><code># 4. \u5b9e\u73b0group_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528group_norm\u5e76\u9a8c\u8bc1API\n\nbatch_size = 4\nchannels = 6\nh,w = 2,2\ninputx = torch.randn(batch_size,channels,h,w)\n\nnum_groups = 3\ngroup_norm_op = torch.nn.GroupNorm(num_groups,channels,affine=False)\ngn_y = group_norm_op(inputx)\nprint(gn_y.shape)  # torch.Size([4, 6, 2, 2])\n## \u624b\u5199group_norm\n# BCHW\ngroup_inputxs = torch.split(inputx,split_size_or_sections=channels//num_groups,dim=1)\nresults = []\nfor g_inputx in group_inputxs:\n    gn_mean = g_inputx.mean(dim=(1,2,3),keepdim=True)\n    print(gn_mean.shape) # 3 \u4e2a torch.Size([4, 1, 1, 1])\n    gn_std = g_inputx.std(dim=(1,2,3),keepdim=True,unbiased=False)\n    gn_result = (g_inputx - gn_mean)/(gn_std + 1e-5)\n    results.append(gn_result)\n\nverify_gn_y = torch.cat(results,dim=1)\nprint(verify_gn_y.shape)  # torch.Size([4, 6, 2, 2])\nprint(torch.allclose(gn_y,verify_gn_y)) # True\n</code></pre>"},{"location":"learning/1/#_10","title":"\u56fe\u793a","text":"<p>6\u901a\u9053\uff0c\u5206\u62103\u7ec4\uff0c\u6bcf\u7ec4\u4e24\u901a\u9053</p> <p></p>"},{"location":"learning/1/#_11","title":"\u6743\u91cd\u5f52\u4e00\u5316","text":"<p>\u6587\u5b57\u3001\u6570\u5b66\u3001\u56fe\u793a\u3001\u4ee3\u7801</p> <p></p> <ul> <li>\u6743\u91cd\u5f52\u4e00\u5316\u4e0e\u4e4b\u524d\u4ecb\u7ecd\u7684\u5f52\u4e00\u5316\u4e0d\u592a\u4e00\u6837</li> <li>\u628a\u6743\u91cd\u8fdb\u884c\u89e3\u8026\uff0c\u5e45\u5ea6\u548c\u65b9\u5411\u89e3\u8026</li> <li>\u770b\u5b98\u7f51api\u7684\u4ecb\u7ecd</li> <li>\u641c\u7d22\uff1a<code>torch.nn.utils.weight_norm</code>  \u662f\u5bf9\u6743\u91cd\u8fdb\u884c\u5f52\u4e00\u5316</li> </ul> <p></p> <ul> <li>\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e14\u662f\u4e00\u4e2a\u5305\u88f9\u7684\u51fd\u6570\uff0c\u5305\u88f9\u7684\u5bf9\u8c61\u662fmodule</li> <li> <p>\u9700\u8981\u7684\u8f93\u5165</p> </li> <li> <p><code>module</code></p> </li> <li>\u89e3\u91ca\u51fd\u6570\u529f\u80fd\uff1a</li> </ul> <p>\u5bf9\u4e00\u4e2a\u53c2\u6570\uff08\u6307\u7684\u662f\u6743\u91cd\u53c2\u6570\u3001\u4e0d\u5305\u62ec\u504f\u7f6e\uff09</p> <p>\u5bf9\u53c2\u6570\u8fdb\u884c \\(w = g \\frac{v}{||v||}\\) \u53d8\u6362\uff0c\u672c\u6765\u7684\u53c2\u6570\u662fv\uff0c\u628av\u5148\u9664\u4ee5 v\u7684\u6a21\uff0c\u5f97\u5230v\u7684\u5355\u4f4d\u5411\u91cf\uff0c\u4e5f\u5c31\u662fv\u7684\u5355\u4f4d\u5411\u91cf\uff0c\u518d\u4e58\u4ee5\u65b0\u7684\u5e45\u5ea6g\uff0cg\u662f\u53ef\u5b66\u4e60\u7684\uff0c\u5f97\u5230\u65b0\u7684\u6743\u91cdw</p> <p>\u5047\u8bbe\u4e00\u4e2alinear\u5c42\uff0c\u6743\u91cd\u4e3av\uff0c\u5957\u4e0a\u4e00\u4e2aweight_norm\u51fd\u6570\u4ee5\u540e\uff0c\u5bf9\u5b83\u65b0\u589e\u4e00\u4e2a\u53c2\u6570g\uff0c\u8fd9\u6837\u65b0\u7684\u6743\u91cd\u53d8\u6210\u4e86w\uff0c  w\u4fdd\u7559\u4e86v\u7684\u65b9\u5411\uff0c\u4f46\u662f\u65b0\u589e\u7684\u4e00\u4e2a\u5e45\u5ea6g</p> <ul> <li>\u8fd4\u56de\u503c\uff0c\u8fd8\u662fmodule</li> </ul> <p></p>"},{"location":"learning/1/#_12","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<pre><code># 5.\u5b9e\u73b0weight_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528weight_norm \u5e76\u9a8c\u8bc1api\nlinear = nn.Linear(embedding_dim,3,bias=False)\nwh_linear = torch.nn.utils.weight_norm(linear)\nwh_linear_output = wh_linear(inputx)\n# print(wh_linear_output.shape) # torch.Size([2, 3, 3])\n## \u624b\u5199weight_norm\nweight_direction = linear.weight/(linear.weight.norm(dim=1,keepdim=True))\nweight_magnitude = wh_linear.weight_g\nprint(weight_direction.shape)  # torch.Size([3, 4])\nprint(weight_magnitude.shape)  # torch.Size([3, 1])\nverify_wh_linear_output = inputx @ (weight_direction.transpose(-1,-2))*(weight_magnitude.transpose(-1,-2))\n\nprint(\"weight norm\",torch.allclose(wh_linear_output,verify_wh_linear_output))\n</code></pre> <p>\u8c03\u7528api\u5b9e\u73b0weight norm</p> <ul> <li>\u8f93\u5165\u53c2\u6570\uff1amodule</li> </ul> <p>\u2460 linear\u5c42\u7684\u8f93\u5165\u5927\u5c0f\u662fembedding_dim=4\u3001\u8f93\u51fa\u5927\u5c0f\u8bbe\u4e3a3</p> <p>\u9996\u5148\uff0c\u9700\u8981\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662fmodule\uff0c\u6240\u4ee5\u9700\u8981\u9996\u5148\u5b9e\u4f8b\u5316module\uff0c\u4ee5linear\u5c42\u4e3e\u4f8b</p> <p>\u200b \u2461 \u4e0d\u8003\u8651bias</p> <p><code>linear = nn.Linear(embedding_dim,3,bias=False)</code></p> <ul> <li>\u5c06\u5b9e\u4f8b\u5316\u597d\u7684linear\uff0c\u4f20\u5165\u6743\u91cd\u5f52\u4e00\u5316\u51fd\u6570 <code>torch.nn.utils.weight_norm</code>\uff0c\u5f97\u5230\u6743\u91cd\u5f52\u4e00\u5316\u4ee5\u540e\u7684linear</li> </ul> <p><code>wh_linear = torch.nn.utils.weight_norm(linear)</code></p> <ul> <li>\u5c06\u8f93\u5165 <code>inputx</code> \u4f20\u5165\u5230 \u6743\u91cd\u5f52\u4e00\u5316\u540e\u7684\u5c42\uff0c\u5f97\u5230\u6743\u91cd\u5f52\u4e00\u5316api\u7684\u7ed3\u679c</li> </ul> <p><code>wh_linear_output = wh_linear(inputx)</code></p> <p>\u624b\u5199weight norm</p> <ul> <li>\u67e5\u770b \u6743\u91cd\u5f52\u4e00\u5316\u7ebf\u6027\u5c42 \u8f93\u51fa\u7684\u5f62\u72b6\uff1a233</li> </ul> <p>print(wh_linear_output.shape) # torch.Size([2, 3, 3])</p> <p>\u89e3\u91ca\u4e3a\u4ec0\u4e48\u662f233\uff1f</p> <p>\u9996\u5148 \u8f93\u5165\u662f234\uff0clinear\u5c42\u662f\u4ece4\u7ef4\u6620\u5c04\u52303\u7ef4\u7684\uff0c\u6240\u4ee5\u7ed3\u679c\u662f233\uff0c\u4e5f\u5c31\u662f \u7ecf\u8fc7\u6743\u91cd\u5f52\u4e00\u5316\u5c42\u7684\u8f93\u51fa\u662f 233</p> <p>233\u662f\u600e\u4e48\u6765\u7684\uff1f</p> <p>\u9996\u5148 \u516c\u5f0f\uff1a\\(w = g \\frac{v}{||v||}\\) ,\u5148\u5bf9\u6743\u91cd v  \u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u5f97\u5230\u65b9\u5411\u5411\u91cf\uff0c\u7136\u540e\u4e58\u4ee5\u4e00\u4e2a \u53ef\u5b66\u4e60\u7684\u5e45\u5ea6\u5411\u91cf g</p> <p>\u6240\u4ee5\u5728\u624b\u52a8\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u9996\u5148 \u5f97\u5230 linear\u7684\u6743\u91cd <code>linear.weight</code>\uff0c\u63a5\u7740 \u5bf9linear\u7684\u6743\u91cd\u9664\u4ee5\u4e00\u4e2a \u8303\u6570</p> <p><code>linear.weight</code> \u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\uff0c\u6839\u636e\u516c\u5f0f\u662f v\u9664\u4ee5v\u7684\u6a21</p> <p>\u5177\u4f53\u6765\u8bf4\uff0c<code>linear.weight/linear.weight.norm(dim=1,keepdim=True)</code></p> <p>\u89e3\u91ca\uff1a\u9664\u4ee5\u6a21 \u5e76\u4e0d\u662f \u9664\u4ee5\u6574\u4e2a\u77e9\u9635\u7684\u6a21\uff0c\u800c\u662f \u8ddf\u6bcf\u4e00\u4e2asample\uff0c\u5185\u79ef\u76f8\u4e58\u7684\u5411\u91cf\u7684\u6a21\uff0c\u6240\u4ee5\u8fd9\u91ccdimension\u53d61\uff0c\u8ba1\u7b97linear\u7684\u65f6\u5019 \u662f x\u4e58\u4ee5 w\u7684\u8f6c\u7f6e\uff0c\u4e5f\u5c31\u662fx\u7684\u6bcf\u4e00\u884c \u4e0e w\u7684\u8f6c\u7f6e\u7684\u6bcf\u4e00\u5217\u76f8\u4e58\uff0c\u4e0ew\u8f6c\u7f6e\u7684\u6bcf\u4e00\u5217\u76f8\u4e58 \u76f8\u5f53\u4e8e\u8ddf w\u7684\u6bcf\u4e00\u884c\u76f8\u4e58\uff0cw\u7684\u6bcf\u4e00\u884c\u5c31\u662fdimension=1\uff0c\uff080\u7ef4\u662fbatchsize\uff09\uff0c\u5e76\u4e14\u8bbe\u7f6ekeepdim=True,\u5e76\u4e14\u5c06 \u53d8\u91cf\u540d\u547d\u540d\u4e3a <code>weight_direction</code></p> <pre><code>weight_direction = linear.weight/(linear.weight.norm(dim=1,keepdim=True))\n</code></pre> <p>\u63a5\u4e0b\u6765 \u8fd8\u9700\u8981\u5e45\u5ea6\u53c2\u6570 <code>weight_magnitude</code>\uff0c\u4e5f\u5c31\u662f \u516c\u5f0f\u4e2d\u7684g\uff0c\u8981\u4fdd\u8bc1\u8ddfapi\u7684g\u4e00\u6837\uff0c\u5728\u5b98\u7f51api\u4e2d\u7ed9\u51fa weight_g\u8868\u793a\u5e45\u5ea6\uff0cweight_v\u8868\u793a\u65b9\u5411</p> <p></p> <pre><code>weight_magnitude = wh_linear.weight_g\n</code></pre> <p>\u67e5\u770b \u65b9\u5411\u7684\u5f62\u72b6\u548c\u5e45\u5ea6\u7684\u5f62\u72b6</p> <pre><code>print(weight_direction.shape)  # torch.Size([3, 4])\nprint(weight_magnitude.shape)  # torch.Size([3, 1])\n</code></pre> <p>\u65b9\u5411\u7684\u5f62\u72b6\u662f 3\u00d74\uff0c\u5e45\u5ea6\u7684\u5f62\u72b6\u662f 3\u00d71</p> <p>\u63a5\u4e0b\u6765 \u6309\u7167 \u516c\u5f0f \\(w = g \\frac{v}{||v||}\\) \u8ba1\u7b97\u51fa\u65b0\u7684 w</p> <pre><code>verify_wh_linear_output = inputx @ (weight_direction.transpose(-1,-2))*(weight_magnitude.transpose(-1,-2))\n</code></pre> <p>\u8f93\u5165\u4e0e\u6743\u91cd\u76f8\u4e58\uff0c\u7136\u540e\u4e58\u4ee5 \u65b9\u5411\u5411\u91cf\uff0cinputx.shape=234\uff0c\u9700\u8981\u5bf9weight\u8f6c\u7f6e\u4e00\u4e0b</p>"},{"location":"learning/1/#_13","title":"\u603b\u7ed3","text":""},{"location":"learning/1/#_14","title":"\u6846\u67b6","text":"<p>batch size\uff1aper channel across minibatch\uff0c\u5f52\u4e00\u5316\u7684\u65f6\u5019\u5bf9\u6bcf\u4e2a\u901a\u9053\u5355\u72ec\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u7b2c\u4e8c\u4e2a\u5f52\u4e00\u5316\u662f\u5c42\u5f52\u4e00\u5316\uff0c\u5bf9\u6bcf\u4e2a\u6837\u672c\u5355\u72ec\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u5e76\u4e14\u5bf9\u6bcf\u4e00\u4e2a\u5c42\u5355\u72ec\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3anlp\u4e2d\u7684\u65f6\u95f4\uff0c\u7b2c\u4e09\u4e2a\u662f\u5b9e\u4f8b\u5f52\u4e00\u5316\uff0c\u98ce\u683c\u8fc1\u79fb\u4e2d\u7ecf\u5e38\u4f7f\u7528\uff0c\u505a\u6cd5\u662fper sample\u3001per channel\uff0c\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u901a\u9053\u5355\u72ec\u505a\uff0c\u7b2c\u56db\u4e2a\u662f\u7fa4\u5f52\u4e00\u5316\uff0c\u5bf9\u6bcf\u4e2a\u6837\u672c\u5206\u7ec4\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u7ec4\u662f\u6307\u5bf9channel\u8fdb\u884c\u5206\u7ec4\uff0c\u7b2c\u4e94\u4e2a\u6743\u91cd\u5f52\u4e00\u5316\uff0c\u5bf9\u6743\u91cd\u8fdb\u884c\u5f52\u4e00\u5316\u518dscale</p> <p>\u603b\u7ed3\u7edf\u8ba1\u91cf\u7ef4\u5ea6\uff1a</p> <p>batchnorm</p> <ul> <li>nlp\uff1aNLC\u2192C</li> <li>cv\uff1aNCHW\u2192C</li> </ul> <p>LayerNorm</p> <ul> <li>nlp\uff1aNLC\u2192NL\uff08per sample\u3001per layer \u4fdd\u7559sample\u7ef4\u5ea6\uff0c\u4fdd\u7559layer\u7ef4\u5ea6\uff09</li> <li>cv\uff1aNCHW\u2192NHW?</li> </ul> <p>\u5b9e\u4f8b\u5f52\u4e00\u5316</p> <ul> <li>nlp\uff1aNLC\u2192NC</li> <li>cv\uff1aNCHW\u2192NC</li> </ul> <p>groupnorm</p> <ul> <li>nlp\uff1aN\uff0cG\uff0cL\uff0cC//G \u2192 N\uff0cG</li> <li>cv\uff1aN\uff0cG\uff0cC//G\uff0cH\uff0cW\u2192N\uff0cG</li> </ul>"},{"location":"learning/1/#_15","title":"\u4ee3\u7801","text":""},{"location":"learning/1/#nlp","title":"nlp","text":"<pre><code>import torch\nimport torch.nn as nn\n\nbatch_size = 2\ntimes_steps = 3\nembedding_dim = 4\n\ninputx = torch.randn(batch_size,times_steps,embedding_dim) # N*L*C\n\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm1d(embedding_dim,affine=False)\nbn_y = batch_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,1),keepdim=True)\nbn_std = inputx.std(dim=(0,1),unbiased=False,keepdim=True)\nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5)\nprint(\"batch norm:\",torch.allclose(bn_y,verify_bn_y)) \n\n# 2. \u5b9e\u73b0layer_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528 layer_norm API\nlayer_norm_op = torch.nn.LayerNorm(embedding_dim,elementwise_affine=False)\nln_y = layer_norm_op(inputx)\n\n## \u624b\u5199layer_norm\nln_mean = inputx.mean(dim=-1,keepdim=True)\nln_std = inputx.std(dim=-1,keepdim=True,unbiased=False)\nverify_bn_y = (inputx - ln_mean)/(ln_std + 1e-05)\nprint(\"layer norm:\",torch.allclose(ln_y,verify_bn_y)) \n\n# 3. \u5b9e\u73b0instance_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528ins_norm\u5e76\u9a8c\u8bc1API\nins_norm_op = torch.nn.InstanceNorm1d(embedding_dim)\nin_y = ins_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199ins_norm\nin_mean = inputx.mean(dim=1,keepdim=True)\nin_std = inputx.std(dim=1,keepdim=True,unbiased=False)\nverify_in_y = (inputx - in_mean)/(in_std+1e-5)\nprint(\"instance norm:\",torch.allclose(in_y,verify_in_y))\n\n# 4. \u5b9e\u73b0group_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528group_norm\u5e76\u9a8c\u8bc1API\nnum_groups = 2\ngroup_norm_op = torch.nn.GroupNorm(num_groups,embedding_dim,affine=False)\ngn_y = group_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199group_norm\ngroup_inputxs = torch.split(inputx,split_size_or_sections=embedding_dim//num_groups,dim=-1)\nresults = []\nfor g_inputx in group_inputxs:\n    gn_mean = g_inputx.mean(dim=(1,2),keepdim=True)\n    # print(gn_mean.shape) # torch.Size([2, 1, 1])\n    gn_std = g_inputx.std(dim=(1,2),keepdim=True,unbiased=False)\n    gn_result = (g_inputx - gn_mean)/(gn_std + 1e-5)\n    results.append(gn_result)\n\nverify_gn_y = torch.cat(results,dim=-1)\nprint(\"group norm:\",torch.allclose(gn_y,verify_gn_y))\n\n\n# 5.\u5b9e\u73b0weight_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528weight_norm \u5e76\u9a8c\u8bc1api\nlinear = nn.Linear(embedding_dim,3,bias=False)\nwh_linear = torch.nn.utils.weight_norm(linear)\nwh_linear_output = wh_linear(inputx)\n# print(wh_linear_output.shape) # torch.Size([2, 3, 3])\n## \u624b\u5199weight_norm\nweight_direction = linear.weight/(linear.weight.norm(dim=1,keepdim=True))\nweight_magnitude = wh_linear.weight_g\nprint(weight_direction.shape)  # torch.Size([3, 4])\nprint(weight_magnitude.shape)  # torch.Size([3, 1])\nverify_wh_linear_output = inputx @ (weight_direction.transpose(-1,-2))*(weight_magnitude.transpose(-1,-2))\n\nprint(\"weight norm\",torch.allclose(wh_linear_output,verify_wh_linear_output))\n</code></pre>"},{"location":"learning/1/#cv","title":"cv","text":"<pre><code>import torch\n\nbatch_size = 4\nchannels = 3\nh,w = 2,2\n\ninputx = torch.randn(batch_size,channels,h,w) # BCHW \u53ea\u8981\u7ef4\u5ea6\u662f\u6b63\u786e\u7684\uff0c\u6570\u5b57\u53ef\u4ee5\u968f\u4fbf\u751f\u6210\n\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm2d(channels,affine=False)\nbn_y = batch_norm_op(inputx) \n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,2,3),keepdim=True) \nbn_std = inputx.std(dim=(0,2,3),unbiased=False,keepdim=True) \nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5)\nprint(\"batch norm:\",torch.allclose(bn_y,verify_bn_y))\n\n\n# 2. \u5b9e\u73b0layer_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528 layer_norm API\nlayer_norm_op = torch.nn.LayerNorm((channels,h,w),elementwise_affine=False)\nln_y = layer_norm_op(inputx) \n\n## \u624b\u5199layer_norm\nln_mean = inputx.mean(dim=(1,2,3),keepdim=True) \nln_std = inputx.std(dim=(1,2,3),keepdim=True,unbiased=False)  \nverify_bn_y = (inputx - ln_mean)/(ln_std + 1e-05)  \nprint(\"layer norm:\",torch.allclose(ln_y,verify_bn_y))\n\n\n# 3. \u5b9e\u73b0instance_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528ins_norm\u5e76\u9a8c\u8bc1API\nins_norm_op = torch.nn.InstanceNorm2d(channels)\nin_y = ins_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n# print(inputx.shape) torch.Size([4, 3, 2, 2])\n## \u624b\u5199ins_norm\nin_mean = inputx.mean(dim=(2,3),keepdim=True)\n# dim=(2,3) print(in_mean.shape) torch.Size([4, 3, 1, 1])\n#dim=1  print(in_mean.shape) torch.Size([4, 1, 2, 2])\nin_std = inputx.std(dim=(2,3),keepdim=True,unbiased=False)\nverify_in_y = (inputx - in_mean)/(in_std+1e-5)\nprint(\"instance norm:\",torch.allclose(in_y,verify_in_y))\n\n# 4. \u5b9e\u73b0group_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528group_norm\u5e76\u9a8c\u8bc1API\n\nbatch_size = 4\nchannels = 6\nh,w = 2,2\ninputx = torch.randn(batch_size,channels,h,w)\n\nnum_groups = 3\ngroup_norm_op = torch.nn.GroupNorm(num_groups,channels,affine=False)\ngn_y = group_norm_op(inputx)\n# print(gn_y.shape)  # torch.Size([4, 6, 2, 2])\n## \u624b\u5199group_norm\n# BCHW\ngroup_inputxs = torch.split(inputx,split_size_or_sections=channels//num_groups,dim=1)\nresults = []\nfor g_inputx in group_inputxs:\n    gn_mean = g_inputx.mean(dim=(1,2,3),keepdim=True)\n    # print(gn_mean.shape) # 3 \u4e2a torch.Size([4, 1, 1, 1])\n    gn_std = g_inputx.std(dim=(1,2,3),keepdim=True,unbiased=False)\n    gn_result = (g_inputx - gn_mean)/(gn_std + 1e-5)\n    results.append(gn_result)\n\nverify_gn_y = torch.cat(results,dim=1)\n# print(verify_gn_y.shape)  # torch.Size([4, 6, 2, 2])\nprint(\"group norm:\",torch.allclose(gn_y,verify_gn_y)) # True\n</code></pre>"},{"location":"learning/10_ResNet/","title":"\u9879\u76ee\u5b9e\u6218\uff1aResNet\u679c\u852c\u5206\u7c7b","text":"<p>\u57fa\u4e8ePyTorch ResNet18\u7684\u679c\u852c\u5206\u7c7b\u9010\u884c\u4ee3\u7801\u8bb2\u89e3</p>"},{"location":"learning/10_ResNet/#1","title":"1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":""},{"location":"learning/10_ResNet/#2","title":"2 \u56fe\u50cf\u9884\u5904\u7406","text":""},{"location":"learning/10_ResNet/#3","title":"3 \u7edf\u8ba1\u8bad\u7ec3\u96c6\u5747\u503c\u548c\u6807\u51c6\u5dee","text":""},{"location":"learning/10_ResNet/#4-datasetdataloader","title":"4 \u6784\u5efadataset\u548cdataloader","text":""},{"location":"learning/10_ResNet/#5-resnet18","title":"5 \u6784\u5efaResNet18\u6a21\u578b\uff08\u5bfc\u5165\u5e93\uff09","text":""},{"location":"learning/10_ResNet/#6","title":"6 \u6784\u5efa\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4ee3\u7801","text":""},{"location":"learning/10_ResNet/#7","title":"7 \u5f00\u59cb\u8bad\u7ec3\u6f14\u793a","text":""},{"location":"learning/10_ResNet/#8-tensorboard","title":"8 tensorboard\u6f14\u793a","text":""},{"location":"learning/11_excelcsvtensor/","title":"\u57fa\u7840\uff1aexcel\\csv\u6587\u4ef6\u2192tensor","text":"<p>Excel/Csv\u6587\u4ef6\u6570\u636e\u8f6c\u6210PyTorch\u5f20\u91cf\u5bfc\u5165\u6a21\u578b\u4ee3\u7801\u9010\u884c\u8bb2\u89e3</p>"},{"location":"learning/12_KLdivergence/","title":"KL divergence","text":"<p>\u4fe1\u606f\u91cf \uff5c\u71b5 \uff5c \u4ea4\u53c9\u71b5 \uff5cKL\u6563\u5ea6 \uff08\u76f8\u5bf9\u71b5\uff09\uff5c\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570</p>"},{"location":"learning/12_KLdivergence/#1","title":"1 \u524d\u8a00","text":"<p>\u4fe1\u606f\u91cf </p> <p>\\(I(x) = \\log_2{\\frac{1}{p(x)}} = -\\log_2{p(x)}\\) </p> <p>\u71b5</p> <p>\\(H(P)=\\sum p_iI_i^p=-\\sum p_i \\log_2(p_i)\\)</p> <p>\u4ea4\u53c9\u71b5</p> <p>\\(H(p,q) = \\sum p_i I_i^q = -\\sum p_i log_2(q_i)\\)</p> <p>\u76f8\u5bf9\u71b5\uff08KL\u6563\u5ea6\uff09</p> <p>\\(D_{KL}(p||q) = \\sum p_i \\log_2 \\frac{p_i}{q_i}\\)</p> <p>\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\uff08Cross Entropy Loss\uff09</p> <p>\\(H(p,q) = -\\log_2 (q_{class})\\)</p> <p>\u9010\u6b65\u9012\u8fdb\uff0c\u540e\u9762\u7684\u6982\u5ff5\u5efa\u7acb\u5728\u524d\u9762\u7684\u6982\u5ff5\u57fa\u7840\u4e4b\u4e0a</p>"},{"location":"learning/12_KLdivergence/#2","title":"2 \u4fe1\u606f\u91cf","text":"<p>\u4fe1\u606f\u91cf Amount of Information</p> <p>\u5b9a\u4e49\uff1a</p> <p>\u4e8b\u4ef6\u5305\u542b\u7684\u4fe1\u606f\u91cf\u5927\u5c0f\uff08\u4e8b\u4ef6\u53d1\u751f\u7684\u96be\u5ea6\u6709\u591a\u5927\uff09</p> <ul> <li>\u5c0f\u6982\u7387\u4e8b\u4ef6\uff0c\u5b83\u53d1\u751f\u7684\u96be\u5ea6\u6bd4\u8f83\u5927\uff0c\u6240\u4ee5\u6709\u8f83\u5927\u7684\u4fe1\u606f\u91cf</li> <li>\u5927\u6982\u7387\u4e8b\u4ef6\uff0c\u5b83\u53d1\u751f\u7684\u96be\u5ea6\u6bd4\u8f83\u5c0f\uff0c\u6240\u4ee5\u6709\u8f83\u5c0f\u7684\u4fe1\u606f\u91cf</li> </ul> <p>\u4f8b\u5b50\uff1a</p> <p></p> <p>\u6027\u8d28\uff1a</p> <p>\u5bf9\u4e8e\u72ec\u7acb\u4e8b\u4ef6A\u3001B\uff1a\\(P(AB)=P(A)P(B)\\)</p> <p>\u4e24\u4e2a\u4e8b\u4ef6\u540c\u65f6\u53d1\u751f\u7684\u4fe1\u606f\u91cf \u7b49\u4e8e \u4e24\u4e2a\u4e8b\u4ef6\u7684\u4fe1\u606f\u91cf \u76f8\u52a0 \uff1a\\(I(AB)=I(A)+I(B)\\)</p> <p>\u4fe1\u606f\u91cf\u516c\u5f0f\uff1a</p> <p>\\(I(x):= \\log_2(\\frac{1}{p(x)})=-log_2(p(x))\\)</p> <ul> <li><code>:=</code>  \u8868\u793a <code>\u5b9a\u4e49\u4e3a</code> \uff0c\u662f\u4e00\u79cd\u4eba\u4e3a\u5b9a\u4e49</li> <li><code>=</code>  \u662f\u6570\u5b66\u610f\u4e49\u4e0a\u7684 <code>\u5de6\u8fb9 = \u53f3\u8fb9</code></li> <li><code>I</code>  : <code>information</code></li> <li>\u516c\u5f0f\u600e\u4e48\u8bbe\u8ba1\u7684\uff1f</li> </ul> <p>\uff081\uff09\\(p(x)\\) \uff1a\u8868\u793a\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\uff0c\u53d6\u503c\u8303\u56f4\uff1a \\(0 \\leq p(x) \\leq 1\\)</p> <p>\uff082\uff09\u6982\u7387 \\(p(x)\\) \u548c \u4fe1\u606f\u91cf \\(I(x)\\)  \u662f \u8d1f\u76f8\u5173\u7684 \\(\\rightarrow\\) \\(I(x):=\\frac{1}{p(x)}\\)</p> <p>\uff083\uff09\u4e24\u4e2a\u4e8b\u4ef6\u540c\u65f6\u53d1\u751f\u7684\u4fe1\u606f\u91cf \u7b49\u4e8e \u4e24\u4e2a \u4e8b\u4ef6\u7684\u4fe1\u606f\u91cf\u76f8\u52a0 \\(I(AB)=I(A)+I(B)\\)</p> <p>\\(I(AB)=log_2\\frac{1}{P(AB)}=log_2\\frac{1}{P(A)P(B)}=log_2\\frac{1}{P(A)}+log_2\\frac{1}{P(B)} = I(A)+I(B)\\)</p> <p>\u4fe1\u606f\u91cf\u7684\u5b9a\u4e49\uff1alog\u4ee52\u4e3a\u5e95\u6982\u7387\u5206\u4e4b\u4e00</p> <p>\uff084\uff09\u4ee52\u4e3a\u5e95\uff0c\u662f\u8f6c\u6362\u5230\u4e8c\u8fdb\u5236\u4e0b\u8868\u793a\u590d\u6742\u5ea6\uff08\u4ee5e\u4e3a\u5e95\u3001\u4ee510\u4e3a\u5e95\u90fd\u53ef\u4ee5\uff0c\u53ea\u662f\u4ee52\u4e3a\u5e95\u66f4\u4f18\uff09</p> <p>\u8ba1\u7b97\u4fe1\u606f\u91cf\u7684\u4f8b\u5b50\uff1a</p> <p></p> <p>\u4fe1\u606f\u91cf\u7684\u5b9a\u4e49\uff1alog\u4ee52\u4e3a\u5e95\u6982\u7387\u5206\u4e4b\u4e00</p>"},{"location":"learning/12_KLdivergence/#3","title":"3 \u71b5","text":"<p>\u71b5 Entropy</p> <p>\u5b9a\u4e49\uff1a</p> <p>1\u3001\u6982\u7387\u5206\u5e03\u7684\u4fe1\u606f\u91cf\u671f\u671b</p> <p>2\u3001\u7cfb\u7edf\u6574\u4f53\u7684\u4fe1\u606f\u91cf</p> <p>\u7cfb\u7edf\u6574\u4f53\u7531\u6240\u6709\u53ef\u80fd\u53d1\u751f\u7684\u4e8b\u4ef6\u6784\u6210</p> <p>\u4f8b\u5b50\uff1a\u629b\u786c\u5e01\uff0c\u6b63\u9762\u548c\u53cd\u9762 \u6784\u6210\u4e00\u4e2a\u7cfb\u7edf</p> <p>\u516c\u5f0f\uff1a</p> <p>\\(H(P)=\\sum p_i I_i = \\sum p_i \\log_2 \\frac{1}{p_i} = -\\sum p_i \\log_2 p_i\\)</p> <p>\u8ba1\u7b97\u5b9e\u4f8b\uff1a</p> <p></p> <p>\u4f5c\u7528\uff1a</p> <p>\u7528\u6765\u8bc4\u4f30 \u6982\u7387\u6a21\u578b \u7684\u4e0d\u786e\u5b9a\u7a0b\u5ea6</p> <ul> <li>\u4e0d\u786e\u5b9a\u6027\u8d8a\u5927\uff0c\u71b5\u8d8a\u5927</li> <li>\u4e0d\u786e\u5b9a\u6027\u8d8a\u5c0f\uff0c\u71b5\u8d8a\u5c0f</li> </ul> <p>\u56fe\u793a\uff1a</p> <p></p> <ul> <li>\u6982\u7387\u5b8c\u5168\u76f8\u7b49\u65f6\uff0c\u6211\u4eec\u5b8c\u5168\u4e0d\u786e\u5b9a\u54ea\u4e2a\u4f1a\u53d1\u751f</li> <li>\u5bf9\u4e8e \u7b2c\u4e8c\u5f20\u56fe\uff0c\u7b2c\u4e09\u4e2a\u4e8b\u4ef6\u7684\u6982\u7387\u9ad8\u4e00\u4e9b\uff0c\u6240\u4ee5\u66f4\u6709\u53ef\u80fd\u53d1\u751f\uff0c\u4e5f\u5c31\u662f\u8bf4 \u8fd9\u4e2a\u7cfb\u7edf\u7684\u71b5\u5c0f\u4e00\u4e9b</li> </ul> <p>\u4f8b\u5b50\uff1a</p> <p></p> <p>\u4ece\u4f8b\u5b50\u4e2d\u53ef\u4ee5\u5f97\u51fa\u7ed3\u8bba\uff1a\uff08\u4f8b1 \u7cfb\u7edf\u7684\u71b5 &gt; \u4f8b2 \u7684\u71b5\uff09</p> <ul> <li>\u82e5\u6982\u7387\u5bc6\u5ea6\u5747\u5300\uff0c\u4ea7\u751f\u7684\u968f\u673a\u53d8\u91cf\u4e0d\u786e\u5b9a\u6027\u5c31\u66f4\u9ad8\uff0c\u5219\u71b5\u7684\u503c\u5c31\u66f4\u5927</li> <li>\u82e5\u6982\u7387\u5bc6\u5ea6\u805a\u62e2\uff0c\u4ea7\u751f\u7684\u968f\u673a\u53d8\u91cf\u4e0d\u786e\u5b9a\u6027\u5c31\u66f4\u4f4e\uff0c\u5219\u71b5\u7684\u503c\u5c31\u66f4\u5c0f</li> </ul> <p>\u603b\u7ed3\uff1a</p> <p>\u5982\u679c\u4e00\u4e2a\u7cfb\u7edf\u4e2d\u53ea\u6709\u4e24\u4e2a\u4e8b\u4ef6A\u3001B\uff0c\u4e14\u4e8b\u4ef6A\u53d1\u751f\u7684\u6982\u7387P(A)\uff0c\u4e8b\u4ef6B\u53d1\u751f\u7684\u6982\u7387 P(B)\uff0c\u90a3\u4e48\u8fd9\u4e2a\u7cfb\u7edf\u7684\u71b5\uff1a</p> <p>H(P) = P(A)I(A)+P(B)I(B)</p> <p>\\(= P(A)log_2\\frac{1}{P(A)} + P(B)log_2\\frac{1}{P(B)}\\)</p> <p>\\(H(P)=\\sum_{i=1}^nP_iI(i)\\) \\(\\sum_iP_i = 1\\)</p>"},{"location":"learning/12_KLdivergence/#4","title":"4 \u4ea4\u53c9\u71b5","text":"<p>\u4ea4\u53c9\u71b5 cross entropy  </p> <p>\u5b9a\u4e49\uff1a</p> <p>\u5047\u8bbe \u771f\u5b9e\u6982\u7387\u5206\u5e03\u4e3a p\uff0c\u9884\u6d4b\u6982\u7387\u5206\u5e03\u4e3a q\uff0c\u5219\u9884\u6d4b\u6982\u7387\u5206\u5e03q \u5bf9 \u771f\u5b9e\u6982\u7387\u5206\u5e03p\u7684\u5e73\u5747\u4fe1\u606f\u91cf\u7684\u4f30\u8ba1\uff0c\u53eb\u505a\u4ea4\u53c9\u71b5</p> <ul> <li>\uff08\u9884\u6d4b\u6982\u7387\u5206\u5e03 \u4e5f\u53eb \u4f30\u8ba1\u6982\u7387\u5206\u5e03\uff09</li> </ul> <p>\u516c\u5f0f\uff1a</p> <p>\\(H(p,q)=\\sum p_iI_i^q=-\\sum p_i \\log_2(q_i)\\)</p> <p>\\(H(p,q) = \\sum p_iI(q_i)\\)</p> <p>\u4f8b\u5b50\uff1a</p> <p></p> <p>\u7531\u4f8b\u5b50\u7684\u7ed3\u679c\uff0c\u89c2\u5bdf\u53ef\u77e5\uff1a</p> <p>\uff081\uff09\u9884\u4f30\u7231\u4fa3\u5206\u5e03 \u4e0e \u771f\u5b9e\u6982\u7387\u5206\u5e03 \u8d8a\u63a5\u8fd1\uff0c\u4ea4\u53c9\u71b5\u8d8a\u5c0f</p> <p>\uff082\uff09\u4ea4\u53c9\u71b5\u7684\u503c \u603b\u662f\u5927\u4e8e \u71b5\u7684\u503c \uff08\u6839\u636e \u5409\u5e03\u65af\u4e0d\u7b49\u5f0f\uff09</p> <p>\uff083\uff09\u5bf9\u4e8e\u4f8b\u5b50\u4e2d\uff0c\\(P(A)=P(B)=\\frac{1}{2}\\)</p> <p>\u71b5 \\(H(P)=1\\)</p> <p>\u4f46\u662f\u8ba1\u7b97\u51fa\u6765\u7684 \u4ea4\u53c9\u71b5 \\(H(p,q)&gt;H(p)\\)</p> <p>\u8865\u5145\uff1a\u5409\u5e03\u65af\u4e0d\u7b49\u5f0f</p> <p>\u82e5 \\(\\sum_{i=1}^np_i=\\sum_{i=1}^nq_i=1\\)\uff0c\u4e14 \\(p_i\uff0cq_i \\in (0,1]\\)\uff0c\u5219\u6709\uff1a</p> <p>\\(-\\sum_{i=1}^np_ilogp_i\\leq-\\sum_{i=1}^np_ilogq_i\\)</p> <p>\u7b49\u53f7\u6210\u7acb\uff0c\u5f53\u4e14\u4ec5\u5f53 \\(p_i =q_i \\ {\\forall} i\\)</p> <p>\u771f\u5b9e\u6982\u7387\u7684\u71b5 \u6c38\u8fdc \u5c0f\u4e8e\u7b49\u4e8e \u4ea4\u53c9\u71b5</p>"},{"location":"learning/12_KLdivergence/#5-kl","title":"5 KL\u6563\u5ea6\uff08\u76f8\u5bf9\u71b5\uff09","text":"<p>\u76f8\u5bf9\u71b5 Relative Entropy\u3001KL\u6563\u5ea6 \uff08KL divergence\u3001Kullback-Leibler divergence\uff09</p> <p>\u4f5c\u7528\uff1a </p> <p>\u7528\u4e8e\u8861\u91cf2\u4e2a\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02</p> <p>\u6839\u636e\u540e\u9762\u7684\u516c\u5f0f\uff0c\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5c31\u662f\u4e24\u4e2a\u5206\u5e03\u4fe1\u606f\u91cf\u7684\u5dee</p> <p>\u516c\u5f0f\uff1a</p> <p>\\(D_{KL}(p||q) = \\sum p_i (I_q-I_p)\\)  \u3001 \\(I_q - I_p\\)  \u4e3a\u4fe1\u606f\u91cf\u4e4b\u5dee</p> <p>\\(=\\sum p_i(log_2\\frac{1}{q_i}-log_2\\frac{1}{p_i})\\)</p> <p>\\(=\\sum p_ilog_2\\frac{1}{q_i}-p_ilog_2\\frac{1}{p_i}\\)</p> <p>\\(=H(p,q)-H(p)\\)  \uff08\u6240\u4ee5\u53eb \u76f8\u5bf9\u71b5\u3001\u6216\u8005 \\(H(p)=H(p,p)\\)\uff09</p> <p>\\(= \\sum p_i log_2\\frac{p_i}{q_i}\\)  (\u5e38\u7528\u7684\u5c55\u5f00\u5f0f)</p> <ul> <li>\\(p\u548cq\u7684\u4ea4\u53c9\u71b5 - p\u7684\u71b5\\)</li> </ul> <p>\u91cd\u8981\u6027\u8d28\uff1a</p> <p>\uff081\uff09\\(D(p||q)\\) \u4e0e \\(D(q||p)\\) \u4e0d\u4e00\u6837\uff0c\u5373 \\(D(p||q) \\neq D(q||p)\\)</p> <ul> <li>\\(D(p||q)\\)  \u8868\u793a \u4ee5\\(p\\)\u4e3a\u57fa\u51c6\uff08\\(p\\)\u4e3a\u771f\u5b9e\u6982\u7387\u5206\u5e03\uff09\uff0c\u4f30\u8ba1\u6982\u7387\u5206\u5e03 \\(q\\) \u4e0e \u771f\u5b9e\u6982\u7387\u5206\u5e03 \\(p\\)\u4e4b\u95f4\u7684\u5dee\u8ddd</li> <li>\\(D(q||p)\\) \u8868\u793a \u4ee5\\(q\\)\u4e3a\u57fa\u51c6\uff08\\(q\\)\u4e3a\u771f\u5b9e\u6982\u7387\u5206\u5e03\uff09\uff0c\u4f30\u8ba1\u6982\u7387\u5206\u5e03\\(p\\)\u4e0e\u771f\u5b9e\u6982\u7387\u5206\u5e03\\(q\\)\u4e4b\u95f4\u7684\u5dee\u8ddd</li> <li>\u524d\u9762\u7684\u5206\u5e03\u4f5c\u4e3a \u771f\u5b9e\u6982\u7387\u5206\u5e03\uff0c\u8ba1\u7b97<code>\u771f\u5b9e\u5206\u5e03\u4e0e\u4f30\u8ba1\u5206\u5e03\u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5-\u771f\u5b9e\u5206\u5e03\u7684\u4fe1\u606f\u71b5</code></li> </ul> <p>\uff082\uff09\\(KL\u6563\u5ea6\u7684\u503c \\geq 0\\) \\(\\iff\\) \\(D_{KL}(p||q) \\geq 0\\)</p> <p>\u5409\u5e03\u65af\u4e0d\u7b49\u5f0f\u8bf4\u660e\uff1a</p> <p></p> <p>\u6240\u4ee5   \\(D_{KL}(p||q) \\geq 0\\)</p> <p>\u7279\u522b\u7684\uff0c\u5f53\u5206\u5e03 \\(q\\)\u4e0e\u5206\u5e03\\(p\\)\u5b8c\u5168\u4e00\u6837\u65f6\uff0c\\(D(p||q)=0\\)</p> <p>\u771f\u5b9e\u6982\u7387p\u7684\u71b5 \\(\\leq\\)  p\u548cq\u7684\u4ea4\u53c9\u71b5</p> <p>\u6240\u4ee5 \\(D_{KL}(p||q)=H(p,q)-H(p) \\geq 0\\)</p>"},{"location":"learning/12_KLdivergence/#6","title":"6 \u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570","text":"<p>\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570 Cross Entropy Loss</p> <p>\u7531\u4e0a\u53ef\u77e5\uff0cKL\u6563\u5ea6\\(D(p||q)\\) \u8868\u793a \u9884\u6d4b\u5206\u5e03q \u4e0e\u771f\u5b9e\u5206\u5e03p \u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6240\u4ee5 \u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5c06 \u635f\u5931\u51fd\u6570 \u5b9a\u4e49\u4e3a KL\u6563\u5ea6\uff1a\\(Loss = D(p||q)\\)</p> <p>\u635f\u5931\u51fd\u6570 = KL\u6563\u5ea6</p> <p>\u5e76\u4e14\u6211\u4eec\u5e0c\u671b \u6a21\u578b\u7684\u9884\u6d4b\u5206\u5e03q \u4e0e \u771f\u5b9e\u5206\u5e03p \u5b8c\u5168\u76f8\u540c\uff0c\u5373\uff1a\u635f\u5931\u51fd\u6570 \\(Loss=D(p||q)=0\\)</p> <p>\u635f\u5931\u51fd\u6570\uff1a</p> <p>\\(Loss = D(p||q)=H(p,q)-H(p)\\)</p> <ul> <li>p\u4e0eq\u7684\u4ea4\u53c9\u71b5 - p\u7684\u4fe1\u606f\u71b5</li> <li>p\u662f\u771f\u5b9e\u5206\u5e03</li> <li>\\(D(p||q)=H(p,q)-H(p)=\\sum p_i log_2(\\frac{1}{q_i})-\\sum p_ilog_2\\frac{1}{p_i}\\)</li> </ul> <p>\u5bf9\u4e8e\u5206\u7c7b\u95ee\u9898\uff1a</p> <ul> <li>\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u4e00\u822c\u7528\u4e8e\u5206\u7c7b\u95ee\u9898\uff0c\u5206\u7c7b\u95ee\u9898\u4e00\u822c\u662f\u4e00\u4e2a\u5355\u70b9\u5206\u5e03</li> <li>\\(\\iff\\) \u7b49\u4ef7\u4e8e \u771f\u5b9e\u7c7b\u522b \u6982\u7387 = 1\uff0c\u5176\u4ed6\u7c7b\u522b\u6982\u7387 = 0</li> </ul> <p>\u771f\u5b9e\u5206\u5e03\u662f\u4e00\u4e2a\u5355\u70b9\u5206\u5e03\uff0c\u771f\u5b9e\u7c7b\u522b\u7684\u6982\u7387\u4e3a1\uff0c\u5176\u4ed6\u7c7b\u522b\u7684\u6982\u7387\u4e3a0\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> \u7c7b\u522b class1 class2 class3 class4 \u6982\u7387 0 0 1 0 <ul> <li>\\(p_{class1} = p_{class2} = p_{class4} = 0\\)</li> <li>$\\log2(\\frac{1}{p_{classs3}}) = 0 $ </li> <li>\u6240\u4ee5\uff0c\\(H(p)=\\sum p_i I(p_i) = \\sum p_i log_2{\\frac{1}{p_i}}=0\\)</li> </ul> <p>\u63a8\u5bfc\uff1a</p> <p>\u9996\u5148\u6709 \u635f\u5931\uff1a</p> <p>\\(Loss = D(p||q)=H(p,q)-H(p) = \\sum p_i log_2(\\frac{1}{q_i})-\\sum p_ilog_2\\frac{1}{p_i}\\)</p> <p>\u7531\uff0c\\(H(p)=0\\)</p> <p>\u2234 \\(Loss = H(p,q)-H(p) = H(p,q)\\)</p> <p>\u7531 \\(H(p,q)\\) \u662f\u4ea4\u53c9\u71b5\uff0c\u6240\u4ee5\u635f\u5931\u51fd\u6570 \u53c8\u79f0\u4e3a \u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\uff1a</p> <p>\\(Cross\\_Entropy\\_Loss = H(p,q) = \\sum p_i log_2\\frac{1}{q_i}\\)</p>"},{"location":"learning/13_RNN/","title":"RNN","text":"<p>ref\uff1a\u301029\u3001PyTorch RNN\u7684\u539f\u7406\u53ca\u5176\u624b\u5199\u590d\u73b0\u3011</p> <p></p> <p>topic\uff1a</p> <p>\uff081\uff09\u4e0d\u540c\u7c7b\u578b\u7684RNN\u7684\u56fe\u793a\u4ee5\u53ca\u5e94\u7528\u573a\u666f\u7684\u56fe\u793a</p> <p>\uff082\uff09\u4ecb\u7ecdpytorch\u4e2dRNN\u7684api\u7684\u4f7f\u7528</p> <p>\uff083\uff09\u901a\u8fc7\u4ee3\u7801\u9a8c\u8bc1 RNN \u5185\u90e8\u662f\u5982\u4f55\u8ba1\u7b97\u7684\uff0c\u901a\u8fc7\u4ee3\u7801\u6765 \u9a8c\u8bc1 pytorch\u7684RNN\u7684api \u5e76\u5bf9\u6bd4\u7ed3\u679c</p>"},{"location":"learning/13_RNN/#k1","title":"k1 \u8bb0\u5fc6\u5355\u5143\u5206\u7c7b","text":"<ul> <li> \u4ec0\u4e48\u662f\u8bb0\u5fc6\u5355\u5143\uff1f</li> </ul> <p>\u8bb0\u5fc6\u5355\u5143\u5c31\u662f \u5b58\u50a8\u7684 \u8fc7\u53bb\u7684\u5386\u53f2\u4fe1\u606f</p> <ul> <li> \u4ec0\u4e48\u662f\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff1f</li> </ul> <p>\u6240\u8c13\u5faa\u73af\u795e\u7ecf\u7f51\u7edc \u5c31\u662f\u8bf4\uff0c\u5728\u5bf9\u5e8f\u5217\u8fdb\u884c\u5efa\u6a21\u7684\u65f6\u5019\uff0c\u5728\u7b97\u6bcf\u4e00\u65f6\u523b\u7684\u8868\u5f81\u7684\u65f6\u5019\uff0c\u4e00\u822c\u8003\u8651\u8fc7\u53bb\u7684 \u5386\u53f2\u4fe1\u606f\u3002\u8fd9\u4e2a\u5386\u53f2\u4fe1\u606f \u5c31\u662f\u901a\u8fc7 \u8bb0\u5fc6\u5355\u5143 \u4fdd\u5b58\u7684\u3002\u7136\u540e\u6bcf\u4e2a\u65f6\u523b \u6211\u4eec\u90fd\u4f1a\u4ece \u8bb0\u5fc6\u5355\u5143\u4e2d \u83b7\u53d6 \u8fc7\u53bb\u7684 \u5386\u53f2\u4fe1\u606f\uff0c\u7136\u540e\u8f85\u52a9\u5f53\u524d\u65f6\u523b \u505a\u9884\u6d4b\u3002</p> <ul> <li> \u8bb0\u5fc6\u5355\u5143\u5206\u7c7b</li> </ul> <p>\u5173\u4e8e\u8bb0\u5fc6\u5355\u5143 \u4e00\u822c\u6709\u4e09\u7c7b</p> <ol> <li>RNN</li> <li>LSTM</li> <li>GRU </li> </ol> <p>\u4e00\u7c7b \u6bd4\u5982\u8bf4 RNN\uff0c\u6bd4\u5982\u8bf4 Simple RNN\uff0c\u7b80\u5355\u7684RNN \u7ed3\u6784\uff0c\u7b49\u4e0b\u5b9e\u73b0\u7684\u4e5f\u662f \u7b80\u5355\u7684RNN\u7ed3\u6784</p> <p>\u53e6\u5916\u4e24\u79cd\u662f GRU\u548cLSTM\uff0c\u8fd9\u4e24\u79cd\u7f51\u7edc\u7684\u8bb0\u5fc6\u6027\u4f1a\u66f4\u5f3a\u4e00\u70b9\uff1b\u8ba1\u7b97\u590d\u6742\u5ea6\u4e5f\u4f1a\u66f4\u9ad8\u4e00\u70b9\uff1b\u4f7f\u7528\u9891\u7387\u4e5f\u4f1a\u66f4\u9ad8\u4e00\u70b9\uff0c\u5c31\u662f\u8bf4 \u73b0\u5728\u5f88\u591a\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6211\u4eec\u57fa\u672c\u4f7f\u7528\u7684\u662fLSTM\u6216\u8005GRU\uff1b\u4f46\u662f\u5b83\u4eec\u90fd\u662fRNN\u7684\u4e00\u4e2a\u53d8\u4f53\uff0c\u6240\u4ee5RNN\u662f\u57fa\u7840\uff1b</p>"},{"location":"learning/13_RNN/#k2","title":"k2 \u6a21\u578b\u7684\u5206\u7c7b","text":"<p>\uff081\uff09\u5355\u5411\u5faa\u73af</p> <p>\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e5f\u53ef\u5206\u4e3a\u5355\u5411\u5faa\u73af\uff0c\u6240\u8c13\u5355\u5411\u5faa\u73af\u5c31\u662f\uff0c\u5f53\u524d\u65f6\u523b\u7684\u9884\u6d4b \u53ea\u8ddf \u8fc7\u53bb\u6709\u5173\uff0c\u4ece\u5de6\u5230\u53f3 \u9012\u5f52\u7684\u8ba1\u7b97\u3002</p> <p>\uff082\uff09\u53cc\u5411\u5faa\u73af</p> <p>\u53cc\u5411\u5faa\u73af\uff0c\u53cc\u5411\u5faa\u73af\u5c31\u662f\u8bf4 \u4e0d\u53ea\u6709 \u4ece\u5de6\u5230\u53f3\u7684 \u4e5f\u6709 \u4ece\u53f3\u5230\u5de6\u7684\uff0c\u5c31\u662f\u8bf4\u6709\u4e24\u6761\u94fe\uff0c\u53e6\u5916\u4e00\u6761\u94fe\uff0c\u5728\u8ba1\u7b97\u5f53\u524d\u65f6\u523b\u7684\u9884\u6d4b\u7684\u65f6\u5019 \u4f1a\u8003\u8651 \u672a\u6765\u4fe1\u606f\u3002</p> <p>\uff083\uff09\u591a\u4e2a\u5355\u5411 \u3001 \u591a\u4e2a\u53cc\u5411</p> <p>\u8fd9\u4e2a\u5c31\u662f\u53cc\u5411\u5faa\u73af\uff1b\u90a3\u8fd8\u53ef\u4ee5\u628a \u591a\u4e2a\u5355\u5411 \u6216\u8005\u8bf4 \u591a\u4e2a\u53cc\u5411 \u53e0\u52a0\u8d77\u6765\uff0c\u4e5f\u5c31\u662fdeep RNN \u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc</p> <p></p> <p>\uff081\uff09\u5355\u5411\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc</p> <p></p> <p>\u53ef\u4ee5\u5206\u4e3a\u4e09\u5c42\uff1a</p> <ol> <li>\u6700\u4e0b\u9762\u4e00\u5c42\u662f input layer\uff0c\u4e5f\u5c31\u662f\u8f93\u5165\u5c42\uff1b</li> <li>\u4e2d\u95f4\u662f\u9690\u542b\u5c42\uff1b</li> <li>\u6700\u540e\u662f\u8f93\u51fa\u5c42\uff1b</li> </ol> <p>\u4e0b\u9762\u7684\u8f93\u5165\u5c42\u6bcf\u4e00\u4e2a\u795e\u7ecf\u5143 \u53ef\u4ee5\u770b\u505a \u6bcf\u4e00\u4e2a\u65f6\u523b\uff1b</p> <p>\u4e5f\u5c31\u662f\u8bf4 \u6bcf\u4e00\u4e2a\u65f6\u523b \u4e0d\u4ec5\u8ddf\u5f53\u524d\u65f6\u523b\u7684\u8f93\u5165\u6709\u5173\uff0c\u8fd8\u8ddf\u4e0a\u4e00\u65f6\u523b\u7684\u8bb0\u5fc6\u5355\u5143\u6709\u5173\uff1b</p> <p>\u5e76\u4e14\u5728\u5355\u5411\u5faa\u73af\u795e\u7ecf\u7f51\u7edc \u4e2d \u59cb\u7ec8\u662f \u4ece\u5de6\u5230\u53f3\u7684\uff1b</p> <p>\u5c31\u662f\u8bf4\u5f53\u524d\u65f6\u523b\u7684\u9884\u6d4b \u53ea\u8ddf \u8fc7\u53bb\u7684\u8bb0\u5fc6\u5355\u5143 \u6709\u5173\uff0c\u8ddf\u672a\u6765\u7684 \u662f\u65e0\u5173\u7684\uff1b</p> <p>\uff082\uff09\u53cc\u5411\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc</p> <p></p> <ol> <li>\u6709\u4e24\u6761\u94fe</li> <li>\u5206\u4e3a4\u4e2a\u90e8\u5206\uff1a  input layer\u3001output layer\u3001forward layer\u3001backward layer</li> <li>\uff08forward layer\uff09  forward layer\u662f\u4ece\u5de6\u5230\u53f3\u7684\u5faa\u73af \uff0c\u610f\u601d\u5c31\u662f\u8bf4 \u5728 forward layer\u7684\u8f93\u51fa\u4e2d\uff0c\u5b83\u7684\u8f93\u51fa\u4e0d\u4ec5\u8ddf\u5f53\u524d\u8f93\u5165\u6709\u5173 \u4e5f\u8ddf\u8fc7\u53bb\u7684\u8bb0\u5fc6\u5355\u5143\u6709\u5173\uff1b</li> <li>\uff08backward layer\uff09  backward layer\u5f53\u4e2d\uff0c\u5b83\u7684\u5f53\u524d\u65f6\u523b\u7684\u8f93\u51fa \u4e0d\u4ec5\u8ddf\u5f53\u524d\u65f6\u523b\u7684\u8f93\u5165\u6709\u5173\uff0c\u8fd8\u8ddf\u672a\u6765\u65f6\u523b\u7684\u8bb0\u5fc6\u5355\u5143\u6709\u5173\uff0c\u6240\u4ee5\u662f \u4ece\u53f3\u5230\u5de6\u7684 \u9012\u5f52\u8fd0\u7b97\u7684\u3002</li> <li>**\uff08\u5c06forward\u548cbackward\u7ed3\u5408\uff09**\u8d77\u6765\u6709\u4ec0\u4e48\u597d\u5904\u5462\uff1f \u5c31\u662f\u8bf4 \u65e2\u80fd\u770b\u5230\u8fc7\u53bb \u53c8\u80fd\u770b\u5230\u672a\u6765</li> </ol>"},{"location":"learning/13_RNN/#k3","title":"k3 \u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u6027\u80fd\u6bd4\u8f83","text":"<p>\u8fd9\u5f20\u8868\u683c \u6765\u81ea\u67d0\u7bc7\u8bba\u6587\uff0c\u8fd9\u5f20\u8868\u683c \u5f88\u597d\u7684 \u5c55\u793a\u4e86 RNN\u3001LSTM\u3001 \u53cc\u5411 \u5355\u5411\u3001MLP\u3001\u4ee5\u53ca\u662f\u5426delay\u7b49 \u5728\u53c2\u6570\u6570\u91cf\u76f8\u7b49\u7684\u60c5\u51b5\u4e0b \u5728\u8bed\u97f3\u8bc6\u522b\u4e0a\u7684\u8868\u73b0\uff1b\u53ef\u4ee5\u770b\u5230 \u7b2c\u4e8c\u5217 \u7b2c\u4e09\u5217 \u5206\u522b\u662f\u8bad\u7ec3\u8bef\u5dee\u548c\u6d4b\u8bd5\u8bef\u5dee\uff1b </p> <p>\u901a\u8fc7\u8868\u683c \u53ef\u4ee5\u770b\u5230 \u4e0d\u540c\u7684\u6a21\u578b\u5728 \u8bed\u97f3\u8bc6\u522b \u8fd9\u79cd \u5e8f\u5217\u5efa\u6a21\uff0c\u5e8f\u5217\u5206\u7c7b\u8fd9\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0</p> <p></p> <p>\uff081\uff09\u7b2c\u4e00\u884c\u662fMLP\uff0cMLP\u5c31\u662f\u7b80\u5355\u7684DNN \u662fno window\u7684\uff08\u4ec0\u4e48\u610f\u601d\uff1f\uff09</p> <p>\u6211\u4eec\u628a\u8bed\u97f3 \u5206\u6210\u5f88\u591a\u5e27\uff0c\u6bd4\u65b9\u8bf4\u4e00\u5e27\u662f 15\u6beb\u79d2 \u6216\u8005 20\u6beb\u79d2\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u5e27 \u63d0\u53d6\u4e00\u4e2a\u7279\u5f81 \u6bd4\u5982\u8bf4 \u5085\u91cc\u53f6\u53d8\u6362 \u5f97\u5230\u4e00\u4e2a\u9891\u8c31\u7279\u5f81\uff0c\u7136\u540e \u6211\u4eec\u5bf9\u6bcf\u4e00\u5e27 \u8fdb\u884c\u5355\u72ec\u5efa\u6a21\uff0c\u6240\u8c13 no window\u5c31\u662f \u6211\u4eec\u4e0d\u8003\u8651 \u5468\u56f4\u7684\u5e27\uff0c\u53ea\u8003\u8651 \u5f53\u524d\u8fd9\u4e2a15\u6beb\u79d2\uff0c\u7136\u540e \u6211\u4eec \u628a\u5b83\u9001\u5165 DNN\u4e2d\uff0c\u6765\u53bb \u8fdb\u884c\u4e00\u4e2a \u9884\u6d4b \u5206\u7c7b\uff0c\u8fd9\u6837\u505a\u7684\u8bdd \u5b83\u7684 \u8bad\u7ec3\u8bef\u5dee \u548c\u6d4b\u8bd5\u8bef\u5dee \u5927\u6982\u90fd\u662f\u5728 40% \u5de6\u53f3\uff1b</p> <p></p> <p>\uff082\uff09\uff0810 frame window\u3001stride\uff09</p> <p>\u7b2c\u4e8c\u884c MLP 10\u5e27\u4f5c\u4e3a\u4e00\u4e2a\u7a97 \u610f\u601d\u662f \u6211\u4eec\u73b0\u5728 \u540c\u6837\u8fd8\u662fMLP\uff0c\u4f46\u662f \u73b0\u5728MLP\u7684 \u8f93\u5165 \u4e0d\u4ec5\u662f \u53ea\u6709\u4e00\u5e27\u7684\u7279\u5f81\uff0c\u800c\u662f\u628a \u6bcf10\u5e27 \u653e\u5230\u4e00\u8d77\uff0c\u90a3\u4e48\u8fd9\u91cc\u662f\u5426\u6709stride\uff0c\u5c31\u662f\u8bf4 \u8fd910\u5e27 \u5230\u5e95\u6709\u6ca1\u6709\u4ea4\u53e0 \u5e76\u6ca1\u6709\u4ecb\u7ecd\uff0c\u603b\u4e4b \u7b2c\u4e8c\u884c\u8fd9\u4e2a \u8f93\u5165 \u6bd4 \u7b2c\u4e00\u884c \u8986\u76d6\u7684 \u65f6\u95f4\u7a97\u53e3 \u4f1a\u66f4\u5927\u4e00\u70b9 \uff1b</p> <p>\u90a3\u4e48\u8fd9\u6837\u53ef\u4ee5\u770b\u5230 \u8fd9\u4e2a\u8bef\u5dee\uff0c\u663e\u8457\u7684\u4ece 46% \u964d\u5230 32%\uff0c\u8fd9\u4e2a\u7ed3\u679c\u8bf4\u660e \u5728\u8bed\u97f3\u8bc6\u522b \u8fd9\u4e2a\u5e8f\u5217\u5efa\u6a21 \u4efb\u52a1\u4e2d\uff0c\u5f53\u6211\u4eec\u628a \u4e0a\u4e0b\u6587\u7279\u5f81 \u4e00\u8d77\u8003\u8651\u7684\u8bdd \u6548\u679c\u4f1a \u66f4\u597d\uff1b\u8fd9\u662f\u7b2c\u4e8c\u884c\u3002</p> <p></p> <p>\uff083\uff09delay</p> <p>\u7b2c\u4e09\u884c\uff0c\u5c06MLP\u6362\u6210\u4e86 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u4e00\u4e2a\u7b80\u5355\u7684RNN \u6a21\u578b\uff0c\u5e76\u4e14\u62ec\u53f7 delay 0\uff0c\u7b49\u4e0b\u4f1a\u89e3\u91ca \u4ec0\u4e48\u53ebdelay\uff0c\u8fd9\u91cc\u7684\u610f\u601d\u5c31\u8bf4\uff0c\u5c31\u662f\u8bf4 \u628a \u6bcf\u4e00\u5e27\u7279\u5f81 \u50cf \u7b2c\u4e00\u5e45\u56fe\u4e00\u6837\uff0c\u6bd4\u5982\u8bf4</p> <p></p> <p>\u8fd9\u91cc\u662f\u7b2c\u4e00\u5e27\u7684\u7279\u5f81\uff0c\u8fd9\u91cc\u662f\u7b2c\u4e8c\u5e27\u7684\u7279\u5f81\uff0c\u8fd9\u91cc\u662f\u7b2c\u4e09\u5e27\u7684\u7279\u5f81\uff0c\u6211\u4eec\u628a\u6bcf\u4e00\u5e27\u7684\u7279\u5f81 \u9001\u5165\u5230RNN\u4e2d\uff0c\u901a\u8fc7\u4e2d\u95f4\u7684\u9690\u542b\u5c42 \u5bf9\u5386\u53f2\u4fe1\u606f \u8fdb\u884c\u66f4\u65b0\uff0c\u8fd9\u6837\u7684\u7f51\u7edc \u9519\u8bef\u7387\u4e5f\u662f\u76f8\u6bd4MLP \u66f4\u8fdb\u4e00\u6b65\uff0c\u770b\u5230\u8bad\u7ec3\u8bef\u5dee\u523030%\uff0c\u6d4b\u8bd5\u8bef\u5dee\u662f35%\uff0c\u76f8\u6bd4\u4e8e\u4e0a\u9762 10\u5e27\u7684MLP\uff0c\u6548\u679c\u66f4\u597d\u3002</p> <p>\uff084\uff09LSTM</p> <p></p> <ul> <li>\u63a5\u4e0b\u6765 \u5982\u679c\u6211\u4eec\u628aRNN\uff0c\u66ff\u6362\u6210LSTM\uff0c\u6548\u679c\u66f4\u8fdb\u4e00\u6b65</li> <li>\u90fd\u662fdelay 0</li> </ul> <p>\uff085\uff09LSTM+backwards</p> <p></p> <p>\u518d\u4e0b\u9762\u4e00\u6b65\uff0c\u8fd8\u662fLSTM\uff0c\u53ea\u662f\u628a\u8f93\u5165 \u7ffb\u8f6c\u8fc7\u6765\uff0c\u4e5f\u5c31\u662f\u628ainput\u5e8f\u5217\u5012\u8fc7\u6765\uff0c\u518d\u8f93\u5165\u5230\u7f51\u7edc\u4e2d\uff0c\u8bef\u5dee\u662f\u5dee\u4e0d\u591a\u7684\uff0c\u6240\u4ee5 \u4ec5\u4ec5\u662f\u4e00\u6761\u94fe\u7684\u8bdd\uff0c\u4e0d\u8bba\u662f\u6b63\u5411\u8bc6\u522b\uff0c\u8fd8\u662f\u53cd\u5411\u8bc6\u522b \u5176\u5b9e\u6548\u679c\u662f\u5dee\u4e0d\u591a\u7684</p> <p>\uff086\uff09RNN delay 3</p> <p></p> <p>\u5bf9\u8f93\u5165\u8fdb\u884c\u6539\u9020\uff0c\u9996\u5148\u53ef\u4ee5\u770b\u5230 \u540c\u6837\u662f\u7528 RNN\u7f51\u7edc\uff0c\u8fd9\u91cc \u5bf9\u5b83 \u8fdb\u884c delay \u4e09\u5e27\uff0c\u7136\u540e\u53ef\u4ee5\u770b\u5230 \u5b83\u7684\u6548\u679c \u76f8\u6bd4\u4e8e\u539f\u672c\u7684 RNN \u4ece30% \u964d\u4f4e\u5230 29%\uff0c\u6d4b\u8bd5\u8bef\u5dee \u4e5f\u662f\u4ece 35% \u964d\u4f4e\u5230 34%\uff1b</p> <ul> <li> \u90a3\u4e48\u8fd9\u4e2a delay 3 \u5e27\u662f\u4ec0\u4e48\u610f\u601d\u5462\uff1f</li> </ul> <p></p> <p>delay 3 \u5e27\u7684\u610f\u601d\u5c31\u662f\u8bf4\uff0c\u5f53 \u5582\u5165 \u4e09\u5e27 \u4f5c\u4e3a \u8f93\u5165\u7684\u65f6\u5019\uff0c\u524d\u9762 \u8fd9\u4e09\u4e2a\u8f93\u51fa\uff0c\u5148\u4e0d\u8981\uff0c</p> <p>\u5c31\u662f\u8bf4 \u5148\u62ff \u4e09\u5e27\u8f93\u5165 \u9001\u5165\u5230\u7f51\u7edc\u4e2d \u8ba9\u5b83\u5148\u5bf9\u8bb0\u5fc6\u5355\u5143 \u53bb \u66f4\u65b0\u4e09\u6b65 \uff0c\u7136\u540e\u5230\u7b2c\u56db\u6b65\uff08\u5e27\uff09\u7684 \u8f93\u5165\u7684\u65f6\u5019\uff0c\u624d \u628a \u8f93\u51fa\u62ff\u51fa\u6765\uff0c \u4f5c\u4e3a \u7b2c\u4e00\u5e27\u7684\u9884\u6d4b\u503c\uff0c\u8fd9\u4e2a\u5c31\u662fdelay 3\u7684\u610f\u601d</p> <ul> <li> \u4e3a\u4ec0\u4e48 delay 3 \u5e27\u6548\u679c\u6709\u6548\uff1f</li> </ul> <p>\u5982\u679c \u4e0d\u505adelay\u7684\u8bdd \uff0c\u5728 \u8f93\u5165 \u7b2c\u4e00\u5e27\u7684 \u7279\u5f81\u7684\u65f6\u5019\uff0c\u5b83\u7684\u9884\u6d4b\u7684\u8f93\u51fa \u53ea\u80fd \u770b\u5230\u5f53\u524d\u7684\u7b2c\u4e00\u5e27\uff0c\u8303\u56f4\u5c31\u5f88\u5c0f\uff1b</p> <p>\u4f46\u662f\u5f53 delay \u4e09\u5e27\u7684\u65f6\u5019 \u9884\u6d4b\u7b2c\u4e00\u5e27\u7684\u8f93\u51fa \u5176\u5b9e\u5c31\u770b\u5230\u4e86 \u4e09\u5e27\uff0c\u5b83\u770b\u5230\u4e86 \u7b2c\u4e00\u5e27\u3001\u7b2c\u4e8c\u5e27\u3001\u7b2c\u4e09\u5e27 \u90fd\u8fdb\u5165\u4e86 \u8bb0\u5fc6\u5355\u5143\u4e2d\uff1b</p> <p>\u4ee5\u4e0a\u5c31\u662f delay RNN\u7684\u7ed3\u6784\uff1b</p> <ul> <li> \u518d\u6b21\u89e3\u91ca delay</li> </ul> <p>delay \u80fd\u591f\u5728 \u77ed\u6682 \u7684 \u727a\u7272 \u65f6\u5ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u770b\u5230\u66f4\u5bbd\u7684\u4e0a\u4e0b\u6587</p> <p>\u6709 delay \u7684\u8bdd\uff0c\u5728\u9884\u6d4b\u7b2c\u4e00\u5e27\u7684\u8f93\u51fa\u7684\u65f6\u5019 \u80af\u5b9a\u4f1a \u7a0d\u5fae \u5ef6\u8fdf\u4e00\u70b9\uff0c\u56e0\u4e3a \u5982\u679c \u4e0d\u505a delay\u7684\u8bdd\uff0c\u6211\u4eec\u5c31\u76f4\u63a5 \u7b97\u4e00\u6b65\u5c31\u597d\u4e86\uff0c\u5982\u679cdelay \u4e09\u5e27\u7684\u8bdd\uff0c\u90a3\u5728\u9884\u6d4b\u7b2c\u4e00\u5e27\u7684\u8f93\u51fa\u7684\u65f6\u5019\uff0c\u9700\u8981\u8ba1\u7b97 \u4e09\u6b65\uff0c\u6240\u4ee5\u80af\u5b9a\u4f1a\u6709 \u4e00\u5b9a\u65f6\u5ef6\u7684\u3002\u4f46\u662f\u8fd9\u4e2a\u65f6\u5ef6 \u786e\u5b9e\u80fd\u591f \u4f7f\u5f97 \u9884\u6d4b\u7684\u6548\u679c\u66f4\u597d\uff0c\u56e0\u4e3a\u5b83\u770b\u5230\u7684\u4e0a\u4e0b\u6587 \u4f1a \u66f4\u5bbd\u4e00\u70b9\uff1b\u4ee5\u4e0a\u662fdelay\u7684\u610f\u601d\u3002</p> <p>\uff087\uff09B </p> <p></p> <p>\u53cc\u5411\u7684LSTM\u3001RNN</p> <p></p> <ul> <li>RNN delay\u4e09\u5e27 \u548cLSTM delay \u4e94\u5e27 \u6548\u679c\u90fd\u6709\u4e0d\u540c\u7a0b\u5ea6\u7684\u589e\u52a0\uff1b</li> <li>\u53cc\u5411\u7684\u7ed3\u679c\u6bd4delay \u548c \u5355\u5411\u7684 \u6548\u679c\u90fd\u8981\u597d\uff1b</li> <li>\u8bad\u7ec3\u96c6 \u9519\u8bef\u7387\u4ece29%\u964d\u4f4e\u523024%\uff1b</li> <li>\u6d4b\u8bd5\u96c6\u9519\u8bef\u7387\u4e5f\u662f\u660e\u663e\u964d\u4f4e\uff1b</li> </ul> <p>\u53cc\u5411\u3001delay </p> <ul> <li>\u8868\u793a \u770b\u5230\u4e86\u672a\u6765\u7684\u4fe1\u606f\uff1b</li> <li>\u5f53 delay\u4e09\u5e27\u7684\u8bdd\uff0c\u5728\u9884\u6d4b\u7b2c\u4e00\u5e27\u7684\u8f93\u51fa\u7684\u65f6\u5019 \u5176\u5b9e\u662f\u770b\u5230\u4e86\u7b2c\u4e8c\u5e27\u3001\u7b2c\u4e09\u5e27\u3001\u7b2c\u56db\u5e27  \u6307\u7684\u662f \u770b\u5230\u4e86\u672a\u6765\u7684\u4e09\u5e27\u7684</li> <li>\u5f53\u9884\u6d4b \u7b2c\u4e8c\u5e27\u7684\u8f93\u51fa\u7684\u65f6\u5019 \u540c\u6837 \u770b\u5230\u7b2c\u4e09\u5e27\u3001\u7b2c\u56db\u5e27\u3001\u7b2c\u4e94\u5e27</li> <li> <p>\u867d\u7136\u4e5f\u770b\u5230\u4e86\u672a\u6765\u7684\u4fe1\u606f\uff0c\u4f46\u770b\u5230\u672a\u6765\u7684\u4fe1\u606f\u8fd8\u662f\u4e0d\u591f\u957f\uff1b</p> </li> <li> <p>\u5982\u679c\u628a\u5355\u5411 \u6362\u6210\u53cc\u5411\u7684\u7f51\u7edc\u7684\u8bdd\uff0c\u90a3\u4e48\u6574\u4e2a\u672a\u6765\u7684\u7279\u5f81 \u548c \u8fc7\u53bb\u7684 \u7279\u5f81\uff0c\u7f51\u7edc\u90fd\u80fd\u770b\u5230\uff0c\u8fd9\u5c31\u662f\u8bf4\u53cc\u5411\u7684\u8303\u56f4 \u66f4\u5927\u4e00\u70b9\uff1b</p> </li> </ul> <ul> <li> <p>\u5355\u5411delay 3\uff1a\u8f93\u51fa\u7b2c\u4e00\u5e27\u770b\u5230\u7684\u662f \u8f93\u5165\u7b2c\u4e00\u5e27\u3001\u7b2c\u4e8c\u5e27\u3001\u7b2c\u4e09\u5e27</p> </li> <li> <p>\u53cc\u5411delaye 3\uff1a\u8f93\u51fa\u7b2c\u4e00\u5e27\uff0c\u770b\u5230\u7684\u662f\u7b2c\u4e00\u5e27\u3001\u7b2c\u4e8c\u5e27\u3001\u7b2c\u4e09\u5e27+\u7b2c\u56db\u5e27\u3001\u7b2c\u4e94\u5e27\u3001\u7b2c\u516d\u5e27 </p> </li> </ul> <p>\u53cc\u5411\u7684\u7f3a\u70b9</p> <p>\u9700\u8981\u5b8c\u5168\u7684 \u628a\u6574\u4e2a\u8f93\u5165\u7279\u5f81\u5e8f\u5217 \u9001\u5165\u5230\u7f51\u7edc\u4e2d \uff0c\u6700\u540e\u624d\u80fd\u5f97\u5230\u8f93\u51fa</p> <p>\u800c\u5355\u5411\u5e26\u65f6\u5ef6\u7684\u60c5\u51b5\u5c31\u4e0d\u9700\u8981\u628a\u6574\u4e2a\u7279\u5f81 \u90fd\u7b97\u51fa\u6765 \u624d\u80fd\u9884\u6d4b\u7b2c\u4e00\u5e27\uff0c\u53ea\u8981\u6709\u4e09\u5e27\u4e86\uff0c\u5c31\u53ef\u4ee5\u9884\u6d4b\u7b2c\u4e00\u5e27\u4e86\uff1b</p> <p>\u6240\u4ee5\u5355\u5411\u5e26\u65f6\u5ef6\u7684\uff0c\u54cd\u5e94\u901f\u5ea6\u4f1a\u66f4\u5feb\uff1b</p> <p>\u53cc\u5411\u7684\u54cd\u5e94\u901f\u5ea6\u80af\u5b9a\u662f\u6700\u6162\u7684\uff1b</p> <p>\u6240\u4ee5\u5728\u901f\u5ea6 \u548c\u6548\u679c\u4e0a \u9700\u8981 \u53d6\u5f97\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684\u5e73\u8861 \u624d\u80fd\u6ee1\u8db3\u5177\u4f53\u7684\u4e1a\u52a1\u9700\u6c42\u3002</p>"},{"location":"learning/13_RNN/#k4","title":"k4 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u7f3a\u70b9","text":"<p>\u4e00\u3001\u4f18\u70b9</p> <p>\uff081\uff09\u6743\u91cd\u5171\u4eab\u53ef\u4ee5\u5904\u7406\u53d8\u957f\u5e8f\u5217</p> <p>\uff082\uff09\u6a21\u578b\u7684\u5927\u5c0f \u4e0e \u5e8f\u5217\u957f\u5ea6\u65e0\u5173</p> <p>\uff083\uff09\u8ba1\u7b97\u91cf\u4e0e\u5e8f\u5217\u957f\u5ea6\u5448\u73b0\u7ebf\u6027\u5173\u7cfb</p> <p>\uff084\uff09\u8003\u8651\u5386\u53f2\u4fe1\u606f</p> <p>\uff085\uff09\u4fbf\u4e8e\u6d41\u5f0f\u8f93\u51fa</p> <p>\uff086\uff09\u6743\u91cd\u65f6\u4e0d\u53d8</p> <p>\u4e8c\u3001\u7f3a\u70b9</p> <p>\uff081\uff09\u4e32\u884c\u8ba1\u7b97\u901f\u5ea6\u6162</p> <p>\uff082\uff09\u65e0\u6cd5\u83b7\u53d6\u592a\u957f\u7684\u5386\u53f2\u4fe1\u606f</p> <p>\u7b2c\u4e00\u70b9</p> <p>\u4f18\u70b9\u53ef\u4ee5\u5904\u7406\u53d8\u957f\u5e8f\u5217</p> <p>\u8fd9\u4e2a\u662fDNN\u548cCNN\u5904\u7406\u4e0d\u4e86\u7684\uff0c\u6bd4\u5982DNN\uff0c\u8f93\u5165\u7684\u7279\u5f81\u662f\u56fa\u5b9a\u7684\uff0c\u800cCNN\u7684\u4e0d\u4ec5\u548ckernel size\u6709\u5173\uff0c\u8fd8\u8ddf\u8f93\u5165\u7684\u901a\u9053\u6570\u6709\u5173\uff0c\u6240\u4ee5\u5982\u679cCNN \u8f93\u5165\u901a\u9053\u6570\u6709\u53d8\u5316\u7684\u8bdd \u8fd8\u9700\u8981\u91cd\u65b0\u642d\u5efa\u4e00\u4e2a\u7f51\u7edc\uff0c\u800cRNN \u662f\u53ef\u4ee5\u5904\u7406\u53d8\u957f\u5e8f\u5217\u7684</p> <ul> <li> \u4e3a\u4ec0\u4e48RNN \u80fd\u5904\u7406\u53d8\u957f\u5e8f\u5217\u5462\uff1f</li> </ul> <p></p> <p>\u539f\u56e0\u662f\u56e0\u4e3a\uff0c\u53ef\u4ee5\u770b\u5230\u56fe\u4e2d \u6709\u4e00\u4e2aw</p> <p>\u4e5f\u5c31\u662f \u6743\u91cd\uff0c\u8fd9\u4e2aw\u5728\u6bcf\u4e2a\u65f6\u523b \u90fd\u662f\u76f8\u7b49\u7684\uff0c\u6b63\u662f\u56e0\u4e3a \u6240\u6709\u7684\u6743\u91cd\uff0c\u5728\u6bcf\u4e00\u4e2a\u65f6\u523b\u90fd\u662f\u76f8\u7b49\u7684\uff1b\u4e0d\u8bba\u662f \u8f93\u5165 \u8ddf\u65e2\u6709\u5355\u5143\u7684\u8fde\u63a5\uff0c\u8fd8\u662f\u5386\u53f2\u4fe1\u606f \u8ddf\u5f53\u524d\u7684\u795e\u7ecf\u5143\u7684\u8fde\u63a5 \u5b83\u7684\u6743\u91cd\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u6b63\u662f\u56e0\u4e3a \u6743\u91cd \u5728\u6bcf\u4e00\u65f6\u523b \u5171\u4eab\uff0c\u6240\u4ee5 RNN \u80fd\u591f\u5904\u7406\u53d8\u957f\u5e8f\u5217\uff1b</p> <p>\u4e00\u65e6\u53bb\u6389\u4e86 \u6743\u91cd \u5171\u4eab \u8fd9\u4e2a\u5f52\u7eb3\u504f\u7f6e\u7684\u8bdd\uff0c\u5c31\u662f\u8bf4\uff0c\u5982\u679c\u6bcf\u4e00\u65f6\u523b \u90fd\u6709\u4e00\u4e2a \u4e0d\u4e00\u6837\u7684 w\u7684\u8bdd\uff0c\u8fd9\u4e2a\u65f6\u5019 \u5c31\u4e0d\u80fd\u5904\u7406 \u53d8\u957f\u5e8f\u5217\u4e86\uff0c\u5c31\u7c7b\u4f3c position embedding \u4e00\u6837\uff0c\u53ea\u8981\u9047\u5230\u4e86 \u957f\u5ea6 \u6bd4\u8bad\u7ec3\u96c6\u5927\u7684\uff0c\u90a3\u5c31\u5904\u7406\u4e0d\u4e86\u4e86\uff08\u4e5f\u4e0d\u662f\uff0c\u4e09\u89d2\u53d8\u6362\uff09\uff1b</p> <p>\u7b2c\u4e8c\u70b9</p> <p></p> <p>\u7b2c\u4e8c\u70b9\uff0c\u6a21\u578b\u7684\u5927\u5c0f \u4e0e \u5e8f\u5217\u957f\u5ea6\u65e0\u5173\uff0c\u8fd9\u91cc\u8bf4\u7684\u662f \u6a21\u578b\u7684\u5927\u5c0f\uff0c\u662f\u8bf4\u6a21\u578b\u7684\u53c2\u6570\u6570\u91cf \u4e0e \u957f\u5ea6\u65e0\u5173\uff0c\u6a21\u578b\u7684\u5168\u90e8\u53c2\u6570 \u548c\u5e8f\u5217\u957f\u5ea6 \u90fd\u662f\u65e0\u5173\u7684\uff0c\u53ea\u8f93\u5165\u7279\u5f81 \u548c\u8f93\u5165\u901a\u9053\u6570 \u4ee5\u53caRNN\u7684\u9690\u542b\u5355\u5143\u6709\u5173</p> <p>\u7b2c\u4e09\u70b9</p> <p></p> <p>\u7b2c\u4e09\u4e2a\u4f18\u70b9\u5c31\u662f RNN\u7684\u8ba1\u7b97\u91cf \u8ddf \u5e8f\u5217\u957f\u5ea6 \u5448\u7ebf\u6027\u589e\u957f\uff0c\u7c7b\u6bd4Transformer\uff0c\u5728\u539f\u672c\u7684Transformer\u4e2d \u6700\u5927\u7684\u4e00\u4e2a \u8bdf\u75c5\u7684\u5730\u65b9 \u5c31\u662f \u8ba1\u7b97\u590d\u6742\u5ea6 \u8ddf\u5e8f\u5217\u957f\u5ea6 \u662f\u5448\u4e00\u4e2a\u5e73\u65b9\u5173\u7cfb\u7684\uff0c\u4f46\u662f\u5728RNN\u4e2d\uff0c\u8ba1\u7b97\u91cf \u662f\u8ddf\u957f\u5ea6 \u5448\u73b0 \u7ebf\u6027\u589e\u957f\u7684\uff1b</p> <p>\u4e3e\u4f8b\u5b50\uff1a</p> <p>\u5f53 \u5e8f\u5217\u957f\u5ea6 \u4e3a2\u7684 \u65f6\u5019\uff0c\u8ba1\u7b97\u91cf \u53ef\u80fd\u5c31\u662f2t</p> <p>\uff08t\u6307\u7684\u662f\u65f6\u95f4\uff1f\u6a21\u578b \u56fa\u6709\u7684\u8ba1\u7b97\u91cf\uff09</p> <p>\u5f53\u5e8f\u5217\u957f\u5ea6\u4e3a3 \u7684\u65f6\u5019\uff0c\u8ba1\u7b97\u91cf \u5c31\u662f3t\uff0c\u5c31\u4e0d\u662f\u8bf4 \u4ece 4\u53d8\u62109\uff0c\u5448\u73b0 \u5e73\u65b9\u5173\u7cfb\u3002</p> <p>\u5728RNN\u4e2d \u5448\u73b0 \u7ebf\u6027\u5173\u7cfb\uff1b\u8fd9\u662f\u8ddf Transformer \u5728\u8ba1\u7b97\u91cf\u4e0a \u4e00\u4e2a\u660e\u663e\u7684\u533a\u522b\u3002</p> <p>\u7b2c\u56db\u70b9</p> <p></p> <p>\u76f8\u6bd4DNN\u800c\u8a00\uff0cRNN\u662f\u53ef\u4ee5\u8003\u8651\u5230 \u5386\u53f2\u4fe1\u606f\u7684\uff0c\u56e0\u4e3a\u6709\u94fe\u5f0f\u7684\u7ed3\u6784\uff0c\u53ef\u4ee5\u901a\u8fc7\u9690\u542b\u5c42 \u6765\u79ef\u7d2f \u5386\u53f2\u4fe1\u606f\uff1b</p> <p>\u7b2c\u4e94\u70b9</p> <p></p> <p>\u6d41\u5f0f \u8f93\u51fa\uff0c\u53ef\u4ee5\u770b\u5230\uff1a</p> <p></p> <ul> <li> \u6d41\u5f0f\u8f93\u51fa\u662f\u4ec0\u4e48\uff1f</li> </ul> <p>\u6bcf \u8ba1\u7b97\u4e00\u6b65\uff0c\u90fd\u53ef\u4ee5\u5f97\u5230 \u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd9\u4e2a\u8f93\u51fa \u53ef\u4ee5\u76f4\u63a5 \u9001\u7ed9 \u7528\u6237\uff0c\u8fd9\u5c31\u662f \u6d41\u5f0f \u7684\u610f\u601d\u3002</p> <p>\u4f46\u662f\u5bf9\u4e8e Transformer\u800c\u8a00\u7684\u8bdd\uff0c\u7531\u4e8e\u5b83\u662f\u8003\u8651\u5230\u5168\u5c40\u7684\u4fe1\u606f \u8ba1\u7b97\u4e00\u4e2a \u5168\u5c40\u7684self attention\uff0c\u6240\u4ee5\u5c31\u4e0d\u80fd\u5355\u6b65 \u7684\u8ba1\u7b97 \u6bcf\u4e00\u6b65\u7684 \u8f93\u51fa\uff0c\u8fd9\u5c31\u662f Transformer\u7684\u4e00\u4e2a\u7f3a\u70b9\uff0c\u4e0d\u80fd\u76f4\u63a5\u7684 \u5e94\u7528\u5230 \u6d41\u5f0f\u7684\u573a\u666f\uff1b</p> <p>\u4f46\u662f\u5728\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u53ea\u8981\u6bcf\u7b97\u4e00\u6b21 \u9012\u5f52\u8fd0\u7b97\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u8f93\u51fa\uff0c\u8fd9\u4e2a \u8f93\u51fa\u5c31\u53ef\u4ee5\u76f4\u63a5\u8fd4\u56de\u7ed9\u7528\u6237\uff0c\u8fd9\u5c31\u662f\u6d41\u5f0f\u7684\uff0c\u4e5f\u5c31\u662f \u4e0d\u9700\u8981 \u628a \u6574\u4e2a\u5e8f\u5217 \u90fd\u7b97\u5b8c \u624d\u8fd4\u56de\u7ed9\u7528\u6237\uff0c\u800c\u662f\u8bf4 \u6bcf\u7b97\u51fa\u4e00\u4e2a \u65f6\u523b \u90fd\u53ef\u4ee5\u8fd4\u56de\u7ed9\u7528\u6237</p> <p>\u7b2c\u516d\u70b9</p> <p></p> <p>\u6743\u91cd\u65f6\u4e0d\u53d8</p> <p>\u6743\u91cd\u662f \u65f6\u4e0d\u53d8\u7684\uff0c\u6b63\u662f\u56e0\u4e3aRNN \u6743\u91cd \u65f6\u4e0d\u53d8\uff0c\u6240\u4ee5RNN \u53ef\u4ee5\u5904\u7406 \u53d8\u957f\u5e8f\u5217\uff1b</p> <p>\u4e8c\u3001\u7f3a\u70b9</p> <p></p> <ul> <li> \u4e3a\u4ec0\u4e48\u8bf4 \u4e32\u884c\u8ba1\u7b97\u6162</li> </ul> <p>\u56e0\u4e3a \u5728\u7b97 \u6bcf\u4e00\u65f6\u523b\u7684\u65f6\u5019 \u90fd\u9700\u8981\u7b49 \u4e0a\u4e00\u65f6\u523b\u7684\u5386\u53f2\u4fe1\u606f\uff0c\u7b49\u4e0a\u4e00\u65f6\u523b\u7684\u7b97\u51fa\u6765 \u624d\u80fd\u7b97 \u4e0b\u4e00\u65f6\u523b\uff0c\u662f\u4e00\u4e2a \u4e32\u884c\u7684\u8fc7\u7a0b\uff0c\u6bd4\u8f83\u6162</p> <ul> <li> \u600e\u4e48\u7406\u89e3 RNN \u4e5f\u662f\u65e0\u6cd5\u83b7\u53d6\u592a\u957f\u7684\u5386\u53f2\u4fe1\u606f</li> </ul> <p>\u4e5f\u5c31\u662f\u8bf4 \u7531\u4e8e\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\uff0c\u5bfc\u81f4RNN\u65e0\u6cd5 \u4ece\u5f53\u524d\u65f6\u523b \u83b7\u53d6\u5f88\u4e45\u8fdc\u7684\u4fe1\u606f</p> <p>RNN \u7531\u4e8e\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u83b7\u5f97\u592a\u957f\u7684\u5386\u53f2\u4fe1\u606f\u3002</p> <p>\u8fd9\u4e00\u70b9\u6b63\u662fTransformer\u7684\u4f18\u70b9\u3002</p> <p>Transformer\u7684\u5f52\u7eb3\u504f\u7f6e \u662f\u6bd4\u8f83\u5f31\u7684\uff0c\u662f\u901a\u8fc7\u4e00\u4e2a \u5168\u5c40\u7684self attention\uff0c\u6765\u8ba1\u7b97 \u4e24\u4e24\u4f4d\u7f6e\u4e4b\u95f4\u7684\u4e00\u4e2a\u76f8\u5173\u6027\uff0c\u6240\u4ee5Transformer\u662f\u53ef\u4ee5\u4e0a\u4e0b\u53bb\u6355\u6349 \u5f88\u957f\u7684\u5386\u53f2\u5173\u8054\u6027\u7684\u3002</p>"},{"location":"learning/13_RNN/#k5-rnn","title":"k5 RNN \u7684\u5e94\u7528\u573a\u666f","text":"<p>\uff081\uff09\u751f\u6210\u4efb\u52a1</p> <p>\u751f\u6210\u4efb\u52a1\uff0c\u6bd4\u5982\u6b4c\u8bcd\u751f\u6210\u3001\u5bf9\u8054\u751f\u6210\u3001\u50cfGPT\u4e00\u6837\u5199\u5c0f\u8bf4</p> <p>\u751f\u6210\u4efb\u52a1\uff0c\u5982\u679c\u7528\u4e00\u5e45\u56fe\u6765\u8868\u793a\uff1a</p> <p></p> <p>1\u3001\u5982\u56fe\u8868\u793aRNN\u5728\u8bd7\u6b4c\u3001\u8bed\u97f3\u3001\u7b26\u53f7\u751f\u6210\u4e2d\u7684\u8868\u793a</p> <p>2\u3001\u8fd9\u7c7b\u4efb\u52a1\u53ef\u4ee5\u770b\u6210one to many\u7684\u8fc7\u7a0b\uff0c\u4e5f\u5c31\u662f\u8bf4 \u53ea\u8981\u7ed9\u4e86 \u4e00\u4e2a\u8f93\u5165\uff0c\u6216\u8005\u4e00\u4e2a\u5f88\u77ed\u7684 \u8f93\u5165\uff0cRNN\u5c31\u53ef\u4ee5\u5229\u7528\u81ea\u5df1\u7684 \u9012\u5f52\u673a\u5236 \u4e0d\u65ad\u7684\u9884\u6d4b \u65b0\u7684\u8f93\u51fa\uff0c\u5c31\u6bd4\u5982 \u7ed9\u51fa \u4e00\u4e24\u53e5\u8bdd\uff0cRNN \u5199\u51fa\u4e00\u6bb5\u8bdd \u6216\u8005 \u4e00\u7bc7\u6587\u7ae0\uff0c\u5c31\u662f one to many\uff0cRNN\u5728\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5e94\u7528</p> <p>\uff082\uff09\u60c5\u611f\u5206\u7c7b</p> <p>RNN\u4e5f\u80fd\u505a\u60c5\u611f\u5206\u7c7b</p> <p>\u6bd4\u5982\u8bf4\u5f88\u53e4\u8001\u7684\u4e00\u4e2a\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\uff0c\u5bf9\u5f71\u8bc4\u8fdb\u884c\u5206\u7c7b\uff0c\u5224\u65ad\u4e00\u53e5\u8bdd\u662f\u6b63\u5411\u60c5\u611f\u8fd8\u662f\u8d1f\u5411\u60c5\u611f\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\uff0c\u53ef\u4ee5\u770b\u6210many to one\u7684\u4efb\u52a1</p> <p></p> <p>\u8f93\u5165\u662f\u4e00\u6bb5\u8bdd\u6216\u8005\u8bf4\u4e00\u7bc7\u6587\u7ae0\uff0c\u4f46\u662f\u8f93\u51fa \u53ea\u6709\u4e00\u4e2a\uff0c\u53ea\u9700\u8981\u5bf9\u4e00\u6bb5\u8bdd\u9884\u6d4b\u4e00\u4e2a\u7c7b\u522b\u5c31\u597d\u4e86\uff0c\u8fd9\u4e2a\u5c31\u662fmany to one\u7684\u4efb\u52a1\uff0c\u5178\u578b\u7684\u5e94\u7528\u573a\u666f\u5c31\u662f\u53bb\u60c5\u611f\u5206\u7c7b</p> <p></p> <p>many to many\u7684\u4efb\u52a1\uff1a</p> <ul> <li>\u8bcd\u6cd5\u8bc6\u522b</li> <li>\u673a\u5668\u7ffb\u8bd1</li> </ul> <p>\u8bcd\u6cd5\u8bc6\u522b\u5c31\u662f\u8bc6\u522b\u5f53\u524d\u8fd9\u4e2a\u8bcd\u662f\u540d\u8bcd\u8fd8\u662f\u52a8\u8bcd\uff0c\u5f53\u524d\u8fd9\u4e2a\u5355\u8bcd\u591a\u97f3\u5b57\u7b49\u7b49</p> <p>\u673a\u5668\u7ffb\u8bd1\uff0c\u5728Transformer\u4e2d\u662f\u5e94\u7528\u6bd4\u8f83\u591a\u7684\uff1b</p> <p>\u4f46\u662f\u8fd9\u4e24\u79cd many  to many\u7684\u6a21\u578b\u7ed3\u6784\u8fd8\u662f\u6709\u4e00\u4e9b\u533a\u522b\u7684\uff0c\u53ef\u4ee5\u770b\u5230\u4e0b\u9762\u4e24\u5e45\u56fe\uff1a</p> <p>\uff08\u4e00\uff09\u8bcd\u6cd5\u8bc6\u522b</p> <p></p> <ul> <li> <p>\u8bc6\u522b\u4e00\u53e5\u8bdd\u4e2d\uff0c\u6bcf\u4e2a\u5b57\u7684\u62fc\u97f3\u662f\u4ec0\u4e48\uff0c\u6216\u8005\u8bc6\u522b\u6bcf\u4e2a\u8bcd\u7684\u8bcd\u6027\uff0c\u8fd9\u79cd\u5c31\u662fmany to many</p> </li> <li> <p>\u5c5e\u4e8e\u76f4\u8fdb\u76f4\u51fa\u7684many to many</p> </li> </ul> <p>\uff08\u4e8c\uff09\u673a\u5668\u7ffb\u8bd1</p> <p></p> <ul> <li>sequence to sequence \u7ed3\u6784\uff1b</li> <li>\u6709\u7f16\u7801\u5668\uff0c\u6709\u89e3\u7801\u5668\uff0c\u4e2d\u95f4\u4f9d\u9760\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6765\u5e2e\u52a9\u89e3\u7801\u5668\u9884\u6d4b\u6bcf\u4e00\u65f6\u523b\u7684\u8f93\u51fa\uff0c\u4e5f\u662fmany to many\uff1b</li> <li>\u5e38\u89c1\u7684\u5e94\u7528\u573a\u666f\uff1a\u673a\u5668\u7ffb\u8bd1\u3001\u8bed\u97f3\u5408\u6210\u7b49</li> </ul> <p></p> <p>\u8bed\u8a00\u6a21\u578b RNNLM\uff1b</p> <p>\u603b\u4e4b\u5c31\u662f</p> <ul> <li>one to one</li> <li>Many to one</li> <li>many to many</li> </ul>"},{"location":"learning/13_RNN/#k6-rnn","title":"k6  RNN\u6846\u56fe","text":""},{"location":"learning/13_RNN/#torchnnrnn","title":"torch.nn.RNN","text":"<ul> <li> <p>\u53ef\u4ee5\u7528\u6765\u6784\u9020\u4e00\u5c42 \u6216\u8005\u591a\u5c42 \u7b80\u5355\u7684RNN\u7ed3\u6784\uff1b </p> </li> <li> <p>RNN\u8fd8\u6709\u53e6\u5916\u4e00\u79cd\u7ed3\u6784\uff1a\u6fc0\u6d3b\u51fd\u6570\uff0c\u53ef\u4ee5\u7528tanh\u6fc0\u6d3b\u51fd\u6570 \u6216\u8005 ReLU\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f7f\u5f97RNN\u6709\u66f4\u5f3a\u7684\u975e\u7ebf\u6027\u5efa\u6a21\u80fd\u529b\uff1b</p> </li> <li> <p> RNN \u8ba1\u7b97\u516c\u5f0f\u662f\u4ec0\u4e48\u5462\uff1f</p> </li> </ul> <p></p> <ul> <li> <p>\u6bcf\u4e00\u65f6\u523b\u7684\u8f93\u51fa\uff0c\u6216\u8005\u8bf4\u6bcf\u4e00\u65f6\u523b\u7684\u72b6\u6001</p> </li> <li> <p>\u5728\u7b80\u5355RNN\u4e2d\uff0c\u8f93\u51fa\u662f\u7b49\u4e8e\u72b6\u6001\u7684\uff0c \\(h_t\\)\u4e5f\u5c31\u662f \\(t\\) \u65f6\u523b\u7684\u8f93\u51fa\uff1b</p> </li> <li> <p>\u6216\u8005\u8bf4 t \u65f6\u523bRNN\u7684\u72b6\u6001 \u7b49\u4e8e tanh\u51fd\u6570\uff0c\u5c31\u662f\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u91cc\u9762\u5206\u522b\u662f \\(W_{ih}\u00d7x_t\\) \u518d\u52a0\u4e0a \\(b_{ih}\\)\uff0c\u90a3\u4e48\u8fd9\u91cc\u7684\\(x_t\\)\uff0c\u5c31\u662f\u5f53\u524d\u65f6\u523b\u7684\u8f93\u5165\uff0c\u7136\u540e\\(w_{ih}\\)\uff0c\u5c31\u662f\u5728\u8fd9\u4e2aRNN\u4e2d\uff0c\u5b83\u5bf9\u8f93\u5165\u7684\u6743\u91cd\u77e9\u9635\uff0c\u5c31\u662f \u4f1a\u7528\u8fd9\u4e2a\u77e9\u9635 \u6765\u5bf9\u6743\u91cd \u505a\u4e00\u4e2a\u6620\u5c04\uff0c\u7136\u540e\u6574\u4f53\u4e0a\uff0c\u8fd9\u4e2a\u4e1c\u897f \u53ef\u4ee5\u770b\u505a linear\u5c42\uff0c\u6709\u6743\u91cd \u8fd8\u6709 \u504f\u7f6e\uff0c\\(b_{ih}\\)\uff0c\u5c31\u662f\u5173\u4e8e \u6743\u91cd\u7684\u4e00\u4e2a\u504f\u7f6e</p> </li> <li> <p>\u540e\u9762 \u8fd8\u6709\u4e00\u9879\uff0c\u8ddf \u5386\u53f2\u72b6\u6001\u6709\u5173\u7684\uff0c\u8ddf \\(h_{t-1}\\) \u6709\u5173\u7684</p> </li> <li>\u4e5f\u5c31\u662f\u8bf4\uff0c\u9700\u8981\u5c06 \u4e0a\u4e00\u65f6\u523b\u7684 \u8f93\u51fa \u6216\u8005\u8bf4 \u4e0a\u4e00\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001 \u62ff\u8fc7\u6765\uff0c\u7136\u540e\u5bf9\u5b83\u8fdb\u884c\u4e00\u4e2a \u6620\u5c04\uff0c\u7528 \\(w_{hh}\\) \u7684\u6743\u91cd \u6765\u8fdb\u884c\u76f8\u4e58\uff0c\u6765\u8fdb\u884c\u6620\u5c04\uff0c\u7136\u540e\u518d\u52a0\u4e0a\u4e00\u4e2a\u504f\u7f6e</li> <li>\u603b\u4f53\u800c\u8a00 \u5c31\u662f\u8bf4 \u6bcf\u4e00\u65f6\u523b\u7684\u8f93\u51fa \u6216\u8005\u8bf4 \u9690\u542b\u72b6\u6001 \u4e0d\u5149\u8ddf\u5f53\u524d\u65f6\u523b \u7684 \u8f93\u5165 \\(x_t\\) \u6709\u5173\uff0c\u540c\u65f6\u4e5f\u8ddf\u4e0a\u4e00\u65f6\u523b\u7684\u8bb0\u5fc6\u5355\u5143  \\(h_{t-1}\\)\u6709\u5173\uff0c\u5e76\u4e14\u90fd\u662f\u7ebf\u6027\u7ec4\u5408\u7684\u5173\u7cfb\uff0c\u6700\u540e\u901a\u8fc7\u4e00\u4e2a\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u5c31\u80fd\u5f97\u5230\u5f53\u524d\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001\uff1b</li> </ul> <p></p> <ul> <li> \u89e3\u91ca\uff1a</li> </ul> <p>\\(h_t\\) \u662f \\(t\\)\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001</p> <p>\\(x_t\\)\u662f t \u65f6\u523b\u7684\u8f93\u5165</p> <p>\\(h_{t-1}\\)\u662f  \\(t-1\\)\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001</p> <p>\\(h_0\\) \u8868\u793a\u521d\u59cb\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001</p> <p>pytorch\u4e2d\u4e5f\u63d0\u4f9b\u4e86\u4e24\u79cd \u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff1atanh\u548crelu\u6fc0\u6d3b\u51fd\u6570\uff0c\u9ed8\u8ba4\u7528tanh\u6fc0\u6d3b\u51fd\u6570</p> <p></p> <ul> <li>\u8fd9\u662f\u4e00\u4e2a class</li> <li>\u5728\u7528RNN\u65f6\u5019\uff0c\u9996\u5148\u8981 \u5b9e\u4f8b\u5316 \u8fd9\u4e2aclass</li> <li>\u5b9e\u4f8b\u5316 class\u4ee5\u540e\uff0c\u5f97\u5230RNN\u7684\u4e00\u4e2a\u6a21\u578b</li> <li>\u7136\u540e \u518d\u628a \u8f93\u5165 \u5582\u5165\u5230 \u6a21\u578b\u4e2d\uff0c\u800c\u4e0d\u76f4\u63a5\u628a \u8f93\u5165 \u5582\u5165\u5230\u6a21\u578b\u4e2d\uff1b</li> <li> <p>\u4e00\u822c\u6240\u6709\u6a21\u578b\u7684 class\uff0c\u90fd\u9700\u8981 \u5148\u8fdb\u884c\u4e00\u4e2a\u5b9e\u4f8b\u5316\uff0c\u7136\u540e\u624d\u80fd\u5f97\u5230\u4e00\u4e2alayer\uff1b</p> </li> <li> <p> \u5b9e\u4f8b\u5316RNN\u6240\u9700\u8981\u7684\u53c2\u6570</p> </li> </ul> <p></p> <ul> <li> <p>\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f <code>input_size</code>,\u4e5f\u5c31\u662f \u8f93\u5165\u7279\u5f81\u7684\u5927\u5c0f\uff0c\u4e5f\u5c31\u662f <code>x</code> \u7684\u7279\u5f81\u7684\u7ef4\u5ea6</p> </li> <li> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f <code>hidden_size</code>\uff0c<code>hidden_size</code>\u51b3\u5b9a\u4e86 \\(h_t\\)\u7684\u5927\u5c0f\uff0c\u5c31\u662f\u6bcf\u4e00\u65f6\u523b\u7684 \\(h_t\\)\u5c31\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u5bf9\u4e8e\u5355\u4e00\u6837\u672c\u800c\u8a00\uff0c\u6bcf\u4e00\u65f6\u523b \\(h_t\\)\u5c31\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u90a3\u4e48\u8fd9\u4e2a\u5411\u91cf\u957f\u5ea6\u662f\u591a\u5c11\u5462\uff1f\u5c31\u662f\u7531 <code>hidden_size</code> \u6765\u51b3\u5b9a</p> </li> <li> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570 \u5c31\u662f <code>num_layers</code>\uff0c\u5c31\u662f\u8bf4 \u8fd9\u4e2aRNN\uff0c\u53ef\u4ee5\u9ed8\u8ba4\u5b9e\u4f8b\u5316\u7684\u65f6\u5019 \u53ea\u6709\u4e00\u5c42\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u6539\u53d8 <code>num_layers</code>\u7684\u503c\uff0c\u53d8\u6210\u591a\u5c42\uff0c\u5806\u53e0\u8d77\u6765\u7684\u7ed3\u6784\uff0c\u4e4b\u524d\u5728\u4ecb\u7ecd\u7684\u65f6\u5019\u4e5f\u8bb2\u8fc7\uff0c\u53ef\u4ee5\u5806\u53e0\u8d77\u6765\uff0c\u5355\u5411\u7684\u53ef\u4ee5\u5806\u53e0\uff0c\u53cc\u5411\u7684 \u4e5f\u53ef \u5806\u53e0</p> </li> <li> <p>\u7b2c\u56db\u4e2a\u53c2\u6570 \u5c31\u662f \u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u8fd9\u91cc\u9ed8\u8ba4\u662f<code>tanh</code>\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u6539\u6210 <code>ReLu</code>\u51fd\u6570</p> </li> <li>\u7b2c\u4e94\u4e2a\u662f<code>bias</code> \u4e00\u822c\u4f1a\u52a0\u4e0a \u8fd9\u4e24\u4e2abias</li> <li>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f <code>batch first</code>\uff0c\u8fd9\u4e2a\u9700\u8981\u6ce8\u610f\u4e00\u4e0b\uff0c\u8fd9\u4e2a\u53c2\u6570\u5c31\u51b3\u5b9a\u4e86 \u8f93\u5165\u548c\u8f93\u51fa\u7684\u683c\u5f0f</li> </ul> <ul> <li>\u5982\u679c\u8bbe\u7f6e <code>batch first=true</code>\u7684\u8bdd\uff1a</li> </ul> <p>\u63d0\u4f9b\u7684\u8f93\u5165\u5f20\u91cf \u548c \u8f93\u51fa\u5f20\u91cf\u7684 \u683c\u5f0f\u5c31\u662f <code>batch \u00d7 sequence length\u00d7feature</code> \u8fd9\u6837\u7684\u683c\u5f0f</p> <p>\u9ed8\u8ba4\u662f<code>false</code>\u7684\uff0c\u5982\u679c\u662f <code>false</code>\u7684\u60c5\u51b5\u4e0b\uff1a</p> <p>\u9700\u8981\u4fdd\u8bc1 \u8f93\u5165\u7684\u683c\u5f0f\u662f <code>sequence length</code>\uff0c\u4e5f\u5c31\u662f\u5e8f\u5217\u957f\u5ea6 \u5728\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\uff0c<code>batch size</code>\u5728\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\uff0c<code>feature size</code>\u5728\u7b2c\u4e09\u4e2a\u7ef4\u5ea6</p> <ul> <li>\u7b2c\u4e03\u4e2a\u53c2\u6570 <code>dropout</code></li> <li>\u6700\u540e\u4e00\u4e2a\u53c2\u6570<code>bidirectional</code>\uff0c\u6700\u540e\u4e00\u4e2a\u53c2\u6570 \u8868\u793a \u53cc\u5411\u7684\u610f\u601d</li> </ul> <p>\u4e5f\u5c31\u662f\u628a\u8fd9\u4e2a\u53c2\u6570\u8bbe\u7f6e\u4e3a <code>true</code>\u7684\u8bdd\uff0c\u5c31\u53ef\u4ee5\u6784\u5efa\u4e00\u4e2a\u53cc\u5411\u7684RNN\u7ed3\u6784\uff1b</p> <p>\u65e2\u7136\u662f \u53cc\u5411RNN\u7ed3\u6784\uff0c\u8f93\u51fa\u7684\u7279\u5f81\u5927\u5c0f\u5c31\u662f<code>2\u00d7feature size</code>\uff0c\u5c31\u662f2\u500d\u7684<code>feature size</code>\uff1b</p> <p>\u53cc\u5411\u7ed3\u6784\u56fe</p> <p></p> <ul> <li> <p>\u8fd9\u5e45\u56fe \u5c31\u662f \u53cc\u5411\u7684\uff0c\u4e00\u65e6\u628aRNN\u8bbe\u7f6e\u6210 \u53cc\u5411\u7684\u8bdd\uff0c\u6700\u7ec8\u7684\u8f93\u51fa \u662f\u7531<code>forward\u8f93\u51fa</code>\u548c<code>backward\u8f93\u51fa</code>\u4e00\u8d77\u62fc\u8d77\u6765\u7684\uff0c\u6240\u4ee5\u8fd9\u4e2a \u8f93\u51fa\u72b6\u6001\u662f \u4e8c\u500d\u7684 <code>hidden size</code>\uff0c\u53ef\u4ee5\u6307\u5b9a <code>concat</code>\u548c<code>sum</code>\uff0c\u4e00\u822c\u7528 <code>concat</code> \u66f4\u591a\u4e00\u70b9</p> </li> <li> <p>\u4e5f\u5c31\u662f\u8bf4 \u5982\u679c \u8bbe\u7f6e <code>hidden size\u662f16</code>\u7684\u8bdd\uff0c\u90a3\u4e48 <code>output layer</code>\u5927\u5c0f\uff0c\u5c31\u662f32\uff0c\u5982\u679c\u662f\u53cc\u5411\u7684\u8bdd</p> </li> </ul> <p>\u4ee5\u4e0a\u662fRNN\u5b9e\u4f8b\u5316\u7684\u53c2\u6570\u8bb2\u89e3\uff1b</p> <ul> <li>\u5f53\u5b9e\u4f8b\u5316\u5b8c\u4ee5\u540e\uff0c\u5c31\u5f97\u5230\u4e86RNN\u5c42</li> <li>\u7136\u540e\u5c31\u53ef\u4ee5 \u63d0\u4f9b \u8f93\u5165 \u548c \u521d\u59cb\u7684\u9690\u542b\u72b6\u6001\uff0c\u6765\u53bb\u9012\u5f52\u7684\u7b97\u51fa \u6bcf\u4e00\u65f6\u523b\u7684 \u8f93\u5165 \u6240\u5bf9\u5e94\u7684\u8f93\u51fa\u662f\u4ec0\u4e48\uff1b</li> </ul> <p>\u5f53\u5b9e\u4f8b\u5316\u5b8c \u4e00\u4e2aRNN\uff0c\u5c31\u53ef\u4ee5 \u63d0\u4f9b <code>input</code> \u548c \\(h_0\\)\uff0c\u6765\u7ed9\u51fa\u771f\u6b63\u7684\u8f93\u5165\u5e8f\u5217\uff1a</p> <p></p> <ul> <li> \u89e3\u91cainput</li> </ul> <p>\u8f93\u5165\u4e00\u822c\u662f\u4e09\u7ef4\u7684\uff1a</p> <p></p> <p>\u5982\u679c\u8bbe\u7f6e\u7684<code>batch size first\u7b49\u4e8etrue</code>\u7684\u8bdd\uff0c\u90a3\u5bf9\u5e94\u7684\u8f93\u5165\u683c\u5f0f\u5c31\u662f <code>batch size\u00d7sequence length\u00d7hidden size</code>\uff1b</p> <p>\u53cd\u4e4b \u5982\u679c<code>\u6ca1\u6709\u8bbe\u7f6ebatch size\u7b49\u4e8etrue</code>\u7684\u8bdd\uff0c\u63d0\u4f9b\u7684\u683c\u5f0f\u5c31\u662f <code>sequence length\u00d7batch size\u00d7hidden size</code></p> <ul> <li> <p> \u89e3\u91ca \\(h_0\\)</p> </li> <li> <p>\\(h_0\\)\u7684\u683c\u5f0f\u662f (\\(d\u00d7{num\\_layers}\\)\uff0c \\(N\\)\uff0c\\(H_{out}\\) )</p> </li> <li> <p>\\(h_0\\) \u662f \u521d\u59cb\u72b6\u6001\uff0c\u53ea\u6709 \u8fd9\u4e00\u4e2a\u65f6\u523b\uff0c\u6240\u4ee5\u8fd9\u91cc\u4e0d\u9700\u8981\u8003\u8651 <code>sequence_length</code> \u8fd9\u4e2a\u7ef4\u5ea6</p> </li> <li> <p> \u90a3\u8fd9\u91cc\u4e5f\u662f \u4e09\u4e2a\u7ef4\u5ea6\uff0c\u4e3a\u4ec0\u4e48\u5462\uff1f</p> </li> </ul> <p>\u56e0\u4e3a  RNN \u53ef\u4ee5\u662f \u591a\u5c42 \u4e5f\u53ef\u4ee5\u662f \u53cc\u5411\uff0c\u6240\u4ee5\u7b2c\u4e00\u4e2a\u7ef4\u5ea6 \u5176\u5b9e\u5c31\u662f \u662f\u5426\u662f \u53cc\u5411  \u8ddf \u591a\u5c42 \u8fd9\u4e24\u4e2a\u56e0\u7d20 \u51b3\u5b9a\u7684\uff1b</p> <p><code>case1\uff1a</code>\u5982\u679c\u6a21\u578b\u662f\u4e00\u5c42\uff0c\u5e76\u4e14\u662f\u5355\u5411\u7684\u8bdd\uff0c\u90a3\u4e48\u7b2c\u4e00\u4e2a\u7ef4\u5ea6 \u5c31\u662f 1 \uff1b</p> <p><code>case2\uff1a</code>\u5982\u679c\u662f \u6709\u4e24\u5c42\uff0c\u5e76\u4e14\u662f \u5355\u5411\u7684\u8bdd\uff0c\u90a3\u4e48\u5c31\u662f 1\u00d72\uff1b</p> <p><code>case3\uff1a</code>\u5982\u679c\u662f\u53cc\u5411 \u5e76\u4e14\u662f \u4e24\u5c42\u7684\u8bdd\uff0c\u90a3\u5c31\u662f 2\u00d72=4\uff1b</p> <p>\u6240\u4ee5\u8fd9\u91cc\u7684 \u7b2c\u4e00\u4e2a\u7ef4\u5ea6 \\(d \\times num\\_layers\\) \u7531\u662f\u5426\u53cc\u5411 \u4ee5\u53ca \u5c42\u6570\u6709\u5173</p> <p></p> <p>\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6 \\(N\\)\uff0c\u5c31\u662f <code>batch size</code>\uff0c\u6bcf\u4e2a\u6837\u672c \u90fd\u53ef\u4ee5 \u8bbe\u7f6e\u4e00\u4e2a \u521d\u59cb\u72b6\u6001</p> <p>\u7b2c\u4e09\u4e2a\u7ef4\u5ea6 \\(H_{out}\\) \u5c31\u662f <code>hidden size</code>\u7684\u5927\u5c0f\uff0c\u56e0\u4e3a \u521d\u59cb\u72b6\u6001 \u5c31\u662f\u4e00\u4e2a\u5411\u91cf,\u7b2c\u4e09\u7ef4 \u5c31\u662f \u5411\u91cf\u7684\u957f\u5ea6</p>"},{"location":"learning/13_RNN/#_1","title":"\u4ee3\u7801\u793a\u4f8b","text":""},{"location":"learning/13_RNN/#1-rnn","title":"1 \u5355\u5c42\u5355\u5411 RNN","text":"<p>\u8fd9\u4e2aRNN \u662f\u4e00\u4e2a class</p> <p>\u6240\u4ee5\uff0c\u9996\u5148\u5b9e\u4f8b\u5316\u4e00\u4e2a\u5355\u5411\u5355\u5c42\u7684RNN</p> <p>step1\uff1aimport  torch.nn as nn</p> <p>step2\uff1a\u5b9e\u4f8b\u5316 nn.RNN</p> <p>step3\uff1a\u4f20\u5165 \u5b9e\u4f8b\u5316\u53c2\u6570\uff1b</p> <ul> <li> <p>input_size=4</p> </li> <li> <p>hidden_size\u4e5f\u53ef\u4ee5 \u968f\u4fbf\u5199\u4e00\u4e2a hidden_size=3 </p> </li> <li> <p>num_layers\u53ef\u4ee5\u4f20\u51651</p> </li> <li>batch first\u8bbe\u7f6e\u6210true</li> <li>\u5b9a\u4e49\u4e3a<code>single_rnn</code></li> </ul> <pre><code>import torch\nimport torch.nn as nn\n# 1.\u5355\u5411\u3001\u5355\u5c42RNN\nsingle_rnn = nn.RNN(input_size=4,hidden_size=3,num_layers=1,batch_first=True)\n</code></pre> <p>\u4ee5\u4e0a\u662f \u5355\u5c42\u5355\u5411RNN\uff0c\u63a5\u4e0b\u6765\u6784\u5efa\u4e00\u4e2a\u8f93\u5165</p> <p>\u8f93\u5165\u7684\u7ef4\u5ea6\u4e00\u822c\u662f <code>batch_size\u00d7sequence length\u00d7\u8f93\u5165\u7279\u5f81</code></p> <p>\u8f93\u5165\u7279\u5f81\u5c31\u662fRNN\u5b9e\u4f8b\u5316\u65f6\u7684 <code>input size=4\uff0cbatch size=1\uff0csequence length=2\uff0c\u7279\u5f81\u7ef4\u5ea6=4</code></p> <p>\u4ee5\u4e0a\u6784\u5efa\u597d\u4e86input\u5e8f\u5217\uff0c\u5206\u522b\u662f\uff1a <code>batch_size \u00d7 sequence length\u00d7\u8f93\u5165\u7279\u5f81</code></p> <pre><code>input = torch.randn(1,2,4) \n# batch_size*sequence_length*feature_size\n</code></pre> <p>\u628a\u8fd9\u4e2a<code>input</code>\u4f5c\u4e3a <code>single_rnn</code>\u7684\u8f93\u5165\uff1b</p> <p>\u4e5f\u53ef\u4ee5\u4e0d\u4f20\u5165\\(h_0\\),\u5b83\u9ed8\u8ba4\u4ee5\\(0\\)\u5411\u91cf\u586b\u5145</p> <p></p> <p>\u540c\u65f6\u4e5f\u53ef\u4ee5\u770b\u770b \u5b98\u7f51 api \u8f93\u51fa\u662f\u4ec0\u4e48</p> <p></p> <p>\u8f93\u51fa\u662f\u4e24\u4e2a\u503c\uff0c\u4e00\u4e2a\u662f\u6574\u4e2a\u7684\uff0c\u6240\u6709\u65f6\u523b\u7684\u8f93\u51fa\uff1b</p> <p>\u53e6\u5916\u4e00\u4e2a\u8f93\u51fa\u7684\u91cf\u5c31\u662f\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u72b6\u6001\uff0c\u8981\u5b9a\u4e49\u53d8\u91cf\u63a5\u6536\u8f93\u51fa</p> <pre><code>output,h_n = single_rnn(input)\n</code></pre> <p>\u8fd9\u6837\u6574\u4e2a\u8f93\u51fa\u5c31\u7b97\u5b8c\u4e86\uff0c\u63a5\u4e0b\u6765\u770b\u4e00\u4e0b\\(output\\)\u548c \\(h_n\\)</p> <p></p> <p>\u4ee3\u7801\u89e3\u8bfb\uff1a</p> <p>\uff081\uff09 <code>input</code>\u7684\u5f62\u72b6 <code>1\u00d72\u00d74 = batch size\u00d7sequence length\u00d7feature dim</code></p> <p>\uff082\uff09<code>single_rnn</code> \u7684\u53c2\u6570\u542b\u4e49\uff1a<code>4,3,1=input_size,hidden_size;num_layers</code></p> <p>\uff083\uff09<code>output</code>\u5927\u5c0f\u5c31\u662f <code>1\u00d72\u00d73</code></p> <ul> <li>1\u8868\u793a batch size\uff0c\u8f93\u5165batch size=1\uff0c\u8f93\u51fa batch size\u4e5f\u662f1\uff0c\u6ca1\u6709\u6539\u53d8</li> <li>2\u662f sequence length\uff0c\u5e8f\u5217\u957f\u5ea6\uff0c\u6211\u4eec\u5582\u5165\u7684\u8f93\u5165\u957f\u5ea6\u662f2\uff0c\u6240\u4ee5\u8f93\u51fa\u7684\u957f\u5ea6\u4e5f\u662f2</li> <li>3\uff0c\u7b2c\u4e09\u4e2a\u7ef4\u5ea6\u4e3a\u4ec0\u4e48\u662f3\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u8bbe\u7f6e\u7684hidden size=3\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6bcf\u4e2a\u8f93\u51fa\u7684\u72b6\u6001\u5411\u91cf \u957f\u5ea6\u662f3</li> </ul> <p>\uff084\uff09\\(h_n\\)\uff1a \u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001\uff0c\u5728\u7b80\u5355RNN\u4e2d\uff0c\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001\u7b49\u4e8e\u6700\u540e\u65f6\u523b\u7684\u8f93\u51fa\u7684\uff0coutput\u6700\u540e\u4e00\u884c\u7684\u503c \u7b49\u4e8e \\(h_n\\)</p> <p></p>"},{"location":"learning/13_RNN/#2-rnn","title":"2 \u53cc\u5411\u3001\u5355\u5c42RNN","text":"<pre><code>single_rnn = nn.RNN(input_size=4,hidden_size=3,num_layers=1,batch_first=True)\n</code></pre> <ul> <li> <p>input size\u4e0d\u53d8</p> </li> <li> <p>hidden size\u4e0d\u53d8</p> </li> <li>num_layers\u4e0d\u53d8</li> <li>batch first\u4e5f\u4e0d\u53d8</li> <li>\u4f46\u662f\u9700\u8981\u65b0\u589e\u4e00\u4e2a\u53c2\u6570\uff0c\u53eb\u505a\uff1a</li> </ul> <p></p> <p>\uff1abidirectional\uff0c\u8fd9\u4e2a\u53c2\u6570\u9ed8\u8ba4\u662ffalse\uff0c\u628a\u5b83\u7f6e\u6210true</p> <p>\u7136\u540e\u547d\u540d\u4e3a bidirectional_rnn\uff1a</p> <pre><code>bidirectional_rnn = nn.RNN(input_size=4,hidden_size=3,num_layers=1,batch_first=True,bidirectional=True)\n</code></pre> <p>\u4ee5\u4e0a\u662f\u5b9e\u4f8b\u5316\u7684\u53cc\u5411RNN</p> <ul> <li>\u8f93\u5165\u7279\u5f81\u5927\u5c0f\u662f4</li> <li>\u8f93\u51fa or \u9690\u542b\u5c42\u5927\u5c0f\u662f3</li> <li>\u53ea\u6709\u4e00\u5c42</li> <li>batch first=true</li> <li>\u5e76\u4e14\u8fd8\u662f\u53cc\u5411\u7684</li> </ul> <p>\u540c\u6837\u628a\u4e0a\u9762\u7684\u8f93\u5165 \u9001\u5165\u53cc\u5411RNN\u4e2d\uff0c\u4ee5<code>input</code>\u4f5c\u4e3a\u8f93\u5165<code>bidirectional_rnn(input)</code>\uff0c\u56e0\u4e3a\u65e0\u8bba\u53cc\u5411\u3001\u5355\u5411\uff0c\u8f93\u51fa\u90fd\u662f\u4e00\u6837\u7684\uff0c\u90fd\u662f<code>output</code> \u548c <code>h_n</code>\uff0c\u8868\u793a\u533a\u522b\u52a0\u524d\u7f00<code>bi</code></p> <pre><code>bi_output,bi_h_n = bidirectional_rnn(input)\n</code></pre> <p>\u9996\u5148 \u6253\u5370 output\u7684\u5f62\u72b6</p> <pre><code>bi_output.shape\n</code></pre> <p></p> <p>\u8fd8\u6709h_n\u7684\u5f62\u72b6\uff1a</p> <pre><code>bi_h_n.shape\n</code></pre> <p></p> <p>\u5bf9\u6bd4\uff0c\u628a\u5355\u5411\u5355\u5c42RNN\u7684output\u7684\u5f62\u72b6\uff0ch_n\u7684\u5f62\u72b6\uff0c\u90fd\u6253\u5370\u51fa\u6765\uff1a</p> <p></p> <ul> <li>\u9996\u5148\u4ece\u8f93\u51fa\u4e0a\u6765\u8bb2\uff1a</li> </ul> <p>\uff081\uff09\u5355\u5411\u7684\u8f93\u51fa\u5927\u5c0f\u662f 1\u00d72\u00d73\u7684</p> <p>\uff082\uff09\u53cc\u5411\u7684\u8bdd\u53d8\u6210\u4e86 1\u00d72\u00d76\uff08\u4e00\u4e2abatch size\uff1b2\u4e2asequence length\uff1b6\u4e2a\u7279\u5f81\u7ef4\u5ea6\uff09</p> <p>\u8fd9\u662f\u4e3a\u4ec0\u4e48\u5462\uff1f</p> <p>\u8fd9\u662f\u56e0\u4e3a\u5728\u53cc\u5411RNN\u4e2d\u6700\u540e\u662f\u628a<code>forward layer</code>\u548c<code>backward layer</code>\u4e24\u4e2a\u8f93\u51fa\u62fc\u8d77\u6765\uff0c\u6240\u4ee5\u7279\u5f81\u5927\u5c0f\u53d8\u6210\u4e86\u4e24\u500d\u7684<code>hidden size</code>\uff1b</p> <ul> <li>\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u72b6\u6001\u4e5f\u662f\u4e0d\u4e00\u6837\u7684</li> </ul> <p>\uff081\uff09\u5728\u53cc\u5411RNN\u4e2d\uff0c\u5b83\u7684\u7ef4\u5ea6\u662f 2\u00d71\u00d73\uff08\u524d\u5411\u7684\u8f93\u51fa\u662f\u4e2a 1\u00d73\uff0c\u540e\u5411\u7684\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a1\u00d73\uff09</p> <p>\uff082\uff09\u5728\u5355\u5411\u4e2d\uff0c\u7ef4\u5ea6\u662f1\u00d71\u00d73</p> <p>\u4e3a\u4ec0\u4e48\u5462\uff1f</p> <p>\u56e0\u4e3a\u53cc\u5411\u4e2d\uff0c\u5176\u5b9e\u662f\u6709\u4e24\u4e2a\u5c42\u7684\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u72b6\u6001\uff0c\u6709\u4e00\u4e2a<code>forward layer</code>\u548c\u4e00\u4e2a<code>backward layer\uff0c</code>\u8fd9\u4e24\u4e2a\u72b6\u6001\u5728\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u4e0a\u62fc\u8d77\u6765\u4e86\uff0c\u4f46\u662f\u5728\u5355\u5411\u4e2d\uff0c\u53ea\u6709\u4e00\u5c42\u7684\u6700\u540e\u4e00\u4e2a\u72b6\u6001\uff1b</p>"},{"location":"learning/13_RNN/#3-rnn-api","title":"3 RNN api \u4ee3\u7801\u6c47\u603b","text":""},{"location":"learning/13_RNN/#4-rnnrnn","title":"4 \u5355\u5411RNN&amp;\u53cc\u5411RNN \u4ece\u77e9\u9635\u8fd0\u7b97\u7684\u89d2\u5ea6\u5b9e\u73b0","text":"<p>\u6ce8\u610f\uff1a\u4ee5\u4e0b\u6f14\u793a\u4e2d\uff0c\u6ca1\u6709\u8bbe\u7f6e\u591a\u5c42\uff0c num layers\u90fd\u5b9a\u4e49\u76841\u5c42</p> <p>\uff081\uff09\u5f15\u5165\u5e93\uff0c\u53ef\u4ee5\u4f7f\u7528\u5e38\u89c1\u7684pytorch\u51fd\u6570</p> <pre><code>import torch\nimport torch.nn as\n</code></pre> <p>\uff082\uff09\u5b9a\u4e49\u5e38\u91cf</p> <p>\u7136\u540e\u5b9a\u4e49\u4e00\u4e9b\u5e38\u91cf\uff0c\u6bd4\u5982batch size\u3001\u5e8f\u5217\u957f\u5ea6</p> <pre><code>bs,T = 2,3  #\u6279\u5927\u5c0f \u548c \u5e8f\u5217\u957f\u5ea6\n</code></pre> <p>\u8fd8\u9700\u8981\u5b9a\u4e49 input size\u548chidden size\uff0c\u5206\u522b\u8868\u793a\u8f93\u5165\u7279\u5f81\u5927\u5c0f \u548c \u9690\u542b\u5c42 \u7279\u5f81\u5927\u5c0f</p> <pre><code>input_size,hidden_size=2,3 #\u8f93\u5165\u7279\u5f81\u5927\u5c0f\uff0c\u9690\u542b\u5c42\u7279\u5f81\u5927\u5c0f\n</code></pre> <p>\u6709\u4e00\u4e2a\u95ee\u9898\uff1a\u600e\u4e48\u7406\u89e3 \u65f6\u5e8f\u6a21\u578b\u4e2d\u7684 batchsize\uff1f</p> <p>\uff083\uff09\u751f\u6210 input</p> <p>\u6709\u4e86\u8fd9\u4e9b\u91cf\u4ee5\u540e\uff0c\u751f\u6210\u4e00\u4e2a  input \uff0c\u8fd8\u662f\u8003\u8651batch first\u7b49\u4e8etrue\u7684\u60c5\u51b5\uff1a\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u5199batch size\u3001\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e\u5199\u5e8f\u5217\u957f\u5ea6\u3001\u7b2c\u4e09\u4e2a\u4f4d\u7f6e\u5199feature dim\uff0c\u4e5f\u5c31\u662f input size</p> <pre><code>input = torch.randn(bs,T,input_size) # \u968f\u673a\u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u5165\u7279\u5f81\u5e8f\u5217\n</code></pre> <p>\uff084\uff09\u521d\u59cb\u5316\u9690\u72b6\u6001</p> <p>\u521d\u59cb\u5316\u4e00\u4e2a\u521d\u59cb\u7684\u9690\u542b\u72b6\u6001 <code>h_0</code>\uff0c\u521d\u59cb\u7684\u9690\u542b\u72b6\u6001\u4e00\u822c\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u5982\u679c\u8003\u8651\u4e86<code>batch size</code>\uff0c\u5c31\u5e94\u8be5\u662f <code>batch size</code>\u4e2a\u8fd9\u6837\u7684\u72b6\u6001\uff0c\u4e5f\u53ef\u4ee5\u5148\u5199\u62100\uff1a</p> <pre><code>h_prev=torch.zeros(bs,hidden_size)  # \u6bcf\u4e00\u4e2a\u72b6\u6001\u5411\u91cf\u5927\u5c0f\u662f hidden size\n</code></pre> <p>\u4e5f\u5c31\u662f\u5728\u7b2c\u4e00\u4e2a\u65f6\u523b\u7684\u65f6\u5019\uff0c\u9700\u8981\u4e00\u4e2a\u521d\u59cb\u7684\u9690\u542b\u72b6\u6001\u6765\uff0c\u6765\u4f5c\u4e3a\u7b2c0\u65f6\u523b\u7684\u521d\u59cb\u72b6\u6001</p> <p></p> <p>\uff085\uff09\u8c03\u7528pytorch RNN\u7684API</p> <p>\u8fd8\u662f\u7528<code>nn.RNN()</code>\u7684api\uff0c\u9700\u8981\u4f20\u5165<code>input_size</code>\uff0c<code>hidden size</code>\u8fd8\u6709<code>batch first=True</code>\uff0c\u8fd9\u6837\u6211\u4eec\u5f97\u5230\u4e00\u4e2arnn</p> <pre><code>rnn = nn.RNN(input_size,hidden_size,batch_first=True)\n</code></pre> <p>\uff086\uff09\u4f20\u5165\u53c2\u6570</p> <p>\u9700\u8981\u628a input \u4ee5\u53ca\u521d\u59cb\u72b6\u6001\u4e5f\u4f20\u5165RNN\u4e2d\uff0c\u4f46\u662f\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0capi\u4e2d\u521d\u59cb\u72b6\u6001\u662f\u4e09\u7ef4\u7684</p> <p></p> <p>\u521a\u521a\u521d\u59cb\u5316\u7684\u662f \u540e\u9762\u4e24\u7ef4\uff0c\u7b2c\u4e09\u7ef4 \u6211\u4eec\u6ca1\u6709\u521d\u59cb\u5316\uff0c\u56e0\u4e3a\u8fd9\u91cc\u662f\u5355\u5411\u7684 \u5e76\u4e14 \u53ea\u6709\u4e00\u5c42\u7684\uff0c\u6240\u4ee5\u5bf9\u5b83\u6269\u4e00\u7ef4\u5c31\u597d\u4e86\uff0c\u62690\u7ef4\uff0c\u5f97\u5230rnn output\u548ch_finall\uff0c\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u72b6\u6001\uff0c\u6216\u8005\u53ebstate_finall</p> <pre><code>rnn_output,state_finall = rnn(input,h_prev.unsqueeze(0))\n</code></pre> <p>\u8fd9\u4e2a\u662f\u8c03\u7528pytorch \u5b98\u65b9\u7684api\uff0c\u8fd0\u884c\u6253\u5370\uff0c\u770b\u7ed3\u679c</p> <p></p> <p>\uff087\uff09\u624b\u5199RNN forward \u51fd\u6570</p> <p>\u5b9a\u4e49<code>RNN forward</code>\u51fd\u6570\uff0c\u5b9e\u73b0RNN\u8ba1\u7b97\u539f\u7406 <code>def rnn_forward():</code>\uff0c\u5bf9\u4e8e\u8fd9\u4e2a\u51fd\u6570 \u9996\u5148\u8981\u4f20\u5165\u53c2\u6570\uff1a</p> <p></p> <p>\u6839\u636e\u516c\u5f0f\uff0c\u8981\u60f3\u7b97\u51fa\\(h_t\\)\u7684\u8bdd\uff1a</p> <ul> <li>\u9700\u8981\u6709\\(x\\)\uff0c\\(x\\)\u5c31\u662f\u8f93\u5165\uff0c\u6240\u4ee5\u7b2c\u4e00\u4e2a\u53c2\u6570\uff0c\u9700\u8981\u5199\u7684\u662f\\(input\\)</li> <li>\u8f93\u5165\u9700\u8981\u4e00\u4e2a\u6295\u5f71\u77e9\u9635\uff0c\u5c31\u662f\\(W_{ih}\\)\uff0c\u9700\u8981\u4e00\u4e2aweight</li> <li>\u540c\u65f6\u8fd8\u9700\u8981\u504f\u7f6e\u9879\\(\\mathrm{bias_{ih}}\\)</li> <li>\u8fd8\u6709\u4e0a\u4e00\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001 \uff1a \\(W_{hh}\\) </li> <li>\u8fd8\u6709 \\(b_{hh}\\)</li> <li>\u516c\u5f0f\u4e2d\u8fd8\u6709 \\(h_{t-1}\\) \uff0c\u5199\u6210 <code>h_prev</code> \uff0c\u5c31\u662f\u524d\u4e00\u65f6\u523b\u7684\u72b6\u6001</li> </ul> <p>\u4ee5\u4e0a\uff0c\u5c31\u80fd\u7b97\u51faRNN\u7684\u8f93\u51fa</p> <p>\u7b2c\u4e00\u6b65\uff1a\u83b7\u53d6\u5f53\u524d\u65f6\u523b\u7684\u8f93\u5165\u7279\u5f81\u5f97\u5230<code>x</code></p> <pre><code>def rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev):\n</code></pre> <ul> <li>input \u9ed8\u8ba4 \u4e09\u7ef4\u7684\u7ed3\u6784\uff0c\u5148\u628ainput\u7684\u5f62\u72b6\u62c6\u89e3\u51fa\u6765\uff0c\u5f62\u72b6\u5e94\u8be5\u662f <code>batch size\u00d7sequence length\u00d7input size</code> \uff0c\u8c03\u7528 <code>input.shape</code></li> </ul> <pre><code>bs,T,input_size = input.shape\n</code></pre> <ul> <li>\u901a\u8fc7\u62c6\u89e3 <code>input</code> \uff0c\u8fd8\u53ef\u4ee5\u77e5\u9053 <code>hidden size</code>\uff0c\u4e5f\u5c31\u662f<code>h_dim</code>\uff0c\u4e5f\u5c31\u662f <code>weight_ih</code>\uff0c\u53ef\u4ee5\u6839\u636e\u5b83\u7684\u6743\u91cd\u6240\u5f97\u5230\uff0c\u4e5f\u5c31\u662f<code>weight_ih.shape</code>\uff0c\u90a3\u5230\u5e95\u662f<code>shape[0]</code>\u8fd8\u662f <code>shape[1]</code>\u5462\uff1f\u770b\u516c\u5f0f\uff1a</li> </ul> <p></p> <p><code>weight ih</code>\u8ddf <code>xt</code> \u662f\u5de6\u4e58\u7684\u5173\u7cfb\uff0c\u6240\u4ee5<code>weight</code>\u7684\u7b2c2\u4e2a\u7ef4\u5ea6\u8ddf<code>x</code>\u662f\u76f8\u540c\u7684\uff0c\u6240\u4ee5\u7b2c\u4e00\u4e2a\u7ef4\u5ea6 \u5c31\u662f\u9690\u542b\u5355\u5143\u7684\u7ef4\u5ea6\uff0c\u6240\u4ee5\u5199\u6210<code>.shape[0]</code>\uff0c\u5f97\u5230<code>hidden dim \uff1a</code></p> <pre><code>h_dim = weight_ih.shape[0]\n</code></pre> <p>\u4ee5\u4e0a\u662f\u5f97\u5230\u4e86\u4e00\u4e9b\u7ef4\u5ea6\uff0c\u63a5\u4e0b\u6765\uff0c\u53ef\u4ee5\u5199\u51fa <code>h out</code>\uff0c\u9996\u5148 \u521d\u59cb\u5316\u4e00\u4e2a \u8f93\u51fa\uff0c\u8f93\u51fa\u5927\u5c0f\u662f <code>batch size\u00d7T\u00d7h dim</code>\uff0c\u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u51fa\u77e9\u9635 \u6216\u8005 \u72b6\u6001\u77e9\u9635</p> <pre><code>h_out = torch.zeros(bs,T,h_dim)  # \u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u51fa\uff08\u72b6\u6001\uff09\u77e9\u9635\n</code></pre> <ul> <li> <p><code>bs</code>\u8ddf\u8f93\u5165\u662f\u4e00\u6837\u7684</p> </li> <li> <p>\u5e8f\u5217\u957f\u5ea6 \u6216\u8005\u53eb \u65f6\u95f4\u957f\u5ea6 \u4e5f\u662f\u8ddf \u8f93\u5165\u4e00\u6837\u7684\u7ef4\u5ea6</p> </li> <li>\u9700\u8981\u6539\u6210 <code>hidden size</code>\u8fd9\u4e2a\u7ef4\u5ea6</li> </ul> <p>\u63a5\u4e0b\u6765 \u6839\u636e\u8fd9 6 \u4e2a\u53c2\u6570\uff0c\u7b97\u51fa <code>h out</code></p> <p></p> <p>RNN\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8ba1\u7b97\uff0c\u6240\u4ee5\u9700\u8981\u6839\u636e<code>x1</code>\u8ba1\u7b97<code>h1</code>\uff0c\u6839\u636e<code>x2</code>\u8ba1\u7b97<code>h2</code>\u7b49\u7b49\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a<code>for</code>\u5faa\u73af <code>for t in range(T):</code> </p> <p>\u56e0\u4e3aRNN\u7684\u8ba1\u7b97\u590d\u6742\u5ea6 \u8ddf\u5e8f\u5217\u957f\u5ea6 \u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u6240\u4ee5\u5bf9\u5e8f\u5217\u957f\u5ea6\u8fdb\u884c\u904d\u5386\u5c31\u597d\u4e86</p> <pre><code>for t in range(T):\n</code></pre> <p>\u9996\u5148\u5f97\u5230\u5f53\u524d\u65f6\u523b\u7684\u8f93\u5165\u5411\u91cf\uff0c<code>input</code>\uff0c\u56e0\u4e3ainput\u662f\u4e09\u7ef4\uff1a</p> <ul> <li>\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u662f batch size\uff0c\u5168\u90fd\u53d6\u51fa\u6765</li> <li>\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u662f\u65f6\u95f4\uff0c\u5c31\u62ff\u5f53\u524d t \u65f6\u523b\u7684\u8f93\u5165\u5411\u91cf</li> <li>\u7b2c\u4e09\u7ef4\u662f\u7279\u5f81\u7ef4\u5ea6\uff0c\u4e5f\u662f\u5168\u90e8\u62ff\u51fa\u6765</li> </ul> <pre><code>x = input[:,t,:]  # \u83b7\u53d6\u5f53\u524d\u65f6\u523b\u8f93\u5165\u7279\u5f81\uff0cbs*input_size\n</code></pre> <p>\u4ee5\u4e0a\u662f\u7b2c\u4e00\u6b65\uff1a\u83b7\u53d6\u5f53\u524d\u65f6\u523b\u7684\u8f93\u5165\u7279\u5f81\u5f97\u5230<code>x</code></p> <p>\u7b2c\u4e8c\u6b65\uff1a\u6269\u5145 batch \u7ef4\u5ea6</p> <p>\u6839\u636e\u516c\u5f0f\uff0c\u8ba9<code>w</code>\u8ddf<code>x</code>\u8fdb\u884c\u76f8\u4e58</p> <ul> <li>\u8fd9\u91cc<code>weight</code>\u4e00\u822c\u9ed8\u8ba4\u4f20\u5165 \u662f\u4e8c\u7ef4\u7684</li> <li>\u800c<code>x</code>\u7684\u5927\u5c0f\uff0c\u9ed8\u8ba4\u662f <code>batch size\u00d7input size</code></li> </ul> <p><code>weight ih</code>\u7684\u5f62\u72b6\u662f <code>h dim\u00d7input size</code></p> <p>\u6240\u4ee5\u4e3a\u4e86\u8fdb\u884c<code>batch</code>\u7ef4\u5ea6\u65e0\u5173\u7684\u4e58\u6cd5\u8fd0\u7b97\u7684\u8bdd\uff1a</p> <p>\u9996\u5148\u5bf9<code>weight ih</code>\u8fdb\u884c\u4e00\u4e2a\u6269\u5145\uff0c\u628a<code>weight</code>\u53d8\u6210\u4e00\u4e2a <code>batch</code>\u7684\u5f62\u5f0f\uff0c<code>weight ih</code>\u662f<code>hidden size\u00d7input size</code>\u7684\u5927\u5c0f\uff0c\u5bf9\u5b83 \u589e\u52a0\u4e00\u7ef4\uff0c<code>batch</code> \u7ef4\u5ea6\uff0c\u5bf9\u5b83\u8fdb\u884c\u590d\u5236\uff0c\u590d\u5236\u6210\u8ddf<code>input</code>\u4e00\u6837\u7684\u5927\u5c0f\uff0c\u5927\u5c0f\u5c31\u53d8\u6210\u4e86 <code>batch size\u00d7h dim\u00d7input size</code></p> <pre><code>w_ih_batch = weight_ih.unsqueeze(0).tile(bs,1,1) \n# bs*h_dim*input_size\n</code></pre> <p>\u8fd9\u662f <code>w ih</code>\uff0c\u53d8\u6210 <code>batch</code>\u7684\u5f62\u72b6</p> <p>\u540c\u6837\u5bf9\u4e8e<code>weight hh</code>\uff0c\u4e5f\u662f\u4e00\u6837\u7684\uff0c\u4e5f\u8f6c\u6362\u4e00\u4e0b\uff0c\u5bf9\u5b83\u589e\u52a0\u4e00\u4e2a<code>batch</code>\u7ef4\u5ea6\uff0c\u7136\u540e\u628a\u5b83\u7684<code>batch</code>\u7ef4\u5ea6\u6269\u5145\u6210 <code>batch size</code>\u7ef4\u5ea6\u5927\u5c0f</p> <pre><code>w_hh_batch = weight_hh.unsqueeze(0).title(bs,1,1)\n# bs * h_dim * h_dim\n</code></pre> <p>\u8fd9\u91cc <code>w hh</code>\u5927\u5c0f\u5c31\u662f <code>batch size\u00d7 h dim\u00d7h dim</code>\uff0c\u56e0\u4e3a\u8ddf<code>hidden state</code>\u76f8\u8fde\u7684\uff0c\u6240\u4ee5\u662f\u4e00\u4e2a\u65b9\u9635</p> <p>\\(h_t = \\mathrm{tanh(W_{ih}x_t+b_{ih}+W_{hh}h_{t-1}+b_{hh})}\\)</p> <p>\u7b2c\u4e09\u6b65\uff1a\u5f00\u59cb\u8ba1\u7b97 \\(w_{ih}\u00d7 x_t\u3001w_{hh}\u00d7 h_{t-1}\\)</p> <p>\u7b2c\u4e00\u9879\uff1a<code>w_times_x</code></p> <p>\u9996\u5148\u8ba1\u7b97 <code>x</code>\uff0c\u5c31\u662f\u8ba1\u7b97 <code>Wih</code>\u4e58\u4ee5<code>x</code>\u8fd9\u4e2a\u91cf <code>w_times_x</code>\u8fd9\u4e2a\u91cf\uff0c\u53ef\u4ee5\u8c03\u7528 <code>torch.bmm</code>\u8fd9\u4e2a\u51fd\u6570</p> <p><code>batch matrix multiplication</code>\uff0c\u662f\u542b\u6709\u6279\u5927\u5c0f\u7684\u77e9\u9635\u76f8\u4e58\uff0c\u4e0e \u6279 \u65e0\u5173\u7684 \u8ba1\u7b97\u77e9\u9635\u76f8\u4e58</p> <ul> <li>\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u4f20\u5165 <code>w ih batch</code></li> <li>\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e \u4f20\u5165 <code>x</code></li> </ul> <p>\u5f53\u524d\u8fd9\u4e2a<code>x</code>\u662f<code>batch size\u00d7 input_size</code>\u7684\uff0c\u4e3a\u4e86\u8ddf <code>w ih batch</code>\u76f8\u4e58\uff0c\u9700\u8981\u5c06\u5b83 \u6269\u5145\u4e00\u7ef4\uff0c\u6269\u5145\u6210 <code>batch size\u00d7input size\u00d71</code>\u7684\uff0c\u8fd9\u91cc \u9700\u8981 \u5bf9\u5b83 \u6269\u5145\u4e00\u4e0b\uff0c\u8c03\u7528\u4e00\u4e0b<code>unsqueeze</code></p> <pre><code>x = input[:,t,:].unsqueeze(2)\n</code></pre> <p>\u672c\u6765\u662f\u4e8c\u7ef4\u7684\uff0c\u73b0\u5728\u5728\u7b2c\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u6269\u5145\uff0c\u53d8\u6210 <code>batch size\u00d7input size\u00d71</code>\uff0c\u6b64\u65f6\u8ddf<code>x</code>\u76f8\u4e58\uff0c\u5f97\u5230 <code>batch size\u00d7 h dim\u00d71</code>\uff0c\u6700\u540e<code>1</code>\u7684\u7ef4\u5ea6\u53bb\u6389\uff0c\u8c03\u7528<code>unsqueeze</code>\u51fd\u6570\uff0c\u5f97\u5230\u7684\u7ed3\u679c <code>batch size\u00d7h dim</code>\uff0c\u5f97\u5230<code>w times x</code>\u7684\u7ed3\u679c\uff0c\u504f\u7f6e\u6700\u540e\u518d\u52a0</p> <pre><code>x = input[:,t,:].unsqueeze(2)  # \u83b7\u53d6\u5f53\u524d\u65f6\u523b\u7684\u8f93\u5165\u7279\u5f81 bs*input_size*1\nw_ih_batch = weight_ih.unsqueeze(0).tile(bs,1,1) #bs*h_dim*input_size\nw_hh_batch = weight_hh.unsqueeze(0).tile(bs,1,1) #bs*h_dim*h_dim\n\nw_times_x = torch.bmm(w_ih_batch,x).squeeze(-1) # bs*h_dm\n</code></pre> <p>\u7b2c\u4e8c\u9879 <code>w_times_h</code></p> <p><code>Whh</code> \u77e9\u9635 \u8ddf\u4e0a\u4e00\u65f6\u523b\u7684\u72b6\u6001\u76f8\u4e58\u7684\u7ed3\u679c</p> <p>\u540c\u6837\u8c03\u7528 <code>torch.bmm</code>\u51fd\u6570\uff0c\u5e26\u6709\u6279\u5927\u5c0f\u7684\u77e9\u9635\u76f8\u4e58\uff0c\u8ddf\u4e0a\u4e00\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001 \u8fdb\u884c\u76f8\u4e58</p> <p>\u540c\u6837\u5bf9<code>h prev</code>\u8fdb\u884c\u6269\u5145\uff0c<code>h_prev.unsqueeze(2)</code>\uff0c\u628a\u5b83\u6269\u5145\u4e09\u7ef4</p> <p>\u56e0\u4e3a<code>h prev</code>\u672c\u6765\u662f\uff0c<code>batch size\u00d7hidden size</code>\uff0c\u73b0\u5728\u53d8\u6210 <code>batch size\u00d7hidden size\u00d71</code>\uff0c\u4e58\u51fa\u6765\u4ee5\u540e \u662f <code>batch size\u00d7hidden size \u00d71</code>\uff0c\u6700\u540e\u518d\u628a1 \u53bb\u6389\uff0c\u6324\u6389</p> <p>\u8fd9\u91cc\u4e58\u7684\u6743\u91cd\u662f\u65b9\u9635\uff0c\u4e0d\u6539\u53d8\u5927\u5c0f\uff0c\u6240\u4ee5\u8fd8\u662f <code>h prev</code>\u7684\u5f62\u72b6</p> <pre><code>w_times_h = torch.bmm(w_hh_batch,h_prev.unsqueeze(2)).squeeze(-1)\n</code></pre> <p>\u8c03\u7528 <code>squeeze</code>\u51fd\u6570\uff0c\u628a\u6700\u540e\u76841\u53bb\u6389 \u6700\u540e\u53d8\u6210\u4e86 <code>batch size\u00d7 h dim</code></p> <p>\u8fd9\u662f\u8fd9\u4e24\u4e2a\u91cf\uff0c\u6700\u540e\u628a\u8fd9\u4e9b\u4e1c\u897f\u5168\u90e8\u52a0\u8d77\u6765\uff0c\u8ddf<code>bias</code>\u52a0\u8d77\u6765\uff0c\u7136\u540e\u901a\u8fc7\u4e24\u4e2atanh\u51fd\u6570</p> <p></p> <p>\u9996\u5148\u662f <code>w_times_x</code>\u8fd9\u4e2a\u91cf \u7136\u540e\u52a0\u4e0a <code>bias ih</code>\uff0c\u6700\u540e\u52a0\u4e0a <code>w times h</code>\uff0c\u4e0a\u4e00\u65f6\u523b\u9690\u542b\u72b6\u6001\u76f8\u5173\u7684\uff0c\u6700\u540e\u662f<code>bias hh</code>\uff0c\u7136\u540e\u8fc7\u4e00\u4e2a <code>tanh</code>\u6fc0\u6d3b\u51fd\u6570\uff0c\u6700\u7ec8\u5f97\u5230\u5f53\u524d\u65f6\u523b\u7684\u8fd9\u4e00\u72b6\u6001</p> <pre><code>torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n</code></pre> <p>\u5b9a\u4e49\u4e3a<code>h_prev</code>\uff0c\u56e0\u4e3a\u8fdb\u884c\u7684\u662f\u9012\u5f52\u7684\u8fd0\u7b97</p> <pre><code>h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n</code></pre> <p>\u73b0\u5728\u8ba1\u7b97\u4e86\\(t\\)\u65f6\u523b\u7684\u8f93\u51fa\uff0c\u63a5\u7740\u628a\\(t\\)\u65f6\u523b\u7684\u8f93\u51fa\uff0c\u653e\u5165\u5230 <code>h out</code>\u4e2d\uff0c</p> <p>\u600e\u4e48\u653e\uff0c\u53ea\u8981\u653e\u5230\u65f6\u95f4\u957f\u5ea6\u8fd9\u4e00\u7ef4\uff0c\\(t\\)\u884c\u5373\u53ef</p> <pre><code>h_out[:,t,:] = h_prev\n</code></pre> <p>\u4ee5\u4e0a\u5b8c\u6210\u4e86\u9012\u5f52\u7684\u8fd0\u7b97\uff0c\u6700\u540e\u8fd4\u56de \u8ddf pytorch\u5b98\u65b9api\u4e00\u6837</p> <p>\u9996\u5148\u8fd4\u56de<code>h_out</code></p> <p>\u7136\u540e\u8fd4\u56de \u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001\uff0c\u5176\u5b9e\u4e5f\u5c31\u662f<code>h_prev</code></p> <p>\u4f46\u662f\u8fd9\u91cc\u7684<code>h_prev</code>\u662f\u4e8c\u7ef4\u7684\uff0c\u5b98\u65b9api\u662f\u4e09\u7ef4\u7684\uff0c\u6240\u4ee5\u8981 \u6269\u4e00\u7ef4\uff0c\u6269\u4e00\u7ef4\u7684\u539f\u56e0\u5c31\u662f\u56e0\u4e3a \u81ea\u5df1\u5b9e\u73b0\u7684\u662f \u5355\u5411\u3001\u5355\u5c42\u7684\uff0c\u6240\u4ee5\u5728 \u7b2c0\u7ef4 \u6269\u5145\u4e00\u4e2a1 \u5c31\u597d\u4e86</p> <pre><code>return h_out,h_prev.unsqueeze(0)\n</code></pre> <p>\u4ee5\u4e0a\u662f\u6240\u6709\u5168\u624b\u5199\u7684RNN forward\u51fd\u6570\uff0c\u5176\u5b9e\u5c31\u662f\u5355\u5411\u7684RNN</p>"},{"location":"learning/13_RNN/#torchtile","title":"torch.tile\u51fd\u6570","text":"<p>\u8865\u5145 torch.tile\u51fd\u6570\uff1a\u6cbf\u6307\u5b9a\u7ef4\u5ea6\u91cd\u590d\u5f20\u91cf\u51fd\u6570</p> <p>\u4f8b\u5b50\uff1a</p> <pre><code>import torch\n\n# \u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\nweight_hh = torch.tensor([[1, 2], [3, 4]])\n\n# \u5047\u8bbe\u6279\u91cf\u5927\u5c0f\u4e3a3\nbs = 3\n\n# \u4f7f\u7528 unsqueeze \u5728\u7b2c0\u7ef4\u5ea6\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u7136\u540e\u4f7f\u7528 tile \u6cbf\u7b2c0\u7ef4\u5ea6\u91cd\u590d bs \u6b21\nw_hh_batch = weight_hh.unsqueeze(0).tile(bs, 1, 1)\n\nprint(\"\u539f\u59cb\u5f20\u91cf:\")\nprint(weight_hh)\nprint(\"\u589e\u52a0\u7ef4\u5ea6\u5e76\u91cd\u590d\u540e\u7684\u5f20\u91cf:\")\nprint(w_hh_batch)\n</code></pre> <p>\u5728\u8fd9\u4e2a\u793a\u4f8b\u4e2d\uff1a</p> <ol> <li><code>weight_hh</code> \u662f\u4e00\u4e2a\u5f62\u72b6\u4e3a <code>[2, 2]</code> \u7684\u5f20\u91cf\u3002</li> <li><code>weight_hh.unsqueeze(0)</code> \u5728\u7b2c0\u7ef4\u5ea6\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u4f7f\u5176\u5f62\u72b6\u53d8\u4e3a <code>[1, 2, 2]</code>\u3002</li> <li><code>tile(bs, 1, 1)</code> \u6cbf\u7b2c0\u7ef4\u5ea6\u91cd\u590d <code>bs</code> \u6b21\uff08\u8fd9\u91cc <code>bs</code> \u4e3a3\uff09\uff0c\u4f7f\u5176\u5f62\u72b6\u53d8\u4e3a <code>[3, 2, 2]</code>\u3002</li> </ol> <p>\u8f93\u51fa\u7ed3\u679c\uff1a</p> <pre><code>\u539f\u59cb\u5f20\u91cf:\ntensor([[1, 2],\n        [3, 4]])\n\u589e\u52a0\u7ef4\u5ea6\u5e76\u91cd\u590d\u540e\u7684\u5f20\u91cf:\ntensor([[[1, 2],\n         [3, 4]],\n\n        [[1, 2],\n         [3, 4]],\n\n        [[1, 2],\n         [3, 4]]])\n</code></pre> <p>\u8fd9\u6837\uff0c<code>w_hh_batch</code> \u5c31\u662f\u4e00\u4e2a\u5f62\u72b6\u4e3a <code>[3, 2, 2]</code> \u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6279\u6b21\u90fd\u5305\u542b\u539f\u59cb\u7684 <code>weight_hh</code> \u5f20\u91cf</p>"},{"location":"learning/13_RNN/#5","title":"5 \u9a8c\u8bc1","text":"<p>\u9a8c\u8bc1\u601d\u8def\uff1a</p> <p>\u628a\u4e4b\u524d\u5b9e\u4f8b\u5316\u7684RNN\u7f51\u7edc\uff0c\u53c2\u6570\u62ff\u51fa\u6765\uff0c\u586b\u5145\u5230\u81ea\u5b9a\u4e49\u7684\u7f51\u7edc\u4e2d</p> <p>\u7136\u540e\u7b97\u51fa\u6765\u7684\u7ed3\u679c \u5982\u679c\u662f\u8ddf\u5b98\u65b9API\u7ed3\u679c\u4e00\u81f4\u7684\u8bdd\uff0c\u5c31\u8868\u660e\u81ea\u5b9a\u4e49\u7684\u51fd\u6570\u662f\u6b63\u786e\u7684</p> <p>\u9996\u5148\uff0c\u62ff\u51faRNN\u7684\u53c2\u6570\uff1a</p> <p>\uff081\uff09RNN\u6709\u54ea\u4e9b\u53c2\u6570\uff1f</p> <p><code>nn.Module</code>\u8fd9\u4e2a\u7c7b\uff0c</p> <p>\u2460 \u5728<code>pytorch</code>\u4e2d \u6240\u6709\u7684\u5c42\uff0c\u90fd\u662f\u7ee7\u627f\u81ea<code>nn.Module</code>\u8fd9\u4e2a\u7c7b</p> <p>\u2461 <code>nn.Module</code>\u7684\u51fd\u6570\uff1a<code>name.parameters</code>\u8fd9\u4e2a\u51fd\u6570\uff0c\u67e5\u770b RNN\u4e2d \u6709\u54ea\u4e9b\u53c2\u6570</p> <p><code>name.parameters</code>\u662f\u4e00\u4e2a\u751f\u6210\u5668\uff0c\u53ef\u4ee5\u7528\u5faa\u73af\u5f97\u5230 <code>for p,n in</code> </p> <p>p\uff1a\u53c2\u6570</p> <p>n\uff1aname</p> <p><code>in rnn.named_parameters():</code> \u5c31\u80fd\u770b\u5230 rnn\u6709\u54ea\u4e9b\u53c2\u6570</p> <pre><code>for p,n in rnn.named_parameters():\n</code></pre> <p>\u6253\u5370\u67e5\u770b\u7ed3\u679c\uff1aRNN\u6709\u54ea\u4e9b\u53c2\u6570 \u4ee5\u53ca \u5b83\u7684\u540d\u79f0</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230 RNN\u7684\u6240\u6709\u7684\u53c2\u6570\u3001\u540d\u79f0\u3001\u5177\u4f53\u5730\u5f20\u91cf\u7684\u6570\u503c </p> <p>\u4e00\u5171\u6709\u56db\u4e2a\u53c2\u6570\uff0c\u5206\u522b\u662f</p> <p>\u2460\u7b2c\u4e00\u4e2a\u53c2\u6570\uff1a <code>weight ih l0</code></p> <ul> <li><code>weight ih</code> \uff1a \u516c\u5f0f\u91cc\u7684 <code>wih</code></li> <li><code>l0</code>\uff1a\u7f51\u7edc\u5b9a\u4e49\u53ea\u6709\u4e00\u5c42\uff0c\u5c42\u6570\u662f\u4ece\\(0\\)\u5f00\u59cb\u7684\uff0c\u6240\u4ee5\u662f\u4ece<code>l0</code></li> </ul> <p>\u2461 \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1a<code>weight hh l0</code></p> <p>\u8868\u793a\u5f53\u524d\u5c42 <code>w hh</code>\u7684\u53c2\u6570</p> <p>\u53e6\u5916\u4e24\u4e2a\u5c31\u662f\u504f\u7f6e\u4e86\uff0c\u5206\u522b\u662f</p> <p>\u2462\u7b2c\u4e09\u4e2a\u53c2\u6570\uff1a <code>bias ih</code></p> <p>\u2463\u7b2c\u56db\u4e2a\u53c2\u6570\uff1a <code>bias hh</code> </p> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a</p> <ul> <li>\u524d\u9762\u4e24\u4e2a\u6743\u91cd\u5f20\u91cf \u662f \u4e8c\u7ef4\u5f20\u91cf</li> <li>\u540e\u9762\u4e24\u4e2a\u504f\u7f6e\u662f \u4e00\u7ef4\u7684\u5411\u91cf</li> </ul> <p>\uff082\uff09\u73b0\u5728\u628a\u8fd9\u4e9b\u53c2\u6570 \u4ee3\u5165\u5230\u81ea\u5df1\u5199\u7684<code>RNN forward</code>\u51fd\u6570\u4e2d</p> <p>\u9996\u5148\uff0c\u590d\u5236\u4e00\u4e0b \u81ea\u5df1\u5199\u7684\u51fd\u6570\u7b7e\u540d</p> <pre><code>rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev):\n</code></pre> <ul> <li><code>input</code>\u8fd8\u662f<code>input</code></li> <li><code>weight ih</code>\u53ef\u4ee5\u6539\u6210 <code>rnn.</code>\uff0c\u76f4\u63a5\u7528<code>rnn.\u53c2\u6570\u540d\u79f0</code>\uff0c\u5c31\u53ef\u4ee5\u8bbf\u95ee\u8fd9\u4e2a\u53c2\u6570 \uff0c<code>rnn.weight_ih_l0</code></li> <li><code>weight hh</code>\u4e5f\u662f\u4e00\u6837\uff0c\u7528<code>rnn.</code>\u6765\u8fdb\u884c\u8bbf\u95ee\uff1a<code>rnn.weight_hh_l0</code></li> <li><code>bias</code>\u4e5f\u662f\u4e00\u6837\u7684 \u5bf9\u5e94\u7684\u662f <code>rnn.bias_ih_l0</code></li> <li>\u540c\u6837<code>hh bias</code>\u4e5f\u662f\u4e00\u6837\u7684 <code>rnn.bias_hh_l0</code></li> <li><code>h_prev</code>\uff0c\u5c31\u662f\u81ea\u5b9a\u4e49\u597d\u7684\uff0c\u5c31\u76f4\u63a5\u7528<code>h_prev</code></li> </ul> <pre><code>rnn_forward(input,rnn.weight_ih_l0,rnn.weight_hh_l0,rnn.bias_ih_l0,rnn.bias_hh_l0,h_prev)\n</code></pre> <p>\u53d8\u91cf\u540d\u547d\u4ee4\uff1a</p> <p>\u524d\u9762\u5199\u7684\u662f<code>rnn output</code>\u548c<code>state finall</code></p> <p></p> <p>\u52a0\u524d\u7f00 <code>custom</code>\uff0c\u8868\u793a\u81ea\u5df1\u5199\u7684</p> <pre><code>custom_rnn_output,custom_state_finall = rnn_forward(input,rnn.weight_ih_l0,rnn.weight_hh_l0,rnn.bias_ih_l0,rnn.bias_hh_l0,h_prev)\n</code></pre> <p>\u8fd9\u6837\u5c31\u8c03\u7528\u4e86\u81ea\u5df1\u5199\u7684<code>RNN forward</code>\u51fd\u6570</p> <p>\u7136\u540e\u5bf9\u6bd4<code>pytorch api</code>\u7684\u7ed3\u679c \u548c \u81ea\u5df1\u5199\u7684\u7ed3\u679c</p> <p></p> <p>\u7b2c\u4e00\u4e2a\u5f20\u91cf \u6574\u4f53RNN \u9884\u6d4b\u7684\u8f93\u51fa\uff0c\u662f\u4e00\u81f4\u7684</p> <p>\u7b2c\u4e8c\u4e2a\u5f20\u91cf\u662f\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u8f93\u51fa</p> <p>\u5b98\u65b9\u7684\u7ed3\u679c \u548c \u81ea\u5b9a\u4e49\u7684\u7ed3\u679c\u4e00\u6837 </p>"},{"location":"learning/13_RNN/#rnn_1","title":"\u81ea\u5b9a\u4e49 RNN\u4ee3\u7801","text":""},{"location":"learning/13_RNN/#6-rnn","title":"6 \u9a8c\u8bc1\u53cc\u5411RNN","text":"<pre><code>h_t = tanh(x_t)\n</code></pre> <p>\u53cc\u5411\u7684\u8bdd\u8c03\u7528\u5355\u5411\u7684\u51fd\u6570</p> <p>\u53cc\u5411\u9700\u8981\u6ce8\u610f \u6240\u6709\u7684\u53c2\u6570 \u90fddouble\u4e86\uff0c\u6240\u6709\u7684weight\u548cbias \u90fd\u6709\u4e24\u4e2a</p> <pre><code># step3 \u624b\u5199\u4e00\u4e2abidirectional_rnn_forward\u51fd\u6570\uff0c\u5b9e\u73b0\u53cc\u5411RNN\u7684\u8ba1\u7b97\u539f\u7406\n</code></pre> <ul> <li>\u53cc\u5411\u8981\u8003\u8651\u4e24\u500d\u7684 <code>forward\u51fd\u6570</code>\u548c<code>backward\u5c42</code></li> <li><code>weight</code>\u6709<code>forward</code>\u5c42\u548c<code>backward</code>\u5c42</li> <li><code>bias</code>\u4e5f\u6709<code>forward</code>\u5c42\u548c<code>backward</code>\u5c42</li> <li><code>h prev</code>\u4e5f\u662f\u6709\u4e24\u4efd\u7684</li> </ul> <p>\u7b2c\u4e00\u4efd\u662f <code>forward layer</code>\uff0c\u8fd8\u6709 <code>backward</code>\uff0c\u590d\u5236\u7136\u540e\u6539\u540d\uff0c\u6309\u7167\u5b98\u65b9\u7684\u540d\u79f0\uff0c\u6539\u6210<code>reverse</code></p> <p>\u8fd9\u65f6\u5019\u6240\u6709\u7684\u53c2\u6570\u90fd\u662f\u4e24\u4efd\u7684\uff1a</p> <p><code>forward</code>\u4e00\u4efd\uff0c<code>backward</code>\u4e00\u4efd</p> <p>RNN\u662f\u6bd4\u8f83\u7b80\u5355\u7684\uff0c\u5982\u679c\u662f<code>LSTM</code> \u3001<code>GRU</code> \u66f4\u590d\u6742</p> <p>\u51fd\u6570\u7b7e\u540d\u5199\u6210\uff1a</p> <p></p> <p>\u63a5\u4e0b\u6765\uff0c\u8fd8\u662f\u4e00\u6837\u7684\uff0c\u5f97\u5230\u4e00\u4e9b\u57fa\u672c\u7684\u4fe1\u606f</p> <p>\u9996\u5148\uff0c\u4e0a\u9762\u590d\u5236\u4e0b\u6765\uff1a</p> <p></p> <p>\u7b2c\u4e00\u6b65 <code>batch size</code>\uff0c<code>\u65f6\u95f4</code> \u548c <code>input size</code></p> <p>\u7136\u540e\uff0c\u5f97\u5230 <code>hidden size</code>  \u3001<code>h_dim</code></p> <p>\u5173\u4e8e<code>h_out</code>\uff0c\u8fd9\u91cc<code>batch size</code>\u4e0d\u53d8\uff0c<code>T</code>\u4e0d\u53d8\uff0c\u4f46\u662f<code>h dim</code>\u8981\u53d8\u6210\u4e24\u500d\uff0c\u56e0\u4e3a\u662f\u53cc\u5411\u7684\u7ed3\u6784\uff1a</p> <pre><code>h_out = torch.zeros(bs,T,h_dim*2)\n# \u521d\u59cb\u5316\u8f93\u51fa\u72b6\u6001\u77e9\u9635\uff0c\u6ce8\u610f\u53cc\u5411\u662f\u4e24\u500d\u7684\u7279\u5f81\u5927\u5c0f\n</code></pre> <p>\u8be5\u5b9a\u4e49\u7684\u5b9a\u4e49\u597d\u4e86\uff0c\u63a5\u4e0b\u6765 \u8c03\u7528<code>RNN forward</code>\u51fd\u6570</p> <p>\u8c03\u7528\u4e24\u6b21<code>RNN forward</code>\u51fd\u6570</p> <p>\u7b2c\u4e00\u6b65\u4e00\u6a21\u4e00\u6837</p> <p></p> <p>\u7ea2\u6846\u662f\u9700\u8981\u4f20\u5165\u7684\u53c2\u6570</p> <p>\u8fd9\u662f<code>forward</code>\u5c42\u7684\u8c03\u7528\uff0c\u53d6\u540d\u4e3a <code>forward_output</code>\uff0c\u8fd9\u91cc\u53ea\u53d6 \u7b2c\u4e00\u4e2a\u8fd4\u56de\u503c\uff0c\u6240\u4ee5\u52a0\u4e2a[0]</p> <p></p> <p>\u5f97\u5230 <code>forward layer</code></p> <p>\u4e0b\u9762 <code>backward layer</code></p> <p>\u8fd9\u91cc\u8981\u53d8\u6362\u4e00\u4e0b\uff0c\u9664\u4e86\u6240\u6709\u7684\u53c2\u6570\u90fd\u7528reverse\u7248\u672c\u7684\uff0c\u5bf9input \u4e5f\u8981reverse\u4e00\u4e0b\uff0c\u5c31\u662f\u56e0\u4e3a\u5982\u679c\u662f\u53cd\u5411\u7684\u8bdd\uff0c\u8981\u4fdd\u8bc1\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u4e0a\uff0cinput\u662f\u6700\u540e\u4e00\u4e2a\u5143\u7d20\uff1b\u5bf9input \u9700\u8981 \u5728\u957f\u5ea6\u8fd9\u4e00\u7ef4 \u8fdb\u884c\u7ffb\u8f6c\uff1a</p> <p></p> <p>\u8c03\u7528 <code>torch.flip  api</code>\uff0c\u8fd9\u4e2a<code>api</code>\uff0c\u5bf9\u5f20\u91cf\u8fdb\u884c\u7ffb\u8f6c\uff1a</p> <p></p> <p>\u6709\u4e24\u4e2a\u53c2\u6570\uff1a</p> <ul> <li> <p>\u4e00\u4e2a\u662finput</p> </li> <li> <p>\u4e00\u4e2a\u662fdim\uff0c\u4e5f\u5c31\u662f\u8bf4 \u4f20\u5165\u7684\u662f \u54ea\u4e2a dim\uff0c\u5c31\u4f1a\u5bf9\u54ea\u4e2a dim \u8fdb\u884c\u7ffb\u8f6c\uff0c\u5b8c\u5168\u76f8\u53cd\u7684\u987a\u5e8f</p> </li> </ul> <p>\u8fd8\u662f\u5148\u62f7\u8d1d\u6240\u6709\u7684\u53c2\u6570\uff0c\u8c03\u7528 rnn_forward\u51fd\u6570\uff1a</p> <p></p> <ul> <li>\u7b2c\u4e00\u4e2a\u53c2\u6570 <code>input</code>\uff0c\u8fdb\u884c\u7ffb\u8f6c\uff0c\u8c03\u7528 <code>torch.flip</code>\uff0c<code>flip</code>\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f <code>input</code>\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f<code>\u7ef4\u5ea6</code>\uff0c\u7ef4\u5ea6\u5b98\u65b9api\u4e2d\u89c4\u5b9a\uff1a</li> </ul> <p></p> <p>\u8981\u4e48\u662f\u5217\u8868 \u8981\u4e48\u662f\u5143\u7ec4</p> <p>\u8fd9\u91cc\u7684input\u662f\u4e09\u7ef4\uff0c\u8981\u7ffb\u8f6c\u7684\u662f \u4e2d\u95f4\u8fd9\u4e00\u7ef4\uff0c<code>T</code>\u8fd9\u7ef4\uff1a</p> <p></p> <p>\u4f20\u5165\u4e00\u4e2a\u5217\u8868\uff0c<code>1</code>\u8fd9\u4e00\u7ef4\u5ea6\uff0c\u8868\u793a\u4e2d\u95f4\u8fd9\u4e00\u7ef4\u5ea6\uff0c\u8fdb\u884c\u7ffb\u8f6c</p> <pre><code>rnn_forward(torch.flip(input,[1]),\n            weight_ih_reverse,\n            weight_hh_reverse,\n            bias_ih_reverse,\n            bias_hh_reveerse,\n            h_prev_reverse)\n</code></pre> <p>\u540c\u6837 \u5bf9\u5b83\u7684\u8c03\u7528 \u4e5f\u53ea\u53d6 <code>output</code>\uff0c\u5b9a\u4e49\u4e3a <code>backward output</code></p> <pre><code>backward_output = rnn_forward(torch.flip(input,[1]),\n                              weight_ih_reverse,\n                              weight_hh_reverse,\n                              bias_ih_reverse,\n                              bias_hh_reveerse,\n                              h_prev_reverse)[0] # backward layer\n</code></pre> <p>\u4ee5\u4e0a \u5f97\u5230\u4e86 <code>forward output</code>\u548c<code>backward output</code></p> <p>\u4e3a\u4ec0\u4e48 \u53ea\u4fdd\u7559\u4e86 <code>h_out</code>\uff0c\u6ca1\u6709\u4fdd\u7559<code>h prev</code>\u5462\uff1f</p> <p>\u56e0\u4e3a\u5728RNN\u4e2d\uff0c<code>h prev</code>\u53ef\u4ee5\u4ece <code>h out</code>\u4e2d\u5f97\u5230\uff0c\u6240\u4ee5\u4e3a\u4e86\u65b9\u4fbf \u53ea\u53d6\u4e86 <code>h out</code></p> <p></p> <p>\u63a5\u4e0b\u6765\uff0c\u628a <code>forward output</code> \u548c <code>backward output</code> \u586b\u5145\u5230 <code>h out</code>\u4e2d</p> <p>\u9996\u5148 <code>h out</code>\u662f\u4e09\u7ef4\u7684\uff0c\u5e76\u4e14\u6700\u540e\u4e00\u7ef4 \u7531 <code>forward \u548c backward</code> \u586b\u5145\u8d77\u6765\u7684\uff0c\u6240\u4ee5\u586b\u5145\u65f6\uff0c\u7d22\u5f15\u7684\u5199\u6cd5\uff1a\u4ece\\(0\\)\u5230 <code>h_dim</code></p> <pre><code>h_out[:,:,:h_dim] = forward_output\n</code></pre> <p>\u4ece<code>h_dim:</code>\u5230\u6700\u540e</p> <p>\u524d\u5411\u7684\u8f93\u51fa\uff0c\u586b\u5145\u5230\u524d\u4e00\u534a\u4e2d\uff0c\u540e\u4e00\u534a\u7684\u7ef4\u5ea6\uff0c\u7528<code>backward output</code>\u586b\u5145</p> <pre><code>h_out[:,:,h_dim:] = backward_output\n</code></pre> <p>\u628a <code>\u524d\u5411\u8f93\u51fa</code> \u548c <code>\u540e\u5411\u8f93\u51fa</code> \u62fc\u8d77\u6765\uff0c\u7136\u540e\u8fd4\u56de</p> <p>\u540c\u6837\u6309\u7167<code>\u5b98\u65b9api</code>\uff0c\u8fd4\u56de\u4e24\u4e2a\u6570\uff1a</p> <ul> <li>\u7b2c\u4e00\u4e2a\u6570\u662f <code>h_out</code></li> <li>\u7b2c\u4e8c\u4e2a\u6570\u5c31\u662f <code>state finall</code></li> </ul> <p></p> <p><code>Sate finall</code>\u7ef4\u5ea6\u662f \\(D*num\\_layers\\) \u00d7 N \u00d7 \\(H_{out}\\)</p> <ul> <li>\u524d\u9762\u8868\u793a \u53cc\u5411 \u548c \u5c42\u6570\u7684\u4e58\u79ef</li> <li>\u4e2d\u95f4\u662f<code>batch size</code></li> <li>\u540e\u9762\u662f <code>H_out</code></li> </ul> <p>\u600e\u4e48\u5199\u5462\uff1f</p> <p>\u9996\u5148 \u8981\u53d6\u51fa  <code>h out</code>\u7684\u6700\u540e\u4e00\u4e2a\u65f6\u523b\uff0c\u56e0\u4e3a\u65f6\u523b\u662f\u5728\u4e2d\u95f4\u90a3\u4e2a\u7ef4\u5ea6\uff0c\u6240\u4ee5\u7528 <code>-1</code>\u7d22\u5f15</p> <pre><code>return h_out,h_out[:,-1,:].reshape(())\n</code></pre> <p>\u5148\u53d6\u51fa \u6700\u540e\u4e00\u4e2a\u65f6\u523b\uff0c\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u72b6\u6001\u5411\u91cf\uff0c\u5f62\u72b6 \\(batch \\_size\u00d72\u500d\u7684h\\_dim\\)\uff0c\u5148<code>reshape</code>\uff0c\u628a2\u5355\u72ec\u62ce\u51fa\u6765\uff0c\u7136\u540ereshape\uff1a</p> <ul> <li>Batch size\u4e0d\u53d8</li> <li>2\u5355\u72ec\u62ce\u51fa\u6765</li> <li>h dim\u5c31\u5199\u6210 h dim</li> </ul> <p>\u9996\u5148\u628a\u4e8c\u7ef4\u5f20\u91cf \u53d8\u6210\u4e09\u7ef4\u5f20\u91cf</p> <pre><code>return h_out,h_out[:,-1,:].reshape((bs,2,h_dim))\n</code></pre> <p>\u7136\u540e \u628a2\u63d0\u5230\u524d\u9762\uff0c\u6839\u636e\u5b98\u65b9api\uff1a</p> <ul> <li>2 \u5728\u524d\u9762</li> </ul> <p></p> <ul> <li>batch size\u5728\u4e2d\u95f4</li> </ul> <p>\u6240\u4ee5\u628a2 \u63d0\u5230\u524d\u9762\uff0c\u8c03\u7528\u4e00\u4e0b\u8f6c\u7f6e\u51fd\u6570\uff0c\u5c31\u662f\u628a <code>\u7b2c0\u7ef4\u5ea6</code> \u548c <code>\u7b2c1\u7ef4\u5ea6</code> \u4ea4\u6362\u4e00\u4e0b\uff1a</p> <pre><code>return h_out,h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)\n</code></pre> <p>\u4ee5\u4e0a\u53cc\u5411\u81ea\u5b9a\u4e49RNN \u51fd\u6570\u7684\u5b9e\u73b0</p>"},{"location":"learning/13_RNN/#rnn_2","title":"\u81ea\u5b9a\u4e49\u53cc\u5411 RNN\u4ee3\u7801","text":"<pre><code># step3 \u624b\u5199\u4e00\u4e2a bidirectional_rnn_forward\u51fd\u6570\uff0c\u5b9e\u73b0\u53cc\u5411RNN\u7684\u8ba1\u7b97\u539f\u7406\ndef bidirectional_rnn_forward(input,\n                              weight_ih,\n                              weight_hh,\n                              bias_ih,\n                              bias_hh,\n                              h_prev,\n                              weight_ih_reverse,\n                              weight_hh_reverse,\n                              bias_ih_reverse,\n                              bias_hh_reverse,\n                              h_prev_reverse):\n    bs,T,input_size = input.shape\n    h_dim = weight_ih.shape[0]\n    h_out = torch.zeros(bs,T,h_dim*2) # \u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u51fa\uff08\u72b6\u6001\uff09\u77e9\u9635\uff0c\u6ce8\u610f\u53cc\u5411\u662f\u4e24\u500d\u7684\u7279\u5f81\u5927\u5c0f\n\n    forward_output = rnn_forward(input,\n                                 weight_ih,\n                                 weight_hh,\n                                 bias_ih,\n                                 bias_hh,\n                                 h_prev)[0]  # forward layer\n    backward_output = rnn_forward(torch.flip(input,[1]),\n                                  weight_ih_reverse,\n                                  weight_hh_reverse,\n                                  bias_ih_reverse, \n                                  bias_hh_reverse,\n                                  h_prev_reverse)[0] # backward layer\n\n    # \u5c06input\u6309\u7167\u65f6\u95f4\u7684\u987a\u5e8f\u7ffb\u8f6c\n    h_out[:,:,:h_dim] = forward_output\n    h_out[:,:,h_dim:] = torch.flip(backward_output,[1]) #\u9700\u8981\u518d\u7ffb\u8f6c\u4e00\u4e0b \u624d\u80fd\u548cforward output\u62fc\u63a5\n\n\n    h_n = torch.zeros(bs,2,h_dim)  # \u8981\u6700\u540e\u7684\u72b6\u6001\u8fde\u63a5\n\n    h_n[:,0,:] = forward_output[:,-1,:]\n    h_n[:,1,:] = backward_output[:,-1,:]\n\n    h_n = h_n.transpose(0,1)\n\n    return h_out,h_n\n    # return h_out,h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)\n\n# \u9a8c\u8bc1\u4e00\u4e0b bidirectional_rnn_forward\u7684\u6b63\u786e\u6027\nbi_rnn = nn.RNN(input_size,\n                hidden_size,\n                batch_first=True,\n                bidirectional=True)\nh_prev = torch.zeros((2,bs,hidden_size))\nbi_rnn_output,bi_state_finall = bi_rnn(input,h_prev)\n\nfor k,v in bi_rnn.named_parameters():\n    print(k,v)\n</code></pre> <p>\u4ee3\u7801\u601d\u8def\uff1a</p> <ol> <li>\u9996\u5148\u628a <code>input</code>\u4f20\u5165\u5230 <code>forward layer</code>\u4e2d</li> <li>\u7136\u540e\u518d\u628a<code>input</code> \u6309\u7167 \u65f6\u95f4\u7684\u987a\u5e8f \u7ffb\u8f6c\u4e00\u4e0b\uff0c\u518d\u4f20\u5165<code>backwardward layer</code>\u4e2d</li> <li>\u518d\u628a <code>forward output</code>\u548c<code>backward output</code>\u62fc\u8d77\u6765\uff0c\u5f62\u6210\u6574\u4f53\u7684<code>h out</code></li> <li>\u6700\u540e\u8fd4\u56de\u5e8f\u5217 \u6574\u4f53\u7684\u9690\u542b\u72b6\u6001\u548c \u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u72b6\u6001</li> </ol> <p>\u73b0\u5728\u9a8c\u8bc1  \u53cc\u5411 rnn  forward \u6b63\u786e\u6027</p> <p>\u9996\u5148 \u5b9e\u4f8b\u5316\u53cc\u5411RNN \u5c42</p> <p>\u590d\u5236\u4e0b\u6765\uff0c\u5e76\u8bbe\u7f6e bidirection=True</p> <pre><code># \u9a8c\u8bc1\u4e00\u4e0b bidirectional_rnn_forward\u7684\u6b63\u786e\u6027\nbi_rnn = nn.RNN(input_size,hidden_size,batch_first=True,bidirectional=True)\n</code></pre> <p>\u540c\u6837\u5b9a\u4e49\u4e00\u4e2a<code>h_prev</code></p> <pre><code>h_prev = torch.zeros()\n</code></pre> <p>\u5927\u5c0f\u662f <code>2\u00d7 batch size\u00d7 hidden size</code></p> <p></p> <pre><code>h_prev = torch.zeros(2,bs,hidden_size)\n</code></pre> <p>\u8c03\u7528RNN\uff0c\u4f20\u5165<code>input</code>\u548c<code>h_prev</code>\uff0c\u5f97\u5230\u53cc\u5411RNN\u7684<code>output</code>\u548c\u53cc\u5411<code>state finall</code></p> <pre><code>bi_rnn_output,bi_state_finall = bi_rnn(input,h_prev)\n</code></pre> <p>\u5f97\u5230\u5b98\u65b9api\u7684\u7ed3\u679c</p> <p></p> <p>\u5bf9\u4e8eRNN \u67e5\u770b\u4e00\u4e0b \u53c2\u6570\u7684\u540d\u5b57\uff0c\u7136\u540e\u628a\u8fd9\u4e9b\u53c2\u6570\u4ee3\u5165\u5230\u81ea\u5b9a\u4e49\u7684\u53cc\u5411RNN\u51fd\u6570\u4e2d\u53bb</p> <pre><code>for k,v in bi_rnn.named_parameters():\n    print(k,v)\n</code></pre> <p></p> <p>\u53ef\u4ee5\u770b\u5230\u5728pytorch\u53cc\u5411RNN \u4e2d\u7684\u53c2\u6570\uff1a</p> <ol> <li>weight ih l0</li> <li>weight hh l0</li> <li>bias ih l0</li> <li>bias hh l0</li> <li>weight ih l0 reverse</li> <li>weight hh l0 reverse</li> <li>bias ih l0</li> <li>bias hh l0 reverse</li> </ol> <p>\u4e00\u5171\u67098\u4e2a\u53c2\u6570\uff0c\u8fd9\u662f\u56e0\u4e3a <code>forward layer</code>\u67094\u4e2a\u53c2\u6570\uff0c<code>reverse layer</code>\u4e5f\u67094\u4e2a\u53c2\u6570</p> <p>\u6709\u4e86\u8fd98\u4e2a\u53c2\u6570\uff0c\u5c31\u53ef\u4ee5\u628a\u8fd98\u4e2a\u53c2\u6570\u4f20\u5165\u5230\u53cc\u5411RNN\u4e2d</p> <p>\u9996\u5148\u628a \u7b7e\u540d copy\u4e0b\u6765\uff1a</p> <p></p> <pre><code>bidirectional_rnn_forward(input,\n                          weight_ih,\n                          weight_hh,\n                          bias_ih,\n                          bias_hh,\n                          h_prev,\n                          weight_ih_reverse,\n                          weight_hh_reverse,\n                          bias_ih_reverse,\n                          bias_hh_reverse,\n                          h_prev_reverse)\n</code></pre> <ul> <li><code>input</code>\u4e0d\u53d8</li> <li><code>weight ih</code>\u6539\u6210<code>weight ih l0</code></li> <li><code>weight hh</code>\uff0c\u540c\u6837<code>weight hh l0</code></li> </ul> <p>\u8fd8\u8981\u52a0\u4e0a<code>bi_rnn.</code>\uff0c\u4e5f\u5c31\u662f\u8bf4\u628a\u5b9e\u4f8b\u5316\u7684RNN\u5c42\u4f20\u8fdb\u6765</p> <p><code>bi_rnn.bias ih l0</code> </p> <p><code>bi_rnn.bias hh l0</code></p> <p></p> <p><code>h prev</code>\u9700\u8981\u6ce8\u610f\uff1a\u662f\u4e09\u7ef4\u7684</p> <p>\u524d\u9762\u6709\u4e2a 2 \uff0c\u53ea\u9700\u8981\u4f20\u5165\u7b2c\u4e00\u4e2a\u5c31\u597d\u4e86 <code>h prev[0]</code></p> <p>\u53cd\u5411\u7684\u4e5f\u662f\u7c7b\u4f3c\u7684</p> <p><code>bi_rnn.weight ih l0 reverse</code></p> <p>\u540e\u9762\u4e5f\u662f\u4e00\u6837 <code>bi_rnn.weight hh l0 reverse</code></p> <p><code>bi_rnn.bias ih l0 reverse</code> </p> <p><code>bi_rnn.bias hh l0 reverse</code></p> <p><code>h prev reverse</code>\uff0c\u7528<code>h prev [1]</code></p> <p></p> <p>\u5b9a\u4e49  <code>custom_bi_rnn_output,custom_bi_state_finall</code>\u63a5\u6536\u8f93\u51fa</p> <p>\u63a5\u4e0b\u6765\u5206\u522b\u6253\u5370api\u7684\u7ed3\u679c \u548c \u81ea\u5df1\u5199\u7684\u51fd\u6570\u7684\u7ed3\u679c\uff1a</p> <p></p> <p>\u8fd9\u4e2a \u7ed3\u679c\u6709\u95ee\u9898\uff0c\uff08\u540e\u9762\u6539\u4e86 \u5c31\u662f\u5404\u79cd\u7ffb\u8f6c ) </p> <p>\u7531\u4e8e\u662f\u53cc\u5411\u7684 <code>hidden size=3</code>\uff0c\u4f46\u662f\u8f93\u51fa\u72b6\u6001\u957f\u5ea6\u662f6\uff0c\u8fd9\u662f\u56e0\u4e3a\u53cc\u5411\u7684\u6709\u62fc\u63a5</p>"},{"location":"learning/13_RNN/#_2","title":"\u6c47\u603b\u6240\u6709\u4ee3\u7801","text":"<pre><code>import torch\nimport torch.nn as nn\n</code></pre> <pre><code>bs,T=2,3  # \u6279\u5927\u5c0f\uff0c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\ninput_size,hidden_size = 2,3 # \u8f93\u5165\u7279\u5f81\u5927\u5c0f\uff0c\u9690\u542b\u5c42\u7279\u5f81\u5927\u5c0f\ninput = torch.randn(bs,T,input_size)  # \u968f\u673a\u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u5165\u7279\u5f81\u5e8f\u5217\nh_prev = torch.zeros(bs,hidden_size) # \u521d\u59cb\u9690\u542b\u72b6\u6001\n</code></pre> <pre><code># step1 \u8c03\u7528pytorch RNN API\nrnn = nn.RNN(input_size,hidden_size,batch_first=True)\nrnn_output,state_finall = rnn(input,h_prev.unsqueeze(0))\n\nprint(rnn_output)\nprint(state_finall)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[[-0.7709,  0.7301, -0.9299],\n         [-0.6976, -0.8241, -0.1903],\n         [-0.6485, -0.2633, -0.1093]],\n\n        [[-0.2035,  0.7439, -0.1369],\n         [-0.4805, -0.5790,  0.1787],\n         [-0.6185,  0.4854, -0.4907]]], grad_fn=&lt;TransposeBackward1&gt;)\ntensor([[[-0.6485, -0.2633, -0.1093],\n         [-0.6185,  0.4854, -0.4907]]], grad_fn=&lt;StackBackward0&gt;)\n</code></pre> <pre><code># step2 \u624b\u5199 rnn_forward\u51fd\u6570\uff0c\u5b9e\u73b0RNN\u7684\u8ba1\u7b97\u539f\u7406\ndef rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev):\n    bs,T,input_size = input.shape\n    h_dim = weight_ih.shape[0]\n    h_out = torch.zeros(bs,T,h_dim) # \u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u51fa\uff08\u72b6\u6001\uff09\u77e9\u9635\n\n    for t in range(T):\n        x = input[:,t,:].unsqueeze(2)  # \u83b7\u53d6\u5f53\u524d\u65f6\u523b\u7684\u8f93\u5165\u7279\u5f81\uff0cbs*input_size*1\n        w_ih_batch = weight_ih.unsqueeze(0).tile(bs,1,1) # bs * h_dim * input_size\n        w_hh_batch = weight_hh.unsqueeze(0).tile(bs,1,1)# bs * h_dim * h_dim\n\n        w_times_x = torch.bmm(w_ih_batch,x).squeeze(-1) # bs*h_dim\n        w_times_h = torch.bmm(w_hh_batch,h_prev.unsqueeze(2)).squeeze(-1) # bs*h_him\n        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n\n        h_out[:,t,:] = h_prev\n\n    return h_out,h_prev.unsqueeze(0)\n</code></pre> <pre><code># \u9a8c\u8bc1\u7ed3\u679c\ncustom_rnn_output,custom_state_finall = rnn_forward(input,\n                                                    rnn.weight_ih_l0,\n                                                    rnn.weight_hh_l0,\n                                                    rnn.bias_ih_l0,\n                                                    rnn.bias_hh_l0,\n                                                    h_prev)\nprint(custom_rnn_output)\nprint(custom_state_finall)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[[-0.7709,  0.7301, -0.9299],\n         [-0.6976, -0.8241, -0.1903],\n         [-0.6485, -0.2633, -0.1093]],\n\n        [[-0.2035,  0.7439, -0.1369],\n         [-0.4805, -0.5790,  0.1787],\n         [-0.6185,  0.4854, -0.4907]]], grad_fn=&lt;CopySlices&gt;)\ntensor([[[-0.6485, -0.2633, -0.1093],\n         [-0.6185,  0.4854, -0.4907]]], grad_fn=&lt;UnsqueezeBackward0&gt;)\n</code></pre> <pre><code>print(torch.allclose(rnn_output,custom_rnn_output))\nprint(torch.allclose(state_finall,custom_state_finall))\n</code></pre> <p>\u8f93\u51fa\uff1aTrue\u3001True</p> <pre><code># step3 \u624b\u5199\u4e00\u4e2a bidirectional_rnn_forward\u51fd\u6570\uff0c\u5b9e\u73b0\u53cc\u5411RNN\u7684\u8ba1\u7b97\u539f\u7406\ndef bidirectional_rnn_forward(input,\n                              weight_ih,\n                              weight_hh,\n                              bias_ih,\n                              bias_hh,\n                              h_prev,\n                              weight_ih_reverse,\n                              weight_hh_reverse,\n                              bias_ih_reverse,\n                              bias_hh_reverse,\n                              h_prev_reverse):\n    bs,T,input_size = input.shape\n    h_dim = weight_ih.shape[0]\n    h_out = torch.zeros(bs,T,h_dim*2) # \u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u51fa\uff08\u72b6\u6001\uff09\u77e9\u9635\uff0c\u6ce8\u610f\u53cc\u5411\u662f\u4e24\u500d\u7684\u7279\u5f81\u5927\u5c0f\n\n    forward_output = rnn_forward(input,\n                                 weight_ih,\n                                 weight_hh,\n                                 bias_ih,\n                                 bias_hh,\n                                 h_prev)[0]  # forward layer\n    backward_output = rnn_forward(torch.flip(input,[1]),\n                                  weight_ih_reverse,\n                                  weight_hh_reverse,\n                                  bias_ih_reverse, \n                                  bias_hh_reverse,\n                                  h_prev_reverse)[0] # backward layer\n\n    # \u5c06input\u6309\u7167\u65f6\u95f4\u7684\u987a\u5e8f\u7ffb\u8f6c\n    h_out[:,:,:h_dim] = forward_output\n    h_out[:,:,h_dim:] = torch.flip(backward_output,[1]) #\u9700\u8981\u518d\u7ffb\u8f6c\u4e00\u4e0b \u624d\u80fd\u548cforward output\u62fc\u63a5\n\n\n    h_n = torch.zeros(bs,2,h_dim)  # \u8981\u6700\u540e\u7684\u72b6\u6001\u8fde\u63a5\n\n    h_n[:,0,:] = forward_output[:,-1,:]\n    h_n[:,1,:] = backward_output[:,-1,:]\n\n    h_n = h_n.transpose(0,1)\n\n    return h_out,h_n\n    # return h_out,h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)\n\n# \u9a8c\u8bc1\u4e00\u4e0b bidirectional_rnn_forward\u7684\u6b63\u786e\u6027\nbi_rnn = nn.RNN(input_size,hidden_size,batch_first=True,bidirectional=True)\nh_prev = torch.zeros((2,bs,hidden_size))\nbi_rnn_output,bi_state_finall = bi_rnn(input,h_prev)\n\nfor k,v in bi_rnn.named_parameters():\n    print(k,v)\n</code></pre> <p>\u8f93\u51fa</p> <pre><code>weight_ih_l0 Parameter containing:\ntensor([[ 0.5458,  0.5512],\n        [-0.5077, -0.0750],\n        [ 0.3572,  0.1419]], requires_grad=True)\nweight_hh_l0 Parameter containing:\ntensor([[-0.4093,  0.2012,  0.0746],\n        [-0.5619, -0.3820, -0.4060],\n        [-0.4412,  0.2706, -0.2816]], requires_grad=True)\nbias_ih_l0 Parameter containing:\ntensor([-0.5063, -0.1391, -0.0587], requires_grad=True)\nbias_hh_l0 Parameter containing:\ntensor([ 0.0343, -0.2352,  0.3234], requires_grad=True)\nweight_ih_l0_reverse Parameter containing:\ntensor([[ 0.1298,  0.5538],\n        [ 0.4151,  0.2533],\n        [-0.4401,  0.5322]], requires_grad=True)\nweight_hh_l0_reverse Parameter containing:\ntensor([[-0.4232,  0.2246,  0.4265],\n        [ 0.3016, -0.4142, -0.3064],\n        [-0.1960,  0.2845,  0.3770]], requires_grad=True)\nbias_ih_l0_reverse Parameter containing:\ntensor([-0.4372, -0.2452,  0.4506], requires_grad=True)\nbias_hh_l0_reverse Parameter containing:\ntensor([ 0.3957, -0.4655, -0.2143], requires_grad=True)\n</code></pre> <pre><code>custom_bi_rnn_output,custom_bi_state_finall = bidirectional_rnn_forward(input,\n                                                                        bi_rnn.weight_ih_l0,\n                                                                        bi_rnn.weight_hh_l0,\n                                                                        bi_rnn.bias_ih_l0,\n                                                                        bi_rnn.bias_hh_l0,\n                                                                        h_prev[0],\n                                                                        bi_rnn.weight_ih_l0_reverse,\n                                                                        bi_rnn.weight_hh_l0_reverse,\n                                                                        bi_rnn.bias_ih_l0_reverse,\n                                                                        bi_rnn.bias_hh_l0_reverse,\n                                                                        h_prev[1])\n</code></pre> <pre><code>print(\"Pytorch API output\")\nprint(bi_rnn_output)\nprint(bi_state_finall)\n\nprint(\"\\n custom bidirectional_rnn_forward function output:\")\nprint(custom_bi_rnn_output)\nprint(custom_bi_state_finall)\nprint(torch.allclose(bi_rnn_output,custom_bi_rnn_output))\nprint(torch.allclose(bi_state_finall,custom_bi_state_finall))\n</code></pre> <pre><code>True\nTrue\n</code></pre>"},{"location":"learning/14_LSTM/","title":"LSTM","text":""},{"location":"learning/14_LSTM/#recall","title":"Recall","text":"<p>RNNCELL\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a \u5355\u6b65 \u7684\u8fed\u4ee3</p> <p>\u56e0\u4e3a\u6240\u6709\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc \u90fd\u662f\u6709\u5f88\u591a\u6b65 \u53bb\u8fed\u4ee3\uff0c\u6700\u7ec8\u628a\u6bcf\u4e00\u6b65\u7684\u72b6\u6001 \u53d6\u51fa\u6765 \u4f5c\u4e3a \u8f93\u51fa</p> <p>\u8fd9\u91cc\u7684RNNCELL\uff0c\u4e5f\u5c31\u662f\u8bf4 \u591a\u4e2a\uff0c\u6bcf\u4e2a\u65f6\u523b\u7684\u8ba1\u7b97 \u5c31\u662f\u4e00\u4e2aRNNCELL\uff0c\u7136\u540e\u628a \u591a\u4e2aRNNCELL \u8fde\u8d77\u6765 \uff0c\u5176\u5b9e\u5c31\u6784\u6210\u4e86 \u4e00\u4e2aRNN\uff0c\u6240\u4ee5 \u65e0\u8bba\u662fRNN\u4e5f\u597d\uff0c\u8fd8\u662f GRU\u4e5f\u597d\uff0c\u8fd8\u662f LSTM\u4e5f\u597d\uff0c\u5b83\u4eec \u90fd\u6709\u5404\u81ea\u7684CELL\uff0c\u7136\u540e\u6bcf\u4e2aCELL\uff0c\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a \u5355\u6b65\u7684\u8fd0\u7b97\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a \u5355\u4e2a\u65f6\u523b\u7684\u8fd0\u7b97\uff0c\u4e0b\u9762 \u6709\u4e00\u4e2a\u4f8b\u5b50</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u9996\u5148 \u5b9e\u4f8b\u5316\u4e86 \u4e00\u4e2a RNNCELL\uff1b\u8fd9\u4e2aRNNCELL\u7684 input size\u548chidden size\u5206\u522b\u4e3a10\u548c20\uff1b\u7136\u540e \u6211\u4eec \u5b9a\u4e49\u4e00\u4e2a input \u7684\u8bad\u7ec3\u7279\u5f81\uff0cbatch size\u662f3\uff0c\u7136\u540e \u65f6\u95f4\u957f\u5ea6\u662f6\uff0c\u7136\u540e\u7279\u5f81\u7ef4\u5ea6\u662f10\uff0c\u5e76\u4e14\u5b9a\u4e49\u4e86 \u4e00\u4e2a \u521d\u59cb\u7684 hidden state[hx],\u7136\u540e\u5c31\u53ef\u4ee5\u7528RNNCELL\uff0c\u6765\u53bb\u505a\u6bcf\u4e00\u6b21\u8fed\u4ee3\uff0c\u6240\u4ee5\u6211\u4eec \u770b\u5230 \u8fd9\u91cc\u6709\u4e00\u4e2a for\u5faa\u73af\uff0c\u7136\u540e \u6bcf\u4e00\u6b65 \u4f1a\u8c03\u7528 \u8fd9\u4e2aRNNCELL\u7684 \u5b9e\u4f8b\u5316\u7684\u64cd\u4f5c\uff0c\u7b97\u51fa\u6bcf\u4e00\u65f6\u523b\u7684\u9690\u542b\u72b6\u6001 hx=rnn(input[i],hx)  \u8fd9\u513f\u5e94\u8be5\u662f \\(h_x\\)</p>"},{"location":"learning/15_ContrastiveLearning/","title":"\u5bf9\u6bd4\u5b66\u4e60","text":"<p>ContrastiveLearning</p>"},{"location":"learning/16_YOLO/","title":"YOLO","text":"<p>YOLO\u539f\u7406\u53ca\u4ee3\u7801\u5b9e\u73b0</p>"},{"location":"learning/17_DETR/","title":"DETR","text":"<p>DETR \u539f\u7406\u53ca\u4ee3\u7801\u5b9e\u73b0</p>"},{"location":"learning/18_DINO/","title":"DINO","text":"<p>DINO \u539f\u7406\u53ca\u4ee3\u7801\u5b9e\u73b0</p>"},{"location":"learning/19_GPT/","title":"GPT","text":"<p>\u539f\u7406\u53ca\u4ee3\u7801\u5b9e\u73b0</p>"},{"location":"learning/2/","title":"\u56fe\u89e3LayerNorm &amp; BatchNorm","text":""},{"location":"learning/2/#batchnorm1dnlp","title":"BatchNorm1D\u3001NLP","text":"<p>b=2\uff0cn=3\uff0cd=4</p> <ul> <li> \u7406\u89e3 3\u7ef4\u5f20\u91cf\u30014\u7ef4\u5f20\u91cf\uff1a</li> </ul> <p>\\(\\begin{cases} \u77e9\u9635\u6cd5:2\u4e2a3\u00d74\u7684\u77e9\u9635 \\\\  \\\\ \u62bd\u5c49\u6cd5:\u4e00\u76f4\u5212\u5206\u540c\u4e00\u4e2a\u65b9\u5411\uff0c\u76f4\u5230\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u753b\u53e6\u4e00\u4e2a\u65b9\u5411 \\end{cases}\\)</p> <p>\u90fd\u662f\u6a2a\u6392\u548c\u7ad6\u6392\u7684\u6570\u5b57\u6392\u5217\uff0c\u4e0d\u540c\u7ef4\u5ea6\u7684\u5f20\u91cf\u8868\u793a\u4e0d\u540c\u7684\u5206\u7ec4</p>"},{"location":"learning/2/#nlp","title":"\u600e\u4e48\u7406\u89e3nlp\u4e2d\u7684\u4e09\u7ef4\u5f20\u91cf\uff1f","text":"<ul> <li> \u77e9\u9635\u6cd5\uff1a</li> </ul> <p>2\u4e2a3\u00d74\u7684\u77e9\u9635\uff0c\u56fe\u89e3\uff1a</p> <p></p> <ul> <li> \u62bd\u5c49\u6cd5</li> </ul> <p>\uff08\u7b2c\u4e00\u6b21\u771f\u6b63\u5f00\u59cb\u7406\u89e3\u591a\u7ef4\u5f20\u91cf\u662f\u597d\u670b\u53cb\u62ff\u62bd\u5c49\u7ed9\u6211\u4e3e\u5f97\u4f8b\u5b50\uff0c\u56db\u7ef4\u5f20\u91cf\u5c31\u662f\u62bd\u5c49\u91cc\u9762\u8fd8\u6709\u62bd\u5c49\uff09</p> <p>\u8981\u70b9\uff1a\u4e00\u76f4\u6cbf\u540c\u4e00\u4e2a\u65b9\u5411\u5212\u5206\uff0c\u76f4\u5230\u6700\u540e\u4e00\u7ef4\u6362\u65b9\u5411</p> <p>2\u00d73\u00d74</p> <p></p> <p>\uff1a\u903b\u8f91\u7406\u987a\u4e86\uff0c\u611f\u89c9\u81ea\u5df1\u505a\u8fd9\u4e9b\u4e1c\u897f\u8822\u8822\u7684......\u4f46\u662f\uff0c\u6211\u8bb0\u6027\u4e0d\u597d\uff0c\u603b\u5fd8......\u7b28\u5c31\u7b28\u5427\uff0c\uff08\u51cc\u5999\u5999\u53e3\u543b\uff1a\u5e08\u5085\u5929\u5929\u8bf4\u4eba\u5bb6\u7b28\uff0c\u53ef\u4eba\u5bb6\u672c\u6765\u5c31\u662f\u7b28\uff09</p> <ul> <li> \u5b9e\u9645\u610f\u4e49</li> </ul> <p>\u5bf9\u4e8enlp\u6765\u8bf4\uff0cbnd=234\u8868\u793ab\u4e2a\u53e5\u5b50\uff0c\u6bcf\u4e2a\u53e5\u5b50n\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd\u6709d\u4e2a\u7ef4\u5ea6</p> <p>\\(batch\\_size \u00d7 max\\_sequence\\_length \u00d7 model\\_dim\\)</p> <p>\u5c31\u662f\u8bf4 \u73b0\u5728\u6709 2\u4e2a\u53e5\u5b50\uff0c\u6bcf\u4e2a\u53e5\u5b503\u4e2a\u5355\u8bcd\uff0c\u6bcf\u4e2a\u5355\u8bcd\u75284\u7ef4\u5411\u91cf\u8868\u793a</p> <p></p> <p>\u8fd9\u91cc\u7684\u95ee\u9898\u662f\uff1a\u6211\u4eec\u8fd9\u4e2a\u5143\u7d20\u7684\u4e2a\u6570\u90fd\u662f\u5bf9\u7684\uff0c\u4f46\u662f\u5728\u8ba1\u7b97\u673a\u4e2d\u5b58\u50a8\u4e2d\uff0c\u5e76\u4e0d\u662f\u8fd9\u4e48\u5b58\u7684</p> <ul> <li> \u5b9e\u9645\u610f\u4e49 &amp; \u77e9\u9635\u6cd5 &amp; \u8ba1\u7b97\u673a\u5b58\u50a8\u903b\u8f91</li> </ul> <p></p> <pre><code>batch_size = 2\ntimes_steps = 3\nembedding_dim = 4\n\ninputx = torch.randn(batch_size,times_steps,embedding_dim) # N*L*C\nprint(inputx)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[[ 0.8475, -0.3956, -0.5602,  1.4907],\n         [-0.0746, -0.0021,  0.1291, -0.0343],\n         [-0.3636,  1.8378,  0.1954, -1.0180]],\n\n        [[-1.0256,  1.0202,  0.7321,  0.3294],\n         [-0.6416,  0.5399,  0.8733,  1.7110],\n         [-1.1292,  0.2056,  0.6884,  0.2267]]])\n</code></pre> <ul> <li> <code>torch.randn</code> &amp; <code>torch.rand</code></li> </ul> <p>\u73b0\u5728\u5f00\u59cbnlp&amp;BN\uff0c\u56fe\u5f62\u7ed3\u5408\uff0c\u4f8b\u5b50\uff0c\u6570\u5b66\u4f8b\u5b50\uff0c\u4e0d\u8981\u8131\u79bb\u5b9e\u9645\u610f\u4e49</p> <p>\u4e5f\u5c31\u662f\u8bf4</p> <p>\u8f93\u5165\uff1a\u4e09\u7ef4\u5f20\u91cf\uff0c\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e8c\u7ef4\u6570\u8868\uff0c\u7528\u62ec\u53f7\u5206\u7ec4\uff0c\u6240\u4ee5\u6709\u4e86\u4e0d\u540c\u7684\u610f\u4e49</p>"},{"location":"learning/2/#bn1d","title":"\u600e\u4e48\u8ba1\u7b97BN1D\uff1f","text":"<pre><code>bn_mean = inputx.mean(dim=(0,1),keepdim=True)\nprint(bn_mean)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[[ 0.0695, -0.6811, -0.1232, -0.5339]]])\n</code></pre> <p>\u597d\u561f\uff0c\u8fd9\u4e32\u4ee3\u7801\u662f\u6ca1\u6709\u4efb\u4f55\u95ee\u9898\u4e86</p>"},{"location":"learning/2/#_1","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<p>\u5e93\u51fd\u6570\u8981\u7684\u8f93\u5165\u683c\u5f0f\uff1abnd \\(\\rightarrow\\) bdn</p> <ul> <li> \u8bf4\u4e00\u4e0b\u8fd9\u8fb9\u7684\u8f6c\u7f6e .transpose(-1,-2)</li> </ul> <p></p> <p>\u5b98\u7f51api\u7ed9\u4e86\uff0c\u8f93\u5165bdn\uff0c\u8f93\u51fabdn\uff0c\u6240\u4ee5\u8f93\u51fa\u4ee5\u540e\u518dtranspose\uff0c\u53d8\u6210bnd</p> <p>\u8fd8\u6709\u4e00\u70b9\uff0c\u4e0d\u7ba1\u662fBN\u8fd8\u662fLN\u90fd\u662f\u4e0d\u6539\u53d8\u5f62\u72b6\u7684</p> <pre><code>batch_size = 2\ntimes_steps = 3\nembedding_dim = 4\n\ninputx = torch.randn(batch_size,times_steps,embedding_dim) # N*L*C\n# print(inputx)\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm1d(embedding_dim,affine=False)\nbn_y = batch_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)\n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,1),keepdim=True)\n# print(bn_mean)\nbn_std = inputx.std(dim=(0,1),unbiased=False,keepdim=True)\nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5)\nprint(bn_y)\nprint(verify_bn_y)\nprint(torch.allclose(bn_y,verify_bn_y))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[[ 1.2288e+00,  2.1594e-01, -8.7470e-01,  1.3089e+00],\n         [ 1.2093e+00,  5.8475e-01, -4.2434e-01, -4.7364e-01],\n         [-1.0128e+00, -1.9468e+00, -1.2768e+00,  2.7564e-01]],\n\n        [[-1.1732e+00,  3.5125e-01,  1.5184e+00, -1.3330e+00],\n         [ 4.1865e-01, -4.3382e-01, -4.6577e-04,  1.1563e+00],\n         [-6.7079e-01,  1.2287e+00,  1.0579e+00, -9.3418e-01]]])\ntensor([[[ 1.2288e+00,  2.1594e-01, -8.7470e-01,  1.3089e+00],\n         [ 1.2093e+00,  5.8474e-01, -4.2434e-01, -4.7363e-01],\n         [-1.0128e+00, -1.9468e+00, -1.2768e+00,  2.7564e-01]],\n\n        [[-1.1732e+00,  3.5125e-01,  1.5183e+00, -1.3330e+00],\n         [ 4.1865e-01, -4.3382e-01, -4.6576e-04,  1.1563e+00],\n         [-6.7079e-01,  1.2287e+00,  1.0579e+00, -9.3418e-01]]])\nTrue\n</code></pre>"},{"location":"learning/2/#bn2d","title":"BN2D","text":"<p>\u56fe\u7247\u7684\u5b58\u50a8\u683c\u5f0f\uff1abchw</p> <p>\u8ba1\u7b97BN</p> <p>\u81ea\u5df1\u8111\u888b\u91cc\u60f3\u7684\uff1a</p> <p></p> <p>\u8ba1\u7b97\u673a\u8ba4\u8bc6\u7684\uff1a</p> <p></p> <p>\u52a0\u62ec\u53f7\uff0c\u5c31\u53d8\u6210\u4e86\u5f20\u91cf</p> <p>\u8ba1\u7b97\u673a\u4e0e\u5b9e\u9645\u610f\u4e49\u8054\u7cfb\u8d77\u6765\uff1a\u4f18\u5148\u7ad6\u6392\uff0c\u6700\u540e\u4e00\u7ef4\u6a2a\u6392</p> <p>\u5982\u56fe\uff0c4\u00d73\u00d72\u00d72</p> <p></p>"},{"location":"learning/2/#bncv","title":"BN\u4e0eCV","text":"<p>bchw   \\(\\rightarrow\\) 1c11</p> <p>\u5bf9 bhw\u4e2a\u6570\u6c42\u548c \u8ba1\u7b97 \u5747\u503c\u548c\u65b9\u5dee</p> <p></p> <p></p> <p>step1</p> <p></p> <p>step2</p> <p></p> <p>step3</p> <p>\u7b2c\u4e09\u4e2a\u901a\u9053\u7684\u5747\u503c\u548c\u2f45\u5dee\uff1a</p> <p></p> <p></p>"},{"location":"learning/2/#_2","title":"\u4ee3\u7801","text":"<pre><code>import torch\nimport torch.nn as nn\n# \u6a21\u62df\u2f00\u4e2a\u8f93\u2f0a\u5f20\u91cf\nb, c, h, w = 4, 3, 2, 2 # \u4f8b\u5982\uff0c4\u4e2a\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u67093\u4e2a\u901a\u9053\uff0c\u6bcf\u4e2a\u901a\u9053\u7684\u2f24\u2f29\u4e3a2x2\n# \u5b9a\u4e49\u2f00\u4e2a\u6279\u91cf\u5f52\u2f00\u5316\u5c42\nbatch_norm = nn.BatchNorm2d(num_features=c) # c \u662f\u8f93\u2f0a\u7684\u901a\u9053\u6570\ninput_tensor = torch.arange(48).reshape((4,3,2,2)).float()\nprint(input_tensor)\n# \u5e94\u2f64\u6279\u91cf\u5f52\u2f00\u5316\noutput_tensor = batch_norm(input_tensor)\nprint(output_tensor)\nprint(output_tensor.shape) # \u8f93\u51fa\u7684\u5f62\u72b6\u4ecd\u7136\u662f [b, c, h, w]\n</code></pre> <p>\u5747\u503c &amp; \u65b9\u5dee\u4ee3\u7801\uff1a</p> <pre><code># \u8ba1\u7b97\u6bcf\u4e2a\u901a\u9053\u7684\u5747\u503c\u548c\u2f45\u5dee\nmean = input_tensor.mean(dim=(0, 2, 3))\nvar = input_tensor.var(dim=(0, 2, 3), unbiased=False)\nprint(f\"\u5747\u503c: {mean}\")\nprint(f\"\u2f45\u5dee: {var}\")\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>\u5747\u503c: tensor([19.5000, 23.5000, 27.5000])\n\u2f45\u5dee: tensor([181.2500, 181.2500, 181.2500])\n</code></pre> <p></p>"},{"location":"learning/2/#ln1d","title":"LN1D","text":""},{"location":"learning/2/#_3","title":"\u6587\u5b57\u63cf\u8ff0\u3001\u5b9e\u9645\u610f\u4e49","text":"<ul> <li>LN\u662f\u5bf9\u6bcf\u4e2a\u8bcd\u7684\u6240\u6709\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316</li> </ul> <p>\u7c7b\u6bd4\u5230\u4e8c\u7ef4\u6570\u8868\u662f\u5bf9 \u6a2a\u884c\u6837\u672c\u884c\u8fdb\u884c\u5f52\u4e00\u5316</p> <ul> <li>BN\u662f\u5bf9\u540c\u4e00\u4e2a\u7279\u5f81\u7684\u6240\u6709\u6837\u672c\u8fdb\u884c\u5f52\u4e00\u5316</li> </ul> <p>\u7c7b\u6bd4\u5230\u4e8c\u7ef4\u6570\u8868 \u5c31\u662f\u5bf9\u5217\u8fdb\u884c\u5f52\u4e00\u5316</p> <ul> <li>\u6211\u89c9\u5f97\u7406\u89e3LN\u5c31\u4e00\u53e5\u8bdd\u7262\u7262\u8bb0\u4f4f\uff1aper sample\u3001per layer\uff01\u5c24\u5176\u662fper sample</li> </ul> <p>\u6709\u51e0\u4e2a\u6837\u672c\u3001\u5c31\u6709\u51e0\u4e2a\u5747\u503c&amp;\u65b9\u5dee\u3001\u7c7b\u4f3c\u7684</p> <p>\u6709\u51e0\u4e2a\u8bcd\uff0c\u5c31\u6709\u5747\u503c bnd \u2192 1n1</p> <p>\u6709\u51e0\u5f20\u56fe\u7247\uff0c\u5c31\u6709\u51e0\u4e2a\u5747\u503c  bchw\u2192b111</p> <p>\u6765\uff0c\u5728\u6298\u817e\u4e00\u4e0b\uff0cfor BN\uff0c\u7279\u5f81\u591a\u5c11\u7ef4\uff0c\u5c31\u662f\u51e0\u4e2a\u5747\u503c</p> <p>bnd\u219211d</p> <p>bchw\u21921c11</p> <ul> <li>\u4e8c\u7ef4\u6570\u8868\u7684\u683c\u5f0f\uff1a</li> </ul> <p></p>"},{"location":"learning/2/#_4","title":"\u56fe\u793a","text":"<p>\u5bf9\u6bd4\uff1aBN1D &amp; LN1D</p> <p></p>"},{"location":"learning/2/#_5","title":"\u6570\u5b66\u4f8b\u5b50","text":"<p>\u89c1\uff1a5\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5</p>"},{"location":"learning/2/#_6","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<p>\u89c1\uff1a5\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5</p>"},{"location":"learning/2/#ln2d","title":"LN2D","text":""},{"location":"learning/2/#_7","title":"\u6587\u5b57\u63cf\u8ff0","text":"<p>per sample</p>"},{"location":"learning/2/#_8","title":"\u6570\u5b66\u4f8b\u5b50","text":"<pre><code>import torch\nimport torch.nn as nn\nb, c, h, w = 4, 3, 2, 2 # \u4f8b\u5982\uff0c10\u4e2a\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u67093\u4e2a\u901a\u9053\uff0c\u6bcf\u4e2a\u901a\u9053\u7684\u2f24\u2f29\u4e3a32x32\n# \u5b9a\u4e49\u2f00\u4e2a\u5c42\u5f52\u2f00\u5316\u5c42\nlayer_norm = nn.LayerNorm(normalized_shape=[c, h, w]) # c, h, w \u5206\u522b\u662f\u901a\u9053\u6570\u3001\u2fbc\u5ea6\u548c\u5bbd\u5ea6\n# \u6a21\u62df\u2f00\u4e2a\u8f93\u2f0a\u5f20\u91cf\ninput_tensor = torch.arange(48).reshape((4,3,2,2)).float()\n# \u5e94\u2f64\u5c42\u5f52\u2f00\u5316\noutput_tensor = layer_norm(input_tensor)\n# print(input_tensor)\nprint(output_tensor) # \u8f93\u51fa\u7684\u5f62\u72b6\u4ecd\u7136\u662f [b, c, h, w]\n</code></pre>"},{"location":"learning/2/#_9","title":"\u56fe\u793a","text":"<p>\u5bf9\u6bd4\uff1a</p> <p></p>"},{"location":"learning/2/#_10","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<pre><code>import torch\n\nbatch_size = 4\nchannels = 3\nh,w = 2,2\n\ninputx = torch.randn(batch_size,channels,h,w) # BCHW \u53ea\u8981\u7ef4\u5ea6\u662f\u6b63\u786e\u7684\uff0c\u6570\u5b57\u53ef\u4ee5\u968f\u4fbf\u751f\u6210\n\n# 1. \u5b9e\u73b0batch_norm\u5e76\u9a8c\u8bc1API\n\n## \u8c03\u7528 batch_norm API\nbatch_norm_op = torch.nn.BatchNorm2d(channels,affine=False)\nbn_y = batch_norm_op(inputx) # torch.Size([4, 3, 2, 2])\n\n## \u624b\u5199batch_norm\nbn_mean = inputx.mean(dim=(0,2,3),keepdim=True) # torch.Size([1, 3, 1, 1])\nbn_std = inputx.std(dim=(0,2,3),unbiased=False,keepdim=True) # torch.Size([1, 3, 1, 1])\nverify_bn_y = (inputx - bn_mean)/(bn_std+1e-5) # torch.Size([4, 3, 2, 2])\n\n# print(bn_mean.shape) \n# print(bn_std.shape) \n# print(bn_y.shape)   \n# print(verify_bn_y.shape)    \n# print(torch.allclose(bn_y,verify_bn_y))\n'''\n    torch.Size([1, 3, 1, 1])\n    torch.Size([1, 3, 1, 1])\n    torch.Size([4, 3, 2, 2])\n    torch.Size([4, 3, 2, 2])\n    True\n'''\n# 2. \u5b9e\u73b0layer_norm \u5e76\u9a8c\u8bc1api\n\n## \u8c03\u7528 layer_norm API\nlayer_norm_op = torch.nn.LayerNorm((channels,h,w),elementwise_affine=False)\nln_y = layer_norm_op(inputx)  # torch.Size([4, 3, 2, 2])\n\n## \u624b\u5199layer_norm\nln_mean = inputx.mean(dim=(1,2,3),keepdim=True)  # torch.Size([4, 1, 1, 1])\nln_std = inputx.std(dim=(1,2,3),keepdim=True,unbiased=False)  # torch.Size([4, 1, 1, 1])\nverify_bn_y = (inputx - ln_mean)/(ln_std + 1e-05)   # torch.Size([4, 3, 2, 2])\nprint(ln_mean.shape)\nprint(ln_std.shape)\nprint(ln_y.shape)\nprint(verify_bn_y.shape)\nprint(torch.allclose(ln_y,verify_bn_y))\n'''\n    torch.Size([4, 1, 1, 1])\n    torch.Size([4, 1, 1, 1])\n    torch.Size([4, 3, 2, 2])\n    torch.Size([4, 3, 2, 2])\n    True\n'''\n</code></pre>"},{"location":"learning/2/#_11","title":"\u603b\u7ed3","text":"<p>\u6240\u6709\u7684\u56fe\u793a \u518d\u770b\u4e00\u904d</p> <p>BN\u5e38\u7528\u4e8eCV</p> <p>LN\u5e38\u7528\u4e8eNLP</p> <p></p> <p></p>"},{"location":"learning/3/","title":"\u674e\u6c90 \u76ee\u6807\u68c0\u6d4b\u90e8\u5206","text":""},{"location":"learning/3/#k1","title":"k1 \u8fb9\u754c\u6846","text":""},{"location":"learning/3/#_2","title":"\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6587\u5b57\uff1abounding box</p> <p>\u56fe\u793a\uff1a</p> <p></p>"},{"location":"learning/3/#_3","title":"\u600e\u4e48\u5b9e\u73b0\uff1f","text":"<ol> <li>\u4e24\u89d2\u8868\u793a\u6cd5\uff1a\u7531\u77e9\u5f62\u5de6\u4e0a\u89d2\u7684\u4ee5\u53ca\u53f3\u4e0b\u89d2\u7684x\u548cy\u5750\u6807\u51b3\u5b9a</li> <li>\u4e2d\u5fc3\u5bbd\u5ea6\u8868\u793a\u6cd5\uff1a\u8fb9\u754c\u6846\u4e2d\u5fc3\u7684(x,y)\u8f74\u5750\u6807\u4ee5\u53ca\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6</li> </ol> <p>pytorch\u4e2d\u5e38\u7528\u7684\u683c\u5f0f\uff1a</p> <p>boxes\uff1a</p> <ul> <li>x1, y1, x2, y2 \uff08shape=n\u00d74\uff09</li> <li>cx, cy, w, h\uff08shape=n\u00d74\uff09</li> </ul> <p>n\u8868\u793a\u6846\u7684\u6570\u91cf\uff0c\u4e0d\u6b62\u4e00\u4e2a\u6846</p>"},{"location":"learning/3/#_4","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<p>\u542b\u6d4b\u8bd5\u4ee3\u7801</p> <pre><code>import numpy as np\n\n#@save\ndef box_corner_to_center(boxes):\n    \"\"\"\u4ece\uff08\u5de6\u4e0a\uff0c\u53f3\u4e0b\uff09\u8f6c\u6362\u5230\uff08\u4e2d\u95f4\uff0c\u5bbd\u5ea6\uff0c\u9ad8\u5ea6\uff09\"\"\"\n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    cx = (x1 + x2) / 2\n    cy = (y1 + y2) / 2\n    w = x2 - x1\n    h = y2 - y1\n    boxes = np.stack((cx, cy, w, h), axis=-1)\n    return boxes\n\n#@save\ndef box_center_to_corner(boxes):\n    \"\"\"\u4ece\uff08\u4e2d\u95f4\uff0c\u5bbd\u5ea6\uff0c\u9ad8\u5ea6\uff09\u8f6c\u6362\u5230\uff08\u5de6\u4e0a\uff0c\u53f3\u4e0b\uff09\"\"\"\n    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    x1 = cx - 0.5 * w\n    y1 = cy - 0.5 * h\n    x2 = cx + 0.5 * w\n    y2 = cy + 0.5 * h\n    boxes = np.stack((x1, y1, x2, y2), axis=-1)\n    return boxes\n\n# bbox\u662f\u8fb9\u754c\u6846\u7684\u82f1\u6587\u7f29\u5199\ndog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]\n\nboxes = np.array((dog_bbox, cat_bbox))\nbox_center_to_corner(box_corner_to_center(boxes)) == boxes\n</code></pre> <p>OUT\uff1a</p> <pre><code>array([[ True,  True,  True,  True],      \n       [ True,  True,  True,  True]])\n</code></pre> <p>\u8fd9\u91cc\u60f3\u5f3a\u8c03\u7684\u4e00\u70b9\uff1a</p> <p>\u6211\u4eec\u7528\u7684\u7535\u8111\u5c4f\u5e55\u548cpytorch\u4e2d\uff0c\u9ed8\u8ba4\u7684\u539f\u70b9\u5728\u5de6\u4e0a\u89d2</p> <p>\u56fe\u50cf\u4e2d\u5750\u6807\u7684\u539f\u70b9\u662f\u56fe\u50cf\u7684\u5de6\u4e0a\u89d2\uff0c\u5411\u53f3\u7684\u65b9\u5411\u4e3ax\u8f74\u7684\u6b63\u65b9\u5411\uff0c\u5411\u4e0b\u7684\u65b9\u5411\u4e3ay\u8f74\u7684\u6b63\u65b9\u5411\u3002</p> <p></p>"},{"location":"learning/3/#k2","title":"K2 \u951a\u6846","text":""},{"location":"learning/3/#_5","title":"\u951a\u6846\u662f\u4ec0\u4e48\uff1f","text":"<ul> <li>anchor box</li> <li>\u4ee5\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\uff0c\u751f\u6210\u591a\u4e2a\u7f29\u653e\u6bd4\u548c\u5bbd\u9ad8\u6bd4\uff08aspect ratio\uff09\u4e0d\u540c\u7684\u8fb9\u754c\u6846</li> <li>\u5e94\u7528\uff1a\u57fa\u4e8e\u951a\u6846\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b</li> <li>\u662f\u4e00\u79cd \u533a\u57df\u91c7\u6837\u65b9\u6cd5</li> </ul> <p>\u56fe\u793a\uff1a</p> <p></p>"},{"location":"learning/3/#_6","title":"\u751f\u6210\u591a\u4e2a\u951a\u6846","text":"<p>\u8fd9\u91cc\u8981\u89e3\u51b3\u7684\u95ee\u9898\uff1a\u7ed9\u5b9a\u8f93\u5165\u56fe\u50cf\u9ad8\u5ea6h\u3001\u5bbd\u5ea6w\uff0c\u7f29\u653e\u6bd4s\uff0c\u5bbd\u9ad8\u6bd4r\uff0c\u95ee\u9898\u951a\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\uff1f</p> <p>\u7f29\u653e\u6bd4\\(s\\)\uff1a\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u7f29\u653e\uff0c\u8f93\u5165\u56fe\u50cf\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u5206\u522b\u7f29\u653es\u500d\uff0c\u5f97\u5230\u951a\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6</p> <p>\u5bbd\u9ad8\u6bd4\\(r\\)</p> <p>\u2460 \u6307\u7684\u662f\u951a\u6846\u7684\u5bbd\u548c\u9ad8\u4e4b\u6bd4</p> <p>\u2461 \\(r\\)\u662f\u5bf9\u4e8e\u8f93\u5165\u56fe\u50cf\u5bbd\u9ad8\u6bd4\u7684\u653e\u7f29\uff0c\u4f8b\u5982\u8f93\u5165\u56fe\u50cf\u7684\u5bbd\u9ad8\u6bd4\u662f\\(w/h\\)\uff0c\u6dfb\u52a0r\u540e\u53d8\u6210\u4e86\\((w/h)*r\\)\uff0c\u5f97\u5230\u951a\u6846\u7684\u5bbd\u9ad8\u6bd4\\((w/h)*r\\)</p> <p>\u5176\u5b9e\u6211\u89c9\u5f97\u8fd9\u4e2ar\u53ef\u4ee5\u7406\u89e3\u4e3a\u8f93\u5165\u56fe\u50cf\u5bbd\u9ad8\u6bd4\u7684\u7f29\u653e\u6bd4\u7387\uff0c\u53ea\u4e0d\u662f\u8fd9\u4e2a\u6bd4\u7387 \\(r&gt;0\\)\u5373\u53ef\uff0c\u56e0\u4e3a\u53ef\u4ee5\u5728\u539f\u59cb\u56fe\u50cf\u5bbd\u9ad8\u6bd4\u7684\u57fa\u7840\u4e0a\u653e\u5927\u5bbd\u9ad8\u6bd4 \u6216\u8005 \u51cf\u5c11\u5bbd\u9ad8\u6bd4</p> <p>\u603b\u7ed3\uff1a</p> <p>s\uff1ascale</p> <p>r\uff1aaspect ratio</p> <p>\u4ece\u539f\u59cb\u56fe\u50cf\u5230\u786e\u5b9a\u951a\u6846\uff0c\u9700\u8981\u77e5\u9053\u951a\u6846\u7684......</p> <p>\u4e66\u4e0a\uff1a </p> <p></p> <p>\u8981\u641e\u6e05\u695a\u7684\u70b9\uff1a</p> <ul> <li> \u7f29\u653e\u6bd4s\u7684\u610f\u4e49</li> <li> \u5bbd\u9ad8\u6bd4r\u7684\u610f\u4e49</li> <li> \u951a\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u600e\u4e48\u6765\uff1f</li> <li> \u4ec0\u4e48\u53eb\u505a\u539f\u59cb\u56fe\u50cf\u5f52\u4e00\u5316\u4e3a\u6b63\u65b9\u5f62\uff1f</li> </ul> <p>\u56e0\u4e3a\u951a\u6846\u7684\u751f\u6210\u662f\u5728\u5355\u4f4d\u50cf\u7d20\u5185\u8fdb\u884c\u7684\uff0c\u53c2\u7167\u7cfb\u53d8\u4e86 </p> <p>\u9996\u5148\u89e3\u91ca(from \u8ba8\u8bba\u533a)\u8fd9\u91cc\u951a\u6846\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\uff1a</p> <p></p> <p>\u4e66\u4e0a\uff1a </p> <p></p> <p>\u95ee\u9898\uff1a\u8981\u751f\u6210\u591a\u4e2a\u951a\u6846</p> <p>\u89e3\u51b3\uff1a\u8bbe\u7f6e\u591a\u4e2a\\(s\\)\u548c\\(r\\)\u5373\u53ef\uff0c\u7136\u540e\u8003\u8651\u6bcf\u4e2a\u53ef\u80fd\u7684s\u4e0er\u7684\u7ec4\u5408\uff0c\u4f46\u662f\u5b9e\u8df5\u4e2d\u5f80\u5f80\u53ea\u8003\u8651s1\u548cr1\u7684\u6240\u6709\u7ec4\u5408</p>"},{"location":"learning/3/#_7","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<ul> <li> todo\uff1a\u4e3a\u4ec0\u4e48\u9700\u8981\u534a\u5bbd\u3001\u534a\u9ad8\uff0c\u540e\u9762\u90fd\u662f\u4ec0\u4e48\u610f\u601d\uff1f\u951a\u6846\u751f\u6210\u7684\u6b65\u9aa4\u662f\u600e\u4e48\u6837\u7684\uff1f</li> </ul> <pre><code># \u9664\u4ee52\u6765\u83b7\u5f97\u534a\u9ad8\u548c\u534a\u5bbd\nanchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n                                    in_height * in_width, 1) / 2\n\n# \u6bcf\u4e2a\u4e2d\u5fc3\u70b9\u90fd\u5c06\u6709\u201cboxes_per_pixel\u201d\u4e2a\u951a\u6846\uff0c\n# \u6240\u4ee5\u751f\u6210\u542b\u6240\u6709\u951a\u6846\u4e2d\u5fc3\u7684\u7f51\u683c\uff0c\u91cd\u590d\u4e86\u201cboxes_per_pixel\u201d\u6b21\nout_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n            dim=1).repeat_interleave(boxes_per_pixel, dim=0)\noutput = out_grid + anchor_manipulations\n</code></pre> <ul> <li> in_height, in_width = data.shape[-2:]</li> </ul> <p>data format\uff1ab c h w</p> <ul> <li> device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)</li> </ul> <p>num_sizes\u8868\u793ascale\u3001num_ratios\u8868\u793aaspect ratio</p> <ul> <li> <p> boxes_per_pixel = (num_sizes + num_ratios - 1)</p> </li> <li> <p> center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h \u4e58 steps_h\uff1f</p> </li> <li> <p>\u56e0\u4e3a\u539f\u70b9\u5728\u5de6\u4e0a\u89d2\uff0c\u6240\u4ee5\u662f\u951a\u6846\u4e2d\u5fc3\u70b9\u5206\u522b\u52a0offset_h\u3001offset_w</p> </li> <li>\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u751f\u6210\u951a\u6846\uff0c\u6bcf\u4e2a\u50cf\u7d20\u70b9\u90fd\u6709\u751f\u6210 num_sizes+num_ratios-1\u7684\u951a\u6846</li> <li>\u6240\u4ee5\u5bf9\u4e8e\u5355\u4e2a\u50cf\u7d20\u70b9\u6765\u8bf4\uff0c\u53c2\u8003\u70b9\u53d8\u4e86\uff0c\u5168\u90e8\u53d8\u6210\u4e86\u5355\u4f4d\u5750\u6807\u4e0b\u7684\u5ea6\u91cf\uff0c\u6240\u4ee5\u4e58\u4ee5 steps_h \u548c steps_w</li> <li> <p>steps_h = 1/\u539f\u59cb\u9ad8\u5ea6 &amp;  steps_w = 1/\u539f\u59cb\u5bbd\u5ea6</p> </li> <li> <p> torch.meshgrid\uff1f</p> </li> </ul> <p>shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')</p> <p>shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)</p>"},{"location":"learning/3/#torchmeshgrid","title":"torch.meshgrid()","text":"<ul> <li>\u8f93\u5165\uff1a\u4e00\u7ec4\u4e00\u7ef4\u5f20\u91cf</li> <li>\u8f93\u51fa\uff1a\u4e00\u4e2a\u591a\u7ef4\u7f51\u683c\u5750\u6807</li> <li>deal\uff1a\u6bcf\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684\u5f62\u72b6\u4e0e\u8f93\u5165\u5f20\u91cf\u7684\u5f62\u72b6\u76f8\u540c\uff0c\u4f46\u6bcf\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684\u503c\u662f\u6cbf\u7740\u76f8\u5e94\u7ef4\u5ea6\u7684\u7f51\u683c\u5750\u6807\u3002</li> </ul> <p>\u4e3e\u4f8b\uff1a</p> <pre><code>import torch\n\n# \u5b9a\u4e49\u4e00\u7ef4\u5f20\u91cf\nx = torch.tensor([1, 9])\ny = torch.tensor([4, 5,7,10])\n\n# \u751f\u6210\u7f51\u683c\u5750\u6807\ngrid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n\nprint(\"grid_x:\")\nprint(grid_x)\nprint(\"grid_y:\")\nprint(grid_y)\n</code></pre> <p>Out:</p> <pre><code>grid_x:\ntensor([[1, 1, 1, 1],\n        [9, 9, 9, 9]])\ngrid_y:\ntensor([[ 4,  5,  7, 10],\n        [ 4,  5,  7, 10]])\n</code></pre> <p>\u89e3\u91ca\u4e3a\u4ec0\u4e48\u7684\u8fd9\u6837\u7684\u8f93\u51fa\uff1a</p> <ul> <li>\u7ed3\u679c\u89e3\u8bfb\uff1a</li> </ul> <p>grid_x \u548c grid_y \u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u6784\u6210\u4e00\u5bf9\u5750\u6807\uff0c\u6240\u4ee5\u53ef\u4ee5\u770b\u5230 grid_x \u548c grid_y \u5f62\u72b6\u662f\u76f8\u540c\u7684</p> <p>\u5177\u4f53\u6765\u8bf4\u5c31\u662f\uff0cx\u7684\u6240\u6709\u53d6\u503c\u548cy\u7684\u6240\u6709\u53d6\u503c\uff0c\u80fd\u6784\u6210\u591a\u5c11\u5bf9\u5750\u6807</p> <p>x={1,9}\uff0cy={4, 5,7,10}</p> <p>\u6240\u4ee5 x \u548c y \u7684\u6240\u6709\u5750\u6807\u7684\u96c6\u5408\uff1a</p> <p>(1,4)(1,5)(1,7)(1,10)</p> <p>(9,4)(9,5)(9,7)(9,10)</p> <p>\u7136\u540e\u628a\u6240\u6709\u7684x\u53d6\u51fa\u6765\uff0c\u5f97\u5230grid_x\uff1a</p> <p>[1,1,1,1]</p> <p>[9,9,9,9]</p> <p>\u540c\u7406\uff0c\u7531y\u5f97\u5230grid_y\uff1a</p> <p>[4,5,7,10]</p> <p>[4,5,7,10]</p>"},{"location":"learning/3/#_8","title":"\u5168\u90e8\u4ee3\u7801","text":"<pre><code>import torch\n\ndef multibox_prior(data, sizes, ratios):\n    \"\"\"\u751f\u6210\u4ee5\u6bcf\u4e2a\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u5177\u6709\u4e0d\u540c\u5f62\u72b6\u7684\u951a\u6846\"\"\"\n    in_height, in_width = data.shape[-2:] # b c h w\n    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n    boxes_per_pixel = (num_sizes + num_ratios - 1)\n    # boxes_per_pixel = num_sizes * num_ratios\n    size_tensor = torch.tensor(sizes, device=device) # sizes\uff1ascale input h &amp; w \u5206\u522b\u7f29\u653e size\n    ratio_tensor = torch.tensor(ratios, device=device) # ratio\uff1aaspect ratio\n\n    # \u4e3a\u4e86\u5c06\u951a\u70b9\u79fb\u52a8\u5230\u50cf\u7d20\u7684\u4e2d\u5fc3\uff0c\u9700\u8981\u8bbe\u7f6e\u504f\u79fb\u91cf\u3002\n    # \u56e0\u4e3a\u4e00\u4e2a\u50cf\u7d20\u7684\u9ad8\u4e3a1\u4e14\u5bbd\u4e3a1\uff0c\u6211\u4eec\u9009\u62e9\u504f\u79fb\u6211\u4eec\u7684\u4e2d\u5fc30.5\n    offset_h, offset_w = 0.5, 0.5\n    steps_h = 1.0 / in_height  # \u5728y\u8f74\u4e0a\u7f29\u653e\u6b65\u957f\n    steps_w = 1.0 / in_width  # \u5728x\u8f74\u4e0a\u7f29\u653e\u6b65\u957f\n\n    # \u751f\u6210\u951a\u6846\u7684\u6240\u6709\u4e2d\u5fc3\u70b9\n    # \u56e0\u4e3a\u539f\u70b9\u5728\u5de6\u4e0a\u89d2\uff0c\u6240\u4ee5\u662f\u951a\u6846\u4e2d\u5fc3\u70b9\u5206\u522b\u52a0offset_h\u3001offset_w\n    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n    # \u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u751f\u6210\u951a\u6846\uff0c\u6bcf\u4e2a\u50cf\u7d20\u70b9\u90fd\u6709\u751f\u6210 num_sizes+num_ratios-1\u7684\u951a\u6846\uff0c\u6240\u4ee5\u5bf9\u4e8e\u5355\u4e2a\u50cf\u7d20\u70b9\u6765\u8bf4\n    # \u53c2\u8003\u70b9\u53d8\u4e86\uff0c\u5168\u90e8\u53d8\u6210\u4e86\u5355\u4f4d\u5750\u6807\u4e0b\u7684\u5ea6\u91cf\uff0c\u6240\u4ee5\u9664\u4ee5 steps_h \u548c steps_w\n    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\n    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n\n    # \u751f\u6210\u201cboxes_per_pixel\u201d\u4e2a\u9ad8\u548c\u5bbd\uff0c\n    # \u4e4b\u540e\u7528\u4e8e\u521b\u5efa\u951a\u6846\u7684\u56db\u89d2\u5750\u6807(xmin,xmax,ymin,ymax)\n    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n                   * in_height / in_width  # \u5904\u7406\u77e9\u5f62\u8f93\u5165\n    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n                   sizes[0] / torch.sqrt(ratio_tensor[1:])))\n    # \u9664\u4ee52\u6765\u83b7\u5f97\u534a\u9ad8\u548c\u534a\u5bbd\n    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n                                        in_height * in_width, 1) / 2\n\n    # \u6bcf\u4e2a\u4e2d\u5fc3\u70b9\u90fd\u5c06\u6709\u201cboxes_per_pixel\u201d\u4e2a\u951a\u6846\uff0c\n    # \u6240\u4ee5\u751f\u6210\u542b\u6240\u6709\u951a\u6846\u4e2d\u5fc3\u7684\u7f51\u683c\uff0c\u91cd\u590d\u4e86\u201cboxes_per_pixel\u201d\u6b21\n    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n                dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n    output = out_grid + anchor_manipulations\n    return output.unsqueeze(0)\n\nh = 561\nw = 728\nX = torch.rand(size=(1, 3, h, w)) # \u968f\u673a\u751f\u6210\u8f93\u5165\u56fe\u50cf b c h w\nY = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\nY.shape\n</code></pre>"},{"location":"learning/3/#torchmeshgrid-reshape","title":"torch.meshgrid &amp; reshape","text":"<pre><code>import torch\n\nin_height, in_width = 3, 3\ndevice = torch.device('cpu')\noffset_h, offset_w = 0.5, 0.5\nsteps_h = 1.0 / in_height\nsteps_w = 1.0 / in_width\n\ncenter_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\ncenter_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\nshift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\n\nprint(\"shift_y (before reshape):\")\nprint(shift_y)\nprint(\"shift_x (before reshape):\")\nprint(shift_x)\n\nshift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n\nprint(\"shift_y (after reshape):\")\nprint(shift_y)\nprint(\"shift_x (after reshape):\")\nprint(shift_x)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>shift_y (before reshape):\ntensor([[0.1667, 0.1667, 0.1667],\n        [0.5000, 0.5000, 0.5000],\n        [0.8333, 0.8333, 0.8333]])\nshift_x (before reshape):\ntensor([[0.1667, 0.5000, 0.8333],\n        [0.1667, 0.5000, 0.8333],\n        [0.1667, 0.5000, 0.8333]])\nshift_y (after reshape):\ntensor([0.1667, 0.1667, 0.1667, 0.5000, 0.5000, 0.5000, 0.8333, 0.8333, 0.8333])\nshift_x (after reshape):\ntensor([0.1667, 0.5000, 0.8333, 0.1667, 0.5000, 0.8333, 0.1667, 0.5000, 0.8333])\n</code></pre> <p>Test </p> <pre><code>torch.arange(in_height, device=device) + offset_h\n</code></pre> <p>\u8f93\u51fa\uff1atensor([0.5000, 1.5000, 2.5000])</p> <pre><code>center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\ncenter_h    \n</code></pre> <p>\u8f93\u51fa\uff1atensor([0.1667, 0.5000, 0.8333])</p> <ul> <li> \u521b\u5efa\u951a\u6846\u7684\u56db\u89d2\u5750\u6807(xmin,xmax,ymin,ymax)</li> </ul> <pre><code>'''\n    h = 561\n\n    w = 728\n\n    X = torch.rand(size=(1, 3, h, w)) # \u968f\u673a\u751f\u6210\u8f93\u5165\u56fe\u50cf b c h w\n\n    Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n'''\n\nw = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n                sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n                * in_height / in_width  # \u5904\u7406\u77e9\u5f62\u8f93\u5165\nh = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n                sizes[0] / torch.sqrt(ratio_tensor[1:])))\n\n# print(size_tensor)  tensor([0.7500, 0.5000, 0.2500])\n# print(ratio_tensor[0])  tensor(1.)\n# print(size_tensor * torch.sqrt(ratio_tensor[0])) tensor([0.7500, 0.5000, 0.2500])\n\n# print(sizes[0]) 0.75\n# print(ratio_tensor[1:]) tensor([2.0000, 0.5000])\n# print(sizes[0] * torch.sqrt(ratio_tensor[1:]))  tensor([1.0607, 0.5303])\n\n# print(torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n#                sizes[0] * torch.sqrt(ratio_tensor[1:]))))\n# tensor([0.7500, 0.5000, 0.2500, 1.0607, 0.5303])\n</code></pre>"},{"location":"learning/3/#_9","title":"\u516c\u5f0f\u7684\u6574\u4f53\u903b\u8f91\u662f\u4ec0\u4e48\uff1f","text":"<p>\u4e0b\u6b21\u518d\u770b\uff1a\u8f93\u5165\u4ec0\u4e48\u4e1c\u897f\uff1f\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898\uff1f</p> <ul> <li> <p> todo</p> </li> <li> <p>\u89e3\u91ca1</p> </li> <li> <p>\u89e3\u91ca2</p> </li> </ul> <p>\u5168\u90e8\u6765\u81ea\u8bc4\u8bba\u533a</p> <p>\u7b97\u4e86\uff0c\u641e\u4e0d\u61c2 <code>* in_height / in_width</code>\uff0c\u8fd9\u6b65\u5f88\u8ff7\u60d1\uff0c\u5927\u6982\u7684\u610f\u601d\u5c31\u662f\u5904\u7406\u8f93\u5165\u56fe\u50cf\u662f\u77e9\u5f62\u7684\u60c5\u51b5\uff0c\u4f46\u7b97\u4e86\uff0c\u5b9e\u9645\u5904\u7406\u65f6\uff0c\u5927\u90e8\u5206\u8f93\u5165\u56fe\u50cf\u90fd\u662f \u65b9\u5f62\u7684\uff0c\u6bd4\u503c=1\uff0c\u5176\u4f59\u7684\uff0c\u9047\u5230\u4e86\u518d\u770b\u5427\u3002</p> <p>me\uff1a\u4e5f\u8bb8\u5e38\u8bc6\u662f\uff1a\u8f93\u5165\u56fe\u50cf\u5bbd\u9ad8\u6bd4=1\uff0c\u951a\u6846\u8981\u751f\u6210\u5404\u79cd\u5f62\u72b6\u7684\uff0c\u800c\u4e0d\u80fd\u4ec5\u4ec5\u5c40\u9650\u4e8e\u65b9\u5f62\uff1b</p> <p>\u6211\u7684\u7406\u89e3\uff1a\u5982\u679c\u8f93\u5165\u56fe\u50cf\u662f\u77e9\u9635\uff08\u975e\u65b9\u5f62\uff09\u7684\u8bdd\uff0c\u4f46\u662f\u6bcf\u4e2a\u50cf\u7d20\u70b9\u7684\u53c2\u7167\u7cfb\u8fd8\u662f\u65b9\u5f62\u7684\uff0c\u4e14\u662f\u5355\u4f4d\u65b9\u5f62\uff0c\u5de6\u4e0a\u89d2\u4e3a\u539f\u70b9\uff0c\u90a3\u4e48 \u4ece\u539f\u6765\u7684\u5bbd\u548c\u9ad8 \u53bb\u7f29\u653e \u5230\u5355\u4f4d\u50cf\u7d20\u70b9\uff0c\u5c31\u9700\u8981\u8fd9\u4e2a\u4e1c\u897f\u3002\u8fd8\u662f\u4ece\u4f8b\u5b50\u51fa\u53d1\uff1a</p> <p></p> <p>\u8fd8\u662f\u4e0d\u660e\u767d\uff0c\u7b97\u4e86\u3002</p> <p>\u89e3\u91ca1</p> <p>\\(w = s\\times \\sqrt{r}\\)</p> <p>\\(h = \\frac{s}{ \\sqrt{r}}\\)</p> <p></p> <p></p> <p></p> <p>\u60f3\u5f3a\u8c03\u4e00\u70b9\uff1a\u5b9e\u9645\u5728\u6211\u4eec\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u65f6\uff0c\u7279\u5f81\u56fe\u957f\u548c\u5bbd\u90fd\u662f\u76f8\u540c\u7684\uff0c\u6bd4\u5982 (19, 19)\u3001(7, 7)\uff0c\u6240\u4ee5 <code>in_height / in_width</code> \u6052\u7b49\u4e8e 1\uff0c\u56e0\u6b64\u5bf9\u4e8e\u5b9e\u9645\u7684\u4f7f\u7528\u5e76\u4e0d\u4f1a\u5e26\u6765\u526f\u4f5c\u7528\u3002</p> <p>\u89e3\u91ca2\uff1a</p> <p></p> <p>\u4e3b\u8981\u770b <code>\u4e09\u3001d2l\u4ee3\u7801\u89e3\u6790</code></p> <ul> <li> \u89e3\u91ca\u6700\u540e\u4e00\u53e5\uff0c<code>\u5206\u522b\u5f52\u4e00\u5316</code> \u662f\u4ec0\u4e48\u610f\u601d \uff1f <code>\u79fb\u9879</code></li> </ul> <p></p>"},{"location":"learning/3/#_10","title":"\u7ec6\u8282\uff1a\u4e3a\u4ec0\u4e48\u7d22\u5f15\u662f\u8fd9\u6837\uff1f\uff1f","text":"<ul> <li> solved</li> </ul> <p>w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),                 sizes[0] * torch.sqrt(ratio_tensor[1:]))) *in_height / in_width  # \u5904\u7406\u77e9\u5f62\u8f93\u5165</p> <p>sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]</p> <p>\u89e3\u91ca\u8fd9\u91cc\u7684\u7d22\u5f15\uff0c\u4e4b\u524d\u5df2\u7ecf\u8bf4\u8fc7\u4e86 \u53ea\u8003\u8651 sizes\u548cratios \u7684\u7b2c\u4e00\u5143\u7d20 \u7684\u7ec4\u5408\uff1a</p> <p>(0.75,1)(0.75,2)(0.75,0.5)</p> <p>(0.75,1)   (0.5,1)(0.25,1)  \u53bb\u91cd\u3001\u5220\u6389\u4e00\u4e2a</p> <p>\u951a\u6846\u8981\u751f\u6210\u4e0d\u540c\u5bbd\u9ad8\u6bd4\u7684\uff0cratio\u8868\u793a\u65b9\u5f62\uff0c\u4e00\u822c\u5728\u8bbe\u7f6e\u5bbd\u9ad8\u6bd4\u7684\u65f6\u5019\uff0cratio\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u4e00\u822c\u90fd\u662f1\uff0c\u4f46\u662f\u951a\u6846\u7684\u8986\u76d6\u8303\u56f4\u4e0d\u80fd\u4ec5\u4ec5\u662f\u65b9\u5f62\uff0c\u8fd9\u4e00\u70b9\u7684\u5b9e\u73b0\u662f\u901a\u8fc7scale\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u4e0eratio\u7684\u5404\u4e2a\u5143\u7d20\u7ec4\u5408\u5b9e\u73b0\u7684</p> <p>\u2234 \u4ee3\u7801\u7684\u5b9e\u73b0\u903b\u8f91\u5982\u4e0b\uff08\u89e3\u91ca\u7d22\u5f15\u4e3a\u4ec0\u4e48\u662f\u90a3\u6837\u7684\uff09</p> <p><code>size_tensor * torch.sqrt(ratio_tensor[0])</code> \uff1a (0.75,1) (0.5,1)(0.25,1)</p> <p><code>sizes[0] * torch.sqrt(ratio_tensor[1:])</code> \uff1a(0.75,2)(0.75,0.5)</p> <ul> <li> ......</li> </ul>"},{"location":"learning/3/#k3","title":"K3 \u4ea4\u5e76\u6bd4","text":"<ul> <li>\u4ec0\u4e48\u662f\u4ea4\u5e76\u6bd4</li> <li>\u4e3a\u4ec0\u4e48\u9700\u8981\u4ea4\u5e76\u6bd4</li> <li>\u600e\u4e48\u5b9e\u73b0\u4ea4\u5e76\u6bd4</li> </ul>"},{"location":"learning/3/#_11","title":"\u4ec0\u4e48\u662f\u4ea4\u5e76\u6bd4","text":"<p>\u50cf\u7d20\u96c6\u7684\u6770\u5361\u5fb7\u7cfb\u6570=\u4ea4\u5e76\u6bd4</p> <p></p> <ul> <li> \u56fe\u89e3\uff1a</li> </ul> <p></p>"},{"location":"learning/3/#_12","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u4ea4\u5e76\u6bd4\uff1f","text":"<p>\u2460 \u4f7f\u7528\u4ea4\u5e76\u6bd4\u6765\u8861\u91cf\u951a\u6846\u548c\u771f\u5b9e\u8fb9\u754c\u6846\u4e4b\u95f4\u3001\u4ee5\u53ca\u4e0d\u540c\u951a\u6846\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6</p> <p>\u2461 \u67d0\u4e2a\u951a\u6846\u201c\u8f83\u597d\u5730\u201d\u8986\u76d6\u4e86\u56fe\u50cf\u4e2d\u7684\u72d7\u3002 \u5982\u679c\u5df2\u77e5\u76ee\u6807\u7684\u771f\u5b9e\u8fb9\u754c\u6846\uff0c\u90a3\u4e48\u8fd9\u91cc\u7684\u201c\u597d\u201d\u8be5\u5982\u4f55\u5982\u4f55\u91cf\u5316\u5462\uff1f\u5c31\u662f\u4ea4\u5e76\u6bd4</p>"},{"location":"learning/3/#_13","title":"\u600e\u4e48\u5b9e\u73b0\u4ea4\u5e76\u6bd4\uff1f","text":"<p>Tip</p> <p>\u4e00\u5b9a\u8981\u6ce8\u610f\u539f\u70b9\u5728\u5de6\u4e0a\u89d2\uff0c\u6240\u4ee5\uff1a</p> <pre><code>inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\ninter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n</code></pre> <p>\u5de6\u4e0a\u53d6\u6700\u5927\u3001\u53f3\u4e0b\u89d2\u53d6\u6700\u5c0f\u503c</p> <ul> <li> <code>.clamp</code> \u4ec0\u4e48\u610f\u601d\uff1f</li> </ul> <p><code>inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)</code></p> <pre><code>#@save\ndef box_iou(boxes1, boxes2):\n    \"\"\"\u8ba1\u7b97\u4e24\u4e2a\u951a\u6846\u6216\u8fb9\u754c\u6846\u5217\u8868\u4e2d\u6210\u5bf9\u7684\u4ea4\u5e76\u6bd4\"\"\"\n    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n                              (boxes[:, 3] - boxes[:, 1]))\n    # boxes1,boxes2,areas1,areas2\u7684\u5f62\u72b6:\n    # boxes1\uff1a(boxes1\u7684\u6570\u91cf,4),\n    # boxes2\uff1a(boxes2\u7684\u6570\u91cf,4),\n    # areas1\uff1a(boxes1\u7684\u6570\u91cf,),\n    # areas2\uff1a(boxes2\u7684\u6570\u91cf,)\n    areas1 = box_area(boxes1)\n    areas2 = box_area(boxes2)\n    # inter_upperlefts,inter_lowerrights,inters\u7684\u5f62\u72b6:\n    # (boxes1\u7684\u6570\u91cf,boxes2\u7684\u6570\u91cf,2)\n    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n    # inter_areasandunion_areas\u7684\u5f62\u72b6:(boxes1\u7684\u6570\u91cf,boxes2\u7684\u6570\u91cf)\n    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n    union_areas = areas1[:, None] + areas2 - inter_areas\n    return inter_areas / union_areas\n</code></pre> <ul> <li> ...</li> </ul>"},{"location":"learning/3/#_14","title":"\u76ee\u6807\u68c0\u6d4b\u7684\u6b65\u9aa4","text":""},{"location":"learning/4_GAN/","title":"GAN","text":"<p>About GAN</p> <ul> <li> cycleGAN </li> <li> starGAN </li> <li> C-RNN-GAN</li> </ul> <p>\u89c6\u9891\u94fe\u63a5</p> <p></p> <p>\u6587\u751f\u56fe\u6a21\u578b</p> <p></p> <p>\u4ea4\u4e92\u5f0f\u7684demo</p> <p>text2image\u7684\u6a21\u578b \u6216\u8005\u53eb caption2image\uff1a\u53ef\u4ee5\u600e\u4e48\u6784\u9020\u8fd9\u6837\u4e00\u4e2a\u6a21\u578b\u5462\uff1f</p> <p>\u524d\u63d0\uff1a\u7b97\u529b\u591f\u3001\u6570\u636e\u591f\uff0c\u6709\u5927\u91cf\u7684\u56fe\u50cf\u6587\u672c\u5bf9</p> <p>\u6587\u672c\u8f93\u5165\u5230bert\u4e2d\uff0c\u63d0\u53d6\u6587\u672c\u7279\u5f81\uff0c\u901a\u8fc7Transformer\u6a21\u578b\u751f\u6210\u56fe\u50cfpatch\uff0c\u7136\u540e\u628apatch\u62fc\u8d77\u6765\u6784\u6210\u4e00\u5f20\u56fe\u7247\uff0c\u5047\u8bbe\u91c7\u7528 \u8fd9\u6837\u7684\u6a21\u578b\uff0cLOSS\u8be5\u600e\u4e48\u8bbe\u8ba1\uff1f</p> <p>\u6700\u5e38\u7528\u7684loss\uff0c\u6bd4\u5982L1 loss\uff0cL2 loss\uff0c\u5f52\u4e00\u5316\u52300~1\u4e4b\u95f4\uff0c\u5c06\u9884\u6d4b\u7684\u56fe\u50cf\u50cf\u7d20\u70b9\u503c\u8ddf\u771f\u5b9e\u7684target\u50cf\u7d20\u70b9\u503c\u4f5c\u5dee\uff0c\u7528\u5dee\u7684\u7edd\u5bf9\u503c \u6216\u8005\u5e73\u65b9\u4f5c\u4e3aloss </p> <p>\u53e6\u5916\u4e00\u79cd\u7528\u6cd5\uff1a\u628a\u751f\u6210\u7684\u7167\u7247\u7528\u5728\u53e6\u5916\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u6bd4\u5982\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u5224\u65ad\u7167\u7247\u662f\u751f\u6210\u7684\u7167\u7247\u8fd8\u662f\u8bc6\u522b\u7684\u7167\u7247\uff0c\u8fd9\u79cd\u7f51\u7edc\u7684\u601d\u60f3\u5c31\u662f GAN \u751f\u6210\u5bf9\u6297\u7f51\u7edc  \uff08topic\uff09</p> <p></p> <p>DALLE\u7684\u7528\u7684\u662f diffusion process\uff0cDiffusion process \u4e3b\u8981\u7528\u968f\u673a\u566a\u58f0\u751f\u6210\u76ee\u6807\u5206\u5e03\uff0c\u6570\u5b66\u516c\u5f0f\u6bd4\u8f83\u590d\u6742\uff0c\u901f\u5ea6\u6709\u5f85\u63d0\u5347\uff0c\u751f\u6210\u56fe\u7247\u7684\u8d28\u91cf\u8ddf\u8fed\u4ee3\u6b21\u6570\u6709\u5173\uff0c\u8fed\u4ee3\u6b65\u9aa4\u8d8a\u5927\uff0c\u751f\u6210\u8d28\u91cf\u8d8a\u597d\u3002</p> <p></p> <p>GAN\u601d\u60f3\uff1a\u751f\u6210\u7684\u7167\u7247\u653e\u5230\u8bc6\u522b\u7f51\u7edc\u4e2d\uff0c\u5224\u65ad\u7167\u7247\u6765\u81ea\u771f\u5b9e\u7684\u8fd8\u662f\u751f\u6210\u7684</p> <p>\u8bba\u6587\u5bfc\u8bfb &amp; \u4ee3\u7801\u5b9e\u73b0</p> <p>2024\u5e74\u7684\u8bba\u6587</p> <p>\u5f15\u7528\u91cf\u76ee\u524d4\u4e07\u591a\u3001\u6b8b\u5dee\u7f51\u7edc\u5f15\u7528\u91cf10\u4e07\u591a\u3001\u4e00\u534a\u7684\u5173\u7cfb\uff0c\u56e0\u4e3aGAN\u7f51\u7edc\u4e3b\u8981\u5728\u751f\u6210\u7f51\u7edc\u4e2d\u4f7f\u7528\uff0c\u5bf9\u4e8e\u8bc6\u522b\u5206\u7c7b\u4efb\u52a1\uff0cGAN\u4e0d\u592a\u9002\u7528\uff0c\u800c\u6b8b\u5dee\u7f51\u7edc\u7684\u5e94\u7528\u8303\u56f4\u66f4\u5e7f\u6cdb</p>"},{"location":"learning/4_GAN/#_1","title":"\u6458\u8981","text":"<p>We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: </p> <p>\u63d0\u51fa\u4e86\u65b0\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6297\u8fc7\u7a0b\u4f30\u8ba1\u751f\u6210\u6a21\u578b</p> <p>a generative model G that captures the data distribution, </p> <p>\u751f\u6210\u6a21\u578bG\u6355\u6349\u6570\u636e\u5206\u5e03</p> <p>and a discriminative model D that estimates the probability that a sample came from the training data rather than G. </p> <p>\u5224\u522b\u6a21\u578bD\u4f30\u8ba1\u6837\u672c\u6765\u81ea\u6765\u81ea\u8bad\u7ec3\u6570\u636e\u8fd8\u662f\u751f\u6210\u5668G</p> <p>The training procedure for G is to maximize the probability of D making a mistake.</p> <p>This framework corresponds to a minimax two-player game.</p> <p>\u8be5\u6846\u67b6\u5bf9\u5e94\u6781\u5927\u6781\u5c0f\u4e24\u4eba\u535a\u5f08</p> <p>In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \\(\\frac{1}{2}\\) everywhere.</p> <p>\u5728\u4efb\u610f\u51fd\u6570G\u548cD\u7684\u7a7a\u95f4\u4e2d\uff0c\u5b58\u5728\u552f\u4e00\u89e3\uff0cG\u6062\u590d\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0cD\u5904\u5904\u7b49\u4e8e \\(\\frac{1}{2}\\)</p> <p>D\u6b64\u65f6\u65e0\u6cd5\u533a\u5206\u6570\u636e\u662f\u6765\u81ea\u771f\u5b9e\u7684\u6570\u636e\u8fd8\u662f\u771f\u5b9e\u7684\u6570\u636e</p> <p>In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation.</p> <p>G\u548cD\u90fd\u662f\u7531\u5168\u8fde\u63a5\u5c42\u6784\u6210\u7684</p> <p>There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples.</p> <p>Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.</p>"},{"location":"learning/4_GAN/#intro","title":"Intro","text":"<p>\u4e00\u4e2a\u6bd4\u55bb\uff1a\uff08\u539f\u6587\u7b2c\u4e8c\u6bb5\uff09</p> <p></p> <p>\u5bf9\u6297\u7f51\u7edc\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u5177\u4f53\u5730\u65b9\u6cd5\u662f\u53ef\u4ee5\u81ea\u5df1\u586b\u5145\u7684</p>"},{"location":"learning/4_GAN/#adversarial-nets","title":"\u6a21\u578b\u65b9\u6cd5 Adversarial nets","text":"<p>\u7b26\u53f7\u8bf4\u660e\uff1a</p> <p>\u751f\u6210\u5668\u7684\u5206\u5e03\uff1a \\(p_g\\)  \u751f\u6210\u5668\u5b66\u4e60 \\(x\\) \u7684\u5206\u5e03</p> <p>\u539f\u59cb\u6570\u636e\uff1a\\(x\\)</p> <p>\u8f93\u5165\u566a\u58f0\u53d8\u91cf\uff1a\\(p_z(z)\\)</p> <p>\u6570\u636e\u7a7a\u95f4\u7684\u6620\u5c04\uff1a\\(G(z;\\theta_g)\\)</p> <p>\u6a21\u578b\u5f00\u59cb\u7684\u5148\u9a8c\u5206\u5e03\u662f \\(p_z(z)\\),\u901a\u8fc7 \\(G(z;\\theta_g)\\)\uff0c\u5b66\u4e60\u5230  \\(p_g\\)\uff0c \\(p_g\\) \u8868\u793a\u4e86 \\(x\\) \u7684\u5206\u5e03</p> <p>\\(G(z;\\theta_g)\\) \u53d8\u91cf\u662f \\(z\\)\uff0c\u53c2\u6570\u662f \\(\\theta_g\\) </p> <p>\u8f93\u5165\u566a\u58f0\u53d8\u91cf \\(z\\)\uff0c\\(z\\)\u6765\u81ea\\(p_z\\)</p> <p>G \u662f\u7531 \u53c2\u6570\u4e3a \\(\\theta_g\\) \u7684\u591a\u5c42\u611f\u77e5\u673a\u8868\u793a\u7684\u53ef\u5fae\u51fd\u6570</p> <p>\u591a\u5c42\u611f\u77e5\u5668 \\(D(x;\\theta_d)\\)  \u8f93\u51fa\u4e00\u4e2a\u6807\u91cf\uff1b\u8868\u793a\u4e00\u4e2a\u6982\u7387\uff0c\u5224\u65ad\u6570\u636e\u6765\u81ea\u751f\u6210\u5668\u8fd8\u662f\u5224\u522b\u5668</p> <p>D \u7684\u8f93\u5165\u662f \\(x\\)\uff0c\u53c2\u6570\u662f \\(\\theta_d\\)\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a\u6807\u91cf\uff0c\u53cd\u6620\u6570\u636e...</p> <p>\u8bad\u7ec3 D \u6700\u5927\u5316 \u7ed9\u8bad\u7ec3\u6837\u672c\u548c\u6765\u81ea \\(G\\) \u7684\u6837\u672c\u5206\u914d\u6b63\u786e\u6807\u7b7e\u7684\u6982\u7387</p> <p>D\u7684\u76ee\u7684\u662f\u80fd\u591f\u6b63\u786e\u5206\u7c7b \u8bad\u7ec3\u6837\u672c \u548c G\u7684\u6837\u672c\u6982\u7387</p> <p>\u8bad\u7ec3 G \u6700\u5c0f\u5316 \\(log(1-D(G(z)))\\)</p> <p>\\(log\\)\u5355\u8c03\u9012\u589e\uff0c\u62ec\u53f7\u91cc\u9762 \\(1-D(G(z))\\) \u6700\u5c0f\uff0c\u6240\u4ee5 \\(D(G(z))\\)  \u6700\u5927</p> <p>\\(D(G(z))\\) \u6700\u5927\u8868\u793a \u751f\u6210\u5668\u6240\u751f\u6210\u7684\u6837\u672c \u8f93\u5165\u5230\u5224\u522b\u5668\u7684\u65f6\u5019\uff0c\u5224\u522b\u5668\u5c06\u5b83\u5224\u522b\u6210 \\(1\\)</p> <p>\u4e5f\u5c31\u662f\u8bf4 \u751f\u6210\u5668\u751f\u6210\u7684\u6837\u672c\uff0c\u5224\u522b\u5668\u5c06\u5b83\u5224\u522b\u6210\u6765\u81ea\u771f\u5b9e\u6837\u672c\uff0c\u7b49\u4e8e1\uff0c\u8fd9\u65f6\u5019\u751f\u6210\u5668\u7684\u76ee\u6807\u5c31\u5df2\u7ecf\u8fbe\u6210\u4e86\uff0c\u4eceG\u751f\u6210\u7684\u6570\u636e\uff0c\u5224\u522b\u5668\u8ba4\u4e3a\u662f\u771f\u5b9e\u7684\u6570\u636e</p> <p></p> <p>\u4ef7\u503c\u51fd\u6570 \\(V(G,D)\\)</p> <p>\u89e3\u91ca\u4ef7\u503c\u51fd\u6570:</p> <p>\\(\\min_G \\max_G V(D,G)\\) </p> <p>min\u662f\u5bf9G\u800c\u8a00\uff0c\\(max\\)\u5bf9\\(D\\)\u800c\u8a00</p> <p>\u2460 \u5bf9\\(D\\)\u800c\u8a00 \u6211\u4eec\u8ba9\\(V\\)\u8fbe\u5230\u6700\u5927\uff0c\u8981\u8ba9\\(V\\)\u8fbe\u5230\u6700\u5927\uff0c\u4e5f\u5c31\u662f  \\(logD(x)\\) \u8fbe\u5230\u6700\u5927\uff0c\u540c\u65f6\\(log 1-D(G(z))\\)  \u8fbe\u5230\u6700\u5927\uff0c\u56e0\u6b64\u5bf9\u4e8e\u5224\u522b\u5668\u800c\u8a00\uff0c\u6211\u4eec\u5e0c\u671b\u5224\u522b\u5668 \u80fd\u591f\u628a\u6765\u81ea\u4e8e\u8bad\u7ec3\u96c6\u7684\u6837\u672c \u628a\u5b83\u5206\u7c7b\u6210\u771f\u5b9e\u7684\uff0c\u6765\u81ea\u751f\u6210\u5668\u7684\u6837\u672c \u5206\u7c7b\u6210\u5047\u7684\uff0c\u8fd9\u4e2a\u5bf9\u4e8e\u5224\u522b\u5668\u7684\u8bad\u7ec3\u76ee\u6807</p> <p>\u2461 \u751f\u6210\u5668\u7684\u8bad\u7ec3\u76ee\u6807\uff0cminG\uff0c\u4e5f\u5c31\u662f\u8bf4 \u5bf9\u4e8e\u751f\u6210\u5668\u800c\u8a00\uff0c\u540e\u9762\u4e24\u4e2a\u8fbe\u5230\u6700\u5c0f\uff0c\u7531\u4e8e\u7b2c\u4e00\u9879\\(logD(x)\\)\u4e0eg\u65e0\u5173\uff0c\u6240\u4ee5\u7b2c\u4e00\u9879\u4e0d\u7528\u770b\uff0c\u5728\u8bad\u7ec3\u96c6\u4e2d \u7b2c\u4e00\u9879 \u76f8\u5f53\u4e8e\u4e00\u4e2a\u5e38\u6570 \u4e0d\u91cd\u8981\u3002\u4e3b\u8981\u770b\u7b2c\u4e8c\u9879\uff0c\u7b2c\u4e8c\u9879\u8fbe\u5230\u6700\u5c0f\u7684\u8bdd\uff0c\u4e5f\u5c31\u662f\u8bf4 \u5e0c\u671b\\(DG(z)\\)\u8fbe\u5230\u6700\u5927\uff0c\u4e5f\u5c31\u662f\u8bf4 \u8bad\u7ec3G\u7684\u76ee\u6807 \u5c31\u662f\u5e0c\u671b \\(D(G(z))\\) \u8fbe\u5230\u6700\u5927\uff0c\u4e5f\u5c31\u662f\u8bf4 \u4ece\u751f\u6210\u5668\u751f\u6210\u51fa\u6765\u7684\u6837\u672c \u9001\u5165\u5230\u5224\u522b\u5668\u4e2d \u6211\u4eec\u5e0c\u671b\u5224\u522b\u5668\u8fd9\u65f6\u5019 \u7ed9\u51fa\u7684\u5206\u7c7b\u7ed3\u679c \u5206\u7c7b\u6210 \u771f\u5b9e\u7684\u6837\u672c\uff1b\u5f53\u8fbe\u5230\u8fd9\u6837\u7684\u7ed3\u679c\u65f6\uff0c\u8bf4\u660eG\u8bad\u7ec3\u7684\u7ed3\u679c\u662f\u5f88\u597d\u7684</p> <p>\\(\\mathbb{E}_{{x \\sim p_{data}(x)}}\\)  x\u670d\u4ecep data(x)\uff0c\u4e5f\u5c31\u662f\u8bad\u7ec3\u96c6\uff0cx\u4e5f\u5c31\u662f\u8bad\u7ec3\u96c6\u4e2d\u7684\u6837\u672c</p> <p>\\(\\mathbb{E}_{{x \\sim p_{data}(x)}}[logD(x)]\\) \u8bad\u7ec3\u96c6\u4e2d\u7684\u6837\u672c\u9001\u5165\u5230\u5224\u522b\u5668\u4e2d\uff0c\u4e5f\u5c31\u662f\u6982\u7387\u53d6\u4e00\u4e2a\\(log\\)\uff0c\u4e5f\u5c31\u662f\u5bf9\u6570\u4f3c\u7136</p> <p>\\(\\mathbb{E}_{{z \\sim p_{z}(z)}}\\) \u7b2c\u4e8c\u90e8\u5206\\(z\\)\u670d\u4ece \\(p_z(z)\\)\uff0c\\(z\\)\u5c31\u662f\u521d\u59cb\u7684\u968f\u673a\u7684\u566a\u58f0\uff0c\\(z\\)\u9001\u5165\u5230\u751f\u6210\u5668\u4e4b\u4e2d \u5f97\u5230\\(G(z)\\)</p> <p>\\(\\mathbb{E}_{{x \\sim p_{z}(z)}}[log(1-D(G(z)))]\\) \\(G(z)\\)\u5c31\u662f\u751f\u6210\u5668\u7684\u8f93\u51fa\uff0c\\(G(z)\\)\u8f93\u5165\u5230\\(D\\)\u4e4b\u4e2d\u5f97\u5230\u4e00\u4e2a\u6982\u7387\uff0c\u7136\u540e\u628a1-\u6982\u7387\uff0c\\(1-DG\\)\uff0c\u5c31\u662f\u8868\u793a \u5224\u522b\u5668\u628a \\(G(z)\\) \u5206\u7c7b\u6210 \u5047\u6837\u672c\u7684\u6982\u7387</p> <p></p> <p>\uff081\uff09\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u57fa\u4e8emini batch\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u7b97\u6cd5\uff1b\u8d85\u53c2\u6570k\uff1b</p> <p>\uff082\uff09k\uff1f\u4e00\u822c\u6587\u7ae0\u800c\u8a00\uff0c\u53ef\u4ee5\u5148\u8bad\u7ec3k\u6b65\u7684\u5224\u522b\u5668\uff0c\u7136\u540e\u518d\u8bad\u7ec3\u4e00\u6b65\u751f\u6210\u5668\uff0c\u8fd9\u91cc\u53d6k=1\uff0c\u8868\u793a\u6bcf\u8bad\u7ec3\u4e00\u6b65\u5224\u522b\u5668\uff0c\u5c31\u8bad\u7ec3\u4e00\u6b65\u751f\u6210\u5668\uff0c\u7136\u540e\u518d\u8bad\u7ec3\u4e00\u6b65\u5224\u522b\u5668\uff0c\u518d\u8bad\u7ec3\u4e00\u6b65\u751f\u6210\u5668\uff0c\u4ea4\u66ff\u8fdb\u884c\uff0ck\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u4e00\u822c\u6765\u8bf4k=1\uff1b\u4e5f\u6709\u4e00\u4e9b\u4efb\u52a1\uff0c\u5148\u8bad\u7ec3\u751f\u6210\u5668\uff0c\u8bad\u7ec3\u4e00\u6bb5\u65f6\u95f4\uff0c\u518d\u5f00\u59cb\u4ea4\u66ff\u8bad\u7ec3\u751f\u6210\u5668\u548c\u5224\u522b\u5668</p> <p>\uff083\uff09\u770b\u7b97\u6cd5\u7684\u8fed\u4ee3\u6d41\u7a0b\uff0c\u4e24\u4e2afor\u8bad\u7ec3\uff1b\u7b2c\u4e00\u4e2afor\u5faa\u73af\uff0c\u5faa\u73afepoch\uff0c\u7b2c\u4e8c\u4e2afor\u5faa\u73af\uff0c\u5faa\u73afdataset \u6216\u8005 dataloader \u8fdb\u884c\u904d\u5386</p> <p></p> <p>\uff084\uff09\u9996\u5148\u7b2c\u4e00\u6b65\uff0c\u7b2c\u4e00\u4e2afor\u5faa\u73af\uff0c\u5148\u8bad\u7ec3k\u6b65\u7684\u5224\u522b\u5668\uff0c k\u53ef\u4ee5\u53d61\uff0c\u5728\u8bad\u7ec3\u5224\u522b\u5668\u65f6\uff0c\u9996\u5148\u4ece\u566a\u58f0\u5206\u5e03\u4e2d\uff0c\u968f\u673a\u91c7\u6837m\u4e2a\u6837\u672c\uff0c\u6784\u6210\u4e00\u4e2aminibatch\uff0cm\u4e2a\u6837\u672c\u5206\u522b\u6784\u6210z1\u5230zm\uff0c\u4ece\u5148\u9a8c\u5206\u5e03 \\(p_g(z)\\) \u91c7\u6837\u800c\u6765\uff0c\\(p_g(z)\\)\u53ef\u4ee5\u662f\u4e00\u4e2a\u6b63\u6001\u5206\u5e03\uff1b\u8fd9\u5c31\u662f\u7b2c\u4e00\u6b65\u5148\u91c7\u6837\u8f93\u5165</p> <p></p> <p>\uff085\uff09\u7b2c\u4e8c\u6b65\uff0c\u91c7\u6837\u771f\u5b9e\u7684\u6570\u636e\u5206\u5e03\uff0c\u91c7\u6837m\u4e2a\u6837\u672c\uff0c\u4ece\u8bad\u7ec3\u96c6\u4e2d\u91c7\u6837m\u4e2a\u6837\u672c</p> <p></p> <p>\uff086\uff09\u57fa\u4e8e\u68af\u5ea6\u4e0a\u5347 ascending \u516c\u5f0f\uff0c\u66f4\u65b0\u5224\u522b\u5668 \uff1b\u76ee\u6807\u51fd\u6570\u662f \\(logD+log(1-DG)\\)</p> <p>\u5bf9\u4e8e\u5224\u522b\u5668\u800c\u8a00\uff0c\u5224\u522b\u7684\u6807\u91cf\u6709\u4e24\u4e2a\uff0c\u7b2c\u4e00\u4e2a\u4ee5x\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ece\u8bad\u7ec3\u96c6\u4e2d\u62ff\u51fa\u7684\u6837\u672c\uff0c\u9001\u5230\u5224\u522b\u5668\u4e2d\uff0c\u5f97\u5230\u4e00\u4e2a\u6982\u7387\u503c\uff0c\u8fd9\u65f6\u7684\u6982\u7387\u503c\u53eb\u505a \\(D(x)\\)\uff0c\u7b2c\u4e8c\u4e2a\u4ece\u751f\u6210\u5668\u4e2d\uff0c\u62ff\u5230\u8f93\u51fa\uff0c\u53eb\u505a\\(G(z)\\)\uff0c\\(G(z)\\)\u9001\u5165\u5230\\(D\\)\u4e2d\uff0c\u5f97\u5230\u53e6\u5916\u4e00\u4e2a\u5224\u522b\u7684\u6982\u7387\uff0c\\(1-\u5224\u522b\u6982\u7387\\)\uff0c\u8fdb\u884c\u4e00\u4e2a \\(log\\) \u8fd0\u7b97\uff0c\u5f97\u5230\u7b2c\u4e8c\u9879\uff0c\u5916\u9762\u7684\u6c42\u548c\u662f\u5bf9\u6574\u4e2aminibatch\uff0c\u6bcf\u4e2a\u6837\u672c\u90fd\u8fd9\u6837\u505a\uff0c\u518d\u9664\u4ee5\\(m\\)\uff0c\u5728\u6837\u672c\u7ef4\u5ea6\u53d6\u4e00\u4e2a\u5e73\u5747\u503c\uff0c\u4ee5\u8fd9\u4e2a\u4f5c\u4e3a \\(target\\)\uff0c\u6b64\u65f6\u7684 \\(\\nabla\\)  \u662f\u4f5c\u7528\u5230 \\(\\theta_D\\) \u7684</p> <p>\u4e5f\u5c31\u662f\u8bf4 \u5bf9\u4e8e\u8fd9\u4e2a\u76ee\u6807\u51fd\u6570 \u53ea\u4f1a\u5bf9 \u5224\u522b\u5668\u7684\u53c2\u6570 \u6c42\u68af\u5ea6\uff0c\u6c42\u5b8c\u68af\u5ea6\u4ee5\u540e\uff0c\u518d\u7528\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\uff0c\u6765\u66f4\u65b0\u5224\u522b\u5668\u7684\u53c2\u6570\uff0c\u4ee5\u4e0a\u662f\u7b2c\u4e00\u6b65\u66f4\u65b0\u5224\u522b\u5668</p> <p>\u5982\u679c \\(k&gt;1\\)\uff0c\u9700\u8981\u4e0d\u65ad\u7684\u5faa\u73af\uff0c\u8fde\u7eed\u7684\u66f4\u65b0\u5224\u522b\u5668\uff0c\u66f4\u65b0k\u6b65\uff0c\u66f4\u65b0\u5b8c\u5224\u522b\u5668\u4ee5\u540e\uff0c\u66f4\u65b0\u751f\u6210\u5668\u90e8\u5206\uff1a</p> <p></p> <p>\uff081\uff09\u751f\u6210\u5668\u90e8\u5206\uff0c\u7b2c\u4e00\u6b65\uff0c\u540c\u6837\u53d6m\u4e2a\u566a\u58f0\u6837\u672c\uff0c\u6784\u6210\u4e00\u4e2aminibatch\uff0c\u540c\u6837\u7684\u4ece\u4e00\u4e2a\u5148\u9a8c\u5206\u5e03\u4e2d\u53d6\uff0c\u5f3a\u8c03\u4e00\u70b9\uff1a</p> <p></p> <p>\uff082\uff09\u7b2c\u4e8c\u6b65\uff0c\u91c7\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5(descending)\uff0c\u66f4\u65b0\u751f\u6210\u5668\uff0c\u6b64\u65f6\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4e5f\u5c31\u662f\u6700\u5c0f\u5316\u51fd\u6570\u662f \\(log(1-DG)\\) \uff0c\u53ea\u9700\u8981\u628a\u751f\u6210\u5668\u751f\u6210\u7684 \\(G\\)\uff0c\u9001\u5165\u5230\u5224\u522b\u5668\u4e4b\u4e2d\uff0c\u5f97\u5230\\(D\\)\uff0c\\(DG\\)\u5c31\u662f\u5224\u522b\u5668\u9884\u6d4b\u7684\u6982\u7387\uff0c\\(1-\u6982\u7387\\)\u53d6log\uff0c\u5f97\u5230 \\(log(1-DG)\\)\uff0c\u7136\u540e\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u635f\u5931\u6c42\u548c\uff0c\u518d\u9664\u4ee5m\uff0c\u53d6\u5e73\u5747\uff0c\u8fd9\u662f\u68af\u5ea6\u53ea\u5bf9\u751f\u6210\u5668\u7684\u53c2\u6570\u8ba1\u7b97\uff0c\u5728\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u65b0 \u751f\u6210\u5668\u7684\u53c2\u6570\uff0c\u8fd9\u65f6\u4e0d\u9700\u8981\u66f4\u65b0\u5224\u522b\u5668\u7684\u53c2\u6570</p> <p>\u4e0a\u9762\u7684\u5f0f\u5b50\uff0c\u53ea\u66f4\u65b0\u5224\u522b\u5668\u7684\u53c2\u6570\uff0c\u4e0d\u9700\u8981\u66f4\u65b0\u751f\u6210\u5668\u7684\u53c2\u6570</p> <p>\u4e0b\u9762\u7684\u5f0f\u5b50\uff0c\u53ea\u66f4\u65b0\u751f\u6210\u5668\u7684\u53c2\u6570\uff0c\u4e0d\u9700\u8981\u66f4\u65b0\u5224\u522b\u5668\u7684\u53c2\u6570</p> <p>\u4ee5\u4e0a\u662fGNA\u7684\u4e00\u4e2astep\uff0c\u4e0d\u65ad\u7684\u91cd\u590d\uff0c\u4ea4\u66ff\u66f4\u65b0\uff0c\u76f4\u5230\\(D(x)\\)\u548c\\(D(G)\\)\u90fd\u662f0.5\uff0c\u6b64\u65f6\u5224\u522b\u5668\u5df2\u7ecf\u65e0\u6cd5\u518d\u533a\u5206 \u6570\u636e\u662f\u6765\u81ea\u771f\u5b9e\u6570\u636e \u8fd8\u662f\u751f\u6210\u5668\u751f\u6210\u7684\u5047\u6837\u672c</p> <p>\u539f\u65874.1\u8bc1\u660e\uff0c\u8fd9\u6837\u7684\u635f\u5931\u51fd\u6570\u662f\u5426\u80fd\u8ba9 G\u548cD \u540c\u65f6\u627e\u5230\u6700\u4f18\u7684\u503c \u6216\u8005 \u6700\u4f18\u7684\u7ed3\u6784\uff1f</p>"},{"location":"learning/4_GAN/#41","title":"4.1 \u8bc1\u660e","text":"<p>\u8003\u8651\u4efb\u610f\u7ed9\u5b9a\u7684\u751f\u6210\u5668G\uff0c\u6700\u4f18\u7684\u5224\u522b\u5668D\u662f\u600e\u4e48\u6837\u7684</p> <p>\u5b9a\u74061\uff0cG\u56fa\u5b9a\uff0c\u6700\u4f18\u7684\u5224\u522b\u5668D\uff1a</p> <p></p> <p>*\u53f7  \u8868\u793a\u6700\u4f18</p> <p>D_G\uff0c\u8868\u793a G \u662f\u56fa\u5b9a\u7684\uff0cD\u662f\u53d8\u5316\u7684</p> <p>x\u670d\u4ece\u771f\u5b9edata\u7684\u6982\u7387\u3001x\u670d\u4ece\u751f\u6210\u5668g\u7684\u6982\u7387</p> <p></p> <p>\u8bc1\u660e\uff0c\u5bf9\u4e8e\u4efb\u610f\u751f\u6210G\uff0c\u5224\u522b\u5668D\u7684\u8bad\u7ec3\u6807\u51c6\uff0c\u5c31\u662f\u8981\u6700\u5927\u5316\u4ef7\u503c\u51fd\u6570V\uff0c\u4ef7\u503c\u51fd\u6570\u4e4b\u524d\u7528\u671f\u671b\u503c\u8868\u793a\u7684\uff0c\u671f\u671b\u53ef\u4ee5\u5199\u6210\u79ef\u5206\u7684\u5f62\u5f0f\uff0c\u7b2c\u4e00\u4e2a\u79ef\u5206 \\(p_{data}(x)D(x)\\)\uff0c\u5bf9\\(x\\)\u8fdb\u884c\u79ef\u5206\uff0c\u7b2c\u4e8c\u4e2a\u79ef\u5206\\(p_zlog(1-Dg)\\)\uff0c\u5bf9\\(z\\)\u8fdb\u884c\u79ef\u5206\uff0c</p> <p></p> <p>\u5c06\u7b2c\u4e8c\u4e2a\u79ef\u5206\uff0c\u6362\u4e00\u4e0b\u79ef\u5206\u7b26\u53f7  z \u2192 x\uff0c\u7136\u540e\u5408\u5e76</p> <p>\uff08\u6709\u4e2a\u95ee\u9898\uff0c\u4e3a\u4ec0\u4e48 \\(\\int_z p_z(z) (1-D(g(z)))dz\\)    \u2192 $\\int_x p_x(x) (1-D(g(x)))dx $   \u8fd9\u91cc \\(p_x(x)\\)\u53d8\u6210 \\(p_g(x)\\)\uff09</p> <p>\u5f53\u5199\u6210\u8fd9\u6837\u4ee5\u540e\uff1a</p> <p></p> <p>\u5bf9\u4e8e\u8fd9\u6837\u4e00\u4e2a\u51fd\u6570\uff0c\u5bf9\u4e8e\u4efb\u610fa\uff0cb\uff0c\u5c5e\u4e8e\u5b9e\u6570\uff0c\u5e76\u4e14 a,b \u4e0d\u7b49\u4e8e0\uff0c\u6b64\u65f6\u5173\u4e8e y \u7684\u51fd\u6570\uff0c\\(alogy+blog(1-y)\\) \u8fd9\u91cc \\(a=p_{data}(x)\\) \u3001 \\(b=p_g(x)\\)  \u5728  \\(\\frac{a}{a+b}\\) \u8fbe\u5230\u6700\u5927\u503c\uff0c\u4e5f\u5c31\u662f \\(\\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}\\) \u8fbe\u5230\u6700\u5927\u503c</p> <p></p> <p>\u6ce8\u610f \u8bad\u7ec3 D\u7684\u76ee\u6807 \u53ef\u4ee5\u89e3\u91ca\u6210 \u5728\u6700\u5927\u5316 P(Y=y|x)\u7684\u5bf9\u6570\u4f3c\u7136</p> <ul> <li>\\(x\\)\u5c31\u662f\u7ed9\u5b9a\u4e00\u4e2a\u8f93\u5165\uff0c\u8f93\u5165\u5230\u5224\u522b\u5668\u4e2d\u7684\u6837\u672c</li> <li>\\(P(Y=y)\\) \\(Y\\)\u8868\u793a\u4e00\u4e2a\u6982\u7387\uff0c\u8868\u793a\\(x\\) \u6765\u81ea\u751f\u6210\u5668\u7684\u6982\u7387\uff0c\u8fd8\u662f\\(data\\)\uff08\u771f\u5b9e\u6570\u636e\uff09\u7684\u6982\u7387</li> </ul> <p>\u516c\u5f0f1 \u53ef\u4ee5\u91cd\u65b0\u5199\u6210 \\(C(G)\\)\uff0c</p> <p>\\(C(G) = \\max_{D}V(G,D)\\)</p> <p>\u5c31\u662f\u628a\u4ef7\u503c\u51fd\u6570 \u91cd\u65b0\u5199\u6210 \u751f\u6210\u5668\u7684\u51fd\u6570\uff0c\u4e5f\u5c31\u662f \\(V(G,D)\\)\u5728\u627e\u5230 \u6700\u4f18\u7684 D \u53d6\u6700\u5927\u503c</p> <p></p> <p>\u91cd\u70b9\u89e3\u91ca\u7b2c\u4e8c\u884c\uff0c\u628a \\(G(z)\\)\u6362\u6210\\(x\\)\uff0c\\(z \\sim p_z\\) \uff0c\u4e5f\u5c31\u662f \\(x \\sim p_g\\)</p> <p>\u4e5f\u5c31\u662f \\(p_z\\) \u53ef\u4ee5\u7528 \\(x\\) \u7684\u5206\u5e03\u8868\u793a</p> <p>\u7136\u540e\u6211\u4eec\u628a\u627e\u5230\u7684 \\(D^*_G(x)\\) \u4ee3\u5165\uff0c\u5f97\u5230\u7b2c\u4e09\u884c\u7684\u4ef7\u503c\u51fd\u6570</p> <p></p> <ul> <li> \u4e0a\u9762\u6211\u770b\u660e\u767d\u4e86</li> </ul> <p>\u7ee7\u7eed\u770b\uff0c\u540e\u9762\u8fd8\u6709\u8bc1\u660e</p> <p></p> <p>\u5f00\u59cb\uff1a</p> <p></p> <p>\u8bad\u7ec3\u6807\u51c6\\(C\\) \u7684\u5168\u5c40\u6700\u5c0f\u503c\uff0c\u4ec5\u4ec5\u5728 \\(p_g = p_{data}\\) \u65f6\uff0c\u8fbe\u5230\uff0c\u5e76\u4e14\u6700\u5c0f\u503c \u662f \\(-log4\\)</p> <p>\u516c\u5f0f4 \u5df2\u7ecf\u5f97\u5230 \\(C(G)\\) \u7684\u8868\u8fbe\u5f0f</p> <p></p> <p>\u63a5\u4e0b\u6765 \u6211\u4eec\u5c31\u7814\u7a76 C(G)\u7684\u6700\u5c0f\u503c\uff0c\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u53d6\u5230\uff0c\u5e76\u4e14\u6700\u5c0f\u503c\u662f\u4ec0\u4e48\u3002\u770b\u8bc1\u660e\uff1a</p> <p></p> <p>\uff081\uff09\u5047\u8bbe \\(p_g=p_{data}\\)\u65f6\uff0c\u4e5f\u5c31\u662f\u751f\u6210\u5206\u5e03\u548c\u771f\u5b9e\u5206\u5e03\u4e00\u6a21\u4e00\u6837\u65f6\uff0c\u6b64\u65f6\u6700\u4f18\u7684 D\u521a\u597d\u7b49\u4e8e \\(\\frac{1}{2}\\)</p> <p>\u770b\u539f\u6587\u7684\u516c\u5f0f(2)</p> <p></p> <p>\uff082\uff09\u56e0\u6b64\uff0c\u6211\u4eec\u628a\u516c\u5f0f(4)\u6700\u540e\u5f97\u5230\u7684\u5f0f\u5b50\uff0c\u5168\u90e8\u6362\u6210 \\(\\frac{1}{2}\\)</p> <p>\u6b64\u65f6\uff1a</p> <p></p> <p></p> <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u5f53\u6211\u4eec\u5047\u8bbe \\(p_g = p_{data}\\) \u65f6\uff0c\u5f97\u5230 \\(C(G)=-log4\\)\uff0c</p> <p>\u5f97\u5230\u8fd9\u4e9b\u4e1c\u897f\u4ee5\u540e\uff0c\u540e\u9762\u600e\u4e48\u505a\u5462\uff1f</p> <p>\u6211\u4eec\u628a\u516c\u5f0f(4)\uff0c\u51cf\u53bb\u4e00\u4e2a \\(-log4\\)\uff1a (ps\uff1a\u6700\u540e\u53c8\u628a \\(-log4\\) \u52a0\u56de\u6765\u4e86)</p> <p>\\(\\mathbb{E}_{\\boldsymbol{x}\\sim p_\\mathrm{data}}\\left[\\log\\frac{p_\\mathrm{data}(\\boldsymbol{x})}{P_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x}\\sim p_g}\\left[\\log\\frac{p_g(\\boldsymbol{x})}{p_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right] -\uff08-log4\uff09\\)</p> <p></p> <p>\u5f97\u5230\uff1a</p> <p>\\(\\mathbb{E}_{\\boldsymbol{x}\\sim p_\\mathrm{data}}\\left[\\log\\frac{p_\\mathrm{data}(\\boldsymbol{x})}{P_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x}\\sim p_g}\\left[\\log\\frac{p_g(\\boldsymbol{x})}{p_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right] + log4\\)</p> <p>= \\(\\mathbb{E}_{\\boldsymbol{x}\\sim p_\\mathrm{data}}\\left[\\log\\frac{p_\\mathrm{data}(\\boldsymbol{x})}{P_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x}\\sim p_g}\\left[\\log\\frac{p_g(\\boldsymbol{x})}{p_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right] + log2 + log2\\)</p> <p>=  \\(\\mathbb{E}_{\\boldsymbol{x}\\sim p_\\mathrm{data}}\\left[\\log\\frac{2 p_\\mathrm{data}(\\boldsymbol{x})}{P_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x}\\sim p_g}\\left[\\log\\frac{2 p_g(\\boldsymbol{x})}{p_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}\\right]\\)</p> <p>= \\(\\mathbb{E}_{\\boldsymbol{x}\\sim p_\\mathrm{data}}\\left[\\log\\frac{ p_\\mathrm{data}(\\boldsymbol{x})}{\\frac{P_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}{2}}\\right]+\\mathbb{E}_{\\boldsymbol{x}\\sim p_g}\\left[\\log\\frac{p_g(\\boldsymbol{x})}{\\frac{p_\\mathrm{data}(\\boldsymbol{x})+p_g(\\boldsymbol{x})}{2}}\\right]\\)</p> <p>\u501f\u52a9 KL\u6563\u5ea6 \u516c\u5f0f\uff1a</p> <p></p> <p>\u5206\u5e03P\u548c\u5206\u5e03Q\u7684KL\u6563\u5ea6\u516c\u5f0f \u5c31\u662f \\(log(\\frac{P(x)}{Q(x)})\\) \u5173\u4e8eP(x) \u7684\u671f\u671b</p> <p>\\(= KL(p_{data}(x) || \\frac{p_{data}(x)+p_g(x)}{2}) +  KL(p_{g}(x) || \\frac{p_{data}(x)+p_g(x)}{2})\\)</p> <p>\u5173\u4e8e\u516c\u5f0f\uff085\uff09\u7684\u5f97\u5230\uff0c\u662f\u4e3a\u4e86C(G)\uff0c\u6211\u4eec\u6700\u5f00\u59cb\\(-(-log4)\uff0c\\)\u4e3a\u4e86\u4fdd\u8bc1\u516c\u5f0f\u603b\u4f53\u4e0d\u53d8\uff0c\u6240\u4ee5\u6700\u540e\u5728 \\(+(-log4)\\) \uff1a</p> <p>\u5f97\u5230\u5b8c\u6574\u7684  \u516c\u5f0f(5)</p> <p></p> <ul> <li> \u4ee5\u4e0a \u516c\u5f0f(5)\u7684\u8bc1\u660e\uff0c\u770b\u61c2\u4e86</li> </ul> <p>\u89e3\u8bfb\u516c\u5f0f5\uff0c\u9996\u5148 \\(KL\u6563\u6b65\u5927\u4e8e0\\)\uff0c\u6052\u6210\u7acb\uff0c\u6240\u4ee5\u6211\u4eec\u770b\u51fa\u5316\u7b80\u51fa\u6765\u7684\u5f0f\u5b50\uff0c\u6700\u5c0f\u503c\u662f \\(-log4\\)</p> <p>\u6700\u5c0f\u503c\u4f55\u65f6\u53d6\u5230\uff0c\u5c31\u662f\\(KL\u6563\u5ea6=0\\) \u4e5f\u5c31\u662f</p> <p>\u2460 \\(p_{data}(x) = \\frac{p_{data}(x)+p_g(x)}{2}\\)</p> <p>\u2461  \\(p_{g}(x) = \\frac{p_{data}(x)+p_g(x)}{2}\\)</p> <p>\u540c\u65f6\u6210\u7acb\uff0c\u4e0d\u5c31\u662f \\(p_{data}=p_{g}\\)</p> <p>\u6211\u4eec \u7528\u8a79\u68ee-\u9999\u519c\u6563\u5ea6\uff0c\u7ee7\u7eed\u5316\u7b80\uff0c\u5f97\u5230\u516c\u5f0f6</p> <p></p> <p>JSD(P||Q)=\\(\u4e8c\u5206\u4e4b\u4e00\u7684\u5206\u5e03M\u548c\u5206\u5e03P\u7684KL\u6563\u5ea6+\\)\\(\u4e8c\u5206\u4e4b\u4e00\u7684\u5206\u5e03M\u4e0e\u5206\u5e03Q\u7684KL\u6563\u5ea6\\)</p> <p>\u5176\u4e2d\\(M=\u4e8c\u5206\u4e4b\u4e00\u7684\u5206\u5e03P\u52a0\u5206\u5e03Q\\)</p> <p>\u57fa\u4e8e \u8a79\u68ee\u9999\u519c\u6563\u5ea6\u7684\u516c\u5f0f\uff0c\u628a\u516c\u5f0f(5)\u5316\u7b80\u6210 \u516c\u5f0f(6)</p> <p></p> <p>\u540c\u7406\uff0c\\(JSD\\)\u4e5f\u662f\u4e00\u4e2a \u5927\u4e8e\u7b49\u4e8e0 \u7684\u4e00\u4e2a\u503c\uff0c\u5e76\u4e14\u4ec5\u5728 \\(p_{data}=p_g\\)\u65f6\u53d60</p> <ul> <li> \u4ee5\u4e0a \u516c\u5f0f(6)\u7684\u8bc1\u660e\uff0c\u61c2\u4e86</li> </ul> <p>\u4ee5\u4e0a\u8bf4\u660e\uff0c\\(p_{data}=p_g\\)\uff0c\u4e5f\u5c31\u662f\u751f\u6210\u5668\u751f\u6210\u7684\u5206\u5e03\uff0c\u521a\u597d\u7b49\u4e8e \u771f\u5b9e\u6570\u636e\u7684\u5206\u5e03\u65f6\uff0c\u6211\u4eec C(G) \u4f1a\u53d6\u6700\u5c0f\u503c \\(-log4\\)\uff0c\u4e5f\u5c31\u662f \\(G\\)\u7684\u4f18\u5316\u8fbe\u5230\u4e86\u6700\u4f18</p> <p>\u672c\u6587\u5b9e\u9a8c\u624b\u5199\u6570\u5b57\u8bc6\u522b\uff0c\u901a\u8fc7GAN\u7f51\u7edc\uff0c\u624b\u5199\u7167\u7247\u7684\u5206\u5e03\uff0c\u968f\u673a\u751f\u6210\u9ad8\u65af\u53d8\u91cf\u9001\u5165\u5230\u751f\u6210\u5668\u4e2d\uff0c\u751f\u6210\u5668\u751f\u6210\u4e00\u5f20\u624b\u5199\u6570\u5b57\u7684\u7167\u7247\uff0c\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5b66\u4e60\u65b9\u6cd5</p> <p></p>"},{"location":"learning/4_GAN/#_2","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<p>\u5168\u90e8\u4ee3\u7801\uff1a</p> <pre><code>\"\"\" \u57fa\u4e8eMNIST \u5b9e\u73b0\u5bf9\u6297\u751f\u6210\u7f51\u7edc (GAN) \"\"\"\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\n\nimage_size = [1, 28, 28]\nlatent_dim = 96\nbatch_size = 64\nuse_gpu = torch.cuda.is_available()\n\nclass Generator(nn.Module):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            torch.nn.BatchNorm1d(128),\n            torch.nn.GELU(),\n\n            nn.Linear(128, 256),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.GELU(),\n            nn.Linear(256, 512),\n            torch.nn.BatchNorm1d(512),\n            torch.nn.GELU(),\n            nn.Linear(512, 1024),\n            torch.nn.BatchNorm1d(1024),\n            torch.nn.GELU(),\n            nn.Linear(1024, np.prod(image_size, dtype=np.int32)),\n            #  nn.Tanh(),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, z):\n        # shape of z: [batchsize, latent_dim]\n\n        output = self.model(z)\n        image = output.reshape(z.shape[0], *image_size)\n\n        return image\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(np.prod(image_size, dtype=np.int32), 512),\n            torch.nn.GELU(),\n            nn.Linear(512, 256),\n            torch.nn.GELU(),\n            nn.Linear(256, 128),\n            torch.nn.GELU(),\n            nn.Linear(128, 64),\n            torch.nn.GELU(),\n            nn.Linear(64, 32),\n            torch.nn.GELU(),\n            nn.Linear(32, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, image):\n        # shape of image: [batchsize, 1, 28, 28]\n\n        prob = self.model(image.reshape(image.shape[0], -1))\n\n        return prob\n\n# Training\ndataset = torchvision.datasets.MNIST(\"mnist_data\", train=True, download=True,\n                                     transform=torchvision.transforms.Compose(\n                                         [\n                                             torchvision.transforms.Resize(28),\n                                             torchvision.transforms.ToTensor(),\n                                             #  torchvision.transforms.Normalize([0.5], [0.5]),\n                                         ]\n                                                                             )\n                                     )\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\ngenerator = Generator()\ndiscriminator = Discriminator()\n\n\ng_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.4, 0.8), weight_decay=0.0001)\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.4, 0.8), weight_decay=0.0001)\n\nloss_fn = nn.BCELoss()\nlabels_one = torch.ones(batch_size, 1)\nlabels_zero = torch.zeros(batch_size, 1)\n\nif use_gpu:\n    print(\"use gpu for training\")\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    loss_fn = loss_fn.cuda()\n    labels_one = labels_one.to(\"cuda\")\n    labels_zero = labels_zero.to(\"cuda\")\n\nnum_epoch = 200\nfor epoch in range(num_epoch):\n    for i, mini_batch in enumerate(dataloader):\n        gt_images, _ = mini_batch\n\n\n        z = torch.randn(batch_size, latent_dim)\n\n        if use_gpu:\n            gt_images = gt_images.to(\"cuda\")\n            z = z.to(\"cuda\")\n\n        pred_images = generator(z)\n        g_optimizer.zero_grad()\n\n        recons_loss = torch.abs(pred_images-gt_images).mean()\n\n        g_loss = recons_loss*0.05 + loss_fn(discriminator(pred_images), labels_one)\n\n        g_loss.backward()\n        g_optimizer.step()\n\n        d_optimizer.zero_grad()\n\n        real_loss = loss_fn(discriminator(gt_images), labels_one)\n        fake_loss = loss_fn(discriminator(pred_images.detach()), labels_zero)\n        d_loss = (real_loss + fake_loss)\n\n        # \u89c2\u5bdfreal_loss\u4e0efake_loss\uff0c\u540c\u65f6\u4e0b\u964d\u540c\u65f6\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u5e76\u4e14\u5dee\u4e0d\u591a\u5927\uff0c\u8bf4\u660eD\u5df2\u7ecf\u7a33\u5b9a\u4e86\n\n        d_loss.backward()\n        d_optimizer.step()\n\n        if i % 50 == 0:\n            print(f\"step:{len(dataloader)*epoch+i}, recons_loss:{recons_loss.item()}, g_loss:{g_loss.item()}, d_loss:{d_loss.item()}, real_loss:{real_loss.item()}, fake_loss:{fake_loss.item()}\")\n\n        if i % 400 == 0:\n            image = pred_images[:16].data\n            torchvision.utils.save_image(image, f\"image_{len(dataloader)*epoch+i}.png\", nrow=4)\n</code></pre> <p>\u4e0a\u9762\u7684\u4ee3\u7801\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c  \\(\\uparrow\\)</p> <p>\u4e0b\u9762\u662f\u8bb2\u89e3\uff0c\u6709\u4e9b\u8bb8\u51fa\u5165\uff0c\u4f46\u6574\u4f53\u601d\u60f3\u662f\u4e00\u81f4\u7684 \\(\\downarrow\\)</p> <p>\u67e5\u770b\u8f93\u51fa\u7ed3\u679c\uff0c\u53ef\u4ee5\u770b\u5230\u751f\u6210\u7684\u56fe\u7247\u9010\u6e10\u6e05\u6670\uff1a</p> <p></p> <p>\u9996\u5148\u4ee3\u7801\u7684\u5927\u6846\u67b6\uff0c\u9996\u5148\u751f\u6210\u751f\u6210\u5668\u7684\u7c7b\uff0c\u7136\u540e\u751f\u6210\u5224\u522b\u5668\u7684\u7c7b\uff0c\u7136\u540e\u8fdb\u884c\u8bad\u7ec3\uff1a</p> <pre><code>'''\u57fa\u4e8eMINIST\u5b9e\u73b0\u5bf9\u6297\u751f\u6210\u7f51\u7edc\uff08GAN\uff09'''\n\nimport torch\nimport torch.nn as nn\n\nclass Generator(nn.Module):\n    pass\nclass Decrimination(nn.Module):\n    pass\n# training\n</code></pre> <p>\u6211\u4eec\u4e3e\u4f8b\u5b50\uff0c\u4ee5\u751f\u6210\u624b\u5199\u6570\u5b57\u7167\u7247\u4e3a\u4f8b\uff1a</p> <p>\u9996\u5148 \u600e\u4e48\u5bfc\u5165 minist\u6570\u636e\u96c6\uff0c\u8c37\u6b4c\u641c\u7d22\uff1a torch vision mnist \u7b2c\u4e8c\u4e2a\uff0cMNIST</p> <p></p> <p>\u770b\u5230\u5b98\u65b9api\uff1a</p> <p></p> <p>\u9996\u5148\u9700\u8981\u7684\u53c2\u6570\uff1a</p> <ul> <li>root\uff1a\u6570\u636e\u5b58\u50a8\u8def\u5f84</li> <li>train\uff1a\u662f\u5426train\u6a21\u5f0f</li> <li>dowload\uff1a\u662f\u5426\u4e0b\u8f7d</li> <li>transform\uff1aPIL image\u683c\u5f0f\u7684\u8f6c\u6362\u6210 \u6d6e\u70b9\u578b\u7684</li> </ul> <p>\u901a\u8fc7\u8fd9\u6837\u7684class \u5f97\u5230MINST\u6570\u636e\u96c6</p> <p>\u9996\u5148\uff0c\u67e5\u770bminist\u6570\u636e\u96c6\u957f\u4ec0\u4e48\u6837\uff1a</p> <pre><code>import torchvision\ndataset = torchvision.datasets.MNIST(\"minist_data\",train=True,download=True)\nprint(len(dataset)) # 60000\n</code></pre> <p>\u628a\u4e0b\u8f7d\u7684minist\u6570\u636e\u96c6\u5b58\u5230 <code>minist_data</code> \u6587\u4ef6\u5939\u4e0b\uff0c\u91c7\u7528\u8bad\u7ec3\u6a21\u5f0f\uff0c\u672c\u5730\u6ca1\u6709\u6240\u4ee5\u8bbe\u7f6e\u4e3aTrue</p> <p>\u4e00\u5171\u67096000\u4e2a\u6837\u672c</p> <p>\u770b\u4e00\u4e0b\u6bcf\u4e2a\u6837\u672c\u957f\u4ec0\u4e48\u6837\uff08\u6253\u5370\u524d5\u4e2a\uff09\uff1a</p> <pre><code>import torchvision\ndataset = torchvision.datasets.MNIST(\"minist_data\",train=True,download=True)\nfor i in range(5):\n    print(dataset[i])\n# (&lt;PIL.Image.Image image mode=L size=28x28 at 0x1030AD100&gt;, 5)\n# (&lt;PIL.Image.Image image mode=L size=28x28 at 0x1030AD100&gt;, 0)\n# (&lt;PIL.Image.Image image mode=L size=28x28 at 0x1030AD100&gt;, 4)\n# (&lt;PIL.Image.Image image mode=L size=28x28 at 0x1030AD100&gt;, 1)\n# (&lt;PIL.Image.Image image mode=L size=28x28 at 0x1030AD100&gt;, 9)\n</code></pre> <p>\u524d5\u4e2a\u6bcf\u4e00\u4e2a\u90fd\u662fimage\u7684\u5bf9\u8c61\uff0c\u5927\u5c0f\u662f28\u00d728\u7684\uff0c\u524d\u9762\u662f \\(x\\) \u540e\u9762\u662f \u6807\u7b7e\uff0c\u6570\u636e\u683c\u5f0f\u662f PIL image</p> <p>\u5982\u679c\u6211\u4eec\u6253\u5370shape\u7684\u8bdd\uff0c\u4f1a\u62a5\u9519</p> <pre><code>import torchvision\ndataset = torchvision.datasets.MNIST(\"minist_data\",train=True,download=True)\nfor i in range(5):\n    print(dataset[i][0].shape)\n# AttributeError: 'Image' object has no attribute 'shape'\n</code></pre> <p>\u8fd9\u65f6\uff0c\u9700\u8981\u8c03\u7528transform  </p> <p></p> <p>\u53ef\u4ee5\u770b\u5230\u4f8b\u5b50\uff1a</p> <p></p> <p>\u4f7f\u7528transforms.Compose()\u4f20\u5165</p> <p>\u6211\u4eec\u53ea\u9700\u8981\u8c03\u6574\u4e00\u4e0b\u5927\u5c0f\uff0c\u4f20\u516528</p> <p></p> <p></p> <p>\u63a5\u7740\uff0c\u628aPIL image\u683c\u5f0f\u8f6c\u5316\u6210 \u6d6e\u70b9\u6570 \u683c\u5f0f</p> <p></p> <p>ToTensor API</p> <p></p> <p>\u5c06PIL image\u683c\u5f0f \u6216\u8005 numpy \u6570\u7ec4 \u8f6c\u6362\u6210tensor\u683c\u5f0f\uff0c\u5e76\u4e14\u53ef\u4ee5\u8f6c\u5316\u5230 0~1\u4e4b\u95f4\u7684\u6d6e\u70b9\u6570</p> <p>\u518d\u6b21\u6253\u5370 shape\uff1a</p> <pre><code>import torchvision\ndataset = torchvision.datasets.MNIST(\"minist_data\",\n                                     train=True,\n                                     download=True,\n                                     transform=torchvision.transforms.Compose(\n                                         [torchvision.transforms.Resize(28),\n                                         torchvision.transforms.ToTensor()]\n                                     ))\nfor i in range(5):\n    print(dataset[i][0].shape)\n# torch.Size([1, 28, 28])\n# torch.Size([1, 28, 28])\n# torch.Size([1, 28, 28])\n# torch.Size([1, 28, 28])\n# torch.Size([1, 28, 28])\n</code></pre> <p>\u6bcf\u4e2a\u6837\u672c \u90fd\u662f 1\u00d728\u00d728\uff1b1\u8868\u793a\u901a\u9053\u6570\uff0cminist\u901a\u9053\u6570=1\uff1b\u4ee5\u4e0a\u770b\u5230\u4e86\u6837\u672c\u957f\u4ec0\u4e48\u6837\uff0c\u53ef\u4ee5\u5e2e\u52a9Generator\u751f\u6210\u7167\u7247</p> <p>\u63a5\u4e0b\u6765\uff0c\u7ee7\u7eed\u770bGenerator\u51fd\u6570\uff0c\u5148\u5199init\uff0c\u518d\u5199forward\uff0cinit\u5b9a\u4e49\u6a21\u5757\uff0cforward\u5c06init\u7684\u6a21\u5757\u4e32\u8054\u8d77\u6765\uff0c\u751f\u6210\u7167\u7247</p> <pre><code>class Generator(nn.Module):\n    def __init__(self):\n        pass\n    def forward(self):\n        pass\n</code></pre> <p>\u5b9e\u4f8b\u5316\u7236\u7c7b\uff0c\u7ee7\u627f\u81eann.Module\uff0c\u7ee7\u627f\u7236\u7c7b\u7684\u6784\u9020\u65b9\u6cd5</p> <pre><code>super(Generator,self).__init__()\n</code></pre> <p>forward\u63a5\u6536z\uff0c\u9ad8\u65af\u968f\u673a\u53d8\u91cf\uff0cz\u7684shape=batchsize\u00d71\u00d728\u00d728,1\u662fchannel\u3001H\u3001W=28\u300128</p> <pre><code>    def forward(self,z):\n        # shape of z:batchsize,1,28,28\n</code></pre> <p>init\u51fd\u6570\uff0c\u4f7f\u7528nn.Sequential()\uff0c\u5b9a\u4e49\u7f51\u7edc\u5c42\uff0c\u5b9a\u4e49\u7b2c\u4e00\u4e2a\u7ebf\u6027\u5c42nn.Linear(in_dim,64)\uff0c64\u662f\u968f\u4fbf\u5b9a\u4e49\u7684\uff0cin_dim\u662f\u7ed9\u7684\uff0c\u7ebf\u6027\u5c42\u540e\u9762\u63a5\u7740\u6fc0\u6d3b\u5c42nn.ReLU()\uff0c\u67e5\u770b\u5b98\u7f51api\uff0c\u9ed8\u8ba4inplace=False\uff0c\u6211\u4eec\u6539\u6210True\uff0c\u7136\u540e\u91cd\u590d\u51e0\u904d\uff0c\u7ebf\u6027\u5c42\u3001\u6fc0\u6d3b\u5c42...</p> <p></p> <pre><code>image_size = [1,28,28]\n\nclass Generator(nn.Module):\n    def __init__(self,in_dim):\n        super(Generator,self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(in_dim,64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64,128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128,256),\n            nn.ReLU(inplace=True),   \n            nn.Linear(256,512),\n            nn.ReLU(inplace=True),   \n            nn.Linear(512,1024),\n            nn.ReLU(inplace=True), \n            nn.Linear(1024,torch.prod(image_size,dtype=torch.int32)),\n            nn.Tanh(),                                                        \n        )\n</code></pre> <p>\u5148\u4e0d\u65ad\u7684\u5347\u7ef4\u5347\u7ef4\u5347\u7ef4\uff0c\u6700\u540e\u964d\u7ef4 1024\u2192 torch.prod(image_size)\uff0c\u6700\u540e\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570Tanh\uff0c\u5c06\u5143\u7d20\u53d8\u6210-1\u52301</p> <p></p> <p>torch.prod \u8fd4\u56de\u8f93\u5165\u5f20\u91cf \u5143\u7d20\u7684\u8fde\u4e58\u79ef</p> <p>\u7136\u540e\u770bforward\u51fd\u6570\uff0c\u5c06z\u4f20\u5165\u5230init\u5b9a\u4e49\u7684self.model()</p> <pre><code>    def forward(self,z):\n        # shape of z:batchsize,1*28*28\n        output = self.model(z)\n</code></pre> <p>\u5c06\u8f93\u51fa\u7684output\u8f6c\u6362\u4e3a\u56fe\u7247\u683c\u5f0f\uff0c\u8c03\u7528reshape\uff0c\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u4ecd\u7136\u7528\uff0cz.shpe[0]\uff0c\u540e\u9762\u7684\u7ef4\u5ea6\u5c31\u7528image_size\uff0cimage_size\u52a0\u661f\u53f7\uff0c\u5c31\u80fd\u628a\u5217\u8868\u4f20\u8fdb\u53bb</p> <pre><code>   def forward(self,z):\n        # shape of z:batchsize,1*28*28\n        output = self.model(z)\n        image = output.reshape(z.shape[0],*image_size)\n        pass\n</code></pre> <p>\u5c06image\u8fd4\u56de\uff0c\u4ee5\u4e0a\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u751f\u6210\u5668\u3002</p> <pre><code>class Generator(nn.Module):\n    def __init__(self,in_dim):\n        super(Generator,self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(in_dim,64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64,128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128,256),\n            nn.ReLU(inplace=True),   \n            nn.Linear(256,512),\n            nn.ReLU(inplace=True),   \n            nn.Linear(512,1024),\n            nn.ReLU(inplace=True), \n            nn.Linear(1024,torch.prod(image_size,dtype=torch.int32)),\n            nn.Tanh(),                                                        \n        )\n\n    def forward(self,z):\n        # shape of z:batchsize,1*28*28\n        output = self.model(z)\n        image = output.reshape(z.shape[0],*image_size)\n        return image\n</code></pre> <p>\u63a5\u4e0b\u6765\u5b9e\u73b0\u5224\u522b\u5668</p> <p>\u5224\u522b\u5668\u63a5\u6536\u7684\u662f\u4e00\u5f20\u56fe\u7247\u4f5c\u4e3a\u8f93\u5165\uff0c\u751f\u6210\u5668\u63a5\u6536\u7684\u662f\u968f\u673a\u566a\u58f0\u4f5c\u4e3a\u8f93\u5165</p> <pre><code>class Decrimination(nn.Module):\n    def __init__(self):\n        super(Decrimination,self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(in_dim,1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024,512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512,256),\n            nn.ReLU(inplace=True),   \n            nn.Linear(256,128),\n            nn.ReLU(inplace=True),   \n            nn.Linear(128,1),\n            nn.Sigmoid(),            \n        )\n        pass\n    def forward(self):\n        pass\n</code></pre> <p>\u5224\u522b\u5668\u7684init\u540c\u6837\u5199\u4e00\u4e2amodel\uff0c\u5224\u522b\u5668\u7684nn.Linear\u7684\u7ef4\u5ea6\u53ef\u4ee5\u53cd\u8fc7\u6765\u5199\uff0c\u4e0a\u9762\u662f\u4e00\u6b65\u6b65\u7684\u589e\u5927\u7ef4\u5ea6\uff0c\u5bf9\u4e8e\u5224\u522b\u5668\u800c\u8a00\uff0c\u4e00\u5f00\u59cb\u53ef\u4ee5\u662f\u5927\u7ef4\u5ea6\uff0c\u7136\u540e\u6162\u6162\u7684\u964d\u7ef4\u5ea6\uff0c\u6700\u540e\u8f93\u51fa\u9884\u6d4b\u7684\u6807\u91cf\uff0c\u6700\u540e\u5c06128\u6620\u5c04\u52301\uff0c\u6700\u540e\u8f93\u51fa\u6982\u7387\uff0c\u5c31\u4e0d\u662fTanh\uff0c\u800c\u662fSigmoid</p> <p>\u5224\u522b\u5668\u63a5\u6536\u4e00\u5f20\u7167\u7247\u4f5c\u4e3a\u8f93\u5165\uff0c\u8f93\u51fa\u662fSigmoid\u51fd\u6570 \u8f93\u51fa\u7684\u6982\u7387\u503c</p> <p>forward\u51fd\u6570 \u63a5\u6536\u7684image\u683c\u5f0f batchsize\u00d71\u00d728\u00d728\uff0c\u5bf9image\u8fdb\u884creshape\uff0creshape\u7684\u7b2c\u4e00\u7ef4\u662fimage.shape[0]\uff0cCHW\u7edf\u4e00\u653e\u5230\u6700\u540e\u4e00\u7ef4</p> <pre><code>prob = self.model(image.reshape(image.shape[0],-1))\n</code></pre> <p>\u4ee5\u4e0a\u5f97\u5230\u4e86\u6982\u7387prob\uff0c\u6700\u540e\u8fd4\u56de\u5373\u53ef\uff0c\u4ee5\u4e0a\u5b9e\u73b0\u4e86\u5224\u522b\u5668\uff08in_dim \u90a3\u91cc\u6709\u9519\u8bef\uff09</p> <pre><code>class Decrimination(nn.Module):\n    def __init__(self):\n        super(Decrimination,self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(in_dim,1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024,512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512,256),\n            nn.ReLU(inplace=True),   \n            nn.Linear(256,128),\n            nn.ReLU(inplace=True),   \n            nn.Linear(128,1),\n            nn.Sigmoid(),            \n        )    \n    def forward(self,image):\n        # shape of image:[batchsize,1,28,28]\n        prob = self.model(image.reshape(image.shape[0],-1))\n        return prob\n</code></pre> <p>\u5224\u522b\u5668\u4ee5\u56fe\u7247\u4f5c\u4e3a\u8f93\u5165\uff0c\u8f93\u51fa\u6982\u7387</p> <p>\u63a5\u4e0b\u6765\u8fdb\u884c\u8bad\u7ec3\u90e8\u5206\uff0c\u7b2c\u4e00\u90e8\u5206\uff0c\u6784\u9020\u6570\u636e\u96c6\uff0cdataset\u5df2\u7ecf\u5199\u597d\u4e86\uff0c\u63a5\u4e0b\u6765\u9001\u5165\u5230dataloader\u4e2d\uff0c\u5fd8\u8bb0\u7528\u6cd5\u5c31\u67e5api</p> <p></p> <p>\u770b\u63a5\u6536\u7684\u8f93\u5165</p> <p></p> <ul> <li>dataset</li> <li>batch_size</li> <li>shuffle</li> </ul> <p>dataloader\u7684\u4f5c\u7528\u5c31\u662f\u628adataset\u7684\u6570\u636e\u53d8\u6210\u4e00\u4e2a\u4e2abatch\uff0c\u540e\u9762\u8fdb\u884c\u6279\u8bad\u7ec3</p> <pre><code>from torch.utils.data import DataLoader\ndataloader = DataLoader(dataset,batch_size=32,shuffle=True)\n</code></pre> <p>\u63a5\u4e0b\u6765\u5f00\u59cb\u4f18\u5316\u5668\uff0c\u9700\u8981\u4e24\u4e2a\u4f18\u5316\u5668\uff0c\u5bf9\u751f\u6210\u5668\u7684\u53c2\u6570\u8fdb\u884c\u4f18\u5316\u548c\u5bf9\u5224\u522b\u5668\u7684\u53c2\u6570\u8fdb\u884c\u4f18\u5316</p> <p></p> <p>\u751f\u6210\u5668\u7684\u4f18\u5316\u5668Adam\uff0c\u67e5\u770bAdam\u9700\u8981\u7684\u53c2\u6570\uff1a</p> <p></p> <ul> <li>params\uff1a\u7b2c\u4e00\u4e2a\u662f\u53ef\u8fed\u4ee3\u7684\u8bad\u7ec3\u53c2\u6570\uff0c\u5c31\u662f\u628a\u6a21\u578b\u7684parameters\u8c03\u7528\u4e00\u4e0b\u5c31\u53ef\u4ee5\u4e86\uff0c\u5f97\u5230\u53ef\u8fed\u4ee3\u7684\u8bad\u7ec3\u53c2\u6570</li> </ul> <pre><code>g_optimizer = torch.optim.Adam(params,lr=0.0001)\n</code></pre> <p>\u56e0\u6b64 \u6211\u4eec\u9700\u8981\u5b9e\u4f8b\u5316Generator,\u4f20\u5165params</p> <p>\u7b2c\u4e00\u4e2a\u7ebf\u6027\u5c42\u4fee\u6539\u4e00\u4e0b\uff0c\u5c11\u4f20\u4e2a\u53c2\u6570\uff1a</p> <pre><code>class Generator(nn.Module):\n    def __init__(self):\n        super(Generator,self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(torch.prod(image_size,dtype=torch.int32),64),\n</code></pre> <p>\u63a5\u4e0b\u6765\u5b9e\u4f8b\u5316\u4e00\u4e2aGenerator</p> <pre><code>generator = Generator()\n</code></pre> <p>\u7136\u540e\uff0cAdam\u4f18\u5316\u5668\u7684\u53c2\u6570\uff0c\u5c31\u662fgenerator.parameters()\uff0c\u53ea\u5bf9\u751f\u6210\u5668\u7684\u53c2\u6570\u8fdb\u884c\u4f18\u5316</p> <pre><code>g_optimizer = torch.optim.Adam(generator.parameters(),lr=0.0001)\n</code></pre> <p>\u540c\u6837\u5b9e\u4f8b\u5316 \u5224\u522b\u5668\uff0c\u540c\u65f6\u4f18\u5316\u5224\u522b\u5668\u7684\u53c2\u6570</p> <pre><code>generator = Generator()\ndiscriminator = Decriminator()\n\ng_optimizer = torch.optim.Adam(generator.parameters(),lr=0.0001)\nd_optimizer = torch.optim.Adam(discriminator.parameters(),lr=0.0001)\n</code></pre> <p>\u751f\u6210\u5668\u53c2\u6570 \u548c \u5224\u522b\u5668\u53c2\u6570 \u5b8c\u5168\u9694\u79bb\uff0c\u4f18\u5316\u4e5f\u5206\u522b\u4f18\u5316</p> <p>\u63a5\u4e0b\u6765 \u5b9a\u4e49loss_fn\uff0c\u903b\u8f91\u56de\u5f52\uff0c\u5224\u65ad\u7167\u7247\u6765\u81ea\u771f\u5b9e\u7684 \u8fd8\u662f \u5047\u7684\uff0closs function \u662fBCELOSS \u4e8c\u6b21\u7684\u4ea4\u53c9\u71b5</p> <p></p> <p>BCE\u63a5\u6536\u7684\u53c2\u6570\uff1a</p> <p></p> <ul> <li>input\u53ef\u4ee5\u662f\u4efb\u610f\u7684\u7ef4\u5ea6</li> <li>output\u4e5f\u662f\u4efb\u610f\u7684\u7ef4\u5ea6\uff0coutput\u9ed8\u8ba4\u60c5\u51b5\u662f\u4e00\u4e2a\u6807\u91cf\uff0c\u5982\u679c\u628areduction\u8bbe\u7f6e\u4e3a\"none\"\u7684\u8bdd\uff0c\u5f62\u72b6\u5c31\u4f1a\u548c\u8f93\u5165\u4e00\u6837</li> <li>target\u4e5f\u662f\u4efb\u610f\u7684\u7ef4\u5ea6</li> <li>\u4e0b\u9762\u7ed9\u51fa\u4f8b\u5b50</li> </ul> <p>\u5b9e\u4f8b\u5316\u4e00\u4e2aloss function</p> <p>input\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a3\u7684\u5411\u91cf</p> <p>target\u4e5f\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a3\u7684\u5411\u91cf</p> <p>\u7136\u540e\u8ddf\u4e00\u4e2aloss\u8ba1\u7b97 \u5f97\u5230\u4e00\u4e2a\u6807\u91cf</p> <p>\u8c03\u7528\u662f\u5f88\u7b80\u5355\u7684\uff0c\u5c31\u8c03\u7528\u4e00\u4e2ann.BCELoss()\u5373\u53ef</p> <ul> <li>BCELoss\u7684\u516c\u5f0f</li> </ul> <p></p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc \\(x_n=1\\)  \u6216\u8005 \\(x_n=0\\) \\(log\\) \u4e0d\u4f1a\u5d29\u6e83\uff0c\u56e0\u4e3apytorch\u5185\u90e8\u4fdd\u8bc1\u4e86\u6570\u503c\u7a33\u5b9a\u6027</p> <p></p> <p>\u6700\u540e\u4e00\u6bb5\uff0c\u5bf9loss function\u8fdb\u884c\u4e86\u622a\u65ad\uff0c\u622a\u65ad\u5230-100\u4e3a\u6b62\uff0c\u907f\u514d\u4e86\u8d1f\u65e0\u7a77\u5927 \u5d29\u6e83\u7684\u95ee\u9898</p> <pre><code>loss_fn = nn.BCELoss()\n</code></pre> <p>\u4ee5\u4e0a\u5b8c\u6210\u4e86\u6240\u6709\u5b9a\u4e49\u7684\u90e8\u5206\uff1a</p> <pre><code># Training\n\ndataset = torchvision.datasets.MNIST(\"minist_data\",\n                                     train=True,\n                                     download=True,\n                                     transform=torchvision.transforms.Compose(\n                                         [torchvision.transforms.Resize(28),\n                                         torchvision.transforms.ToTensor()]\n                                     ))\n\ndataloader = DataLoader(dataset,batch_size=32,shuffle=True)\n\ngenerator = Generator()\ndiscriminator = Decriminator()\n\ng_optimizer = torch.optim.Adam(generator.parameters(),lr=0.0001)\nd_optimizer = torch.optim.Adam(discriminator.parameters(),lr=0.0001)\n\nloss_fn = nn.BCELoss()\n</code></pre> <p>\u63a5\u4e0b\u6765 \u5f00\u59cb\u8bad\u7ec3 \u5c31\u662f\u8bba\u6587\u4e2d\u6240\u63cf\u8ff0\u7684 \u4e24\u4e2a for\u5faa\u73af</p> <p>\u7b2c\u4e00\u4e2afor\u5faa\u73af \u5bf9 epoch\u8fdb\u884c\u5faa\u73af\uff0c\u8868\u793a\u8bad\u7ec3\u591a\u5c11\u4e2a\u5468\u671f</p> <pre><code>num_epoch = 100\nfor epoch in range(num_epoch):\n</code></pre> <p>\u7b2c\u4e8c\u4e2afor\u5faa\u73af \u5bf9dataloader\u8fdb\u884c\u4e00\u4e2a\u679a\u4e3e\u7684\u904d\u5386\uff0c\u91c7\u7528enmerate(dataloader)\uff0c\u8fd4\u56de\u4e24\u4e2a\uff0c\u7b2c\u4e00\u4e2a\u662findex\uff0c\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e\u4e0a\u662fsample \u6216\u8005\u8bf4 mini_batch\uff0cmini_batch \u4e0d\u4ec5\u5305\u542b \\(x\\) \u8fd8\u5305\u542b \\(y\\)\uff0c\u56e0\u6b64 \u6211\u4eec\u8981\u5bf9mini_batch \u8fdb\u884c\u89e3\u6790\u4e00\u4e0b</p> <pre><code>    for i,mini_batch in enumerate(dataloader):\n        gt_image,_ = mini_batch\n</code></pre> <p>labels\u4e0d\u8981\u4e86 \uff0c\u56e0\u4e3a\u8fd9\u91cc\u8fdb\u884c\u7684\u662f\u65e0\u76d1\u7763\u7684\u751f\u6210\u4efb\u52a1\uff0c\u8fd9\u91cc\u7684image\u5c31\u662f\u771f\u5b9e\u7684\u56fe\u7247\uff0c\u6240\u4ee5\u547d\u540dgt_image\uff0cground truth\uff0c\u8868\u793a\u771f\u5b9e\u7684\u7167\u7247</p> <p>\u63a5\u4e0b\u6765\uff0c\u9996\u5148\u968f\u673a\u751f\u6210\u4e00\u4e2az\uff0cz\u670d\u4ece\u6b63\u6001\u5206\u5e03\uff0c\u5f62\u72b6\u662f \\(batch\\_size\u00d7latent\\_dim\\)\uff0c</p> <pre><code>batch_size = 32\nlatent_dim = 64\nfor epoch in range(num_epoch):\n    for i,mini_batch in enumerate(dataloader):\n        gt_image,_ = mini_batch\n        z = torch.randn(batch_size,latent_dim)\n</code></pre> <p>\u5173\u4e8elatent_dim\uff1f</p> <p>z\u7684\u7ef4\u5ea6\u662fbatch size\u00d7latent dim</p> <p>z\u7684\u7ef4\u5ea6\u5728\u6700\u5f00\u59cb\u7684\u5b9a\u4e49\u4e2d\uff0c\u5199\u6210\u7684\u662f\uff1a</p> <p></p> <p>\u5c31\u662f\u5047\u8bbe \u8ddf \u56fe\u7247\u4e00\u6837\u7684\u7ef4\u5ea6\uff0c\u4f46\u662f\uff0c\u4e5f\u53ef\u4ee5\u5047\u8bbe\u662f latent_dim\uff0c\u4e00\u822c\u5728\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u90fd\u662flatent dim\u5305\u62ec\u5728VAE\u4e2d\uff0c\u90fd\u662flatent dim</p> <p></p> <p>z\u662f \u751f\u6210\u5668\u7684\u8f93\u5165</p> <p>\u5c31\u662f\u8bf4z\u7684\u7ef4\u5ea6\uff0c\u53ef\u4ee5\u662f\u4efb\u610f\u7684</p> <p>\u56e0\u6b64 \u6211\u4eec\u9700\u8981 \u5b9a\u4e49\u4e00\u4e2alatent_dim </p> <p>\u5c06latent_dim \u8bbe\u7f6e\u4e3a64\uff0c\u6b64\u65f6z\u7684\u5927\u5c0f\u662f batch size\u00d7latent dim</p> <p>\u7136\u540e\u628a \\(z\\) \u5582\u5165\u5230generator\u4e4b\u4e2d\uff0c\u5f97\u5230prod_image\uff0c\u9884\u6d4b\u51fa\u6765\u7684\u7167\u7247</p> <pre><code>prod_images = generator(z)\n</code></pre> <p>\u5f97\u5230\u9884\u6d4b\u7684\u7167\u7247\uff0c\u7ef4\u5ea6\u662f4\u7ef4\u7684</p> <p></p> <p>batch size\u00d7\u901a\u9053\u00d7\u9ad8\u5ea6\u00d7\u5bbd\u5ea6\uff0c\u8ddfDiscriminator\u7684\u8f93\u5165\u662f\u4e00\u6837\u7684</p> <p></p> <p>\u6240\u4ee5 \u53ef\u4ee5\u628a\u5f97\u5230\u7684\u9884\u6d4b\u7167\u7247 \u9001\u5165\u5230 discriminator\u4e4b\u4e2d\uff0c\u5f97\u5230\u5224\u522b\u7684\u6982\u7387</p> <p></p> <p>\u5c31\u662f\u539f\u6587\u4e2d\u7684D(G(z))</p> <pre><code>pred_images = generator(z)\n</code></pre> <p>\u63a5\u4e0b\u6765 \u628a \u751f\u6210\u5668 \u751f\u6210\u7684\u7167\u7247 \u9001\u5165\u5230 \u5224\u522b\u5668\u4e4b\u4e2d\uff0c\u5f97\u5230\u4e00\u4e2a\u6982\u7387\uff0c\u7136\u540e\u628a\u76ee\u6807\u4e5f\u9001\u5165\u8fdb\u53bb\uff0c\u9001\u5165\u5230BCELoss function\u4e4b\u4e2d\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230g_loss </p> <pre><code>g_loss = loss_fn( discriminator(pred_images) , target )\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u53cd\u5411\u4f20\u64ad\uff0c\u66f4\u65b0\u53c2\u6570</p> <pre><code>        g_loss.backward()\n        g_optimizer.step()\n</code></pre> <p>\u6700\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u9700\u8981\u628a \u68af\u5ea6\u7f6e 0</p> <pre><code>        g_optimizer.zero_grad()\n        g_loss = loss_fn(discriminator(pred_images),target)\n        g_loss.backward()\n        g_optimizer.step()\n</code></pre> <p>\u4f46\u662f target\u8fd8\u6ca1\u6709\u5b9a\u4e49\uff0c\u5199\u62101\u8fd8\u662f \u5199\u62100\uff1f\u76ee\u6807\u662f\u5bf9\u751f\u6210\u5668\u8fdb\u884c\u4f18\u5316\uff0c\u6240\u4ee5\u5e0c\u671b \u5224\u522b\u5668\u628a \u751f\u6210\u5668\u751f\u6210\u7684\u56fe\u7247 \u4f18\u5316\u6210\u771f\u5b9e\u56fe\u7247\uff0c\u6240\u4ee5\u5199\u6210\u9884\u6d4b\u4e3a1\uff0c\u5373\u5b9a\u4e49target = 1\uff0c\u5f62\u72b6\u5c31\u662f batch_size\u00d71</p> <pre><code>        target = torch.ones(batch_size,1)\n</code></pre> <p>\u4ee5\u4e0a\u662f\u5bf9\u751f\u6210\u5668\u7684\u4f18\u5316</p> <pre><code>        g_optimizer.zero_grad()\n\n        target = torch.ones(batch_size,1)\n\n        g_loss = loss_fn(discriminator(pred_images),target)\n        g_loss.backward()\n        g_optimizer.step()\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u5224\u522b\u5668\u7684\u4f18\u5316</p> <ul> <li>\u5224\u522b\u5668\u4e00\u5f00\u59cb\u4e5f\u9700\u8981\u7f6e\u96f6\u64cd\u4f5c</li> </ul> <pre><code>d_optimizer.zero_grad()\n</code></pre> <ul> <li>\u5224\u522b\u5668\u7684\u76ee\u6807\u51fd\u6570\u6709\u4e24\u4e2a\uff1a</li> </ul> <p></p> <p>\u7b2c\u4e00\u9879\u662f\u9700\u8981\u628a\u771f\u5b9e\u7684\u76ee\u6807\u56fe\u7247\u9001\u5165\u8fdb\u53bb</p> <pre><code>target = torch.ones(batch_size,1)        \nd_loss = loss_fn(discriminator(gt_images),target)\n</code></pre> <p>\u4f18\u5316\u7b2c\u4e00\u9879\u7684\u76ee\u6807\u662f\u5224\u522b\u5668\u80fd\u591f\u628a\u771f\u5b9e\u56fe\u7247\u9884\u6d4b\u6b63\u786e\uff0c\u6807\u7b7e\u662ftarget=1</p> <p>\u4ee5\u4e0a\u5199\u597d\u4e86\u7b2c\u4e00\u9879\uff0c\u7b2c\u4e8c\u9879\u628a pred_images\u9001\u5165\u8fdb\u53bb\uff0c\u4e0d\u8fc7\u8981\u628atarget\u7684torch.ones\u6539\u6210torch.zeros\uff0c\u56e0\u4e3a\u7b2c\u4e8c\u9879\u7684\u4f18\u5316\u76ee\u6807\u662f \u5224\u522b\u5668\u628a\u9884\u6d4b\u7167\u7247\u5206\u7c7b\u62100\uff1a</p> <pre><code>d_loss = 0.5*loss_fn(discriminator(gt_images),torch.ones(batch_size,1)) \n        + 0.5*loss_fn(discriminator(pred_images.detach()),torch.zeros(batch_size,1))\n</code></pre> <p>\u8fd9\u91cc\u6709\u9700\u8981\u6ce8\u610f\u7684\u70b9\uff0c\u5728\u66f4\u65b0\u5224\u522b\u5668\u7684\u65f6\u5019\uff0c\u4e0d\u8981\u66f4\u65b0\u751f\u6210\u5668\u7684\u53c2\u6570\uff0c\u6240\u4ee5\u7528 detach\u628a\u53c2\u6570\u9694\u79bb\u51fa\u6765\uff0c\u4ece\u8ba1\u7b97\u56fe\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u800c\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\uff0c\u7531\u4e8e\u662f\u4e24\u4e2aloss</p> <p>\u63a5\u4e0b\u6765\u540c\u6837\u8fdb\u884c backward\u548cstep</p> <pre><code>d_loss.backward()\nd_optimizer.step()\n</code></pre> <p>\u4ee5\u4e0a\u662fg\u548cd\u7684\u4f18\u5316</p> <p>\u8003\u8651\u4fdd\u5b58\u4e2d\u95f4\u7ed3\u679c\uff0c\u6bd4\u5982 \u6bcf\u6b21\u5904\u7406\u5b8c1000\u5f20\u7167\u7247\uff08\u4e00\u51716w\u5f20\u7167\u7247\uff09\uff0c\u4fdd\u5b58\u7167\u7247\u7684\u7ed3\u679c</p> <pre><code>if i%1000 ==0:\n    pass\n</code></pre> <p>\u5b98\u65b9api \uff1atorchvision \u4fdd\u5b58\u7167\u7247</p> <p></p> <p>save_image\u51fd\u6570\uff1a</p> <p></p> <p>\u63a5\u6536\u53c2\u6570\uff1a</p> <ul> <li>tensor\uff1a\u63a5\u53d7\u4e00\u4e2atensor\uff0ctensor\u5c31\u662f\u6211\u4eec\u4fdd\u7559\u7684\u7167\u7247\uff0c\u5982\u679c\u7ed9\u5b9aminibatchbatch\u7684\u8bdd\uff0c\u4e5f\u662f\u53ef\u4ee5\u7684\uff0c\u4f1a\u7528\u7f51\u683c\u72b6\u4fdd\u5b58</li> <li>fp\uff1a\u6587\u4ef6\u540d\u79f0</li> <li>format\uff1a\u786e\u5b9a\u6587\u4ef6\u7684\u540e\u7f00</li> </ul> <p>\u63a5\u4e0b\u6765\u8c03\u7528\u8fd9\u4e2a\u51fd\u6570\uff0c\u4fdd\u5b58\u7167\u7247</p> <p>\u7b2c\u4e00\u4e2a\u53c2\u6570\uff1a\u4f20\u5165pred_images\uff0c\u662f4\u7ef4\u7684\u3001minibatch\u7684\u683c\u5f0f</p> <pre><code>if i%1000 == 0:\n     torchvision.utils.save_image(pred_images,)\n</code></pre> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1a\u6587\u4ef6\u540d\u79f0</p> <p>\u6587\u4ef6\u540d\u79f0\u9700\u8981\u904d\u5386\u547d\u540d\u6bcf\u4e2a\u5355\u72ec\u7684pred_image\uff0c\u91c7\u7528<code>enumerate</code>\u904d\u5386\uff0c\u904d\u5386\u5f97\u5230\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662findex\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7167\u7247</p> <pre><code>        if i%1000 == 0:\n            for index,image in enumerate(pred_images):\n                torchvision.utils.save_image(pred_images,f\"image_{index}.png\")\n</code></pre> <p>\u8865\u5145\u4e4b\u524d\u7684 transform\u8fd8\u9700\u8981\u4e00\u4e2a normalize\u53c2\u6570</p> <p></p> <p></p> <p>\u662f\u56e0\u4e3a\u5728\u8bc6\u522b\u65f6\uff0c\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee</p> <p>\u8fd9\u91cc\u6709\u4e00\u4e2atrick\uff0c\u8ba1\u7b97\u662f0.3\uff0c0.3\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528\u65f6\u7528\u7684\u662f0.5\uff0c0.5\uff0c\u800c\u4e14\u5728\u5b9e\u9645\u7684\u5b9e\u9a8c\u4e2d\uff0c\u786e\u5b9e\u662f0.5\u7684\u5b9e\u9a8c\u6548\u679c\u66f4\u597d</p> <pre><code>dataset = torchvision.datasets.MNIST(\"minist_data\",\n                                     train=True,\n                                     download=True,\n                                     transform=torchvision.transforms.Compose(\n                                         [torchvision.transforms.Resize(28),\n                                         torchvision.transforms.ToTensor(),\n                                         torchvision.transforms.Normalize(mean=[0.5],std=[0.5])]\n\n                                     ))\n</code></pre> <p>\u4ee5\u4e0a\u5b9e\u73b0\u4e86GAN\u7684\u6574\u4f53\u6846\u67b6</p> <ol> <li>\u5148\u5199\u4e00\u4e2a\u751f\u6210\u5668</li> <li>\u7136\u540e\u5199\u5224\u522b\u5668</li> <li>\u6784\u5efa\u6570\u636e</li> <li>\u5b9e\u4f8b\u5316\u4e24\u4e2aoptimizer\uff0c\u5206\u522b\u662f\u751f\u6210\u5668\u7684\u4f18\u5316\u5668\uff0c\u7136\u540e\u662f\u5224\u522b\u5668\u7684\u4f18\u5316\u5668</li> <li>loss function</li> <li>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d \uff0c\u5148\u8bad\u7ec3\u751f\u6210\u5668\u6216\u8005\u5148\u8bad\u7ec3\u5224\u522b\u5668\u90fd\u662f\u53ef\u4ee5\u7684</li> <li>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0d\u8bba\u662f\u751f\u6210\u5668\u8fd8\u662f\u5224\u522b\u5668\u4f18\u5316\u5668\uff0c\u90fd\u9700\u8981\u6307\u5b9a\u597d\u53c2\u6570</li> </ol> <p></p> <p>\u751f\u6210\u5668\u4f18\u5316\u5668\u53ea\u4f18\u5316\u751f\u6210\u5668\u7684\u53c2\u6570\u3001\u5224\u522b\u5668\u4f18\u5316\u5668\u53ea\u4f18\u5316\u5224\u522b\u5668\u7684\u53c2\u6570</p> <pre><code>for epoch in range(num_epoch):\n    for i,mini_batch in enumerate(dataloader):\n        gt_images,_ = mini_batch\n        z = torch.randn(batch_size,latent_dim)\n\n        pred_images = generator(z)\n\n        g_optimizer.zero_grad()\n\n        g_loss = loss_fn(discriminator(pred_images),torch.ones(batch_size,1))\n        g_loss.backward()\n        g_optimizer.step()\n\n        d_optimizer.zero_grad()\n        d_loss = 0.5*loss_fn(discriminator(gt_images),torch.ones(batch_size,1))+ 0.5*loss_fn(discriminator(pred_images).detach(),torch.zeros(batch_size,1))\n        d_loss.backward()\n        d_optimizer.step()\n\n        if i%1000 == 0:\n            for index,image in enumerate(pred_images):\n                torchvision.utils.save_image(pred_images,f\"image_{index}.png\")\n</code></pre> <p>\u4ee3\u7801\u5bf9\u5e94\u7b97\u6cd5\u6d41\u7a0b\uff1a</p> <p></p> <p>\u9996\u5148\u751f\u6210\u5668\u63a5\u6536\u9ad8\u65af\u968f\u673a\u566a\u58f0\u4f5c\u4e3a\u8f93\u5165</p> <pre><code>z = torch.randn(batch_size,latent_dim)\n</code></pre> <p>\u751f\u6210\u5668\u63a5\u6536\u9884\u6d4b\u7684\u7167\u7247\u4f5c\u4e3a\u4f18\u5316</p> <pre><code>        g_optimizer.zero_grad()\n\n        g_loss = loss_fn(discriminator(pred_images),torch.ones(batch_size,1))\n        g_loss.backward()\n        g_optimizer.step()\n</code></pre> <p>\u4f18\u5316\u7684\u76ee\u7684\u662f\u4f7f\u5f97\u751f\u6210\u7684\u7167\u7247\u63a5\u8fd1\u771f\u5b9e\u7167\u7247\uff0c\u4e5f\u5c31\u662f\u5224\u522b\u5668\u7684\u8f93\u51fa <code>discriminator(pred_images)</code> \u63a5\u8fd1\u771f\u5b9e\u6807\u7b7e1    <code>torch.ones(batch_size,1)</code></p> <p>\u5224\u522b\u5668\u7684\u4f18\u5316\u5305\u62ec\u4e24\u90e8\u5206\uff0c\u5c06\u771f\u5b9e\u56fe\u50cf\u5224\u522b\u4e3a1\uff0c\u5c06\u751f\u6210\u56fe\u50cf\u5224\u522b\u4e3a0\uff0c\u4e24\u4e2a\u635f\u5931\u76f8\u52a0\uff0c\u4fdd\u8bc1\u635f\u5931\u7684\u5e73\u8861\uff0c\u6240\u4ee5*0.5</p> <pre><code>        d_optimizer.zero_grad()\n        d_loss = 0.5*loss_fn(discriminator(gt_images),torch.ones(batch_size,1))+ 0.5*loss_fn(discriminator(pred_images).detach(),torch.zeros(batch_size,1))\n        d_loss.backward()\n        d_optimizer.step()\n</code></pre> <p>\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u635f\u5931\u62c6\u5f00\uff1a<code>real_loss</code> \u8868\u793a\u771f\u5b9e\u7167\u7247\u7684\u635f\u5931\uff1b<code>fake_loss</code> \u8868\u793a\u751f\u6210\u56fe\u50cf\u7684\u635f\u5931\uff1b\u63a5\u7740 \\(0.5\u500d\u7684\\mathrm{real\\_loss}\\) + \\(0.5\u500d\u7684 \\mathrm{fake\\_loss}\\) \u5f97\u5230\u6700\u7ec8\u7684loss</p> <pre><code>        real_loss = loss_fn(discriminator(gt_images),torch.ones(batch_size,1))\n        fake_loss = loss_fn(discriminator(pred_images),torch.zeros(batch_size,1))\n        d_loss = 0.5 * real_loss + 0.5 * fake_loss \n        # \u89c2\u5bdf real_loss \u4e0e fake_loss\uff0c\u540c\u65f6\u4e0b\u964d\u540c\u65f6\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u5e76\u4e14\u503c\u5dee\u4e0d\u591a\u5927\uff0c\u8bf4\u660eD\u5df2\u7ecf\u7a33\u5b9a\u4e86   \n</code></pre> <p>\u62c6\u5f00\u5199\u7684\u76ee\u7684\u662f\u901a\u8fc7\u89c2\u5bdf\u635f\u5931 \u5224\u522b \u5224\u522b\u5668\u7684\u8bad\u7ec3\u662f\u5426\u8d8b\u4e8e\u7a33\u5b9a\uff0c\u6807\u51c6\u662f \u89c2\u5bdfreal_loss \u548c fake_loss\uff0c\u540c\u65f6\u4e0b\u964d\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u5e76\u4e14\u503c\u5dee\u4e0d\u591a\u5927\uff0c\u8bf4\u660e D \u5df2\u7ecf\u7a33\u5b9a\u4e86</p> <p>\u540e\u9762\u4ee3\u7801\u7684\u4f18\u5316\uff1a</p> <p>\uff081\uff09\u5f15\u5165batchnorm\u53ef\u4ee5\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\uff0c\u5177\u4f53\u505a\u6cd5\u662f\u5728\u751f\u6210\u5668\u7684Linear\u5c42\u540e\u9762\u6dfb\u52a0BatchNorm1d\uff0c\u6700\u540e\u4e00\u5c42\u9664\u5916\uff0c\u5224\u522b\u5668\u4e0d\u8981\u52a0 </p> <p>\uff082\uff09\u76f4\u63a5\u9884\u6d4b\u30100,1\u3011\u4e4b\u95f4\u7684\u50cf\u7d20\u503c\u5373\u53ef\uff0c\u4e0d\u505a\u5f52\u4e00\u5316\u7684transform\uff1b\u6216\u8005\u4e5f\u53ef\u4ee5\u653e\u5927\uff0c\u9884\u6d4b\u3010-1,1\u3011\u4e4b\u95f4\uff0c\u7528mean=0.5 std=0.5\u8fdb\u884c\u5f52\u4e00\u5316transform\u90fd\u53ef\u4ee5 </p> <p>\uff083\uff09\u5c06\u6fc0\u6d3b\u51fd\u6570ReLU\u6362\u6210GELU\u6548\u679c\u66f4\u597d </p> <p>\uff084\uff09real_loss\u57fa\u4e8e\u771f\u5b9e\u56fe\u7247\uff0cfake_loss\u57fa\u4e8e\u751f\u6210\u56fe\u7247\uff0creal_loss = loss_fn(discriminator(gt_images), torch.ones(batch_size, 1))\uff0cfake_loss = loss_fn(discriminator(pred_images.detach()), torch.zeros(batch_size, 1)) </p> <p>\uff085\uff09\u9002\u5f53\u5f15\u5165\u91cd\u6784loss\uff0c\u8ba1\u7b97\u50cf\u7d20\u503c\u7684L1\u8bef\u5dee </p> <p>\uff086\uff09\u5efa\u8bae\u5f15\u5165loss\u6253\u5370\u8bed\u53e5\uff0c\u5982\uff1a</p> <pre><code>  print(f\"step:{len(dataloader)*epoch+i}, g_loss:{g_loss.item()}, d_loss:{d_loss.item()}, real_loss:{real_loss.item()}, fake_loss:{fake_loss.item()}\") \n</code></pre> <p>\uff087\uff09\u5224\u522b\u5668\u6a21\u578b\u5bb9\u91cf\u4e0d\u5b9c\u8fc7\u5927 </p> <p>\uff088\uff09save_image\u4e2d\u7684normalize\u8bbe\u7f6e\u6210True\uff0c\u76ee\u7684\u662f\u5c06\u50cf\u7d20\u503cmin-max\u81ea\u52a8\u5f52\u4e00\u5230\u30100,1\u3011\u8303\u56f4\u5185\uff0c\u5982\u679c\u5df2\u7ecf\u9884\u6d4b\u4e86\u30100,1\u3011\u4e4b\u95f4\uff0c\u5219\u53ef\u4ee5\u4e0d\u7528\u8bbe\u7f6eTrue </p> <p>\uff089\uff09\u5224\u522b\u5668\u7684\u5b66\u4e60\u7387\u4e0d\u80fd\u592a\u5c0f </p> <p>\uff0810\uff09Adam\u7684\u4e00\u9636\u5e73\u6ed1\u7cfb\u6570\u548c\u4e8c\u9636\u5e73\u6ed1\u7cfb\u6570 betas \u9002\u5f53\u8c03\u5c0f\u4e00\u70b9\uff0c\u53ef\u4ee5\u5e2e\u52a9\u5b66\u4e60\uff0c\u8bbe\u7f6e\u4e00\u5b9a\u6bd4\u4f8b\u7684weight decay</p>"},{"location":"learning/5_Bert/","title":"BERT\u4ece\u96f6\u8be6\u7ec6\u89e3\u8bfb","text":"<p>\u76ee\u5f55\uff1a</p> <p></p>"},{"location":"learning/5_Bert/#1-bert","title":"1 Bert\u7684\u6574\u4f53\u67b6\u6784","text":"<ul> <li>Bert\u7684\u57fa\u7840\u7ed3\u6784\u662fTransformer Encoder\u7684\u90e8\u5206</li> <li>Encoder\u53ef\u4ee5\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a\u8f93\u5165\u90e8\u5206 + \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 + \u524d\u9988\u795e\u7ecf\u7f51\u7edc\u90e8\u5206</li> <li>Bert\u4f7f\u7528\u7684\u662f\u591a\u4e2aTransformer Encoder \u5806\u53e0\u5728\u4e00\u8d77\u7684\uff0c\u5176\u4e2dBert base\u4f7f\u7528\u7684\u662f12\u5c42\u7684Encoder\uff0cBert large\u4f7f\u7528\u7684\u662f24\u5c42\u7684Encoder</li> </ul> <ul> <li> <p>\u56fe\u4e2d\u5c55\u793a\u7684\u662f 12\u5c42\u7684 Encoder \u5806\u53e0\u5728\u4e00\u8d77\u7684Bert base</p> </li> <li> <p>\u5bb9\u6613\u6df7\u6dc6\u7684\u70b9\uff1a</p> </li> </ul> <p>\u8fd9\u91cc\u662f12\u5c42\u7684Encoder\u5806\u53e0\u5728\u4e00\u8d77\u7ec4\u6210\u7684Bert\uff0c\u800c\u4e0d\u662f12\u5c42\u7684Transformer\u5806\u53e0\u5728\u4e00\u8d77\uff0c\u4e00\u5b9a\u8981\u533a\u5206Encoder\u548cTransformer</p> <p>Transformer\u5728\u539f\u8bba\u6587\u4e2d\u662f6\u4e2aEncoder\u5806\u53e0\u5728\u4e00\u8d77\u53d8\u6210\u7f16\u7801\u7aef\uff0c6\u4e2adecoder\u5806\u53e0\u5728\u4e00\u8d77\u53d8\u6210\u89e3\u7801\u7aef</p> <p></p> <p></p> <ul> <li> <p>\u5982\u56fe\u662f\u4e00\u4e2aEncoder\uff0c\u5bf9\u4e8eBert\u7684Encoder\u90e8\u5206\uff0c\u91cd\u70b9\u5173\u6ce8\u8f93\u5165\u90e8\u5206</p> </li> <li> <p>\u5bf9\u4e8eTransformer\u6765\u8bf4\uff0c\u8f93\u5165\u90e8\u5206\u5305\u62ec\u4e24\u90e8\u5206\uff1a</p> </li> </ul> <ul> <li>\u4e00\u90e8\u5206\u662fembedding\uff0c\u5c31\u662f\u505a\u8bcd\u7684\u8bcd\u5411\u91cf\uff0c\u6bd4\u5982\u4f7f\u7528\u968f\u673a\u521d\u59cb\u5316 \u6216\u8005 word2vector</li> <li>\u7b2c\u4e8c\u90e8\u5206\uff1aPosition Encoding\uff0c\u5728Transformer\u4e2d\u4f7f\u7528\u7684\u662fPosition encoding \u4f4d\u7f6e\u7f16\u7801\uff0c\u4f7f\u7528\u7684\u662f\u4e09\u89d2\u51fd\u6570\uff0c\u6b63\u4f59\u5f26\u51fd\u6570</li> </ul> <ul> <li>\u5bf9\u4e8eBert\u6765\u8bf4\uff0c\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a</li> </ul> <ul> <li> <p>\u7b2c\u4e00\u90e8\u5206\u662f token embedding</p> </li> <li> <p>\u7b2c\u4e8c\u90e8\u5206\u662fsegment embedding</p> </li> <li> <p>\u7b2c\u4e09\u90e8\u5206\u662fPositional embedding</p> </li> </ul> <p>\u533a\u5206Transformer\u7684Position encoding\u548cBert\u7684Position embedding</p>"},{"location":"learning/5_Bert/#11-bert","title":"1.1 \u8be6\u7ec6\u770bBert\u7684\u8f93\u5165\u90e8\u5206","text":"<ul> <li> <p>Bert \u7684\u8f93\u5165\u90e8\u5206\u7531\u4e09\u90e8\u5206\u7ec4\u6210\uff0c\u5206\u522b\u662ftoken embedding\u3001segment embedding\u548cPosition embedding</p> </li> <li> <p>\u9996\u5148\u7c89\u8272\u7684\u4e00\u884c\u662f\u8f93\u5165\uff0c\u91cd\u70b9\u5173\u6ce8\u4e24\u4e2a\u90e8\u5206</p> </li> </ul> <p>\u7b2c\u4e00\u90e8\u5206\u662f\u6b63\u5e38\u8bcd\u6c47\uff1a</p> <p></p> <p>\u8fd9\u4e9b\u8bcd\u662fBert\u5206\u8bcd\u5668\u4e4b\u540e\u7684\u8bcd\u6c47</p> <p>\u7b2c\u4e8c\u90e8\u5206\u662f\u7279\u6b8a\u8bcd\u6c47\uff1a</p> <p>[CLS]</p> <p>[SEP]</p> <p>\u4e24\u79cd\u7279\u6b8a\u7b26\u53f7</p> <p>\u8fd9\u4e24\u79cd\u7279\u6b8a\u7b26\u53f7\u7684\u5b58\u5728\u662f\u56e0\u4e3a Bert\u7684\u9884\u8bad\u7ec3\u4e2d \u6709\u4e00\u4e2a\u662f NSP\u4efb\u52a1\uff0cnext sentence prediction\uff0c\u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5177\u4f53\u5730\u4e1c\u897f\u540e\u9762\u8bb2\u89e3</p> <p>[SEP]</p> <p>NSP\u4efb\u52a1\u662f\u5904\u7406\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u56e0\u4e3a\u5904\u7406\u7684\u662f\u4e24\u4e2a\u53e5\u5b50\uff0c\u6240\u4ee5\u9700\u8981\u4e00\u4e2a\u7279\u6b8a\u7b26\u53f7\uff0c\u544a\u8bc9\u6a21\u578b\uff0c\u5728[SEP]\u7b26\u53f7\u4e4b\u524d\u7684\u662f\u4e00\u4e2a\u53e5\u5b50\uff0c\u5728[SEP]\u7b26\u53f7\u4e4b\u540e\u7684\u662f\u4e00\u4e2a\u53e5\u5b50</p> <p>\u4ee5\u4e0a\u662f[SEP]\u7684\u4f5c\u7528</p> <p>[CLS]</p> <p>\u63a5\u7740NSP\u4efb\u52a1\u662f\u4e00\u4e2a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u5c31\u662f\u53e5\u5b50\u4e4b\u95f4\u662f\u4ec0\u4e48\u5173\u7cfb\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u90a3\u4e48\u600e\u4e48\u5b9e\u73b0\u4e8c\u5206\u7c7b\u4efb\u52a1\uff1f\u6240\u4ee5\u5728\u53e5\u5b50\u524d\u9762\u52a0\u4e86\u4e00\u4e2a[CLS]\u7684\u7279\u6b8a\u7b26\u53f7\uff0c\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u5c06[CLS]\u7684\u7279\u6b8a\u5411\u91cf\u63a5\u4e00\u4e2a\u4e8c\u5206\u7c7b\u5668\u505a\u4e00\u4e2a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u662f[CLS]\u7684\u4f5c\u7528</p> <p>\u5173\u4e8e[CLS]\u7684\u6269\u5c55\u5185\u5bb9\uff1a</p> <p>[CLS]\u7684\u5e38\u89c1\u8bef\u89e3\uff1a\u5f88\u591a\u4eba\u8ba4\u4e3a[CLS]\u8fd9\u4e2a\u7279\u6b8a\u5411\u91cf\u8868\u793a\u4e00\u4e2a\u53e5\u5b50\u6216\u8005\u6574\u4e24\u4e2a\u53e5\u5b50\u7684\u8bed\u4e49\u4fe1\u606f</p> <p>\u8fd9\u79cd\u8bf4\u6cd5\u662f\u4e0d\u6b63\u786e\u7684</p> <p>\u5728\u9884\u8bad\u7ec3\u7ed3\u675f\u4ee5\u540e\uff0c[CLS]\u8fd9\u4e2a\u5411\u91cf\u7684\u8f93\u51fa\u5411\u91cf\uff0c\u5e76\u4e0d\u80fd\u8bf4\u4ee3\u8868\u6574\u4e2a\u53e5\u5b50\u7684\u8bed\u4e49\u4fe1\u606f</p> <p>[CLS]\u8fd9\u4e2a\u5411\u91cf\u7528\u5728\u4e86NSP\u4efb\u52a1\u4e2d\uff0cNSP\u4efb\u52a1\u662f\u4e00\u4e2a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u548c\u7f16\u7801\u6574\u4e2a\u53e5\u5b50\u7684\u8bed\u4e49\u4fe1\u606f\u4efb\u52a1 \u662f\u4e0d\u4e00\u6837\u7684\uff0c\u6240\u4ee5\u5728\u7528 [CLS] \u8fd9\u4e2a\u5411\u91cf \u6765\u65e0\u76d1\u7763\u7684\u505a\u6587\u672c\u76f8\u4f3c\u5ea6 \u4efb\u52a1\u7684\u65f6\u5019 \u6548\u679c\u975e\u5e38\u5dee</p> <p>\u53ef\u4ee5\u770b\u4e0b\u9762\u8fd9\u5f20\u56fe</p> <p></p> <p>[CLS]\u5411\u91cf \u5e76\u4e0d\u80fd\u4ee3\u8868\u6574\u4e2a\u53e5\u5b50\u7684\u8bed\u4e49\u4fe1\u606f</p> <p>\u9884\u8bad\u7ec3\u6a21\u578b\u76f4\u63a5\u62ffsentence embedding\u6548\u679c\u751a\u81f3\u4e0d\u5982word embedding\uff0ccls\u7684embedding\u6548\u679c\u6700\u5dee</p> <p>\u4e3a\u4ec0\u4e48\u505a\u65e0\u76d1\u7763\u7684\u6587\u672c\u76f8\u4f3c\u5ea6\uff0ccls\u7684\u8f93\u51fa\u5411\u91cf\u6548\u679c\u5f88\u5dee\uff0c\u6709\u4e13\u95e8\u7684\u8ba8\u8bba</p> <p>\u73b0\u5728\u7684\u95ee\u9898\u662f cls embedding\u662f\u5426\u80fd\u505a\u65e0\u76d1\u7763\u7684\u6587\u672c\u76f8\u4f3c\u5ea6\uff1f\u4e5f\u6709\u7814\u7a76</p> <ul> <li>\u63a5\u4e0b\u6765\u770b\u9ec4\u8272\u7684\u90e8\u5206\uff1atoken embedding</li> </ul> <p></p> <p>token embedding\u5f88\u7b80\u5355\uff0c\u5c31\u662f\u5bf9input\u4e2d\u7684\u6240\u6709\u8bcd\u6c47\uff08\u5305\u62ec\u6b63\u5e38\u8bcd\u6c47\u548c\u7279\u6b8a\u8bcd\u6c47\uff09\u505a\u6b63\u5e38\u7684embedding\uff0c\u6bd4\u5982\u968f\u673a\u521d\u59cb\u5316</p> <ul> <li>\u7b2c\u4e8c\u90e8\u5206\uff0c\u7eff\u8272\u90e8\u5206\uff1asegment embedding</li> </ul> <p></p> <p>\u4e5f\u662f\u7531\u4e8e\u5904\u7406\u7684\u662f\u4e24\u4e2a\u53e5\u5b50\uff0c\u6240\u4ee5\u9700\u8981\u5bf9\u4e24\u4e2a\u53e5\u5b50\u8fdb\u884c\u533a\u5206\uff0c\u6bd4\u5982\u7b2c\u4e00\u4e2a\u53e5\u5b50\u5168\u90e8\u75280\u6765\u8868\u793a\uff0c\u5305\u62ec[CLS]\u5230[SEP]</p> <p></p> <p>\u540e\u9762\u7684\u53e5\u5b50\u5168\u90e8\u75281\u6765\u8868\u793a\uff0c\u5206\u522b\u8868\u793a\u4e86\u4e24\u4e2a\u53e5\u5b50</p> <ul> <li>\u7b2c\u4e09\u90e8\u5206\u662fPosition embeddings\uff0c\u4e5f\u5c31\u662fBert\u7684\u8f93\u5165\u90e8\u5206\u548cTransformer\u7684\u8f93\u5165\u90e8\u5206\u5f88\u5927\u7684\u4e0d\u540c\u70b9</li> </ul> <p>Transformer\u4e2d\u4f7f\u7528\u7684\u662f\u6b63\u4f59\u5f26\u51fd\u6570\uff0c\u8fd9\u91cc\u4f7f\u7528\u7684\u662f\u968f\u673a\u521d\u59cb\u5316\uff0c\u8ba9\u6a21\u578b\u81ea\u5df1\u5b66\u4e60\uff0c\u6bd4\u5982\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u8bbe\u7f6e\u4e3a0\uff0c\u7b2c\u4e8c\u4e2a\u90e8\u5206\u8bbe\u7f6e\u4e3a1\uff0c\u7b2c\u4e09\u4e2a\u90e8\u5206\u8bbe\u7f6e\u4e3a2\u7b49\u7b49\u7b49\uff0c\u4e00\u76f4\u5230511\uff0c512\u8868\u793a\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\uff0c\u7d22\u5f15\u7684\u6700\u5927\u503c\u662f512</p> <p></p> <p>\u6a21\u578b\u81ea\u5df1\u5b66\u4e60 \u6bcf\u4e2a\u4f4d\u7f6e\u7684embedding\u662f\u4ec0\u4e48\u6837\u7684</p> <p>tips</p> <p>encoding\uff1a\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e38\u6570\u4f4d\u7f6e\u7f16\u7801</p> <p>embedding\uff1a\u4f4d\u7f6e\u5d4c\u5165\uff0c\u9700\u8981\u81ea\u5df1\u5b66\u4e60\u4f4d\u7f6e\u8868\u793a</p> <p>\u5173\u4e8e\u4e3a\u4ec0\u4e48\u4f7f\u7528embedding\u800c\u4e0d\u662fencoding\uff0c\u4e5f\u6709\u5f88\u591a\u8ba8\u8bba</p>"},{"location":"learning/5_Bert/#2-mlmnsp","title":"2 \u5982\u4f55\u505a\u9884\u8bad\u7ec3\uff1aMLM+NSP","text":"<p>\u5982\u4f55\u5bf9Bert\u8fdb\u884c\u9884\u8bad\u7ec3\uff1f</p> <p>\u9884\u8bad\u7ec3Bert\u4e3b\u8981\u6d89\u53ca\u4e24\u4e2a\u4efb\u52a1\uff1a</p> <ul> <li>MLM\u4efb\u52a1\uff1amasked language model\u8868\u793a\u63a9\u7801\u8bed\u8a00\u6a21\u578b</li> <li>NSP\u4efb\u52a1\uff1a\u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb</li> </ul>"},{"location":"learning/5_Bert/#21-mlm","title":"2.1 MLM\u4efb\u52a1","text":"<p>\u9996\u5148\u660e\u786eBert\u5728\u9884\u8bad\u7ec3\u65f6 \u4f7f\u7528\u7684\u662f\u5927\u91cf\u7684\u65e0\u6807\u6ce8\u7684\u8bed\u6599\u5e93\uff0c\u968f\u5904\u53ef\u89c1\u7684\u65e0\u6807\u6ce8\u6587\u672c\uff0c\u6240\u4ee5\u5728\u9884\u8bad\u7ec3\u4efb\u52a1\u8bbe\u8ba1\u7684\u65f6\u5019\uff0c\u4e00\u5b9a\u8981\u8003\u8651\u65e0\u76d1\u7763\u6765\u505a\uff0c\u56e0\u4e3a\u662f\u6ca1\u6709\u6807\u7b7e\u7684\uff0c\u5bf9\u4e8e\u65e0\u76d1\u7763\u7684\u76ee\u6807\u51fd\u6570\u6765\u8bf4\uff0c\u6709\u4e24\u79cd\u76ee\u6807\u51fd\u6570 \u6bd4\u8f83\u53d7\u5230\u91cd\u89c6\uff1a</p> <p></p> <p>\u7b2c\u4e00\u79cd\u662fAR\u6a21\u578b\uff0c\u4e5f\u5c31\u662fAuto Regressive\uff0c\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u7279\u70b9\u662f\u53ea\u80fd\u8003\u8651\u5230\u5355\u4fa7\u4fe1\u606f\uff0c\u5178\u578b\u5e94\u7528\u662fGPT</p> <p>\u53e6\u4e00\u79cd\u76ee\u6807\u51fd\u6570\u5c31\u662fAE\u6a21\u578b\uff0cAutoencoding\uff0c\u81ea\u7f16\u7801\u6a21\u578b\uff0c\u7279\u70b9\u662f\u4ece\u635f\u574f\u7684\u8f93\u5165\u6570\u636e\u4e2d\u91cd\u5efa\u539f\u59cb\u6570\u636e\uff0c\u53ef\u4ee5\u4f7f\u7528\u5230\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f</p> <ul> <li> \u6587\u5b57\u63cf\u8ff0</li> <li> \u4f8b\u5b50\uff08\u6570\u5b66\u4f8b\u5b50\u3001\u5b9e\u9645\u4f8b\u5b50\uff09</li> <li> \u4ee3\u7801</li> </ul> <p></p> <p>\u4ee5 <code>\u6211\u7231\u5403\u996d</code> \u8fdb\u884c\u4e3e\u4f8b</p> <p>AR\u6a21\u578b</p> <p></p> <p>\u5047\u8bbe\u539f\u59cb\u8f93\u5165\u8bed\u6599 \u662f <code>\u6211\u7231\u5403\u996d</code>\uff0c\u90a3\u4e48AR\u6a21\u578b\u5728\u505a\u7684\u65f6\u5019\uff0c\u4e0d\u4f1a\u5bf9 <code>\u6211\u7231\u5403\u996d</code> \u672c\u8eab\u8fd9\u53e5\u8bdd\u8fdb\u884c\u64cd\u4f5c\uff0cAR\u6a21\u578b\u7684\u4f18\u5316\u76ee\u6807\u662f \\(P(\u6211\u7231\u5403\u996d)\\)  = P(\u6211) \u3010''\u6211''\u51fa\u73b0\u7684\u6982\u7387\u3011\u00d7 P(\u7231|\u6211)\u3010\u5728\"\u6211\"\u51fa\u73b0\u7684\u6761\u4ef6\u4e0b\uff0c\"\u7231\"\u51fa\u73b0\u7684\u6982\u7387\u3011 \u00d7 P(\u5403|\u6211\u7231)\u3010 \"\u6211\u7231\" \u51fa\u73b0\u7684\u6761\u4ef6\u4e0b \"\u5403\" \u51fa\u73b0\u7684\u6982\u7387\u3011 \u00d7 P(\u996d|\u6211\u7231\u5403)\u3010\u5728 \"\u6211\u7231\u5403\"\u7684\u6761\u4ef6\u4e0b\uff0c\"\u996d\"\u51fa\u73b0\u7684\u6982\u7387\u3011</p> <p>AR\u6a21\u578b\u7684\u4f18\u5316\u76ee\u6807 \u5b58\u5728\u524d\u540e\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u53ef\u4ee5\u770b\u5230AR\u6a21\u578b\u53ea\u7528\u5230\u4e86\u5355\u4fa7\u4fe1\u606f\uff0c\u4e5f\u5c31\u662f\u987a\u5e8f\u8fc7\u6765\u7684\uff0c\u4e5f\u5c31\u662f\u4ece\u5de6\u5230\u53f3\u7684\u5355\u4fa7\u4fe1\u606f\uff0c\u63a5\u4e0b\u6765\u770b AE\u6a21\u578b</p> <p>AE\u6a21\u578b</p> <p></p> <p>AE\u6a21\u578b\u662f\u5bf9\u53e5\u5b50\u505a\u4e00\u4e2a mask</p> <p>mask\u672c\u610f \u9762\u5177 \u7684\u610f\u601d\uff0c\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u7528 \u9762\u5177 \u63a9\u76d6\u53e5\u5b50\u4e2d\u7684 \u67d0\u4e9b \u6216\u8005\u67d0\u51e0\u4e2a \u5355\u8bcd</p> <p>\u6bd4\u5982 mask\u4e4b\u540e \u662f \uff1a\u6211\u7231mask\u996d\uff0c\u6211\u4eec\u77e5\u9053mask\u7684\u4f4d\u7f6e\u662f \"\u5403\"</p> <p>\u6211\u4eec\u7684\u4f18\u5316\u76ee\u6807\u5c31\u662f P(\u6211\u7231\u5403\u996d|\u6211\u7231mask\u996d)   \"\u6211\u7231mask\u996d\" \u6761\u4ef6\u4e0b\uff0c\u51fa\u73b0\u7684\u662f \"\u6211\u7231\u5403\u996d\" \u51fa\u73b0\u7684\u6982\u7387</p> <p>=  P(mask = \u5403|\u6211\u7231\u996d)  \"\u6211\u7231\u996d\"\u7684\u6761\u4ef6\u4e0b\uff0cmask=\"\u5403\" \u7684\u6982\u7387</p> <p></p> <ul> <li> <p>\u4ed4\u7ec6\u4f53\u4f1a\u8fd9\u4e2a AE\u7684\u4f18\u5316\u76ee\u6807\uff0c\u672c\u8d28\u662f\u5728\u6253\u7834\u6587\u672c\u539f\u6709\u7684\u4fe1\u606f\uff0c\u539f\u672c\u662f\"\u6211\u7231\u5403\u996d\"\uff0c\u6211\u4eec\u628a \"\u5403\" \u8fd9\u4e2a\u5b57 \u63a9\u76d6\u6389\uff0c\u8ba9\u6a21\u578b\u4e0d\u77e5\u9053\uff0c\u7136\u540e\u5728 \u8bad\u7ec3\u7684\u65f6\u5019 \u8ba9 \u6587\u672c\u91cd\u5efa</p> </li> <li> <p>\u5728\u505a \u6a21\u578b\u91cd\u5efa\u7684\u65f6\u5019\uff0c\u8ba9\u6a21\u578b \u81ea\u5df1 \u4ece\u524d\u540e\u7684\u8bcd\u4e2d\uff0c\u5b66\u4e60\u5404\u79cd\u4fe1\u606f\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f \u4ece\u524d\u540e\u7684\u4fe1\u606f\u4e2d\u65e0\u9650\u63a5\u8fd1\u7684\u9884\u6d4b\u4e2d \u539f\u672c\u7684\u8bcd\u6c47</p> </li> <li>\u901a\u4fd7\u70b9\u5c31\u662f\u8bf4\uff1a\"\u6211\u7231\"\u4f1a\u544a\u8bc9\u6a21\u578b \u540e\u9762\u5927\u6982\u7387\u8ddf\u7740\u52a8\u8bcd\u8bcd\u7ec4\uff0c\u6bd4\u5982 \"\u6211\u7231\u653e\u98ce\u7b5d\"\uff0c\"\u6211\u7231\u4e0a\u5b66\"\u7b49\u7b49\u7b49\uff0c\"\u996d\" \u4f1a\u544a\u8bc9\u6a21\u578b\u524d\u9762\u5927\u6982\u7387\u662f\u4e2a {\u52a8\u8bcd}\uff0c\u8fd9\u5c31\u662f\u6a21\u578b\u4ece\u6587\u672c\u4e2d\u52aa\u529b\u5b66\u5230\u7684\u89c4\u5f8b\uff0c\u5b66\u5230\u8fd9\u4e9b\u89c4\u5f8b\u4ee5\u540e\uff0c\u6a21\u578b\u5c31\u4f1a\u628a\u6587\u672c\u91cd\u5efa\u51fa\u6765\uff0c\u53d8\u4e3a \"\u5403\"</li> <li>\u63a5\u4e0b\u6765\u8ba8\u8bba mask \u6a21\u578b\u7684\u7f3a\u70b9</li> </ul> <p></p> <p>\u6bd4\u5982\u8bf4 \u6211\u4eecmask\u6389\u4e24\u4e2a\u5355\u5b57\"\u5403\"\u3001\"\u996d\"</p> <p>\u6b64\u65f6\u4f18\u5316\u76ee\u6807\u53d8\u4e3a \\(P(\u6211\u7231\u5403\u996d|\u6211\u7231\\mathrm{mask \\quad mask})\\) \u5c31\u662f \"\u6211\u7231maskmask\"\u6761\u4ef6\u4e0b\uff0c\"\u6211\u7231\u5403\u996d\"\u7684\u6982\u7387\uff0c\u8fd9\u4e2a\u5f0f\u5b50\\(= P(\u5403|\u6211\u7231)P(\u996d|\u6211\u7231)\\) \u901a\u8fc7\u8fd9\u4e2a\u4f18\u5316\u76ee\u6807\uff0c\u53ef\u4ee5\u770b\u51fa \u8fd9\u4e2a\u4f18\u5316\u76ee\u6807\u8ba4\u4e3a \"\u5403\" \u548c \"\u996d \"\u662f\u76f8\u4e92\u72ec\u7acb\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4 AE\u6a21\u578b \u8ba4\u4e3a mask\u548cmask\u4e4b\u95f4 \u662f\u76f8\u4e92\u72ec\u7acb\u7684\uff0c\u4f46\u5176\u5b9emask\u4e4b\u95f4\uff0c\u6bd4\u5982\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0cmask\u5c31\u4e0d\u662f\u76f8\u4e92\u72ec\u7acb\u7684</p> <p>\u4ee5\u4e0a\u8bf4\u660e\u4e86 mask \u6a21\u578b\u7684\u7f3a\u70b9</p> <ul> <li>Bert\u5728\u9884\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u7b2c\u4e00\u4e2a\u4efb\u52a1\u5c31\u662fMLM\uff0c\u5c31\u662f\u7528\u5230\u4e86mask\u7b56\u7565\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cmask\u7684\u6982\u7387\u95ee\u9898\uff0c\u5177\u4f53\u600e\u4e48\u505a\u7684\uff1f \\(\\downarrow\\)</li> </ul> <p></p> <p>\u968f\u673amask 15%\u7684\u5355\u8bcd\uff0c\u4e5f\u5c31\u662f100\u4e2a\u5355\u8bcd\u91cc\u9762\uff0c\u6311\u51fa 15\u4e2a\u5355\u8bcd\u6765mask\uff0c\u4f46\u662f15\u4e2a\u5355\u8bcd\u53c8\u4e0d\u662f\u5168\u90e8\u90fd\u771f\u7684\u6765mask\uff0c\u800c\u662f10%\u66ff\u6362\u6210\u5176\u4ed6\u5355\u8bcd\uff0c10%\u539f\u5c01\u4e0d\u52a8\uff0c80%\u66ff\u6362\u6210\u771f\u6b63\u7684mask\uff0c\u5173\u4e8e\u8fd9\u4e2a\u6982\u7387\u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u7684\uff0c\u60f3\u5b66\u81ea\u5df1\u641c\uff0c\u7406\u89e3\u5c31\u597d</p>"},{"location":"learning/5_Bert/#211mask","title":"2.1.1\u4ee3\u7801\u5b9e\u73b0mask","text":"<ul> <li>\u9996\u5148 \u968f\u673a mask\u6389 15%\u7684\u5355\u8bcd\uff0c\u5bf9\u5e94 <code>mask_indices</code></li> <li>\u63a5\u4e0b\u6765811\u5212\u5206\uff0c\u4e5f\u5c31\u662f 8\u4efd \u66ff\u6362\u6210 mask\uff0c1\u4efd\u539f\u5c01\u4e0d\u52a8\uff0c1\u4efd\u66ff\u6362\u6210\u5176\u4ed6</li> <li>\u5bf9\u5e94\u5230\u4ee3\u7801\uff0c\u5c31\u662f <code>random.randowm()&lt;0.8</code> \u5c31 mask\u6389 \u8fd9\u4e2a\u8bcd\u6c47\uff0c\u66ff\u6362\u6210 \"[mask]\" \u8fd9\u4e2a\u5355\u8bcd</li> <li>\u5269\u4e0b\u7684 20% \u4e2d\u7684\uff0c50%\uff0c\u4e5f\u5c31\u662f\u4e24\u8005\u76f8\u4e58\uff0c\u5c31\u662f\u603b\u4f53\u4e2d\u7684 10% \u4fdd\u6301\u4e0d\u53d8\uff0c\u5269\u4f59\u7684 10% \u968f\u673a\u62bd\u53d6\u4e00\u4e2a\u5355\u8bcd</li> <li>\u4f46\u4ece\u4ee3\u7801\u6765\u770b\uff0c\u8fd9\u4e2a\u968f\u673a\u62bd\u53d6\u7684\u8fc7\u7a0b\uff0c\u4e5f\u6709\u53ef\u80fd\u62bd\u53d6\u5230 \u81ea\u5df1\u672c\u8eab\uff0c\u4e5f\u5c31\u662f \u5b83\u7531\u522b\u7684\u5355\u8bcd\u66ff\u6362\u5b8c\u4ee5\u540e\uff0c\u53c8\u62bd\u53d6\u5230\u8fd9\u4e2a \u66ff\u6362\u540e\u7684\u8bcd\uff0c\u4e5f\u5c31\u662f\u6ca1\u6709\u53d8\uff08\uff1f</li> </ul>"},{"location":"learning/5_Bert/#22-nsp","title":"2.2  NSP\u4efb\u52a1","text":"<p>\u63a5\u4e0b\u6765 \u7b2c\u4e8c\u4e2a\u4efb\u52a1 NSP\u4efb\u52a1</p> <p></p> <ul> <li>\u7406\u89e3NSP\u4efb\u52a1 \u7684\u5173\u952e\u4e00\u70b9\u662f\u7406\u89e3 \u6837\u672c\u7684\u6784\u9020\u6a21\u5f0f</li> <li>NSP\u4efb\u52a1 \u5728\u505a input embedding\u7684\u65f6\u5019\uff0c\u8981\u533a\u5206\uff0c\u6709\u4e24\u4e2a\u7279\u6b8a\u7b26\u53f7\uff0c[CLS]\u548c[SEP]</li> <li>\u63a5\u4e0b\u6765\u770b \u6837\u672c\u7684\u6784\u9020\u6a21\u5f0f\uff1a</li> </ul> <ul> <li>\u7b2c\u4e00\u4e2a\u6b63\u6837\u672c\u662f \u4ece\u8bad\u7ec3\u8bed\u6599\u4e2d\uff0c\u53d6\u51fa\u4e24\u4e2a\u8fde\u7eed\u7684\u6bb5\u843d</li> </ul> <p>\u7406\u89e3\uff1a</p> <p>\u53d6\u51fa\u4e24\u4e2a\u8fde\u7eed\u7684\u6bb5\u843d\uff1a\u8bf4\u660e\u4e24\u4e2a\u6bb5\u843d\u6765\u81ea\u540c\u4e00\u7bc7\u6587\u6863\uff0c\u4e00\u4e2a\u6587\u6863\u662f\u4e00\u4e2a\u4e3b\u9898\uff0c\u5c31\u662f\u8bf4 \u540c\u4e00\u4e2a\u4e3b\u9898\u4e0b \u4e24\u4e2a\u8fde\u7eed\u7684\u6bb5\u843d\uff0c\u987a\u5e8f\u4e5f\u6ca1\u6709\u98a0\u5012\uff0c\u4f5c\u4e3a\u6b63\u6837\u672c</p> <ul> <li>\u8d1f\u6837\u672c\uff0c\u4ece\u4e0d\u540c\u6587\u6863\u4e2d\u968f\u673a\u521b\u5efa\u4e00\u5bf9\u6bb5\u843d \u4f5c\u4e3a\u8d1f\u6837\u672c\uff0c\u533a\u522b\u5c31\u5728 \u8d1f\u6837\u672c\u662f\u4ece\u4e0d\u540c\u7684\u6587\u6863\u4e2d\u62bd\u53d6\u3001\u968f\u673a\u521b\u5efa\u4e00\u5bf9\u6bb5\u843d\uff0c\u4e5f\u5c31\u662f\u4e0d\u540c\u7684\u4e3b\u9898\uff0c\u968f\u4fbf\u521b\u5efa</li> </ul> <ul> <li>\u7f3a\u70b9\u662f\uff1a\u4e3b\u9898\u9884\u6d4b\u548c\u8fde\u8d2f\u6027\u9884\u6d4b\u5408\u5e76\u4e3a\u4e00\u4e2a\u5355\u9879\u4efb\u52a1</li> </ul> <ul> <li> <p>\u4e3b\u9898\u9884\u6d4b \u5c31\u662f \u5224\u65ad\u4e24\u4e2a\u6837\u672c \u662f\u5426\u6765\u81ea\u540c\u4e00\u7bc7\u6587\u6863</p> </li> <li> <p>\u8fde\u8d2f\u6027\u9884\u6d4b\u5c31\u662f \u5224\u65ad\u4e24\u4e2a\u6bb5\u843d \u662f\u4e0d\u662f\u987a\u5e8f\u5173\u7cfb</p> </li> </ul> <ul> <li> <p>\u4e3b\u9898\u9884\u6d4b\u662f\u6bd4\u8f83\u7b80\u5355\u7684\uff0c\u6240\u4ee5\u6574\u4e2a\u4efb\u52a1\u5728\u9884\u6d4b\u7684\u65f6\u5019\uff0c\u5c31\u5f88\u7b80\u5355</p> </li> <li> <p>\u4e5f\u5c31\u662f\u8bf4 \u76f8\u6bd4\u4e8e\u8fde\u8d2f\u6027\u9884\u6d4b\uff0c\u4e3b\u9898\u9884\u6d4b\u975e\u5e38\u5bb9\u6613\u5b66\u4e60</p> </li> <li> <p>\u8fd9\u4e5f\u662f\u540e\u7eed\u5f88\u591a\u4efb\u52a1 \u9a8c\u8bc1 NSP\u4efb\u52a1 \u6ca1\u6709\u6548\u679c\u7684\u539f\u56e0\uff0c\u662f\u56e0\u4e3a \u5b58\u5728\u4e3b\u9898\u9884\u6d4b\u8fd9\u4e00\u4e2a\u5355\u4efb\u52a1\uff0c\u8ba9\u6574\u4e2a\u9884\u6d4b\u4efb\u52a1\u7b80\u5355\u4e86</p> </li> <li> <p>\u800c\u540e\u7eed\u7684\u6539\u8fdb\uff0c\u6bd4\u5982 ALBert\uff0c\u76f4\u63a5\u5c31\u629b\u5f03\u6389\u4e86 \u4e3b\u9898\u9884\u6d4b\uff0c\u800c\u662f\u53bb\u505a\u7c7b\u4f3c\u8fde\u8d2f\u6027\u9884\u6d4b\u7684\u4efb\u52a1\uff0c\u5c31\u662f\u9884\u6d4b \u53e5\u5b50\u987a\u5e8f</p> </li> <li> <p>\u5bf9\u4e8ealbert\u6765\u8bf4\uff0c\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\u90fd\u662f\u6765\u81ea\u4e8e\u540c\u4e00\u7bc7\u6587\u6863</p> </li> </ul> <ul> <li>\u6b63\u6837\u672c\u5c31\u662f \u540c\u4e00\u7bc7\u6587\u6863\u4e2d\uff0c\u4e24\u4e2a\u53e5\u5b50 \u987a\u5e8f\u7684\u53e5\u5b50</li> <li>\u8d1f\u6837\u672c \u5c31\u662f \u4e24\u4e2a\u53e5\u5b50 \u98a0\u5012\u8fc7\u6765</li> <li>\u4f46\u662f \u6b63\u6837\u672c \u548c \u8d1f\u6837\u672c \u90fd\u662f\u6765\u81ea\u540c\u4e00\u6587\u6863</li> </ul>"},{"location":"learning/5_Bert/#3-bertbert","title":"3  \u5982\u4f55\u5fae\u8c03Bert\uff0c\u63d0\u5347Bert\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6548\u679c","text":"<p>\u63a5\u4e0b\u6765\u770b\uff0c\u5982\u4f55\u5728\u4e0b\u6e38\u4efb\u52a1 \u5fae\u8c03bert</p> <p></p> <p>\u4e3b\u8981\u5206\u4e3a\u56db\u79cd\uff1a</p> <ol> <li>\uff08a\uff09\u6587\u672c\u5206\u7c7b\uff0c\u8868\u793a\u53e5\u5b50\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u662f\u53e5\u5b50\u5bf9\u7684\u5206\u7c7b\u4efb\u52a1</li> <li>\uff08b\uff09\u5355\u4e2a\u53e5\u5b50\u7684\u5206\u7c7b\u4efb\u52a1</li> <li>\uff08c\uff09\u95ee\u7b54</li> <li>\uff08d\uff09\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1</li> </ol> <p>\u6700\u5e38\u7528\u7684\uff1a\u6587\u672c\u5206\u7c7b\u3001\u5e8f\u5217\u6807\u6ce8\u3001\u6587\u672c\u5339\u914d</p> <ul> <li>\u4ee5 \u5e8f\u5217\u6807\u6ce8 \u4e3a\u4f8b\uff1a\u628a\u6240\u6709\u7684 token \u8f93\u51fa \u8fdb\u884c\u4e00\u4e2asoftmax\uff0c\u5224\u8bfb \u5c5e\u4e8e \u5b9e\u4f53 \u4e2d\u7684\u54ea\u4e00\u4e2a</li> <li>\u5bf9\u4e8e\u5355\u4e2a\u6837\u672c\u7684 \u6587\u672c\u5206\u7c7b \u5c31\u662f \u4f7f\u7528 [CLS] \u7684\u8f93\u51fa\uff0c\u505a\u4e00\u4e2a\u5fae\u8c03\uff0c\u505a\u4e00\u4e2a\u4e8c\u5206\u7c7b \u6216\u8005 \u591a\u5206\u7c7b\uff0c\u672c\u8d28\u5c5e\u4e8e \u6587\u672c\u5339\u914d\u7684\u4efb\u52a1</li> <li>\u6587\u672c\u5339\u914d \u5c31\u662f\u628a\u4e24\u4e2a\u53e5\u5b50\u62fc\u63a5\u8d77\u6765\uff0c\u5224\u65ad\u53e5\u5b50\u662f\u5426\u76f8\u4f3c\uff0c\u7528[CLS]\u8f93\u51fa\uff0c\"0\"\u8868\u793a\u4e0d\u76f8\u4f3c\uff0c\"1\"\u8868\u793a\u76f8\u4f3c</li> </ul> <p>\u73b0\u5728\u8ba8\u8bba \uff1a</p> <p></p> <p>\u56e0\u4e3a \u5728\u5b9e\u9645\u5e94\u7528\u4e2d \u5f88\u5c11\u81ea\u5df1\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u4e00\u4e2abert\uff0c\u66f4\u591a\u7684\u662f\uff0c\u5229\u7528\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684bert\uff0c\u7136\u540e\u5728\u81ea\u5df1\u7684\u4efb\u52a1\u4e2d\u5fae\u8c03</p> <p></p> <p>\u73b0\u5728\u66f4\u4e00\u822c\u7684\u505a\u6cd5\u662f\uff1a</p> <ul> <li>\u5148\u83b7\u53d6 \u8c37\u6b4c\u4e2d\u6587 \u6216\u8005 \u5176\u4ed6\u516c\u53f8\u7684 bert</li> <li>\u7136\u540e\u57fa\u4e8e\u81ea\u5df1\u7684\u4efb\u52a1\u6570\u636e \u8fdb\u884c\u5fae\u8c03</li> </ul> <p>\u8981\u8ffd\u6c42\u66f4\u597d\u7684\u6027\u80fd\uff0c\u6709\u5f88\u591a\u7684trick\u53ef\u4ee5\u505a\uff0c\ud83d\udc47\ud83c\udffb</p>"},{"location":"learning/5_Bert/#trick","title":"\u7b2c\u4e00\u4e2atrick","text":"<p>\u628a \u4e24\u4e2a\u6b65\u9aa4 \u5206\u4e3a 4\u4e2a\u6b65\u9aa4\uff0c\u6bd4\u5982\u73b0\u5728\u505a \u5fae\u535a\u60c5\u611f\u5206\u6790</p> <ul> <li>\u9996\u5148\u5728\u5927\u91cf\u901a\u7528\u8bed\u6599\u505a\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u4e00\u6b65\u4e0d\u7528\u81ea\u5df1\u505a\uff0c\u901a\u5e38\u7528\u4e00\u4e2a\u4e2d\u6587\u8c37\u6b4c\u7684bert\u5c31\u53ef\u4ee5</li> <li>\u7b2c\u4e8c\u90e8\u5206\uff0c\u5c31\u662f\u5728\u76f8\u540c\u9886\u57df\u7684\u6587\u672c\u4e0a\uff0c\u7ee7\u7eed\u8bad\u7ec3language model\uff0c\u4e5f\u53eb\u505a \u9886\u57df\u7684\u81ea\u9002\u5e94 \u6216\u8005\u8bf4 \u9886\u57df\u8fc1\u79fb\uff0c\u4e5f\u5c31\u662f\u5728\u5927\u91cf\u7684\u5fae\u535a\u6587\u672c\u4e0a\u7ee7\u7eed\u8bad\u7ec3\u8fd9\u4e2abert</li> <li>\u7b2c\u4e09\u4e2a\u5c31\u662f\u5728\u4efb\u52a1\u76f8\u5173\u7684\u5c0f\u6570\u636e\u4e0a\u7ee7\u7eed\u8bad\u7ec3language model\uff0c\u5c31\u662f\u7b2c\u4e8c\u6b65\u4e2d\u5927\u91cf\u7684\u5fae\u535a\u6587\u672c\u4e2d\uff0c\u6709\u5f88\u591a\u4e0d\u5c5e\u4e8e \u5fae\u535a\u60c5\u611f\u5206\u6790\u7684\u6570\u636e\uff0c\u4f7f\u5f97\u6587\u672c\u66f4\u805a\u7126\u4e8e \u4e0e\u5fae\u535a\u60c5\u611f\u5206\u6790\u66f4\u76f8\u5173\u7684\u6570\u636e\uff0c\u7ee7\u7eed\u8bad\u7ec3language model</li> <li>\u7b2c\u56db\u4e2a\u6b65\u9aa4\uff0c\u5728\u5177\u4f53\u7684\u5fae\u535a\u60c5\u611f\u5206\u6790\u4efb\u52a1\uff0c\u6bd4\u5982\u6bd4\u8d5b\u7ed9\u7684\u6570\u636e\u96c6\u8fdb\u884c fine tune</li> <li>\u4e00\u822c\u7684\u7ecf\u9a8c\u5c31\u662f\u5148\u505a\u9886\u57df\u7684\uff0c\u7136\u540e\u518d\u505a\u4efb\u52a1\u7684\uff0c\u6700\u540e\u5fae\u8c03\u6027\u80fd\u662f\u6700\u597d\u7684</li> <li>\u76f8\u5f53\u4e8e\u628a\u4e0a\u9762\u7684\u4e24\u4e2a\u6b65\u9aa4\uff0c\u6269\u5c55\u62104\u4e2a\u6b65\u9aa4\uff0c\u6548\u679c\u4e00\u822c\u53ef\u4ee5\u63d0\u5230 3-4\u4e2a\u70b9\u5de6\u53f3</li> </ul>"},{"location":"learning/5_Bert/#32-trick","title":"3.2 \u7b2c\u4e8c\u4e2atrick","text":"<p>\u7b2c\u4e8c\u4e2a\u90e8\u5206</p> <p></p> <p>\u5728\u5927\u91cf\u7684\u5fae\u535a\u6587\u672c\u4e0a\u8bad\u7ec3bert\uff0c\u4e5f\u76f8\u5f53\u4e8e\u8bad\u7ec3bert</p> <p>\u4e5f\u6709trick\uff0c\u4e24\u4e2a\u5173\u4e8emask\u7684\ud83d\udc47\ud83c\udffb</p> <p></p> <ul> <li>\u7b2c\u4e00\u4e2a\uff0c\u52a8\u6001mask\uff0cbert\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u4f7f\u7528\u7684\u662f\u56fa\u5b9a\u7684mask\uff0c\u5c31\u662f\u628a\u6587\u672cmask\u4e4b\u540e\uff0c\u5b58\u5728\u672c\u5730\uff0c\u7136\u540e\u6bcf\u6b21\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u90fd\u662f\u4f7f\u7528\u540c\u4e00\u4e2a\u6587\u4ef6\uff0c\u5c31\u662f\u6bcf\u6b21\u8bad\u7ec3\u7684\u65f6\u5019\u4f7f\u7528\u7684\u90fd\u662f\u540c\u4e00\u4e2amask\u6807\u8bb0\uff0c\u4ee5 \"\u6211\u7231\u5403\u996d\" \u4e3e\u4f8b\uff0c\u6bcf\u6b21\u8bad\u7ec3\u90fd\u662fmask\u6389 \"\u5403\"\uff0c\u8fd9\u662f\u4e0d\u592a\u597d\u7684\uff1b\u6240\u4ee5\u52a8\u6001mask\u5c31\u662f \u6bcf\u6b21epoch\u8bad\u7ec3\u4e4b\u524d\uff0c\u7136\u540e\u518d\u5bf9\u6570\u636e\u8fdb\u884cmask\uff0c\u76f8\u5bf9\u4e8e\u6bcf\u4e2aepoch\u6765\u8bf4\uff0cmask\u7684\u5355\u8bcd\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u800c\u4e0d\u662f\u4e00\u76f4\u4f7f\u7528\u540c\u4e00\u4e2a\u6587\u4ef6</li> <li>\u7b2c\u4e8c\u4e2a n-gram mask</li> </ul>"},{"location":"learning/5_Bert/#33trick","title":"3.3\u7b2c\u4e09\u4e2atrick","text":"<p>\u53c2\u6570\u7684\u8bbe\u8ba1</p> <p></p>"},{"location":"learning/5_Bert/#4-bert","title":"4 Bert\u4ee3\u7801\u5b9e\u6218","text":"<p>\u6765\u81ea\uff1a</p> <p>\u6570\u5b66\u5bb6\u662f\u6211\u7406\u60f3</p>"},{"location":"learning/5_Bert/#5-bert","title":"5 bert\u539f\u7406","text":""},{"location":"learning/5_Bert/#51","title":"5.1 \u6587\u5b57\u63cf\u8ff0","text":"<p>\u9996\u5148\uff0cbert\u7528\u7684\u662f\u7684Transformer Encoder\u7684\u90e8\u5206</p> <p>\u63a5\u6536\u7684\u8f93\u5165\u662f <code>the cat sat on the mat</code></p> <p></p> <p>\u8f93\u5165\u5230\u7f51\u7edc\u4e2d\uff0c\u9996\u5148\u505a\u4e00\u4e2aembedding</p> <p>embedding\u5728Transformer\u4e2d\u63a5\u6536\u4e24\u4e2a\u4e1c\u897f= word embedding+Position embedding\uff08encoding\uff09</p> <p>\u901a\u8fc7embedding\u4e4b\u540e\uff0c\u5f97\u5230\u6bcf\u4e2a\u8bcd\u5bf9\u5e94\u7684\u8f93\u51fa</p> <p>\u63a5\u7740\u628a\u6bcf\u4e2a\u8bcd\u7684\u8f93\u51fa\uff0c\u9001\u5165\u5230Transformer Encoder\u4e2d\uff1a</p> <p></p> <p>Encoder\u4e2d\u5305\u542b\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5\u3001\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001LayerNorm\uff0c\u518d\u7ecf\u8fc7FFN \u524d\u9988\u5168\u8fde\u63a5\u7f51\u7edc</p> <p>\u63a5\u7740\u5806\u53e0\u8fd9\u4e2aEncoderLayer\u7684\u7ed3\u6784\uff08\u7ee7\u7eed\u6b8b\u5dee\u8fde\u63a5....\uff09\u5806\u53e06\u5c42</p> <p>Bert\uff0c\u9996\u5148\u968f\u673a\u63a9\u76d6\u4e00\u4e9b\u8bcd\uff1a</p> <p></p> <p>\u6bd4\u5982 \u7b2c\u4e8c\u4e2a\u8bcd mask\u6389</p> <p></p> <p>\u7f51\u7edc\u8f93\u51fa \u9884\u6d4b\u7684\u6982\u7387\uff0c\u4f7f\u5f97\u9884\u6d4b\u7684\u6982\u7387\u65e0\u9650\u63a5\u8fd1 cat\u7684\u72ec\u70ed\u7f16\u7801\u5411\u91cf</p> <p></p> <p></p> <p>\u4ee5\u4e0a\u662fbert\u53ef\u4ee5\u89e3\u51b3\u7684\u7b2c\u4e00\u4e2a\u4efb\u52a1</p> <p>\u7b2c\u4e8c\u4e2a\u4efb\u52a1\uff1a\u9884\u6d4b\u4e0b\u4e00\u4e2a\u53e5\u5b50</p> <p></p> <p>\u9884\u6d4b\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u662f\u8fde\u7eed\u53e5\u5b50</p> <p></p> <p>\u6216\u8005\u5224\u65ad\"it was developed by newton and leibniz\"\u662f\u4e0d\u662f \"calculus is a branch of math\"\u7684\u4e0b\u4e00\u4e2a\u53e5\u5b50</p> <p>\u8f93\u51fa\u5e94\u8be5\u662f true\u6216\u8005\u662ffalse</p> <p></p> <p>\u90a3\u5177\u4f53\u600e\u4e48\u505a\u5462\uff1f</p> <p>\u9996\u5148\u5728\u53e5\u5b50\u7684\u5f00\u5934\u52a0\u5165 [CLS] token\uff0c\u5728\u4e24\u4e2a\u53e5\u5b50\u7684\u4e2d\u95f4\u52a0\u5165 [SEP]</p> <p></p> <p>\u7136\u540e\u7ecf\u8fc7 \u4e24\u4e2a Encoder \u5f97\u5230\u4e24\u4e2a\u8f93\u51fa</p> <p>\u7136\u540e\u6211\u4eec\u628a [CLS] token\u7684 \u5bf9\u5e94\u7684 C\u8fdb\u884c\u4e00\u4e2a \u4e8c\u5206\u7c7b \u4efb\u52a1</p> <p></p> <p>\u901a\u8fc7nn.Linear\u6620\u5c04\u4e3a\u4e00\u4e2a\u4e24\u7ef4\u7684?\u5411\u91cf\uff0c\u6bd4\u59820\u6216\u80051</p> <p>\u95ee\u9898\uff1a\u4e3a\u4ec0\u4e48\u4e0d\u7528 first sentence\u7684\u7b2c\u4e00\u4e2a\u8bcd \u6bd4\u5982 calculus \u5bf9\u5e94\u7684 \\(u_1\\) \u8fdb\u884c\u4e8c\u5206\u7c7b\u5462\uff1f\u6216\u8005 \\(u_2\\) \u5462\uff1f</p> <p>\u56de\u7b54</p> <p></p> <p>\u81ea\u6ce8\u610f\u529b\u673a\u5236 \u81ea\u5df1\u5bf9\u81ea\u5df1\u7684\u5173\u6ce8\u6700\u5927\uff0c\u5982\u679c\u6a21\u578b\u66f4\u5173\u6ce8\u6709\u610f\u4e49\u7684\u8bcd\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u7ed3\u679c\uff0c\u6240\u4ee5[CLS]\u76f8\u5f53\u4e8e\u5360\u4f4d\u7b26\uff0c\u5305\u542b\u6240\u6709\u4fe1\u606f</p> <p>\u63a5\u4e0b\u6765\u8ba1\u7b97 loss\uff0c\u7136\u540ebackward\u5373\u53ef</p> <p></p>"},{"location":"learning/5_Bert/#52-bert","title":"5.2 bert\u7684\u5177\u4f53\u65b9\u6cd5\u7ed3\u5408\u4e24\u79cd\u4efb\u52a1","text":"<p>\u6bd4\u5982\u8bf4 \u6709\u4e24\u53e5\u8bdd\uff1a</p> <p></p> <ul> <li>\u5728\u6bcf\u4e00\u53e5\u8bdd\u5f53\u4e2d\uff0c\u8fdb\u884cmask\uff0c\u7136\u540e\u628a\u4e24\u53e5\u8bdd\u62fc\u8d77\u6765</li> <li>\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u7684\u5b66\u4e60\u65b9\u6cd5</li> <li>\u9996\u5148\u5224\u65ad\u662f\u5426\u4e3a\u8fde\u7eed\u7684\u4e24\u53e5\u8bdd\uff0c<code>true or false</code></li> <li>\u5e76\u4e14\u8ba1\u7b97 mask\u7684\u4f4d\u7f6e\u662f\u4ec0\u4e48\u8bcd</li> <li>\u6240\u4ee5bert\u6a21\u578b\u6709\u5f88\u591aloss\uff0c\u6bd4\u5982\u5206\u7c7b\u662f\u4e0d\u662f\u8fde\u7eed\u7684\u4e24\u53e5\u8bdd \u662f\u4e00\u4e2aloss\uff0c\u6709\u591a\u4e2amask\u5c31\u6709\u591a\u4e2aloss\uff0c\u628a\u6240\u6709\u7684loss\uff0c\u5168\u90e8\u52a0\u8d77\u6765\uff0c\u6c42\u548c</li> </ul> <p></p> <ul> <li>\u6c42\u548c\u4e4b\u540e\uff0c\u5728\u8fdb\u884cbackward\uff0c\u5c31\u662f <code>sum of the three loss functions</code></li> </ul> <p>\u4ee5\u4e0a\u662fbert\u7684\u6574\u4e2a\u8fc7\u7a0b</p>"},{"location":"learning/5_Bert/#53-mlm","title":"5.3 MLM\u5b9e\u9645\u4f8b\u5b50","text":"<p>\u63a5\u4e0b\u6765\u7ec6\u8282\uff1a</p> <p>\u9996\u5148\u9884\u6d4b\u54ea\u4e2a\u8bcd\uff1f</p> <p></p> <p>\u5047\u5982\u6709100\u4e2a\u8bcd\uff0c\u6211\u4eec\u9009\u51fa15\u4e2a\u8bcd</p> <p>\u5728\u8fd915\u4e2a\u8bcd\u5f53\u4e2d\uff1a</p> <ol> <li>\u6bcf\u4e2a\u8bcd\u670980%\u7684\u6982\u7387 \u4f1a\u88ab\u6539\u4e3a mask\uff0c\u6bd4\u5982  \"my dog is hairy\" \u6539\u6210 \"my dog is [MASK]\"</li> <li>\u6bcf\u4e2a\u8bcd \u8fd8\u670910%\u7684\u6982\u7387\uff0c\u4f1a\u88ab\u66ff\u6362\u6210\u4efb\u610f\u4e00\u4e2a\u5176\u4ed6\u7684 token\uff0c\u6bd4\u5982 \"my dog is hairy\"\u6539\u6210 \"my dog is apple\"\uff08\u601d\u8003\u4e3a\u4ec0\u4e48\u8fd9\u6837\u505a\uff1f\u540e\u9762\u4f1a\u8bf4\uff09</li> <li>\u6bcf\u4e2a\u8bcd \u8fd8\u6709 10%\u7684\u6982\u7387 \u539f\u5c01\u4e0d\u52a8\uff0c\"my dog is hairy\" \u8fd8\u662f \"my dog is hairy\"</li> </ol> <p>\u7b2c1\u70b9\u548c\u7b2c2\u70b9\u5f88\u597d\u7406\u89e3\uff0c\u90a3\u4e3a\u4ec0\u4e48\u63d0\u51fa\u7b2c3\u70b9\u5462\uff1f</p> <p>\u8fd9\u662f\u5f88\u5de7\u5999\u7684\u5730\u65b9\uff0c\u5e0c\u671b\u6a21\u578b \u8f93\u51fa\u7684 hairy  \u8fd8\u662f hairy\uff0c\u8fd9\u6837\u505a\u7684\u597d\u5904\u5728\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u8ba9\u6a21\u578b\u771f\u6b63\u7406\u89e3\u8fd9\u4e2a\u53e5\u5b50\uff0c\u56e0\u4e3a\"my dog is hairy \"\u53ef\u80fd\u4f1a\u9884\u6d4b\u51fa\u522b\u7684\u8f93\u51fa\uff0c\u6bd4\u5982\"my dog is jack\"\u3001\"my dog is merry\"\uff0c\u5c31\u4e0d\u662f \"hairy\" \u7684\u610f\u601d \uff0c\u4e22\u5931\u539f\u59cb\u7684\u8bed\u4e49</p> <p>\u5173\u4e8e\u7b2c2\u70b9\uff1a</p> <p></p> <p>\u4e00\u53e5\u8bdd\uff1a\u4e3a\u4e86\u9632\u6b62\u6a21\u578b\u8fc7\u4e8e\u76f8\u4fe1 \u5f53\u524d\u7684\u8bcd\uff0c\u800c\u662f\u5e0c\u671b\u6a21\u578b\u53bb\u89c2\u5bdf\u4e0a\u4e0b\u6587</p> <p>\u4ee5\u4e0a\u662fbert\u7684\u7b2c\u4e00\u4e2a\u4efb\u52a1 MLM\uff1a\u63a9\u76d6\u4e00\u4e9b\u8bcd\uff0c\u8fdb\u884c\u9884\u6d4b</p> <p></p>"},{"location":"learning/5_Bert/#531-nsp","title":"5.3.1 NSP\u5b9e\u9645\u4f8b\u5b50","text":"<p>\u9996\u5148\u5bf9\u8f93\u5165\u53e5\u5b50</p> <ol> <li>\u52a0\u5165 \u7279\u6b8a\u5b57\u7b26[SEP]\uff08seperate\uff09[CLS]\uff08class\uff09</li> <li>bert\u6709 \u4e09\u4e2aembedding\u3001Transformer\u67092\u4e2aembedding\uff081\u4e2aembedding+1\u4e2aencoding\uff09\uff0c\u6709\u4e24\u70b9\u4e0d\u540c</li> </ol> <p>\uff081\uff09bert\u591a\u4e86\u4e00\u4e2a segment embedding\uff0c\u4f5c\u7528\u662f \u5224\u65ad\u53e5\u5b50\u5c5e\u4e8e\u7b2c\u4e00\u53e5\u8fd8\u662f\u7b2c\u4e8c\u53e5\uff0c\u5c31\u662f\u5f88\u7b80\u5355\u7684\uff0c\u6bd4\u5982\\(E_A\\)\u5168\u662f\u7b2c\u4e00\u53e5\u7684\uff0c\u90a3\u5c31\u5168\u90e8\u5199\u6210 0\uff0c\\(E_B\\) \u662f\u7b2c\u4e8c\u53e5\uff0c\u90a3\u5c31\u5168\u90e8\u5199\u62101</p> <p></p> <p>\uff082\uff09\u7b2c\u4e8c\u70b9\u4e0d\u540c\uff0cpositional embedding\u5728bert\u4e2d\u662f\u53ef\u8bad\u7ec3\u7684\uff0c\u7c7b\u4f3c <code>nn.embedding</code> \uff0c\u5728Transformer\u4e2d\u662f\u4e09\u89d2\u51fd\u6570\u8868\u793a\u7684\u5e38\u6570\u4f4d\u7f6e\u7f16\u7801</p> <p>\u6700\u540e\u5c06\u4e09\u4e2aembedding\u76f8\u52a0\uff0c\u5f97\u5230embedding layer\u7684\u8f93\u51fa</p> <p></p> <p></p>"},{"location":"learning/5_Bert/#532-multi-task-learning","title":"5.3.2 Multi-Task Learning","text":""},{"location":"learning/5_Bert/#533","title":"5.3.3 \u5fae\u8c03\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1","text":"<p>\u5982\u4f55\u4f7f\u7528 bert fine tune\u5e94\u7528\u5230\u522b\u7684\u4efb\u52a1\u4e0a</p>"},{"location":"learning/5_Bert/#_1","title":"\u7b2c\u4e00\u79cd\u4efb\u52a1\uff1a\u5206\u7c7b\u95ee\u9898","text":"<p>\u60c5\u611f\u5206\u6790\u3001\u6587\u672c\u5206\u7c7b</p> <p>\u9996\u5148[CLS]\u6807\u5fd7\uff0c\u63a5\u7740\u628a\u53e5\u5b50\u6216\u8005\u6587\u7ae0\u653e\u5230\u540e\u9762 \\(w_1\u3001w_2\u3001w_3\\)</p> <p>bert\u8fd9\u91cc\u7684\u53c2\u6570\u53ef\u4ee5\u662ffrozen\u51bb\u4f4f\u7684\uff0c\u6216\u8005fine_tune \u5fae\u8c03\u7684</p> <p>[CLS] \u7684\u8f93\u51fa \u7ecf\u8fc7 \u4e00\u4e2a\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u7ebf\u6027\u5206\u7c7b\u5668 \u662f\u4ece\u5934\u5f00\u59cb\u5b66\u4e60\u7684\uff0c\u5f97\u5230\u4e00\u4e2aclass\u8f93\u51fa</p>"},{"location":"learning/5_Bert/#slot-filling","title":"\u7b2c\u4e8c\u79cd\u4efb\u52a1:slot filling","text":"<p>slot filling?</p> <p>\u6bd4\u5982\u6709\u4e00\u4e2a\u53e5\u5b50 \"arrive Taipei on November 2nd\" \uff0c\u6807\u6ce8\uff1a\"Taipei\" \u662f\" \u5730\u70b9\"\u3001\"November\" \u662f\" \u65f6\u95f4\" </p> <p>\u5c31\u662f\u628a\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8bcd \u8fdb\u884c\u4e00\u4e2a\u5206\u7c7b\uff0c\u6bd4\u5982 \"on\" \u662f \"other\" \u7c7b\u522b\uff0c\"Taipei\" \u662f\" \u5730\u70b9\"\u7c7b\u522b</p> <p>\u6bcf\u4e2a\u8bcd \u90fd\u8fdb\u884c\u4e00\u4e2a\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u5f97\u5230\u4e00\u4e2a\u8f93\u51fa</p>"},{"location":"learning/5_Bert/#_2","title":"\u7b2c\u4e09\u79cd\u4efb\u52a1\uff1a\u81ea\u7136\u8bed\u8a00\u63a8\u7406","text":"<p>\u81ea\u7136\u8bed\u8a00\u63a8\u7406</p> <p>\u5c31\u662f\u7ed9\u5b9a\u4e00\u4e2a\u524d\u63d0\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u5047\u8bbe</p> <p>\u5e0c\u671bbert\u63a8\u7406\u51fa true\u3001false\u3001unknown\uff08\u4e0d\u77e5\u9053\uff09</p> <p>\u4f8b\u5b50\uff1a</p> <p>\u7ed9\u51fa\u524d\u63d0\uff1a\u5730\u7403\u56f4\u7ed5\u592a\u9633\u8f6c</p> <p>\u5047\u8bbe\uff1a\u660e\u5929\u6211\u4eec\u8981\u8003\u8bd5</p> <p>\u4e24\u8005\u4e4b\u95f4\u6ca1\u6709\u4efb\u4f55\u5173\u7cfb\uff0c\u50cf\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5c31\u8981\u5224\u522b\u5047\u8bbe\u662f\u5bf9\u7684\u5417\uff1f\u5176\u5b9e\u662f\u4e0d\u77e5\u9053\uff0c\u6240\u4ee5\u8f93\u51fa unknown</p> <p>\u56e0\u6b64\u8fd9\u4e2a\u5c5e\u4e8e\u4e09\u5206\u7c7b\u7684\u95ee\u9898</p> <p></p> <ul> <li>\u524d\u63d0 \u548c \u5047\u8bbe \u901a\u8fc7 [SEP]\u5206\u9694\u5f00</li> <li>[CLS]  token \u62ff\u51fa\u6765 \u901a\u8fc7\u7ebf\u6027\u5206\u7c7b\u5668 \u9884\u6d4b\uff0c\u5f97\u5230\u8f93\u51fa true\u3001false\u3001unknown</li> </ul>"},{"location":"learning/5_Bert/#_3","title":"\u7b2c\u56db\u79cd\u4efb\u52a1\uff1a\u95ee\u7b54","text":"<p>\u95ee\u7b54</p> <p>\u6bd4\u5982 \u73b0\u5728\u6709\u4e00\u7bc7\u6587\u7ae0\uff0c\u73b0\u5728\u6709\u4e00\u4e2a\u95ee\u9898\uff0c\u8981\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898</p> <p>\u9996\u5148\uff0c\u5047\u8bbe \u95ee\u9898\u7684\u7b54\u6848\u4e00\u5b9a\u662f\u539f\u6587\u4e2d\u51fa\u73b0\u7684\u8bcd \u6216\u8005 \u8fde\u7eed\u7684\u53e5\u5b50</p> <p></p> <p>\u6bd4\u5982\u8fd9\u4e2a\u95ee\u9898\u7684\u7b54\u6848\uff0c\u662f\u539f\u6587\u7684\u7b2c17\u4e2a\u8bcd\u5230\u7b2c17\u4e2a\u8bcd\uff08s\u8868\u793a\u5f00\u59cb\uff0ce\u8868\u793a\u7ed3\u675f\uff09</p> <p>\u5177\u4f53\u6765\u8bf4\uff1a</p> <p></p> <p>\u5148\u628aquestion\u8fd9\u4e2a\u53e5\u5b50 \u653e\u5230\u524d\u9762\uff0c[SEP]\u5206\u9694\u4e24\u4e2a\u53e5\u5b50\uff0cdocument\u653e\u5230\u540e\u9762</p> <p>\u6700\u524d\u9762 \u52a0\u5165 [CLS]</p> <p>\u901a\u8fc7bert\uff0c\u5bf9document\u4e2d \u6bcf\u4e2a\u8bcd\u7684\u8f93\u51fa \u9ec4\u8272\u7684\u5411\u91cf \u4e58\u4ee5\u4e00\u4e2a \u6a59\u8272\u7684\u5411\u91cf\uff0c\u6a59\u8272\u7684\u5411\u91cf \u5c31\u662f nn.Linear\uff0c\u8fdb\u884c\u4e00\u4e2adot product\uff0c\u7136\u540e\u7ecf\u8fc7\u4e00\u4e2asoftmax \uff0c\u5f97\u5230\u6982\u7387\u503c\uff0c\u5176\u4e2d\u6700\u5927\u503c\u6982\u7387\u4e3a0.5\uff0c\u5bf9\u5e94 \u7b2c\u4e8c\u4e2a\u8bcd\uff0c\u4e5f\u5c31\u662f\u8bf4 \u95ee\u9898\u7684\u7b54\u6848\u4ece\u6587\u7ae0\u7684\u7b2c\u4e8c\u4e2a\u8bcd\u4f5c\u4e3a\u5f00\u59cb\uff0c\u90a3\u7ed3\u675f\u5462\uff1f</p> <p></p> <p>\u7ed3\u675f\u5c31\u662f \u84dd\u8272\u7684\u5411\u91cf \u4e58\u4ee5 \u9ec4\u8272\u7684\u5411\u91cf \u8fdb\u884c dot product\uff0c\u7136\u540e\u901a\u8fc7softmax\uff0c\u5f97\u5230\u6982\u7387\u6700\u5927\u503c=0.7\uff0c\u5bf9\u5e94\u7b2c\u4e09\u4e2a\u8bcd\uff0c\u5c31\u8ba4\u4e3ae = d\u7b2c\u4e09\u4e2a\u8bcd</p> <p>\u4e5f\u5c31\u662f\u8bf4\u7b54\u6848 \u4ece \u7b2c\u4e8c\u4e2a\u8bcd \u5230 \u7b2c\u4e09\u4e2a\u8bcd</p> <p>\u8fd8\u6709\u4e00\u4e2a\u95ee\u9898\uff1as&gt;e?</p> <p></p> <p>\u4e3e\u4e2a\u4f8b\u5b50\uff1a\u6587\u6863\u662f \u5b5f\u5b50\u5b54\u5b50\u4e2d\u56fd\u5386\u53f2\uff0c\u95ee\u9898\uff1a\u795e\u5dde\u4e94\u53f7\u4ec0\u4e48\u65f6\u5019\u53d1\u5c04\u3002\u8fd9\u65f6\u5c31\u662f\u6ca1\u6709\u7b54\u6848\u7684\u3002</p>"},{"location":"learning/5_Bert/#54","title":"5.4 \u4ee3\u7801\u5b9e\u6218","text":"<p>Rereference1</p> <p>Reference2</p>"},{"location":"learning/6_Diffusion/","title":"DDPM","text":"<p>Probabilistic Diffusion Model</p> <p>What are Diffusion Models?</p> <p>Probabilistic Diffusion Model\u6982\u7387\u6269\u6563\u6a21\u578b\u7406\u8bba\u4e0e\u5b8c\u6574PyTorch\u4ee3\u7801\u8be6\u7ec6\u89e3\u8bfb</p> <p>\u600e\u4e48\u7406\u89e3\u4eca\u5e74 CV \u6bd4\u8f83\u706b\u7684\u6269\u6563\u6a21\u578b\uff08DDPM\uff09\uff1f</p> <p>Diffusion Model\u5b66\u4e60\u7b14\u8bb0(1)\u2014\u2014DDPM</p> <p>\u53c2\u8003\u6587\u732e\uff1a</p> <p>\uff081\uff092015\u5e74 \uff1a \u57fa\u4e8e\u975e\u5e73\u8861\u70ed\u529b\u5b66\u8fdb\u884c\u6df1\u5ea6\u65e0\u76d1\u7763\u5b66\u4e60</p> <pre><code>@inproceedings{10.5555/3045118.3045358,\nauthor = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},\ntitle = {Deep unsupervised learning using nonequilibrium thermodynamics},\nyear = {2015},\npublisher = {JMLR.org},\nbooktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},\npages = {2256\u20132265},\nnumpages = {10},\nlocation = {Lille, France},\nseries = {ICML'15}\n}\n</code></pre> <p>\u6df1\u5ea6\u65e0\u76d1\u7763\u5b66\u4e60\u5c31\u662f\u751f\u6210\u7167\u7247</p> <p></p> <p>\uff082\uff092020\u5e74\uff1a\u53bb\u566a\u6982\u7387\u6a21\u578b</p> <p></p> <pre><code>@inproceedings{10.5555/3495724.3496298,\nauthor = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},\ntitle = {Denoising diffusion probabilistic models},\nyear = {2020},\nisbn = {9781713829546},\npublisher = {Curran Associates Inc.},\naddress = {Red Hook, NY, USA},\nbooktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},\narticleno = {574},\nnumpages = {12},\nlocation = {Vancouver, BC, Canada},\nseries = {NIPS '20}\n}\n</code></pre> <ul> <li>\u6269\u6563\u6a21\u578b\u5f00\u59cb\u6d41\u884c\u5b9e\u884c2020\u5e74\u7684\u8fd9\u7bc7\u8bba\u6587\uff0c2021\u5e74\u30012022\u5e74\u5f00\u59cb\u6709\u5927\u91cf\u8bba\u6587\u51fa\u73b0</li> <li> <p>\u76ee\u524d\uff1aDDPM 2020\u5e74\u8fd9\u7bc7\u8bba\u6587\u7684\u5f15\u7528\u91cf\u5df2\u7ecf220\u4e07\uff0c2015\u5e74\u8bba\u6587\u7684\u5f15\u7528\u91cf\u4e5f\u6709200\u591a\u4e07</p> </li> <li> <p>\u6269\u6563\u6a21\u578b\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u6a21\u578b\uff1aGAN\u7684\u4efb\u52a1\u3001VAE\u7684\u4efb\u52a1\u3001FLOW\u7684\u4efb\u52a1\u90fd\u53ef\u4ee5\u7528Diffusion Model</p> </li> </ul>"},{"location":"learning/6_Diffusion/#1","title":"1 \u6c47\u603b\u751f\u6210\u6a21\u578b","text":"<p>\uff081\uff09\u7b2c\u4e00\u7c7b\u751f\u6210\u6a21\u578b\uff1aSeq2Seq\u6a21\u578b\uff0c\u81ea\u56de\u5f52\u7684\u89e3\u7801\u6a21\u578b</p> <p>\uff082\uff09\u7b2c\u4e8c\u7c7b\u751f\u6210\u6a21\u578b\uff1a\u57fa\u4e8eGAN\u7684\u6a21\u578b\uff0c\u6ca1\u6709\u663e\u5f0f\u7684\u5bf9\u76ee\u6807\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u53ea\u662f\u5c06\u751f\u6210\u7684\u6570\u636e\u653e\u5230\u4e0b\u6e38\u7684\u5224\u522b\u5668\u4e2d\uff0c\u4ee5\u5bf9\u6297\u7684\u65b9\u5f0f\u4f7f\u5f97\u751f\u6210\u5668\u8fbe\u5230\u7406\u60f3\u7684\u76ee\u6807\u5206\u5e03</p> <p>\uff083\uff09\u7b2c\u4e09\u7c7b\u751f\u6210\u6a21\u578b\uff1aFLOW\u6a21\u578b\uff1aFLOW\u6a21\u578b\u7684\u6570\u5b66\u539f\u7406\u6bd4\u8f83\u4e25\u8c28\uff0c\u662f\u4e00\u4e2a\u5b8c\u5168\u53ef\u9006\u7684\u8fc7\u7a0b\uff0c\u4e3a\u4e86\u8ba9FLOW\u6a21\u578b\u53d8\u5f97\u53ef\u89e3\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u4e9b\u5de7\u5999\u7ed3\u6784\uff0c\u4f7f\u5f97\u5bf9\u6570\u4f3c\u7136\u5b8c\u5168\u53ef\u89e3\uff0c\u4e5f\u662f\u56e0\u4e3a\u8bbe\u8ba1\u7684\u5de7\u5999\u7684\u7ed3\u6784\uff0c\u9650\u5236\u4e86FLOW\u7684\u6027\u80fd</p> <p>\uff084\uff09\u7b2c\u56db\u7c7b\u751f\u6210\u6a21\u578b\uff1aVAE</p> <p>\uff085\uff09\u7b2c\u4e94\u7c7b\u751f\u6210\u6a21\u578b\uff1aDiffusion\uff1a\u5176\u4e2dVAE\u548cDiffusion\u6a21\u578b\u5176\u5b9e\u6709\u70b9\u50cf</p> <p></p>"},{"location":"learning/6_Diffusion/#2","title":"2 \u524d\u7f6e\u6570\u5b66\u77e5\u8bc6","text":""},{"location":"learning/6_Diffusion/#kl","title":"\u4e00\u3001\u6761\u4ef6\u6982\u7387\u516c\u5f0f\u4e0e\u9ad8\u65af\u5206\u5e03\u7684KL\u6563\u5ea6","text":"<p>1\u3001\u6761\u4ef6\u6982\u7387\u7684\u4e00\u822c\u516c\u5f0f</p> <ul> <li> <p> \uff081\uff09P(A,B,C)=P(A)P(B|A)P(C|A,B)</p> </li> <li> <p> \uff082\uff09P(B,C|A) = P(B|A)P(C|A,B)</p> </li> </ul> <p>2\u3001\u57fa\u4e8e\u9a6c\u5c14\u79d1\u592b\u5047\u8bbe\u7684\u6761\u4ef6\u6982\u7387</p> <p>\u4ec0\u4e48\u53eb \u9a6c\u5c14\u79d1\u592b\u5047\u8bbe\uff1f</p> <p>\u5f53\u524d\u65f6\u523b\u7684\u8f93\u51fa \u53ea\u4e0e \u4e0a\u4e00\u65f6\u523b\u6709\u5173\uff0c\u8ddf\u8fc7\u53bb\u4ee5\u53ca\u66f4\u8fdc\u7684\u65e0\u5173</p> <p>\u5982\u6211\u4eec\u73b0\u5728\u6709\u9a6c\u5c14\u79d1\u592b\u94fe\uff1aA\u2192B\u2192C\uff0c\u5219\u4e0a\u9762\u7684\uff081\uff09\u3001\uff082\uff09\u53ef\u4ee5\u8fdb\u884c\u7b80\u5316\uff1a</p> <p>P(A,B,C)\u53ef\u4ee5\u5199\u6210\uff1aP(A,B)P(C|A,B) \u7ee7\u7eed\u5199\u6210\uff1aP(A)P(B|A)P(C|A,B)</p> <p>\u56e0\u4e3a A\u662fB\u7684\u4e0a\u4e00\u65f6\u523b\uff0c\u6240\u4ee5\u4fdd\u7559 P(B|A)</p> <p>\u56e0\u4e3aA\u8ddd\u79bbC\u6bd4\u8f83\u8fdc\uff0c\u4e0d\u518d\u662fC\u7684\u4e0a\u4e00\u65f6\u523b\uff0c\u6240\u4ee5 P(C|A,B)\u5199\u6210P(C|B)</p> <p>\u6240\u4ee5\u5728\u6ee1\u8db3\u9a6c\u5c14\u79d1\u592b\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u6761\u4ef6\u6982\u7387\u5199\u6210\uff1a</p> <ul> <li> \uff083\uff09P(A,B,C)=P(A)P(B|A)P(C|B)</li> </ul> <p>\u540c\u7406\u7b80\u5316P(B,C|A)\uff0c\u5f97\u5230\uff1a</p> <ul> <li> \uff084\uff09P(B,C|A) = P(B|A)P(C|B)</li> </ul> <p>3\u3001\u4e24\u4e2a\u9ad8\u65af\u5206\u5e03\u7684KL\u6563\u5ea6\u516c\u5f0f </p> <p>\u660e\u767dVAE\u7684\u8bdd\uff0cKL\u6563\u5ea6\u516c\u5f0f\u4f1a\u5f88\u6e05\u695a</p> <p></p> <p>\u5bf9\u4e8e\u4e24\u4e2a\u5355\u4e00\u53d8\u91cf\u7684\u9ad8\u65af\u5206\u5e03p\u548cq\u6765\u8bf4\uff0c\u5747\u503c\u662f \\(\\mu_1\\) \u548c \\(\\mu_2\\) \uff0c\u65b9\u5dee\u662f \\(\\sigma_1^2\\)\u3001\\(\\sigma_2^2\\)</p> <p>\u5219\u5b83\u4eec\u7684KL\u6563\u5ea6\u516c\u5f0f\uff1a</p> <p>\\(KL(p,q)=log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma^2+(\\mu_1-\\mu_2)^2}{2\\sigma^2_2}-\\frac{1}{2}\\)</p> <p>4\u3001\u53c2\u6570\u91cd\u6574\u5316\u7684\u6280\u5de7</p> <p>\u4ec0\u4e48\u662f\u53c2\u6570\u91cd\u6574\u5316\uff1f</p> <p>\u5047\u8bbe\u5e0c\u671b\u4ece \u5747\u503c\u4e3a \\(\\mu\\)\u3001\u65b9\u5dee\u4e3a \\(\\sigma^2\\) \u7684\u9ad8\u65af\u5206\u5e03 \\(N(\\mu,\\sigma^2)\\) \u4e2d\u91c7\u6837\uff0c\u5982\u679c\u76f4\u63a5\u91c7\u6837\u7684\u8bdd\uff0c\u4f1a\u9020\u6210\u4ec0\u4e48\u95ee\u9898\u5462\uff1f</p> <p>\u5982\u679c \\(\\mu\\) \u548c  \\(\\sigma\\) \u662f\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u51fa\u6765\u7684\u8bdd\uff0c\u76f4\u63a5\u53bb\u91c7\u6837\\(N(\\mu,\\sigma)\\)\uff0c\u5c31\u4f1a\u5bfc\u81f4\u4e0e \\(\\mu\\) \u548c\\(\\sigma\\) \u53c2\u6570\u65ad\u5f00\u4e86\uff0c\u68af\u5ea6\u65e0\u6cd5\u4f20\u56de\u53bb\u4e86\uff0c\u56e0\u4e3a\u91c7\u6837\u7684\u8fc7\u7a0b\u662f\u4e0d\u53ef\u5bfc\u7684\uff1b\u6240\u4ee5\u4e3a\u4e86\u8ba9\u91c7\u6837\u51fa\u6765\u7684\u6837\u672c\u8ddf \\(\\mu\\) \u548c \\(\\sigma\\) \u4e4b\u95f4\uff0c\u4ecd\u7136\u53ef\u4ee5\u68af\u5ea6\u53ef\u4f20\u64ad\u7684\uff0c\u4ecd\u7136\u662f\u53ef\u5bfc\u7684\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u628a\u8fd9\u4e2a\u91c7\u6837\u8fc7\u7a0b \u7b49\u4ef7\u4e8e \u9996\u5148\u4ece \\(N(0,1)\\) \u7684\u6807\u51c6\u5206\u5e03\u4e2d\uff0c\u91c7\u6837\u51fa z\uff0c\u628a z \u5f53\u6210\u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u518d\u628az\u4e58\u4ee5\u539f\u6765\u5206\u5e03\u7684 \\(\\sigma\\) \u548c \\(\\mu\\) \uff0c\u4fdd\u8bc1\u4e86\u68af\u5ea6\u53ef\u4ee5\u56de\u4f20</p> <p>\u8fd9\u6837\u505a\u7684\u597d\u5904\u662f\uff0c\u8fd9\u6837\u7684\u91c7\u6837\u8fc7\u7a0b\u8f6c\u79fb\u5230\u4e86 z\u4e0a\uff0c   z\u53ef\u4ee5\u770b\u505a\u662f \u7f51\u7edc\u7684\u8f93\u5165    \uff0c\u6216\u8005\u5f53\u6210\u4e00\u4e2a\u5e38\u6570\uff0c\u91c7\u6837\u503c \\(y=\\sigma z+\\mu\\)\uff0c\\(y\\)\u4e0e \\(\\sigma\\) \u548c \\(y\\)\u4e0e \\(\\mu\\) \u4e4b\u95f4\u662f\u5b8c\u5168\u53ef\u5bfc\u7684</p> <p>\u5982\u679c\u4e0d\u8fd9\u6837\u505a\u7684\u8bdd\uff0c\\(y\\)\u5bf9\\(\\mu\\) \u548c \\(y\\)\u5bf9 \\(\\sigma\\) \u7684\u5bfc\u6570\u662f\u7b97\u4e0d\u51fa\u6765\u7684</p> <p>\u4ee5\u4e0a\u79f0\u4e3a\u53c2\u6570\u91cd\u6574\u5316</p> <p></p> <p>\u76ee\u7684\u5c31\u662f\u5e0c\u671b \u91c7\u6837\u51fa\u6765\u7684\u503c\uff0c\u8ddf \\(\\mu\\) \u548c \\(\\sigma\\) \u4e4b\u95f4 \u68af\u5ea6\u662f\u53ef\u5bfc\u7684</p> <p>\u8fd9\u5c31\u662f\u53c2\u6570\u91cd\u6574\u5316\u7684\u6280\u5de7\uff0c\u8fd9\u4e2a\u6280\u5de7\u5728VAE\u548cDiffusion\u4e2d\u5927\u91cf\u4f7f\u7528</p> <p>\u4ee5\u4e0a\u662f\u56de\u987e\uff0c\u63a5\u4e0b\u6765\u8bb2\u89e3\u4ec0\u4e48\u662fVAE\u4ee5\u53ca\u591a\u5c42VAE</p>"},{"location":"learning/6_Diffusion/#vaevae","title":"\u4e8c\u3001VAE\u4e0e\u591a\u5c42VAE\u56de\u987e","text":""},{"location":"learning/6_Diffusion/#1-vae","title":"1 \u5355\u5c42VAE&amp;\u7f6e\u4fe1\u4e0b\u754c","text":"<p>VAE\u7684\u539f\u7406\uff1a</p> <p>\u9996\u5148\u5355\u5c42VAE</p> <p>VAE\u8ba4\u4e3ax\u7531\u67d0\u4e00\u4e2a\u9690\u53d8\u91cfz\u751f\u6210</p> <p></p> <p>\u9690\u53d8\u91cfz\u5982\u4f55\u5f97\u5230\u5462\uff1f</p> <p>\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u901a\u8fc7\\(q_{\\phi}(z|x)\\) \u4ecex\u4e2d\u751f\u6210z\uff0c\u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u4ecez\u9884\u6d4bx</p> <p>\u8865\u5145\uff1a\u4ece z \u5230 x \u662f\u751f\u6210\u8fc7\u7a0b\uff0c\u7f51\u7edc\u8bad\u7ec3\u597d\u4e86\u4ee5\u540e\u53ea\u9700\u8981\u751f\u6210\u5c31\u884c\uff0c\u800c\u4e0d\u9700\u8981\u4ece x \u5230 z \u7684\u7f16\u7801\u8fc7\u7a0b\u4e86</p> <p>\u7b49\u4ef7\u4e8e  \u8bad\u7ec3\u65f6\u8bad\u7ec3\u4e24\u4e2a\u7f51\u7edc\uff0c\u63a8\u7406\u65f6\u53ea\u7528\u5230 z \u5230 x \u7684\u7f51\u7edc</p> <p>\u4ee5\u4e0a\u5c31\u662f\u5355\u5c42VAE</p> <p>\u516c\u5f0f\uff1a</p> <p></p> <p>\u7528\u516c\u5f0f \u6982\u62ec\u76ee\u6807\u6570\u636e\u5206\u5e03\uff1a</p> <p>\uff081\uff09p(x)\u53ef\u4ee5\u5199\u6210\u8054\u5408\u6982\u7387\u5206\u5e03\uff0c\u5bf9z\u8fd9\u4e2a\u53d8\u91cf\u8fdb\u884c\u79ef\u5206\uff0c\u5f97\u5230\u8fb9\u7f18\u5206\u5e03\uff1a</p> <p>\\(p(x) = \\int_zp_{\\theta}(x|z)p(z)\\)</p> <p>\u5199\u6210\u4e00\u4e2a \\(\u6761\u4ef6\u6982\u7387\\) \u548c \\(p(z)\\) \u76f8\u4e58\u7684\u5f62\u5f0f</p> <p>\uff082\uff09\u5bf9\uff081\uff09\u4e58\u4ee5\u540e\u9a8c\u6982\u7387\u5206\u5e03\u7684\u5f62\u5f0f\uff0c\u540e\u9a8c\u662f\u5e0c\u671b\u4ecex\u53bb\u9884\u6d4bz\u8fd9\u4e2a\u9690\u53d8\u91cf\uff0c\u4e0a\u4e0b\u5206\u522b\u4e58\u4ee5 $q_{\\phi} $  z  given  x \u5c31\u662f\\(q_{\\phi}(z|x)\\) \uff1a</p> <p>\\(p(x)= \\int q_{\\phi}(z|x) \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi}(z|x)}\\)</p> <p>\u8fd9\u4e2a\u5f0f\u5b50\u76f8\u5f53\u4e8e  \\(\\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi}(z|x)}\\)  \u5728 \\(q_{\\phi}\\) \u5206\u5e03\u4e0b\u7684\u671f\u671b</p> <p>\uff083\uff09\u5bf9\uff082\uff09\u5de6\u53f3\u4e24\u8fb9\u540c\u65f6\u53d6\\(log\\)\uff0c\u5f97\u5230\\(logp(x)\\)</p> <p>\u53f3\u8fb9\u5199\u6210\u671f\u671b\u7684\u5f62\u5f0f\uff1a</p> <p>\\(logp(x)\\)  \u5c31\u662f\u4e00\u4e2a\u5bf9\u6570\u4f3c\u7136</p> <p>\\(logp(x) = log \\mathbb{E}_{z \\sim q_{\\phi}(z|x)}[\\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi(z|x)}}]\\)</p> <p>\uff084\uff09\u6839\u636e\u8a79\u68ee\u4e0d\u7b49\u5f0f\uff0clog\u79fb\u5230\u671f\u671b\u91cc\u9762\uff0c\u5f97\u5230\uff1a</p> <p>\\(logp(x)  \\ge  \\mathbb{E}_{z \\sim q_{\\phi}(z|x)}[log \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi(z|x)}}]\\) </p> <p>\u7ecf\u8fc7\u4ee5\u4e0a\u7684\u63a8\u5bfc\uff0c\u5f97\u5230\u76ee\u6807\u6570\u636e\u5206\u5e03\u7684\u4e0b\u754c\uff0c\u8bad\u7ec3\u7f51\u7edc\u7684\u76ee\u7684\u662f\u6700\u5927\u5316 \\(logp(x)\\)\uff0c\u6700\u5927\u5316\u5bf9\u6570\u4f3c\u7136\uff0c\u6700\u5927\u5316 \\(logp(x)\\) \u4e0d\u597d\u6c42\uff0c\u5c31\u53ef\u4ee5 \u6700\u5927\u5316  \\(\\mathbb{E}_{z \\sim q_{\\phi}(z|x)}[log \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi(z|x)}}]\\) </p> <p>\u56e0\u4e3a    \\(logp(x)  \\ge  \\mathbb{E}_{z \\sim q_{\\phi}(z|x)}[log \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi(z|x)}}]\\)    \u8fd9\u4e2a\u4e0d\u7b49\u5f0f\u6c38\u8fdc\u6210\u7acb</p> <p>\u63a5\u4e0b\u6765\uff0c\u601d\u8003\uff0c\u600e\u4e48\u6700\u5927\u5316 \\(\\mathbb{E}_{z \\sim q_{\\phi}(z|x)}[log \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi(z|x)}}]\\) \uff1f</p> <p>\u770b\u6210\u4e24\u90e8\u5206</p> <p>\u7b2c\u4e00\u90e8\u5206\uff1a\\(log p_{\\theta}(x|z)\\)</p> <p>\u770b\u6210\u662f\u9884\u6d4b\u7f51\u7edc\uff0c\u57fa\u4e8e\u9884\u6d4b\u51fa\u6765\u7684\u9690\u53d8\u91cfz\u9884\u6d4bx\uff0c\u8fd9\u4e00\u90e8\u5206\u597d\u6c42\uff0c\u4e0e\u76ee\u6807\u6570\u636e\u7684\u5206\u5e03\u4f5c\u5dee\u5373\u53ef</p> <p>\u7b2c\u4e8c\u90e8\u5206\uff1a\\(log\\frac{p(z)}{q_{\\phi}(z|x)}\\)</p> <p>\u8fd9\u90e8\u5206\u5206\u5b50\u3001\u5206\u6bcd\u6c42\u5012\u6570\uff0c\u53d8\u6210\\(-log\\frac{q_{\\phi}(z|x)}{p(z)}\\)</p> <p>\u53c8 \u56e0\u4e3a\u5728 \\(q_{\\phi}\\) \u5206\u5e03\u4e0b\u7684\u671f\u671b \\(\\mathbb{E}_{z \\sim q_{\\phi}(z|x)}\\) \uff0c\u6240\u4ee5\u8fd9\u4e2a\u5f0f\u5b50\u53d8\u6210 \\(q_{\\phi}\\) \u4e0e \\(p_z\\) \u4e4b\u95f4\u7684KL\u6563\u5ea6</p> <p>\u4e8e\u662f\u4e0b\u754c\u53d8\u6210\uff1a</p> <p>\\(\\mathbb{E}_{z \\sim q_{\\phi}(z|x)}[log \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi(z|x)}}]\\)</p> <p>\\(= log p_{\\theta}(x|z) - KL\u6563\u5ea6(q_{\\phi} \u4e0e p)\\)</p> <p>\\(p\\) \u4e00\u822c\u60c5\u51b5\u4e0b\u53ef\u4ee5\u5047\u8bbe\u670d\u4ece\u9ad8\u65af\u5206\u5e03</p> <p>\u56e0\u6b64\uff0c\\(q_{\\phi}\\)  \u53ef\u4ee5\u9884\u6d4b\u9ad8\u65af\u5206\u5e03</p> <p>\u7531\u4e8e\u4e24\u4e2a\u9ad8\u65af\u5206\u5e03\u4e4b\u95f4\u7684KL\u6563\u5ea6\u662f\u53ef\u89e3\u7684\uff0c\u6240\u4ee5\u6574\u4e2aVAE\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4e5f\u662f\u53ef\u4ee5\u5199\u51fa\u6765\u7684\uff0c\u4ee5\u4e0a\u662f\u5355\u5c42VAE\u7684\u539f\u7406</p> <p>\u63a5\u4e0b\u6765\uff0c\u591a\u5c42VAE</p>"},{"location":"learning/6_Diffusion/#2-vae","title":"2 \u591a\u5c42VAE\u53ca\u7f6e\u4fe1\u4e0b\u754c","text":"<p>\u591a\u5c42VAE</p> <p>\u5047\u8bbe\u8fd9\u91cc\u7684x \u4e0d\u662f\u7531\u4e00\u4e2a\u9690\u53d8\u91cf \u751f\u6210\u7684\uff0c\u800c\u662f\u7531 2\u4e2a\u751f\u6210</p> <p></p> <p>\u4e00\u5f00\u59cb\u7684\u9690\u53d8\u91cf\u79f0\u4e3a \\(z_2\\) \uff0c\u901a\u8fc7 \\(z_2\\) \u751f\u6210 \\(z_1\\)\uff0c\u518d\u901a\u8fc7 \\(z_1\\)  \u751f\u6210 \\(x\\)</p> <p>\u8fd9\u6837\u7684\u751f\u6210\u8fc7\u7a0b\u53eb\u505a\u591a\u5c42VAE</p> <p>(1)\u6b64\u65f6\u7684\u76ee\u6807\u5206\u5e03 \\(p(x)\\) \uff0c\u53ef\u4ee5\u5199\u6210\u4e00\u4e2a\u8054\u5408\u6982\u7387\u5206\u5e03\uff0c\u7136\u540e\u628a \\(z_1\\) \u3001\\(z_2\\)  \u79ef\u6389\uff0c\u5f97\u5230 \\(p(x)\\)\uff1a</p> <p>\\(p(x) = \\int_{z_1}\\int_{z_2}p_{\\theta}(x,z_1,z_2)dz_1dz_2\\)</p> <p>(2)\u5206\u5b50\u5206\u6bcd \u540c\u65f6\u4e58\u4ee5 \u540e\u9a8c\u5206\u5e03\uff1a</p> <p>\\(p(x) = \\int_{z_1}\\int_{z_2}q_{\\phi}(z_1,z_2|x)\\frac{p_{\\theta}(x,z_1,z_2)}{q_{\\phi}(z_1,z_2|x)}dz_1dz_2\\)</p> <p>(3)\u7ee7\u7eed\u5c06(2)\u5199\u6210\u671f\u671b\u7684\u5f62\u5f0f\uff1a\\(p(x) = \\mathbb{E}_{z_1,z_2\\sim q_{\\phi}(z_1,z_2|x)}[\\frac{p_{\\theta}(x,z_1,z_2)}{q_{\\phi}(z_1,z_2|x)}]\\)</p> <p>(4) \u540c\u6837\u5bf9(3)\u4e24\u8fb9\u53d6\u4e00\u4e2a\u5bf9\u6570\uff0c\u5229\u7528\u8a79\u68ee\u4e0d\u7b49\u5f0f\uff0c\u5f97\u5230\\(log(p(x))\\) \u7684\u4e0b\u754c\uff1a</p> <p></p> <p>(5)\u5bf9(4)\uff0c\u501f\u52a9\u4e4b\u524d\u8bf4\u8fc7\u7684\u516c\u5f0f\uff0c\u5728\u9a6c\u5c14\u79d1\u592b\u5047\u8bbe\u7684\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u5199\u6210\uff1a</p> <p></p> <p>(6)\u5c06(5)\u4ee3\u5165(4)\uff0c\u5f97\u5230</p> <p></p> <p>\u4ee5\u4e0a\u662f\u591a\u5c42VAE\u7684\u76ee\u6807\u51fd\u6570</p> <p></p> <p>\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u6709\u4e24\u6b21 \u9996\u5148\u4ece \\(x\\)\u4e2d\u63a8\u7406\u51fa\\(z\\)\uff0c\u7136\u540e\u4ece\\(z\\)\u4e2d\uff0c\u63a8\u7406\u51fa\\(x\\)</p> <p>\u8fd9\u4e2a\u8fc7\u7a0b\u8ddfDiffusion\u5f88\u50cf</p>"},{"location":"learning/6_Diffusion/#diffusion","title":"\u4e09\u3001Diffusion\u56fe\u793a","text":"<p>\uff081\uff09</p> <p></p> <p>\uff082\uff09\u8bb2\u89e3\u4e86Diffusion\u662f\u600e\u4e48\u4e00\u56de\u4e8b</p> <p></p> <p>Diffusion\u4ece\u76ee\u6807\u5206\u5e03 \\(x_0\\) \u4e2d\u52a0\u566a\uff0c\u53bb\u751f\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u4e00\u4e2a\u5206\u5e03 \\(x_T\\) \uff0c\u7528\u7684\u65f6\u5019\u4ece\u6700\u7ec8\u5206\u5e03 \\(x_T\\) \uff0c\u9010\u6b65\u9010\u6b65\u7684\u5f97\u5230\u76ee\u6807\u5206\u5e03  \\(x_0\\)</p> <p>\u6240\u4ee5Diffusion\u7684\u76ee\u6807\u51fd\u6570\u8ddf\u591a\u5c42VAE\u7684\u76ee\u6807\u51fd\u6570\u662f\u5f88\u50cf\u7684</p> <p>\u601d\u8003\uff0c\u591a\u5c42VAE &amp; Diffusion\u4e4b\u95f4\u6709\u4ec0\u4e48\u5173\u8054\uff1f</p> <p>Diffusion\u5206\u4e3a\u4e24\u4e2a\u8fc7\u7a0b</p> <p>\u7b26\u53f7\u8bf4\u660e\uff1a\u76ee\u6807\u5206\u5e03\uff08\u8bb0\u4e3a \\(x_0\\)\uff09\uff0c\u566a\u58f0\u5206\u5e03 \\(x_T\\)</p> <p>\u7b2c\u4e00\u4e2a\u8fc7\u7a0b\uff1a\u6269\u6563\u8fc7\u7a0b</p> <p></p> <ul> <li>\u4ece \\(x_0\\)  \u5230 \\(x_T\\) \u71b5\u589e\u8fc7\u7a0b\uff0c\u4ece\u6709\u5e8f\u5230\u65e0\u5e8f</li> <li>\u60f3\u8c61 \u628a\u4e00\u4e2a\u6c34\u6ef4 \u5012\u5165\u5230 \u4e00\u4e2a\u6c60\u5858\u6216\u8005\u6cb3\u6d41\u4e4b\u4e2d\uff0c\u6162\u6162\u7684\u6269\u6563\uff0c\u6700\u540e\u6ca1\u6709\u4e86\u539f\u6765\u7684\u5206\u5e03</li> <li>\u6269\u6563\u8fc7\u7a0b \u5c31\u662f\u539f\u59cb\u7684\u8fc7\u7a0b\u4e2d \u4e0d\u65ad\u7684\u52a0\u566a\uff0c\u76f4\u5230\u6700\u540e\u53d8\u6210\u4e86 \u5404\u9879\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03</li> </ul> <p>\u751f\u6210\u7684\u65f6\u5019\uff0c\u662f\u5e0c\u671b\u4ece\u4e00\u4e2a\u566a\u58f0\u5206\u5e03\u4e2d\uff0c\u9010\u6b65\u9884\u6d4b\u51fa\u76ee\u6807\u5206\u5e03\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53eb\u505a \u9006\u6269\u6563\u8fc7\u7a0b reverse </p> <p>\u7b2c\u4e8c\u4e2a\u8fc7\u7a0b\uff1a\u9006\u6269\u6563\u8fc7\u7a0b</p> <p></p> <p>\u9006\u6269\u6563\u8fc7\u7a0b\uff0c\u5c31\u662f\u57fa\u4e8e\u4e00\u4e2a\u566a\u58f0\uff0c\u80fd\u591f\u628a\u76ee\u6807\u5206\u5e03\u63a8\u5bfc\u51fa\u6765\uff0c\u4ece\u76ee\u6807\u5206\u5e03\u4e2d\uff0c\u91c7\u6837\u65b0\u7684\u6837\u672c\uff0c\u751f\u6210\u65b0\u7684\u56fe\u50cf</p> <p>DIffusion\u8981\u505a\u7684\u5c31\u662f\uff0c\u6709\u4e00\u5806\u76ee\u6807\u5206\u5e03\uff08\u6bd4\u5982\u6709\u4e00\u5806\u8fd9\u4e2a\u4eba\u7684\u7167\u7247\uff09\uff0c\u6211\u4eec\u5e0c\u671b\u80fd\u591f\u628a\u9006\u6269\u6563\u8fc7\u7a0b\uff0c\u4e5f\u5c31\u662f\u4ece\\(x_T\\) \u5230 \\(x_0\\) \u7684\u539f\u7406\u641e\u51fa\u6765\uff0c\u6216\u8005\u8bf4\u516c\u5f0f\u9884\u6d4b\u51fa\u6765\uff0c\u7136\u540e\u968f\u673a\u7684\u751f\u6210\u566a\u58f0\u5206\u5e03\uff0c\u4ece\u800c\u80fd\u591f\u9884\u6d4b\u51fa\u65b0\u7684\u8fd9\u4e2a\u4eba\u7684\u7167\u7247\uff0c\u4ee5\u4e0a\u5c31\u662fDiffusion Model\u505a\u7684\u4e8b\u60c5</p> <p>\u8865\u5145\u4e24\u4e2a\u6761\u4ef6\u6982\u7387\u5206\u5e03\uff1a</p> <p></p> <ul> <li>\\(p_{\\theta}(x_{t-1}|x_t)\\) \uff1a \u8868\u793a\u9006\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03</li> <li>\\(q(x_t|x_{t-1})\\) \uff1a\u8868\u793a\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03</li> </ul> <p>\u800c\u6211\u4eec\u5728\u8fdb\u884c\u63a8\u7406\u7684\u65f6\u5019\uff0c\u53ea\u7528\u5230\u9006\u6269\u6563\u8fc7\u7a0b</p> <p>\u63a5\u4e0b\u6765\u7528\u56fe\u8868\u793a\u8fd9\u4e2a\u8fc7\u7a0b\uff1a</p> <p>\uff083\uff09</p> <p></p> <p>t=0\u65f6\uff0c\u5c31\u662f\u6807\u51c6\u7684\u76ee\u6807\u5206\u5e03\uff0c\u5c31\u662f\u89c4\u5219\u7684\u56fe\u5f62\uff0c\u63a5\u7740\u6211\u4eec\u9010\u6b65\u7684\u52a0\u566a\uff0c\u52a0\u5230\u4e00\u534a\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u56fe\u5f62\u53d8\u5f97\u5f88\u6a21\u7cca\uff0c\u52a0\u5230\u6700\u7ec8\u7684\u65f6\u5019\uff0c\u5206\u5e03\u5df2\u7ecf\u5b8c\u5168\u5931\u53bb\u4e86\u539f\u8c8c\uff0c\u57fa\u672c\u5df2\u7ecf\u53d8\u6210\u4e86\u5404\u9879\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03\uff1a</p> <p></p> <p>\u9006\u6269\u6563\u7684\u8fc7\u7a0b\u4e5f\u662f\u540c\u6837\uff0c\u4e00\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u751f\u6210\u4e00\u5806\u9ad8\u65af\u5206\u5e03\u7684\u566a\u58f0\uff0c\u7136\u540e\u57fa\u4e8e\u6a21\u578b\u4e0d\u65ad\u7684\u8fed\u4ee3\uff0c\u8fed\u4ee3\u5230t=0\u7684\u65f6\u5019\uff0c\u4e5f\u5c31\u662f T\u65f6\u523b\u7684\u8fed\u4ee3\uff0c\u751f\u6210\u65b0\u7684\u6837\u672c\uff0c\u65b0\u7684\u6837\u672c\u7684\u5206\u5e03\u8ddf\u539f\u6765\u6570\u636e\u662f\u4e00\u6837\u7684\uff0c\u4ece\u540c\u4e00\u4e2a\u5206\u5e03\u4e2d\uff0c\u91c7\u6837\u51fa\u6765\u7684\u65b0\u7684\u6837\u672c\uff0c\u4ee5\u4e0a\u662f\u9006\u6269\u6563\u8fc7\u7a0b\uff1a</p> <p></p> <p>\u6269\u6563\u8fc7\u7a0b\u548c\u9006\u6269\u6563\u8fc7\u7a0b\u7684\u5dee\u79f0\u4e3a\u6f02\u79fb\u91cf\uff1a</p> <p></p> <p>\u4ee5\u4e0a\u662f\u6269\u6563\u6a21\u578b\u7684\u5927\u81f4\u8fc7\u7a0b</p> <p>\u4ee5\u4e0a\u662f\u56fe\u793a\uff0c\u4e0b\u9762\u662f\u6b63\u5f0f\u7684\u63a8\u5bfc\u8fc7\u7a0b</p>"},{"location":"learning/6_Diffusion/#_1","title":"\u56db\u3001\u6269\u6563\u8fc7\u7a0b","text":"<p>\u9996\u5148\u660e\u767d\uff0c\u6269\u6563\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u8fc7\u7a0b\uff1a</p> <p>(1)\u6b63\u5411\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u4ece \\(x_0\\) \u5230 \\(x_T\\)</p> <p>(2)\u53cd\u5411\u7684\u9006\u6269\u6563\u8fc7\u7a0b\uff0c\u4ece \\(x_T\\) \u5230 \\(x_0\\)\uff08\u4e5f\u53ef\u4ee5\u53eb\u91cd\u5efa\u8fc7\u7a0b\uff09 \u4ece\u566a\u58f0\u4e2d \u91cd\u5efa\u76ee\u6807\u5206\u5e03</p> <p></p> <p>1\u3001\u9996\u5148\u770b\u6269\u6563\u8fc7\u7a0b\uff1a\u7ed9\u5b9a\u521d\u59cb\u6570\u636e\u5206\u5e03\uff0c\u901a\u4fd7\u4e00\u70b9\u5c31\u662f\u8bad\u7ec3\u96c6\u670d\u4ece\u5206\u5e03 \\(x_0 \\sim q(x)\\)\uff0c\u63a5\u7740\u4e0d\u65ad\u7684\u5411\u5206\u5e03\u4e2d\u6dfb\u52a0\u566a\u58f0\uff0c\u8fd9\u91cc\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u90fd\u662f\u4e0d\u542b\u53c2\u6570\u7684\uff0c\u518d\u6b21\u5f3a\u8c03\uff0c\u6269\u6563\u8fc7\u7a0b\u7684\u6b63\u5411\u8fc7\u7a0b\u662f\u4e0d\u542b\u53c2\u6570\u7684\uff0c\u5c31\u662f\u5f80 \\(x_0\\) \u4e2d \u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\u7684\u65f6\u5019\uff0c\u6bcf\u4e00\u6b65\u7684\u5747\u503c\u7684\u65b9\u5dee\u90fd\u662f\u786e\u5b9a\u7684\uff0c\u5c31\u6bd4\u5982\u5b66\u4e60\u7387\uff0c\u867d\u7136\u6bcf\u4e00\u6b65\u90fd\u5728\u53d8\u5316\uff0c\u4f46\u662f\u662f\u786e\u5b9a\u7684\uff0c\u800c\u4e0d\u662f\u7f51\u7edc\u9884\u6d4b\u7684\uff0c\u5e76\u4e14\u8fd9\u4e2a\u8fc7\u7a0b\u4e5f\u662f\u9a6c\u5c14\u79d1\u592b\u94fe\u7684\u8fc7\u7a0b\uff0c\u5373\u5f53\u524d\u65f6\u523b\u53ea\u4e0e\u4e0a\u4e00\u65f6\u523b\u76f8\u5173\uff0c\u4e0e\u8fc7\u53bb\u66f4\u8fdc\u7684\u65f6\u523b\u65e0\u5173\uff0c\u8fd9\u5c31\u662f\u7b2c\u4e00\u70b9\uff0c\u4e0d\u65ad\u7684\u5f80\u539f\u59cb\u5206\u5e03\u4e2d\uff0c\u53bb\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\uff0c\u4f46\u8fd9\u4e2a\u6dfb\u52a0\u4e0d\u662f\u7b80\u5355\u7684\u52a0\u6cd5\uff0c\u800c\u662f\u4eff\u5c04\u53d8\u6362\u7684\u8fc7\u7a0b\uff0c\u7b49\u4e0b\u4f1a\u57fa\u4e8e\u53c2\u6570\u91cd\u6574\u5316\u751f\u6210\u6bcf\u4e00\u65f6\u523b\u65b0\u7684\u6570\u636e\u5206\u5e03</p> <p></p> <p>2\u3001\u7b2c\u4e8c\u70b9\uff0c\u968f\u7740 t\u7684\u4e0d\u65ad\u589e\u5927\uff0c\u6211\u4eec\u6700\u7ec8\u7684\u6570\u636e\u5206\u5e03\u4f1a\u53d8\u6210 \u5404\u9879\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u4e3a\u4ec0\u4e48\u4f1a\u8fd9\u6837\uff1f\u4e0b\u9762\u5f00\u59cb\u63a8\u5bfc\uff1a</p> <p>\u5728\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u6709\u5b9a\u4e49 \\(q(x_t|x_{t-1})\\)\uff1a\u4ece \\(x_0\\) \u53bb\u9884\u6d4b \\(x_1\\) \uff0c\u6216\u8005\u8bf4 \\(x_1\\) \u9884\u6d4b \\(x_2\\) \u7684\u8bdd\uff0c\u662f\u4e00\u4e2a\u6761\u4ef6\u6982\u7387\u5206\u5e03\uff0c\u5e76\u4e14\u8fd9\u4e2a\u6761\u4ef6\u6982\u7387\u5206\u5e03\u662f\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\uff0c\u8fd9\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\u5747\u503c\u662f \\(\\sqrt{1-\\beta_t}x_{t-1}\\)\uff0c\u65b9\u5dee\u662f \\(\\beta_tI\\) \uff0c\u4e5f\u5c31\u662f\u8bf4 \u6bcf\u6b21\u52a0\u566a\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u53ea\u7531\u5f53\u524d\u65f6\u523b\u7684 \\(x\\) \u548c\u4e00\u4e2a\u786e\u5b9a\u503c \\(\\beta_t\\) \u6709\u5173\uff0c\u662f\u5b8c\u5168\u4e0d\u542b\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\uff0c\u4ee5\u4e0a\u5c31\u662f\u6269\u6563\u8fc7\u7a0b\uff0c\u662f\u4e0d\u542b\u53c2\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6309\u7167\u516c\u5f0f\uff1a</p> <p></p> <p>\u8fed\u4ee3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba1\u7b97 t=\u4efb\u610f\u65f6\u523b\u7684\uff0c\\(x_t\\) \u7684\u5206\u5e03\uff0c\u7136\u540e\u5c31\u53ef\u4ee5\u91c7\u6837\u51fa\u503c</p> <p>\u8fd8\u6709\u4e00\u4e2a\u8fed\u4ee3\u5f0f\uff0c\u7ed9\u5b9a \\(x_0\\) \u6c42\u51fa \\(x_{1:T}\\) \u7684\u8054\u5408\u5206\u5e03\uff0c\u5c31\u662f\u591a\u4e2a\u6761\u4ef6\u6982\u7387\u76f8\u4e58\u7684\u7ed3\u679c\uff0c\u662f\u4e00\u4e2a \u9a6c\u5c14\u79d1\u592b\u8fc7\u7a0b\uff0c\u4ee5\u4e0a\u662f\u4e00\u4e2a\u8054\u5408\u5206\u5e03</p> <p>\u95ee\u9898\uff1a\u600e\u4e48\u7b97\u51fa \\(x_t\\) \u5462\uff1f</p> <p>\u7b54\uff1a\u7528\u4e4b\u524d\u53c2\u6570\u91cd\u6574\u5316\u7684\u6280\u5de7\uff0c\u56e0\u4e3a\\(x_t\\)\u670d\u4ece\u7684\u5206\u5e03\u6709\uff1a\\(x_t|x_{t-1} \\sim \\mathcal{N}(\\sqrt{1-\\beta_t}x_{t-1},\\beta_tI)\\)\uff0c\u53ef\u4ee5\u4ece\u4e00\u4e2a\u6b63\u6001\u5206\u5e03\u4e2d \u751f\u6210\u4e00\u4e2a z\uff0c\u7136\u540e\u628a z\u4e58\u4ee5 \u6839\u53f7\u4e0b \\(\\beta_t\\)\uff0c\u518d\u52a0\u4e0a \\(\\sqrt{1-\\beta_t}x_{t-1}\\)\uff1a</p> <p>\\(\\sigma z + \\mu = \\sqrt{\\beta_t}z+\\sqrt{1-\\beta_t}x_{t-1}\\)\uff0c</p> <p>\u8fd9\u5c31\u662f \\(x_t\\) \u7684\u4e00\u4e2a\u91c7\u6837\u503c\uff0c\u7ecf\u8fc7\u4e0d\u65ad\u7684\u8fed\u4ee3\uff0c\u5f97\u5230 \\(x_{t+1}\\) \u7684\u91c7\u6837\u503c\uff0c\u6700\u7ec8\u5f97\u5230 \\(x_T\\) \u7684\u91c7\u6837\u503c</p> <p>\u95ee\u9898\uff1a\u5927T\u662f\u600e\u6837\u786e\u5b9a\u7684\uff0c\u4ee5\u53ca \\(\\beta_t\\) \u600e\u4e48\u8bbe\u7f6e\uff1f</p> <p>\u7b54\uff1a \\(\\beta_t\\) \u5728\u539f\u8bba\u6587\u4e2d\u662f \u51fa\u4e8e \\(0\\sim 1\\)\u4e4b\u95f4\u7684\u5c0f\u6570\uff0c\u5e76\u4e14\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\\(\\beta_t\\) \u662f\u8d8a\u6765\u8d8a\u5927\u7684</p> <p>\u4e5f\u5c31\u662f \\(\\beta_1 &lt; \\beta_2&lt;...&lt;\\beta_T\\)   \u628a  \\(\\beta_t\\) \u8bbe\u7f6e\u6210\u5c31\u50cf\u5b66\u4e60\u7387\u4e00\u6837\u8bbe\u7f6e\u6210\u4e0d\u65ad\u53d8\u5316\u7684\uff0c\u4e0d\u8fc7\u5b66\u4e60\u7387\u662f\u4e0d\u65ad\u964d\u4f4e\u7684\uff0c\u8fd9\u91cc\u7684 \\(\\beta_t\\) \u662f\u5728\u4e0d\u65ad\u53d8\u5927\u7684</p> <p>\u4ee5\u4e0a\u901a\u8fc7\u8fed\u4ee3\u7684\u65b9\u6cd5\u8ba1\u7b97 \\(x_t\\) \u7684\u91c7\u6837\u503c</p> <p>\u63a5\u4e0b\u6765 \u8bf4\u660e \u5f53 \\(T=\u4ec0\u4e48\\) \u7684\u65f6\u5019\uff0c\\(x_T\\) \u63a5\u8fd1\u4e8e\u4e00\u4e2a\u5404\u9879\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03</p> <p></p> <p>\u4efb\u610f\u65f6\u523b\u7684 \\(q(x_t)\\) \uff0c\u53ef\u4ee5\u4e0d\u7528 \u5b8c\u5168\u4e00\u6837\u7684 \u57fa\u4e8e\u4e0a\u9762\u7684\u516c\u5f0f\uff0c\u6bcf\u4e00\u6b65\u7684\u91c7\u6837\u8fed\u4ee3\u51fa\u6765\uff1a</p> <p></p> <p>\u800c\u662f\u53ef\u4ee5\u5b8c\u5168\u7684\u57fa\u4e8e \\(x_0\\)  \u548c \\(\\beta_t\\) \u8ba1\u7b97\u51fa\u6765\uff0c\u4e5f\u5c31\u662f\u7ed9\u51fa \\(x_0\\) \u521d\u59cb\u7684\u6570\u636e\u5206\u5e03\u548c \\(\\beta_t\\) \u4e00\u4e2a\u53d8\u6362\u503c\uff0c\u5c31\u53ef\u4ee5\u7b97\u51fa\u4efb\u610f\u65f6\u523b\u7684 \\(q(x_t)\\) \uff0c\u5c31\u4e0d\u7528\u4e00\u6b65\u6b65\u8fed\u4ee3\u4e86</p> <p>\u5177\u4f53\u7684\u8ba1\u7b97\u6b65\u9aa4\uff1a</p> <p></p> <p>\uff081\uff09 \\(x_t = \\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}z_{t-1}\\)</p> <p>\u9996\u5148\uff0c\\(x_t\\)  \u7528\u53c2\u6570\u91cd\u6574\u5316\u7684\u6280\u5de7 \u5199\u6210 \\(\\sqrt{\\alpha_t}x_{t-1}+\u6807\u51c6\u5dee\\)</p> <p>\\(\\sigma z + \\mu = \\sqrt{\\beta_t}z+\\sqrt{1-\\beta_t}x_{t-1}\\) </p> <p>\u8fd9\u91cc\u7684 \\(\\alpha_t = 1-\\beta_t\\)</p> <p>\u53ef\u4ee5\u770b\u5230\u539f\u6587\u90fd\u5199\u4e86</p> <p>\u4e5f\u5c31\u662f \\(x_t = \\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}z_{t-1}\\)</p> <p><code>\u5747\u503c + \u6807\u51c6\u5dee \u00d7 \u968f\u673a\u751f\u6210\u7684\u6b63\u6001\u5206\u5e03\u7684\u91cf</code></p> <p>\u4ee5\u4e0a\u5c31\u662f \u53c2\u6570\u91cd\u6574\u5316\u6280\u5de7</p> <p>\\(z_t\\) \u662f\u4ece \\(N(0,1)\\) \u4e2d\u91c7\u6837\u51fa\u6765\u7684 \u968f\u673a\u503c</p> <p>\uff082\uff09</p> <p></p> <p>\\(x_t = \\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}z_{t-1}\\)</p> <p>$=\\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2}+\\sqrt{1-\\alpha_{t-1}}z_{t-2})+\\sqrt{1-\\alpha_t}z_{t-1} $</p> <p>$=\\sqrt{\\alpha_t\\alpha_{t-1}}x_{t-2}+\\sqrt{\\alpha_t}\\sqrt{1-\\alpha_{t-1}}z_{t-2}+\\sqrt{1-\\alpha_t}z_{t-1} $</p> <p>\\(\\sqrt{\\alpha_t}\\sqrt{1-\\alpha_{t-1}}z_{t-2}+\\sqrt{1-\\alpha_t}z_{t-1}\\)</p> <p>\u53ef\u4ee5\u53c2\u6570\u91cd\u6574\u5316\u4e3a \u53ea\u5305\u542b\u4e00\u4e2a \u968f\u673a\u53d8\u91cfz\u7684\u5f62\u5f0f\uff0c\u7406\u7531\uff1a</p> <p></p> <p>\u82e5\u7ed9\u51fa \\(X \\sim N(\\mu_1,\\sigma_1)\\) \\(Y\\sim N(\\mu_2,\\sigma_2)\\)</p> <p>\u5219 \\(aX+bY \\sim N(a\\mu_1+b\\mu_2,a^2\\sigma_1^2+b^2\\sigma_2^2)\\)</p> <p>\u6240\u4ee5 </p> <p>\\(\\sqrt{\\alpha_t}\\sqrt{1-\\alpha_{t-1}}z_{t-2}+\\sqrt{1-\\alpha_t}z_{t-1} \\sim N(0,\\alpha_t-\\alpha_t\\alpha_{t-1}+1-\\alpha_t)=N(0,1-\\alpha_{t-1}\\alpha_t)\\)</p> <p>\uff08\\(z_{t-1}\u3001z_{t-2} \\sim N(0,1)\\)\uff09</p> <p>\u6240\u4ee5 </p> <p>\\(\\sqrt{\\alpha_t}\\sqrt{1-\\alpha_{t-1}}z_{t-2}+\\sqrt{1-\\alpha_t}z_{t-1}\\) \u53ef\u4ee5\u91cd\u6574\u5316\u4e3a \\(\\sqrt{1-\\alpha_t\\alpha_{t-1}}\\bar z_{t-2}\\)</p> <p>\u4ee5\u4e0a\u63a8\u51fa  \u7b2c\u4e8c\u6b65</p> <p>\uff083\uff09</p> <p>\u4e00\u6b21\u7c7b\u63a8\uff0c\u5f97\u5230</p> <p></p> <ul> <li>z\u4ecd\u7136\u662f N(0,1) \u4e2d\u91c7\u6837\u51fa\u6765\u7684</li> <li>\\(x_t = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar\\alpha_t}z\\)</li> <li>\\(\\bar \\alpha_t\\) \u8868\u793a\u8fde\u4e58</li> <li>\\(\\bar{z_{t-2}}\\) \u8868\u793a \u878d\u5408\u4e24\u4e2a\u9ad8\u65af\u5206\u5e03</li> </ul> <p></p> <p>\uff084\uff09</p> <p></p> <p>\u7531\u516c\u5f0f\u53ef\u77e5\uff0c\u5f53\u7ed9\u5b9a \\(x_0\\) \u7684\u65f6\u5019\uff0c\\(q(x_t|x_0)\\) \u7684\u5206\u5e03\u5176\u5b9e\u5c31\u662f \u4ee5 \\(\\sqrt {\\bar\\alpha_t} x_0\\) \u4e3a\u5747\u503c\uff0c\u4ee5 \\(\\sqrt{1-\\bar \\alpha_t }\\)\u4e3a\u65b9\u5dee\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u8fd9\u4e5f\u5c31\u662f\u8bf4 \u5728\u7ed9\u5b9a \\(x_0\\) \u548c \\(\\beta_t\\) \u7684\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u6c42\u51fa \\(q(x_t|x_0)\\)</p> <p>\uff08\u524d\u9762\u6211\u4eec\u5047\u8bbe\u4e86 \\(\\alpha_t = 1-\\beta_t\\)\uff0c\u6240\u4ee5 \\(\\beta_t\\) \u5df2\u7ecf \u5c31\u76f8\u5f53\u4e8e \\(\\alpha_t\\)\u5df2\u77e5\uff0c\u81ea\u7136\u7684 \\(\\bar \\alpha_t\\) \u5df2\u77e5\uff09</p> <p>\\(q(x_t|x_0)=\\mathcal{N}(x_t;\\sqrt{\\bar \\alpha_t}x_0,(1-\\bar \\alpha_t)I)\\)</p> <p>\u4e5f\u5c31\u662f \u53ef\u4ee5\u4ee5  \\(\\mathcal{N}(x_t;\\sqrt{\\bar \\alpha_t}x_0,(1-\\bar \\alpha_t)I)\\) \u8fd9\u4e2a\u9ad8\u65af\u5206\u5e03\u8fdb\u884c\u91c7\u6837\uff0c\u800c\u4e0d\u9700\u8981\u9010\u6b65\u8fed\u4ee3</p> <p>\u2705</p> <p></p> <p>\u2705</p> <p></p> <p>\u4e24\u79cd \u91c7\u6837\u65b9\u6cd5</p> <p>\u4ee5\u4e0a\u5c31\u53ef\u4ee5\u786e\u5b9a\uff0cT\uff0c\u4e5f\u5c31\u662f\u5f53\u52a0\u591a\u5c11\u6b65\u566a\u58f0\u7684\u65f6\u5019\uff0c\u6211\u4eec\u7684 \\(q(x_t|x_0)\\) \u771f\u7684\u53d8\u6210\u4e00\u4e2a\u5404\u9879 \u540c\u6027 \u7684 \u9ad8\u65af\u5206\u5e03\u4e86\uff08\u7b49\u4ef7\u4e8e \u5404\u9879\u72ec\u7acb\uff09\uff0c\u56e0\u4e3a \\(\\alpha_t\\) \u662f\u5df2\u77e5\u7684\uff0c\u662f\u6211\u4eec\u81ea\u5df1\u8bbe\u7f6e\u7684 \u7c7b\u4f3c \u5b66\u4e60\u7387\u7684\u5e38\u6570\uff0c\u56e0\u6b64\u5c31\u80fd\u7b97\u51fa\u6765 \u5f53 \\(t=\u591a\u5c11\\)\u7684\u65f6\u5019\uff0c \\(\\sqrt{\\bar \\alpha_t}\\) \u63a5\u8fd1\u4e8e0\uff0c\\((1-\\bar \\alpha_t)\\) \u63a5\u8fd1\u4e8e1\uff0c\u6b64\u65f6 \\(q(x_t|x_0)\\) \u5c31\u63a5\u8fd1\u4e8e \u6807\u51c6\u6b63\u6001\u5206\u5e03\u3002\u4ee5\u4e0a\u8bf4\u660e\u4e86\u5982\u4f55\u786e\u5b9a t\u5728\u4ec0\u4e48\u65f6\u5019\uff0c\u80fd\u591f\u4f7f\u5f97 \\(q(x_t|x_0)\\) \u63a5\u8fd1\u4e8e \\(N(0,1)\\)</p> <p>\u5c0f\u5c0f\u7684\u603b\u7ed3\u4e00\u4e0b\uff1a</p> <p>(1) \u6269\u6563\u8fc7\u7a0b\u662f\u4e00\u4e2a\u5b8c\u5168\u4e0d\u542b\u53c2\u7684\u6269\u6563\u8fc7\u7a0b</p> <p>(2) \u8fed\u4ee3\u6b21\u6570 \u4ee5\u53ca \\(q(x_t)\\) \u7684\u8ba1\u7b97</p> <p>\u53ea\u8981\u7ed9\u5b9a\u521d\u59cb\u5206\u5e03\uff0c\u4efb\u610f\u65f6\u523b\u7684 \\(q(x_t)\\) \u90fd\u53ef\u4ee5\u628a\u91c7\u6837\u503c \u7b97\u51fa\u6765</p> <p>\u5176\u4e2d \u8ba1\u7b97 \\(q(x_t)\\) \u4e0d\u4e00\u5b9a\u662f \u901a\u8fc7\u8fed\u4ee3\uff1a</p> <p></p> <p>\u4e5f\u53ef\u4ee5\u901a\u8fc7\u76f4\u63a5\u7684\u8ba1\u7b97\u51fa\u6765\uff1a</p> <p></p> <p>\u8fd9\u4e2a\u5f0f\u5b50\u4f7f\u7528\u7684\u65f6\u5019\u9700\u8981 \u9884\u5148\u7684\u77e5\u9053 \\(\\alpha_t\\) \u4e5f\u5c31\u662f \\(\\beta_t\\)</p> <p>(3)\u4e0eVAE\u7684\u533a\u522b\uff1a</p> <p>\u7b2c\u4e00\u70b9\uff1a</p> <p>VAE\u4ece\\(x\\)\u5230\\(z\\)\uff0c\u9996\u5148 \u4e0d\u662f\u4e00\u4e2a \u65e0\u53c2\u6570\u7684 \u8fc7\u7a0b\uff0c\u800c\u662f \u901a\u8fc7 \u540e\u9a8c\u7f51\u7edc \u9884\u6d4b\u51fa\u6765\u7684\uff0c\u5176\u6b21 VAE\u7684\\(z\\)\u5e76\u4e0d\u662f\u5b8c\u5168\u7684\u8ddf\\(x\\)\u65e0\u5173\uff0cDiffusion \u7ecf\u8fc7\u6269\u6563\u4ee5\u540e\u7684 \\(x_t\\) \u662f\u4e00\u4e2a\u57fa\u672c\u5404\u9879\u72ec\u7acb\u7684 \u9ad8\u65af\u5206\u5e03\uff0c\u57fa\u672c\u4e0e \u539f\u59cb\u7684 \\(x_0\\) \u65e0\u5173\u4e86</p> <p></p> <p>\u7b2c\u4e8c\u70b9\uff1a</p> <p>\\(VAE\\)\u4e2d\uff0c\\(x\\)\u8ddf\\(z\\)\u7684\u7ef4\u5ea6\u4e0d\u4e00\u5b9a\u662f\u4e00\u6837\u7684\uff0c\u4f46\u662f\u5728\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u4ece \\(x_0\\) \u5230 \\(x_1\\) \u5230....\u5230 \\(x_t\\) \u7ef4\u5ea6\u59cb\u7ec8\u662f\u4e00\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u6700\u540e\u7684 \\(x_T\\) \u7684\u7ef4\u5ea6 \u548c \\(x_0\\) \u7684\u7ef4\u5ea6\u662f\u4e00\u6837\u7684\uff0c\u662f\u6ca1\u6709\u53d8\u5316\u7684</p> <p>(4)</p> <p>\\(\\beta_t\\) \u600e\u4e48\u6837\u8bbe\u7f6e\uff1f</p> <p></p> <p>\u539f\u6587\uff1b\u5f53\u5206\u5e03\u8d8a\u6765\u8d8a\u63a5\u8fd1\u566a\u58f0\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u8ba9 \\(\\beta_t\\) \u66f4\u5927\u4e00\u70b9\uff0c\u5c31\u662f\u4e00\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u4e0d\u8981\u52a0\u7684\u592a\u5927\uff0c\u4e00\u5f00\u59cb\u7684\u65f6\u5019  \\(\\beta_t\\) \u52a0\u7684\u7684\u5c0f\u4e00\u70b9\uff08\\(\\beta_t\\) \u662f\u63a7\u5236\u4ec0\u4e48\u7684\uff1f\uff09\uff0c\u5230\u540e\u9762\u53ef\u4ee5\u8d8a\u6765\u8d8a\u5927</p> <p>\u53cd\u8fc7\u6765\uff0c\u4ece \\(x_T\\) \u751f\u6210 \\(x_0\\) \uff0c\u5728\u4e00\u5f00\u59cb\u7684\u9636\u6bb5\u53d8\u5316\u5e76\u4e0d\u660e\u663e\uff0c\u59cb\u7ec8\u662f\u6df7\u4e71\u7684\u4e00\u56e2\uff0c\u4f46\u662f\u5728\u6700\u540e\u51e0\u6b65\u7684\u65f6\u5019\uff0c\u53d8\u5316\u7279\u522b\u660e\u663e\uff0c\u6700\u540e\u51e0\u6b65\u53ef\u4ee5\u5f88\u5feb\u7684\u663e\u793a\u539f\u59cb\u6570\u636e\u5206\u5e03</p> <p>\u63a5\u4e0b\u6765\uff0c\u9006\u8fc7\u7a0b\uff0c\u4e5f\u53eb \u9006\u6269\u6563\u8fc7\u7a0b \u6216\u8005 \u91cd\u5efa\u8fc7\u7a0b\uff1areverse process</p>"},{"location":"learning/6_Diffusion/#_2","title":"\u4e94\u3001\u9006\u6269\u6563\u8fc7\u7a0b","text":"<ul> <li>\u6269\u6563\u8fc7\u7a0b\u662f\u5bf9\u539f\u59cb\u6570\u636e\u4e00\u6b65\u6b65\u52a0\u566a\uff0c\u4f7f\u5176\u5f7b\u5e95\u53d8\u6210\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03</li> <li>\u9006\u6269\u6563\u8fc7\u7a0b\uff1a\u4ece\u9ad8\u65af\u5206\u5e03\u4e2d\uff0c\u6062\u590d\u539f\u59cb\u6570\u636e\uff08\u8fd9\u4e5f\u662f DIffusion model\u7684\u76ee\u7684\uff1a\u7ed9\u6211\u4eec\u4e00\u5806\u8bad\u7ec3\u96c6\uff0c\u5e0c\u671b\u6a21\u578b\u80fd\u591f\u4ece\u566a\u58f0\u9884\u6d4b\u51fa\u8bad\u7ec3\u96c6\u7684\u5206\u5e03\uff0c\u8fdb\u800c\u751f\u6210\u65b0\u7684\u6837\u672c\uff09</li> <li>\u5728 \u52a0\u566a\u7684\u8fc7\u7a0b\u4e2d\uff0c \\(\\beta_t\\) \u662f\u6bd4\u8f83\u5c0f\u7684\uff0c\u59cb\u7ec8\u662f 0~1 \u4e4b\u95f4 \u5f88\u5c0f\u7684\u6570\uff0c\u6bcf\u6b21\u52a0\u7684\u9ad8\u65af\u566a\u58f0\u5f88\u5c0f\uff0c\u65e2\u7136\u6bcf\u6b21\u52a0\u7684\u9ad8\u65af\u566a\u58f0\u5f88\u5c0f\uff0c\u53ef\u4ee5\u6709\u7406\u7531\u5047\u8bbe \u9006\u8fc7\u7a0b\uff08\u4ece  \\(x_T\\) \u9010\u6b65\u6062\u590d \\(x_0\\) \u7684\u8fc7\u7a0b \uff09\uff0c\u4e5f\u53ef\u4ee5\u5047\u8bbe\u662f\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\uff0c\u5373 \\(p_{\\theta}(x_{t-1}|x_t)\\) \u4e5f\u662f\u670d\u4ece\u9ad8\u65af\u5206\u5e03</li> <li>\u4f46\u662f\uff0c\u6211\u4eec\u65e0\u6cd5\u76f4\u63a5\u62df\u5408  \\(p_{\\theta}(x_{t-1}|x_t)\\) \u8fd9\u4e2a\u9ad8\u65af\u5206\u5e03\uff0c\u5982\u679c\u6211\u4eec\u8981\u9010\u6b65\u62df\u5408 \\(p_{\\theta}(x_{t-1}|x_t)\\) \uff0c\u9996\u5148\u9700\u8981\u751f\u6210\u4e00\u5806 \\(x_t\\) \uff0c\u7136\u540e\u9010\u6b65\u505a GMM \u7684\u62df\u5408\uff0c\u62df\u5408\u51fa \\(x_{t-1}\\) \u4e4b\u540e\uff0c\u8fd8\u8981\u62df\u5408 \\(x_{t-2}\\) \u7b49\u7b49\uff0c\u9700\u8981\u904d\u5386\u6574\u4e2a\u6570\u636e\u96c6 \u6bd4\u8f83\u9ebb\u70e6\uff0c\u73b0\u5728\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u7f51\u7edc\uff0c\u6765\u8fdb\u884c\u8fd9\u4e2a\u4f30\u8ba1</li> <li>\u5f3a\u8c03\uff1a\u9006\u6269\u6563\u8fc7\u7a0b\u4ecd\u7136\u662f\u4e00\u4e2a\u9a6c\u5c14\u79d1\u592b\u94fe\u8fc7\u7a0b\uff1a</li> </ul> <p>\u73b0\u5047\u8bbe \u6709\u7f51\u7edc\\(\\theta\\)\uff0c\u53ef\u4ee5\u6784\u5efa\u51fa\u6761\u4ef6\u6982\u7387 \\(p_{\\theta}(x_{t-1}|x_t)\\) \uff0c\u5047\u8bbe\u8be5\u6761\u4ef6\u6982\u7387\u5747\u503c\u4e3a \\(\\mu_{\\theta}\\)  \u8fd9\u4e2a\u5747\u503c\u4e0e \\(x_t\\)  \u548c \\(t\\) \u6709\u5173\u7684\uff0c\u4e5f\u5c31\u662f\u8fd9\u4e2a\u7f51\u7edc\uff0c\u4ee5 \\(x_t\\) \u548c \\(t\\) \u4f5c\u4e3a\u8f93\u5165\uff0c\u8fd9\u4e2a\u6761\u4ef6\u6982\u7387\u7684\u65b9\u5dee \u4e5f\u662f\u542b\u53c2\u7684 \\(\\sum_{\\theta}\\) \uff0c\u4e5f\u662f\u7531 \\(x_t\\) \u548c \\(t\\) \u5171\u540c\u4f5c\u4e3a\u8f93\u5165\u7684\uff0c\u540c\u6837\u4e5f\u53ef\u4ee5\u628a\u6574\u4e2a\u8054\u5408\u6982\u7387\u5206\u5e03 \u5199\u6210 \\(p(x_T)\\) \u00d7 \u4e00\u8fde\u4e32\u7684 \u6761\u4ef6\u6982\u7387\u76f8\u4e58</p> <ul> <li>\u4ee5\u4e0a\u662f \u9006\u6269\u6563\u8fc7\u7a0b\uff0c\u5c31\u662f\u4ece \\(x_T\\) \u9010\u6e10\u6062\u590d \\(x_0\\)</li> <li>\u90a3 \\(\\mu_{\\theta}\\)\u548c \\(\\sum_{\\theta}\\) \u5e94\u8be5\u6062\u590d\u6210\u591a\u5c11\u5462\uff1f\uff08\u540e\u9762\u4f1a\u8ba8\u8bba\uff09</li> </ul>"},{"location":"learning/6_Diffusion/#_3","title":"\u516d\u3001\u6269\u6563\u4e2d\u7684\u540e\u9a8c\u6761\u4ef6\u6982\u7387","text":"<p>\u8be5\u90e8\u5206\u6559\u6848\uff1a</p> <p></p> <p></p> <p>\u8be5\u90e8\u5206\u8bb2\u89e3\uff1a</p> <p>\u8fd9\u662f\u4e00\u4e2a\u65b0\u6027\u8d28\uff0c\u6269\u6563\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u5199\u51fa\u540e\u9a8c\u6269\u6563\u6761\u4ef6\u6982\u7387</p> <p>q \u662f\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03</p> <p>\u5982\u679c\u8981\u5199 \\(q(x_{t-1}|x_t,x_0)\\) \u7684\u8bdd\uff0c\u8fd9\u4e2a\u5f0f\u5b50\u53ef\u4ee5\u7528\u4e00\u4e2a\u516c\u5f0f\u8fdb\u884c\u8868\u8fbe</p> <p>\u8fd9\u4e2a\u5c31\u662f\u6269\u6563\u8fc7\u7a0b\u4e2d\u540e\u9a8c\u7684\u6761\u4ef6\u6982\u7387\uff1a\u4e5f\u5c31\u662f\u7ed9\u5b9a \\(x_0\\) \u548c \\(x_t\\)\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u8ba1\u7b97\u51fa \\(x_{t-1}\\)</p> <p></p> <p>\u5f3a\u8c03\uff1a\u8fd9\u91cc\u662f\u9700\u8981\u7ed9\u5b9a \\(x_0\\)  \u7684\uff0c\u800c\u4e0d\u80fd\u5728\u7ed9\u5b9a \\(x_{t}\\) \u5c31\u80fd\u8ba1\u7b97\u51fa \\(x_{t-1}\\) \u5982\u679c\u8fd9\u6837\u7684\u8bdd\uff0c\u4e5f\u5c31\u4e0d\u9700\u8981\u8fd9\u4e2a\u6269\u6563\u7f51\u7edc\u4e86\uff0c\u6240\u4ee5\u4e00\u5b9a\u662f\u540e\u9a8c\u7684\uff0c\u7ed9\u5b9a \\(x_0\\)  \u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u7b97\u51fa \\(x_{t-1}\\)  \u7684\uff0c\u7ed9\u5b9a \\(x_t\\)  \u548c \\(x_0\\)  \u7684\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u51fa \\(x_{t-1}\\)  \u7684\uff0c\u5177\u4f53\u5730\u8ba1\u7b97\u65b9\u6cd5\u5c31\u662f\u57fa\u4e8e \u6761\u4ef6\u6982\u7387 \\(q\\) \uff0c\u516c\u5f0f\uff1a\\(q(x_{t-1}|x_t,x_0)\\)</p> <p></p> <p>\u73b0\u5728\u5047\u8bbe\u5728 \\(x_t\\) \\(x_0\\) \u7ed9\u5b9a\u7684\u6761\u4ef6\u4e0b\uff0c \\(x_{t-1}\\) \u670d\u4ece \u9ad8\u65af\u5206\u5e03\uff0c\u5747\u503c\u4e3a \\(\\tilde{\\mu}\\)  \uff0c\u65b9\u5dee\u4e3a \\(\\tilde{\\beta_t}\\)</p> <p>\u57fa\u4e8e\u8d1d\u53f6\u65af\u516c\u5f0f\uff1a</p> <p></p> <p>\uff081\uff09\u63a8\u5bfc\u7b2c\u4e00\u4e2a\u7b49\u53f7</p> <p>\\(q(x_{t-1}|x_t,x_0) = q(x_t|x_{t-1,x_0})\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\\)</p> <p>\u2460</p> <p>\\(=\\frac{q(x_{t-1},x_t|x_0)}{q(x_t|x_0)}\\)</p> <p>\\(=\\frac{q(x_{t-1}|x_0)q(x_t|x_{t-1},x_0)}{q(x_t|x_0)}\\)</p> <p>\u2461</p> <p>\\(=\\frac{q(x_{t-1},x_t,x_0)}{q(x_t,x_0)}\\)</p> <p>\\(=\\frac{q(x_0)q(x_{t-1}|x_0)q(x_t|x_{t-1},x_0)}{q(x_t,x_0)}\\)</p> <p>\\(=\\frac{q(x_{t-1}|x_0)q(x_t|x_{t-1},x_0)}{\\frac{q(x_t,x_0)}{q(x_0)}}\\)</p> <p>\\(=\\frac{q(x_{t-1}|x_0)q(x_t|x_{t-1},x_0)}{q(x_t|x_0)}\\)</p> <p>\u8bc1\u6bd5</p> <p>\uff082\uff09\u63a8\u5bfc\u7b2c\u4e00\u4e2a\u6b63\u6bd4\u4e8e</p> <p>\\(q(x_t|x_{t-1},x_0)\\)  \u7531\u9a6c\u5c14\u79d1\u592b\u5047\u8bbe\uff0c\\(x_t\\)  \u4e0e \\(x_0\\)  \u65e0\u5173</p> <p>\u2234  \\(q(x_t|x_{t-1},x_0) = q(x_t|x_{t-1})\\)</p> <p>\u7531\uff1a</p> <p></p> <p>\u6240\u4ee5\uff1a\\(q(x_t|x_{t-1}) = \\mathcal{N}(x_t;\\sqrt{1-\\beta_t}x_{t-1},\\beta_tI)\\)  \u5747\u503c= \\(\\sqrt{1-\\beta_t}x_{t-1}\\) </p> <p>\\(\u65b9\u5dee = \\beta_tI\\)</p> <p>\u8bb0 \\(1-\\beta_t=\\alpha_t\\)</p> <p>\u5219</p> <p>\\(q(x_t|x_{t-1})\\propto \\exp(-\\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{2\\beta_t})\\) </p> <p>\u8bc1\u6bd5</p> <p>\uff083\uff09\u63a8\u5bfc\u7b2c\u4e8c\u4e2a\u6b63\u6bd4\u4e8e\u53f7</p> <p>\u7531\uff1a</p> <p></p> <p>\u6240\u4ee5 \\(\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\\)</p> <p>\\(q(x_{t-1}|x_0) \\sim \\mathcal{N}(x_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}}x_0,(1-\\bar{\\alpha}_{t-1})I)\\)</p> <p>\\(q(x_{t}|x_0) \\sim \\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha}_{t}}x_0,(1-\\bar{\\alpha_{t}})I)\\)</p> <p>\u6240\u4ee5\uff1a</p> <p>\\(\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)} \\propto \\exp(-\\frac{1}{2}(\\frac{(x_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}}x_0)^2}{1-\\bar{\\alpha}_{t-1}}-\\frac{(x_t-\\sqrt{\\bar{\\alpha}_{t}}x_0)^2}{1-\\bar{\\alpha_{t}}}))\\)</p> <p>\uff084\uff09\u5408\u5e76\u6b63\u6bd4\u53f7</p> <p>\\(q(x_t|x_{t-1})\\propto \\exp(-\\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{2\\beta_t})\\)</p> <p>\\(\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)} \\propto \\exp(-\\frac{1}{2}(\\frac{(x_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}}x_0)^2}{1-\\bar{\\alpha}_{t-1}}-\\frac{(x_t-\\sqrt{\\bar{\\alpha}_{t}}x_0)^2}{1-\\bar{\\alpha_{t}}}))\\)</p> <p>\\(q(x_{t-1}|x_t,x_0) = \\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\\)</p> <p>\\(\\propto \\exp(-\\frac{1}{2}(\\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{\\beta_t}+\\frac{(x_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}}x_0)^2}{1-\\bar{\\alpha}_{t-1}}-\\frac{(x_t-\\sqrt{\\bar{\\alpha}_{t}}x_0)^2}{1-\\bar{\\alpha_{t}}}))\\)</p> <p>(\u5ffd\u7565\u7cfb\u6570\u3001\u53ea\u4fdd\u7559\u6307\u6570\u90e8\u5206)</p> <p>\u5316\u7b80\uff0c\u4ee5 \\(x_{t-1}\\)  \u4e3a\u57fa\u51c6\u8fdb\u884c\u5408\u5e76\uff0c\u53ea\u6458\u51fa \\(x_{t-1}^2\\)  \u548c \\(x_{t-1}\\) \uff0c\u5176\u4f59\u7684 \\(x_t\\)  \u548c \\(x_0\\) \u5168\u90e8\u5f52\u5230 \\(C(x_t,x_0)\\) \uff1a</p> <p></p> <p>\u6700\u540e\u5b8c\u6210\u5168\u90e8\u7684\u63a8\u5bfc  </p> <p>\u89e3\u91ca \\(C(x_t,x_0)\\) \uff1a</p> <p></p> <p>\uff085\uff09\u6839\u636e\u9ad8\u65af\u5206\u5e03\u7684\u6807\u51c6\u5f62\u5f0f\uff0c\u5f97\u5230\u5747\u503c\u548c\u65b9\u5dee\uff1a</p> <p></p> <p></p> <p>\u7531 \\(ax^2+bx=a(x+\\frac{b}{2a})^2+C\\)</p> <p>\u53ef\u4ee5\u77e5\u9053 \\(\u5747\u503c = -\\frac{b}{2a}\\)</p> <p>\\(\u65b9\u5dee = \\frac{1}{a}\\)</p> <p>\u6240\u4ee5</p> <p></p> <p>\u5747\u503c \\(\\tilde{\\mu}\\)  \u548c \u65b9\u5dee  \\(\\tilde\\beta_t\\)    \u53ef\u6c42</p> <p>\u7ecf\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u5b8c\u6210\u5168\u90e8\u8fd9\u4e00\u90e8\u5206\u7684\u63a8\u5bfc\uff1a</p> <p></p> <p>\u63a5\u4e0b\u6765\uff0c\u7ee7\u7eed\uff1a</p> <p></p> <p>\uff081\uff09\u7531\u53c2\u6570\u91cd\u6574\u5316\uff1a</p> <p></p> <p>\u79fb\u9879 \u53ef\u5bfc \\(x_0\\) \u548c \\(x_t\\)</p> <p>\\(x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(x_t-\\sqrt{1-\\bar{\\alpha}_t} z_t)\\)</p> <p>(2)</p> <p></p> <p>\u6700\u5f00\u59cb\u7684\u5747\u503c\u8868\u8fbe\u5f0f\uff1a</p> <p></p> <p>\u628a \\(x_0\\) \u4ee3\u5165</p> <p>\u6700\u7ec8\u5f97\u5230 \\(\\tilde{\\mu}_t = \\frac{1}{\\alpha_t}(x_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}z_t)\\)</p> <p>\u53ea\u4e0e \\(x_t\\) \u548c \\(z_t\\) \u6709\u5173\uff0c\\(z_t\\) \u8868\u793a  \\(t\\)\u65f6\u523b \u4ece \u6b63\u6001\u5206\u5e03\u4e2d \u7684\u91c7\u6837\u503c</p> <p>\uff083\uff09</p> <p>\u7ecf\u8fc7\u4ee5\u4e0a\uff0c\u5199\u51fa\u4e86\u6269\u6563\u8fc7\u7a0b\u4e2d\uff0c\u540e\u9a8c\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\u7684\u5747\u503c\u548c\u65b9\u5dee\u90fd\u5199\u51fa\u6765\u4e86</p> <p>\u65b9\u5dee\uff1a</p> <p></p> <p>\u53ea\u4e0e \\(\\alpha\\)  \u548c \\(\\beta\\) \u6709\u5173</p> <p>\u5747\u503c\uff1a</p> <p></p> <p>\u4e0e \\(x_t\\)  \u548c \\(z_t\\)  \u6709\u5173</p> <p>\u7b2c\u4e03\u90e8\u5206 \uff0c\u63a8\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u76ee\u6807\u6570\u636e\u7684\u4f3c\u7136\u51fd\u6570\uff0c\u63a8\u5bfc\u51fa\u4f3c\u7136\u51fd\u6570\uff0c\u5c31\u53ef\u4ee5\u4f18\u5316\u7f51\u7edc</p>"},{"location":"learning/6_Diffusion/#_4","title":"\u4e03\u3001\u5bf9\u6570\u4f3c\u7136\u4e0b\u754c\u63a8\u5bfc","text":"<p>\u5185\u5bb9\uff1a</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>\u8bb2\u89e3\uff1a</p> <p>\uff081\uff09</p> <p></p> <p>\\(-logp_{\\theta}(x_0) \\leq  -logp_{\\theta}(x_0) + KL\u6563\u5ea6\\)</p> <p>\\(KL\u6563\u5ea6 \\geq 0\\)</p> <p>\u6240\u4ee5\u6709 </p> <p></p> <p>\\(-logp_{\\theta}(x_0) + DL_{KL}\\) \u662f\u5b83 \\(-logp_{\\theta}(x_0)\\) \u7684\u4e0a\u754c</p> <p>\u6240\u4ee5 \u4f18\u5316 \\(-logp_{\\theta}(x_0) + DL_{KL}\\)  \u76f8\u5f53\u4e8e \u4f18\u5316 \\(-logp_{\\theta}(x_0)\\) </p> <p>\u4e5f\u5c31\u662f  \\(-logp_{\\theta}(x_0) + DL_{KL}\\) \u8fbe\u5230\u6700\u5c0f\uff0c\u5c31\u662f \\(-logp_{\\theta}(x_0)\\)  \u8fbe\u5230\u6700\u5c0f</p> <p>\u79f0\u4e3a \u8d1f\u5bf9\u6570\u4f3c\u7136 \\(-logp_{\\theta}(x_0)\\)</p> <p>\u5148\u9a8c\u77e5\u8bc6\uff1a</p> <p>KL\u6563\u5ea6\u516c\u5f0f\uff1a</p> <p>\\(D(p||q) = H(p,q)-H(p) = \\sum p_i\\log\\frac{1}{q_i}-p_i\\log\\frac{1}{p_i}=\\sum p_i\\log\\frac{p_i}{q_i} = \\mathbb{E}_{x \\sim p}(log\\frac{p}{q})\\)</p> <p> </p> <p>\u7ee7\u7eed\u63a8\u516c\u5f0f\uff1a</p> <p></p> <p>\u7b2c\uff081\uff09\u4e2a \u5c0f\u4e8e\u7b49\u4e8e\u53f7\uff1a</p> <p>K1\uff1a\u4e3a \u4e00\u4e2a \u8d1f\u7684 \u5bf9\u6570\u4f3c\u7136 \u51d1\u9879</p> <p>K2\uff1aKL\u6563\u5ea6 \u5927\u4e8e\u7b49\u4e8e 0 \u6052\u6210\u7acb</p> <p>\u7b2c\uff082\uff09\u4e2a\u7b49\u4e8e\u53f7\uff1a</p> <p>K1\uff1a</p> <p>\\(KL\u6563\u5ea6= D(p||q) = H(p,q)-H(p) = \\sum p_i\\log\\frac{1}{q_i}-p_i\\log\\frac{1}{p_i}=\\sum p_i\\log\\frac{p_i}{q_i} = \\mathbb{E}_{x \\sim p}(log\\frac{p}{q})\\)</p> <p>K2\uff1a</p> <p>\\(-logp_{\\theta}(x_0) \\leq -logp_{\\theta}(x_0) + D_{KL}(q(x_{1:T}|x_0)||p_{\\theta}(x_{1:T}|x_0))\\)</p> <p>\uff081\uff09\u62c6\u51faKL\u6563\u5ea6\uff1a\\(D_{KL}(q(x_{1:T}|x_0)||p_{\\theta}(x_{1:T}|x_0))\\)</p> <p>\\(= \\mathbb{E}_{x_{1:T} \\sim q(x_{1:T}|x_0)}[\\log \\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{1:T}|x_0)}]\\)</p> <p>\uff082\uff09</p> <p>\u62c6\u51fa\u6761\u4ef6\u6982\u7387\uff1a\\(p_{\\theta}(x_{1:T}|x_0)\\)</p> <p>\\(=\\frac{p_{\\theta}(x_{0:T})}{p_{\\theta}(x_0)}\\)</p> <p></p> <p>\u7b2c\u4e8c\u4e2a \u7b49\u4e8e\u53f7\u4e5f \u8bc1\u660e\u51fa\u6765\u4e86</p> <p></p> <ul> <li>\u540e\u9762\u7684\u7b49\u4e8e\u53f7 \u5c31\u6bd4\u8f83\u597d\u8bf4\u4e86\uff0c\u987a\u7740\u5199</li> <li>\u5173\u4e8e \u7b2c\uff083\uff09\u4e2a\u7b49\u4e8e\u53f7\uff0c\u6709\u4e00\u4e2a\u7ec6\u8282\uff0c\u56e0\u4e3a \\(\\mathbb{E}_q logp_{\\theta}(x_0)\\) \u4e2d \\(p_{\\theta}(x_0)\\) \u4e0e \\(q\\) \u65e0\u5173\uff0c\u6240\u4ee5\u53ef\u4ee5\u76f4\u63a5\u62ff\u51fa\u6765\uff0c\u5e76\u4e14\u4e0e\u524d\u9762 \u6d88\u9879</li> </ul> <p></p> <p>\u7ee7\u7eed\u770b</p> <p>\u5728 \u63a8\u5bfc\u51fa </p> <p>\\(-logp_{\\theta}(x_0) \\leq \\mathbb{E}_{{x_{1:T} \\sim q(x_{1:T}|x_0)}}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]\\)</p> <p>\u4ee5\u540e\uff0c\u5de6\u53f3\u4e24\u8fb9 \u5bf9\u4e8e\\(q(x)\\) \u6c42\u671f\u671b\u65f6\uff0c\u4ece \\(x_0\\) \u5f00\u59cb\uff1a</p> <p>\u6362\u53e5\u8bdd\u8bf4\uff1a\u5bf9\\(q(x_0)\\) \u6c42\u671f\u671b</p> <p>\\(-logp_{\\theta}(x_0) \\leq \\mathbb{E}_{{x_{1:T} \\sim q(x_{1:T}|x_0)}}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]\\) \\(\\iff\\) \\(-logp_{\\theta}(x_0) \\leq \\mathbb{E}_{ q(x_{1:T})}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]\\)</p> <p>(\u5728\u4e8e \u4e0b\u6807\u7684\u53d8\u5316)</p> <p>\u2234 \\(-\\mathbb{E}_{q(x_0)}logp_{\\theta}(x_0) \\leq \\mathbb{E}_{{q(x_{0:T})}}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]\\)</p> <p>\u53cd\u8fc7\u6765 \u5c31\u662f \u8bb2\u4e49\u4e0a\u5199\u7684\u4e86</p> <p>\\(\\mathbb{E}_{{q(x_{0:T})}}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]  \\geq -\\mathbb{E}_{q(x_0)}logp_{\\theta}(x_0)\\)</p> <p>\u6ce8\u610f\u5230   \\(-\\mathbb{E}_{q(x_0)}logp_{\\theta}(x_0)\\)  \u8fd9\u4e2a\u4e1c\u897f\u662f\u4ea4\u53c9\u71b5</p> <p>\u6240\u4ee5\u73b0\u5728\u6211\u4eec\u6700\u5c0f\u5316  \u4ea4\u53c9\u71b5 \\(-\\mathbb{E}_{q(x_0)}logp_{\\theta}(x_0)\\) \u5c31\u662f\u6700\u5c0f\u5316 \u4ea4\u53c9\u71b5\u7684\u4e0a\u754c\uff1a\\(\\mathbb{E}_{{q(x_{0:T})}}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]\\) </p> <p>\u603b\u4e4b\uff1a\\(\\mathbb{E}_{{q(x_{0:T})}}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]\\)  \u8fd9\u4e2a\u5f0f\u5b50 \u5c31\u662fLoss\u7684\u4e0a\u754c\uff0c\u6700\u5c0f\u5316\u4e86\u8fd9\u4e2a\u4e0a\u754c\uff0c\u5c31\u662f\u6700\u5c0f\u5316\u4e86Loss\uff0c\u63a5\u4e0b\u6765\u8fdb\u4e00\u6b65\u5316\u7b80\u4e0a\u754c\uff1a\\(\\mathbb{E}_{{q(x_{0:T})}}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]\\)</p> <p>\u4e0a\u754c\u8bb0\u4e3a VLB\uff08why\uff1f\uff09</p> <p></p> <p>\u9996\u5148\uff1a</p> <p>\\(L_{VLB} = \\mathbb{E}_{{q(x_{0:T})}}[log\\frac{q(x_{1:T}|x_0)}{p_{\\theta}(x_{0:T})}]\\)</p> <p>\u63a5\u7740\uff0c\u628a\u5206\u5b50\u5206\u6bcd\u5199\u6210\u5f88\u591a\u6761\u4ef6\u6982\u7387\u76f8\u4e58\u7684\u5f62\u5f0f</p> <p>\\(=\\mathbb{E}\\frac{\\prod q}{\\prod p}\\)</p> <p>\u63a5\u4e0b\u6765\uff0c\u8fd9\u4e00\u5927\u4e32\uff1a</p> <p></p> <p>\u8fd9\u91cc\u7684\u63a8\u5bfc\u5f3a\u8c03\u4e00\u4e0b\uff1a </p> <p>\uff081\uff09</p> <p>\u4ed4\u7ec6\u770b\u4e00\u4e0b\uff0c\u53d8\u4e0e\u4e0d\u53d8 \\(p_{\\theta}(x_{t-1}|x_t)\\) \u662f\u6ca1\u6709\u53d8\u5f97\uff0c\u53ea\u662f \\(q(x_t|x_{t-1})\\) \u53d8\u4e86\uff0c\u63a5\u7740\u518d\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\u5316\u7b80\u5373\u53ef</p> <p></p> <p>\\(q(x_t|x_{t-1})\\)</p> <p>= \\(\\frac{q(x_{t-1}|x_{t})q(x_t)}{q(x_{t-1})}\\)</p> <p>(\u9a6c\u5c14\u79d1\u592b\u6027\u8d28\uff0c\u6240\u4ee5\u53ef\u4ee5\u52a0\u6761\u4ef6 \\(x_0\\))= \\(\\frac{q(x_{t-1}|x_{t},x_0)q(x_t,x_0)}{q(x_{t-1},x_0)}\\)</p> <p>\uff082\uff09</p> <p></p> <p></p> <p>\uff083\uff09</p> <p></p> <p>\uff084\uff09</p> <p>\u4ee5\u4e0b\u4e24\u5f20\u56fe\u7247\uff0c\u8868\u793a\u4e00\u4e2a\u610f\u601d\u3002</p> <p>\u2460 \u4e0a\u9762\u7684\u56fe\uff1a\u6709\u70b9\u5c0f\u95ee\u9898</p> <p>\u6539\u6210\uff1a\\(=D_{KL}(q(x_T|x_0)||p_{\\theta}(x_T)) + \\sum_{t=2}^T D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\\theta}(x_{t-1}|x_t))-\\mathbb{E}_q \\log p_{\\theta}(x_0|x_1)\\)</p> <p>\u8fd9\u6837\uff0c\u4e0a\u56fe\u548c\u4e0b\u56fe\u662f\u4e00\u6837\u7684</p> <p>\u2461 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c</p> <p>KL \u6563\u5ea6\u516c\u5f0f</p> <p></p> <p>\u4e2d\u95f4\u8fd8\u6709\u5220\u6389\u65e0\u5173\u53d8\u91cf\u7684\u6b65\u9aa4\uff1a</p> <p></p>"},{"location":"learning/6_Diffusion/#diffusion-probabilistic-model","title":"\u516b\u3001DIffusion Probabilistic Model \u7b97\u6cd5\u4ee3\u7801","text":"<p>\u6269\u6563\u4e0e\u9006\u6269\u6563\u8fc7\u7a0b\u4f2a\u4ee3\u7801</p> <p></p> <p>pytorch\u5b9e\u73b0\u65e0\u76d1\u7763\u56fe\u50cf\u751f\u6210</p>"},{"location":"learning/6_Diffusion1/","title":"VDM","text":"<p>ref\uff1a\u3010\u516c\u5f0f\u63a8\u5bfc\u3011\u8fd8\u5728\u5934\u75bcDiffusion\u6a21\u578b\u516c\u5f0f\u5417\uff1fDiffusion\u7406\u8bba\u516c\u5f0f\u5582\u996d\u5f0f\u8d85\u8be6\u7ec6\u9010\u6b65\u63a8\u5bfc\u6765\u4e86\uff01</p> <p>\u300aUnderstanding Diffusion Models: A Unified Perspective\u300b\uff1ahttps://arxiv.org/abs/2208.11970 </p> <p>\u300aDenoising Diffusion Probabilistic Models\u300b:https://arxiv.org/abs/2006.11239</p> <p></p> <p></p>"},{"location":"learning/6_Diffusion1/#1-vae","title":"1 VAE","text":""},{"location":"learning/6_Diffusion1/#11","title":"1.1 \u6781\u5927\u4f3c\u7136\u6a21\u578b","text":"<p>\u76ee\u7684\u662f\u4f7f\u5f97\u6a21\u578b\u5b66\u5f97\u6570\u636e\u96c6\u7684\u5206\u5e03</p> <p>\u7ed9\u5b9a\u6570\u636e\u96c6 \\(x_D\\)\uff0c\u901a\u8fc7\u6a21\u578b\u5b66\u5f97 \\(\\phi\\)\u53c2\u6570\uff0c\u5f97\u5230\u6a21\u578b \\(p_{\\phi}(x_D)\\)\uff0c\u8bad\u7ec3\u6a21\u578b\u6700\u5927\u5316 \\(p_{\\phi}(x_D)\\)\uff0c\u4f7f\u5f97 \\(p_{\\phi}(x_D)\\) \u80fd\u591f\u62df\u5408\u6837\u672c \\(x'\\)</p> <p>\u5f97\u5230\u6a21\u578b \\(p_{\\phi}(x_D)\\) \u4ee5\u540e\uff0c\u8fd8\u80fd\u591f\u8fdb\u884c\u91c7\u6837\uff0c\u91c7\u6837\u51fa\u6765\u7684\u6837\u672c\u53ef\u80fd\u4e0d\u5728\u6570\u636e\u96c6\u91cc\u9762\uff0c\u4f46\u662f\u91c7\u6837\u51fa\u6765\u7684\u6837\u672c\u662f\u7b26\u5408\u6982\u7387\u5206\u5e03\u7684\u503c</p>"},{"location":"learning/6_Diffusion1/#12-vae","title":"1.2 VAE","text":"<ul> <li> \u91cd\u70b9\u5728\u4e8e\u7406\u89e3\uff1aVAE\u5c31\u662f\u5efa\u7acb \u771f\u5b9e\u56fe\u7247 \\(x\\) \u4e0e \u9690\u53d8\u91cf\u9ad8\u65af\u5206\u5e03\\(z\\) \u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u5de5\u5177\u662f Encoder\u548cdecoder</li> </ul> <p>\u9996\u5148\uff0c\u751f\u6210\u6a21\u578b\u5fc5\u987b\u8981\u6709\u80fd\u591f\u91c7\u6837\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u80fd\u53ea\u80fd\u56fa\u5b9a\u751f\u6210\u4e00\u4e9b\u6837\u672c</p> <p>\u6240\u4ee5\uff0c\u9996\u5148\u8981\u6709\u4e00\u4e2a\u5206\u5e03\u4f9b\u6211\u91c7\u6837\uff0c\u6709\u4e86\u5206\u5e03\u624d\u80fd\u91c7\u6837\uff0cVAE\u5c31\u501f\u52a9\u4e86\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u5206\u5e03\uff0c\u5c31\u662f\u6807\u51c6\u9ad8\u65af\u5206\u5e03\uff0c\u7528\u516c\u5f0f\u8868\u8fbe\uff1a\u4ee50\u4e3a\u5747\u503c\uff0c\\(I\\) \u4e3a\u65b9\u5dee\u7684\u4e00\u4e2a\u5206\u5e03\uff0c\u6ce8\u610f\u662f\u591a\u7ef4\u7684  \\(N \\sim N(z;\\mathrm{0},\\mathrm{I})\\)</p> <p>VAE\u7684\u7ed3\u6784\u9996\u5148\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1aEncoder+decoder</p> <p>\u6587\u5b57\u8868\u8fbe\uff1a</p> <p>**Encoder\uff1a **\u628a\u4e00\u4e9b\u56fe\u50cf\u901a\u8fc7Encoder\u53d8\u6210\u4e00\u4e9bembedding\uff0c\u4e00\u822c\u662f\u7ef4\u5ea6\u6bd4\u8f83\u4f4e\u7684embedding</p> <p>**decoder\uff1a **decoder\u88ab\u7ef4\u5ea6\u6bd4\u8f83\u4f4e\u7684embedding\u751f\u6210\u4e00\u4e9b\u7ef4\u5ea6\u6bd4\u8f83\u9ad8\u7684\u56fe\u7247</p> <p>\u6570\u5b66\u8868\u8fbe\uff1a</p> <p>Encoder\uff1a\\(q(z|x)\\) \u7ed9\u5b9a\u56fe\u7247 \\(x\\) \u80fd\u591f\u62ff\u5230 \\(z\\)\uff0c\\(z\\)\u8868\u793a\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u7684\u4e00\u4e2a\u6837\u672c</p> <p>Decoder\uff1a\\(p(x|z)\\) \u7ed9\u5b9a\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u7684\u4e00\u4e2a\u503c\uff0c\u901a\u8fc7decoder\u751f\u6210\u4e00\u5f20\u56fe\u7247\\(x\\)</p> <p>VAE\u7684Encoder\u548cdecoder\u5c31\u662f\u5728 \\(x\\)\u548c\\(z\\)\u4e4b\u95f4\u5efa\u7acb\u8d77\u8054\u7cfb</p> <p>\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u505a\u7684\u539f\u56e0\uff1a</p> <p>\u56e0\u4e3a\u8ba9\u6a21\u578b\u76f4\u63a5\u5b66\u4e60 \\(x\\)\u7684\u5206\u5e03\u6bd4\u8f83\u96be\uff0c\\(x\\)\u53ef\u80fd\u5e76\u4e0d\u662f\u5e38\u89c1\u7684\u5206\u5e03\uff0c\u6240\u4ee5\u6ca1\u6709\u529e\u6cd5\u91c7\u6837\uff0c\u4f46\u5982\u679c\u628a\\(x\\)\u4e0e\u4e00\u4e2a\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u8054\u7cfb\u8d77\u6765\uff0c\u90a3\u4e48\u6bcf\u5728\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u4e0a\u91c7\u6837\u4e00\u4e2a\u503c\uff0c\u63a5\u7740\u901a\u8fc7decoder\u5c31\u80fd\u6620\u5c04\u6210\u5728\\(x\\)\u4e5f\u5c31\u662fpixel\u8fd9\u4e2a\u50cf\u7d20\u7a7a\u95f4\u4e0a\uff0c\u751f\u6210\u4e00\u5f20\u56fe\u7247\uff0c\u8fd9\u6837\u5c31\u4f1a\u6709\u5f88\u591a\u6837\u672c\u4ea7\u751f\uff0c\u56e0\u4e3a\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u53ef\u4ee5\u91c7\u6837\u51fa\u65e0\u7a77\u65e0\u5c3d\u4e2a\u6837\u672c</p> <p>\u4e5f\u5c31\u662f\u8fd9\u53e5\u8bdd\uff1a</p> <p></p> <p>z \u8868\u793a\u9690\u53d8\u91cf\uff0c\u4e00\u822c\u91c7\u7528\u9ad8\u65af\u5206\u5e03\uff0c\u4e14\u4e00\u822c\u662f\u591a\u7ef4\u7684\uff0c\u7406\u8bba\u4e0a\u8bb2\u53ef\u4ee5\u662f\u4efb\u610f\u4e00\u4e2a\u80fd\u591f\u8fdb\u884c\u65b9\u4fbf\u91c7\u6837\u7684\u5206\u5e03\uff0c\u53ea\u662f\u56e0\u4e3a\u9ad8\u65af\u90e8\u5206\u6bd4\u8f83\u7b80\u6d01\uff0c\u63a8\u5bfc\u4e5f\u6bd4\u8f83\u7b80\u5355\uff0c\u6240\u4ee5\u4e00\u822c\u91c7\u7528\u9ad8\u65af\u5206\u5e03\u4f5c\u4e3a\u9690\u53d8\u91cf\u3002</p> <p>\u4e00\u53e5\u8bdd\uff0cVAE\u5c31\u662f\u5efa\u7acb \u771f\u5b9e\u56fe\u7247 \\(x\\) \u4e0e \u9690\u53d8\u91cf\u9ad8\u65af\u5206\u5e03\\(z\\) \u4e4b\u95f4\u7684\u8f6c\u6362</p>"},{"location":"learning/6_Diffusion1/#13-vae","title":"1.3 VAE\u7684\u6570\u5b66\u539f\u7406","text":"<p>\u9996\u5148\uff0cVAE\u7684\u4f18\u5316\u76ee\u6807\uff0c\u5b66\u4e60\u5230\u7684\u5206\u5e03 \\(p_{\\phi}(x)\\) \u8d8b\u8fd1\u4e8e \u771f\u5b9e\u5206\u5e03 \\(p(x)\\)</p> <p>\u9996\u5148\u660e\u786e\u4f18\u5316\u76ee\u6807\uff1a  \\(p_{\\phi}(x) \\rightarrow p(x)\\)</p> <p>\u90a3\u5177\u4f53\u600e\u4e48\u5b9e\u73b0\u76ee\u6807\u5462\uff1f\u601d\u60f3\u5c31\u7c7b\u4f3c\u4e8e\u8f85\u52a9\u7ebf &amp; \u4e2d\u95f4\u53d8\u91cf\uff0c\u8fd9\u91cc\u501f\u52a9\u7684\u4e2d\u95f4\u53d8\u91cf\u5c31\u662f \\(z\\)</p> <p>\u9996\u5148\uff0c\u771f\u5b9e\u5206\u5e03 \\(p(x)\\) \u4e00\u822c\u7528\u5bf9\u6570\u4f30\u8ba1\uff0c\u8868\u793a\u6210 \\(logp(x)\\) \uff0c\u4e14\u6709\u4e0b\u754c \\(\\mathbb{E}_{q_{\\phi}(z|x)}[log \\frac{p(x,z)}{q_{\\phi}(z|x)}]\\)</p> <p>\u4ee5\u4e0a\u6b65\u9aa4\u90fd\u5f88\u7b80\u5355\uff0c\u6ca1\u5565\u597d\u8bf4\u7684\uff0c\u503c\u5f97\u6ce8\u610f\u7684\u662f\u7434\u751f\u4e0d\u7b49\u5f0f\uff0c\u4e5f\u5c31\u662f\u628alog\u5185\u79fb</p> <p>\u7406\u89e3\u7434\u751f\u4e0d\u7b49\u5f0f\uff1a</p> <p></p> <p>\u6587\u5b57\u63cf\u8ff0\uff1a\u4efb\u610f\u4e24\u70b9\u7684\u671f\u671b \uff1e \u671f\u671b\u7684\u51fd\u6570\u503c</p> <p>\u56fe\u793a\uff1a\u4efb\u610f\u4e24\u70b9\u7684\u8fde\u7ebf\uff0c\u6c38\u8fdc\u5728\u66f2\u7ebf\u4e0a\u65b9</p> <p>\u51fd\u6570\u503c\u7684\u671f\u671b \uff1e \u671f\u671b\u7684\u51fd\u6570\u503c\uff08\u5bf9\u4e8e\u51f8\u51fd\u6570\u6765\u8bf4\uff1a\u671f\u671b&gt;\u51fd\u6570\u503c\uff09</p> <p>\u800clog\u51fd\u6570\u662f\u4e2a\u51f9\u51fd\u6570\uff0c\u53cd\u8fc7\u6765\u7684\uff0c\u4e5f\u5c31\u662f \u5f26 \u5728 \u66f2\u7ebf\u7684 \u4e0b\u65b9\uff08\u4e5f\u5c31\u662f \u51fd\u6570\u503c &gt; \u671f\u671b\uff09\u501f\u52a9\u8111\u888b\u4e2d\u7684\u56fe\u50cf\u7406\u89e3</p> <p>\u4e0d\u7528\u6b7b\u8bb0\u786c\u80cc\uff0c\u53bb\u60f3\u56fe\u50cf\uff0c\u753b\u4e2a\u56fe\u5373\u53ef\uff0c\u4e5f\u5c31\u662f\u770b\u660e\u767d\u4e86\u516c\u5f0f\uff0c\u8fd9\u4e2a\u516c\u5f0f\u8bc1\u660e\u51fa\u6765\u6709\u4ec0\u4e48\u7528\uff1f</p>"},{"location":"learning/6_Diffusion1/#14-vaeelbo","title":"1.4 \u4e3a\u4ec0\u4e48\u8bf4 \u4f18\u5316VAE\u300a==\u300b\u6700\u5927\u5316ELBO\uff1f","text":"<p>\\(ELBO= \\mathbb{E}_{q_{\\phi}(z|x)}[log \\frac{p(x,z)}{q_{\\phi}(z|x)}]\\)</p> <p>\u518d\u89e3\u91ca\u4e00\u4e0b\uff1a\u7ed9\u5b9a\u6570\u636e\u96c6 \\(x\\)\uff0c\u90a3\u4e48\\(p(x)\\)\u786e\u5b9a\uff0c\u4e5f\u5c31\u662f\\(logp(x)\\)\u786e\u5b9a</p> <p>Encoder\u7684\u4f18\u5316\u76ee\u6807 \\(q_{\\phi}(z|x)=p(z|x)\\)  \u300a====\u300b KL\u6563\u5ea6=0</p> <p>\u90a3\u548c\u53c8\u662f\u56fa\u5b9a\u7684\uff0c\u6240\u4ee5\u62df\u5408\u672a\u77e5\u4f46\u662f\u786e\u5b9a\u7684p(x)\uff0c\u53d8\u6210\u4e86\u6700\u5927\u5316 ELBO</p> <p>VAE\u7684\u76ee\u7684\u5c31\u662f\u5b66\u5230\u771f\u5b9e\\(x\\)\u7684\u5206\u5e03\\(p(x)\\)\uff0c\u4f46\u662f\u53c8\u4e0d\u597d\u76f4\u63a5\u5b66\u5230\uff0c\u6240\u4ee5\u501f\u52a9\u8f85\u52a9\u53d8\u91cfz\uff0c\u901a\u8fc7Encoder\u548cdecoder\u95f4\u63a5\u5b66\u5230\\(x\\)\u7684\u5206\u5e03\uff0c\u53c8\u63a8\u51fa\u4e86\u4e0b\u754c\uff0c\u4e0b\u754c\u5c31\u662f\\(ELBO\\)\uff0c\u4f46\u662f\\(logP(x)=ELBO+KL\u6563\u5ea6\\)\uff0c\\(KL\u6563\u5ea6\\)\u662fEncoder\u7684\u76ee\u6807\uff0c\u7b49\u4e8e\\(0\\)\u6700\u597d\u5c31\u662f \\(q(z|x)\\)\u65e0\u9650\u63a5\u8fd1\\(p(z|x)\\)</p>"},{"location":"learning/6_Diffusion1/#15-elbo","title":"1.5 \u62c6\u89e3ELBO","text":"<p>\u4f7f\u7528\u94fe\u5f0f\u6cd5\u5219\u7684\u539f\u56e0\u662f\u56e0\u4e3a\uff0c\u6211\u4eec\u7528\u7684\u662f\u4e2d\u95f4\u53d8\u91cf\\(z\\)</p> <p></p> <p>\u5c06 ELBO\u62c6\u6210\u4e86 <code>ELBO=\u91cd\u5efa\u9879-\u5148\u9a8c\u5339\u914d\u9879</code></p> <p>\\(=\\mathbb{E}_{q_{\\phi}}(z|x)[logp_{\\theta}(x|z)]-D_{KL}(q_{\\phi}(z|x)||p(z))\\)</p> <ul> <li> \u9996\u5148\uff0c\u4e3a\u4ec0\u4e48\u53eb\u91cd\u5efa\u9879\uff1f</li> </ul> <p>\\(p(x|z)\\) \u4e5f\u5c31\u662f\u5df2\u77e5\\(z\\)\u5b66\u4e60\\(x\\)\uff0c\u5c31\u662f\\(decoder\\)</p> <p>decoder\u7684\u76ee\u7684\u5c31\u662f\u91cd\u5efa\u56fe\u50cf\\(x\\)\uff0c\u6240\u4ee5\u53eb\u91cd\u5efa\u9879</p> <ul> <li> \u4e3a\u4ec0\u4e48\u53eb\u5148\u9a8c\u5339\u914d\u9879\uff1f  \\(D_{KL}(q_{\\phi}(z|x)||p(z))\\)</li> </ul> <p>\\(q_{\\phi}(z|x)\\) \\(x\\)\u5230\\(z\\)\u7684\u5206\u5e03\uff0c\u5c31\u662f\\(Encoder\\)</p> <p>\u4e5f\u5c31\u662f\u8ba9Encoder\u5b66\u5230\u7684\u5206\u5e03\u63a5\u8fd1\u4e8e\\(z\\)\uff0c\\(z\\) \u662f\u4ec0\u4e48\uff1f\\(z\\)\u5c31\u662f\u6211\u4eec\u91c7\u6837\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u4e5f\u5c31\u662f\\(p(z)\\)</p> <p>\u6211\u4eec\u8981\u8ba9 \u6211\u4eec\u7684Encoder\u7f16\u7801\uff0c\u628a\\(x\\)\u6620\u5c04\u5230\\(z\\)\u7a7a\u95f4\u4e0a\uff0c\\(z\\)\u80fd\u6ee1\u8db3\u6307\u5b9a\u7684\u5206\u5e03\uff0c\u4e5f\u5c31\u662fprior matching</p> <p>\\(z\\)\u5c31\u662fprior\uff0c\u4e5f\u5c31\u662fEncoder\u4ee5\u540e\u7684\\(z\\)\uff0c\\(z\\)\u6ee1\u8db3\u6211\u4eec\u81ea\u5df1\u9009\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u4e5f\u5c31\u662f\u628a \u6211\u4eec\u7684 \\(x\\)\u6620\u5c04\u5230\\(z\\)\uff0c\\(z\\)\u6ee1\u8db3\u6211\u4eec\u81ea\u5df1\u9009\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u771f\u662f\u56e0\u4e3a\u6211\u4eec\u7f16\u7801\u540e\u7684\\(z\\)\u6ee1\u8db3\u9ad8\u65af\u5206\u5e03\uff0c\u540e\u7eed\u6211\u4eec\u624d\u53ef\u4ee5\u629b\u5f00Encoder\uff0c\u540e\u7eed\u76f4\u63a5\u5728decoder\u4e0a\u8fdb\u884c\u91c7\u6837\u5f97\u5230\u9ad8\u65af\u5206\u5e03\u503c\uff0c\u7136\u540e\u901a\u8fc7decoder\u5f97\u5230\\(x\\)</p> <p>\u4e00\u53e5\u975e\u5e38\u91cd\u8981\u7684\u8bdd\uff1a</p> <p>Encoder\u628ax\u6620\u5c04\u6210z\uff0c\u5e76\u4e14\u8fd9\u4e2az\u65e0\u9650\u63a5\u8fd1\u9ad8\u65af\u5206\u5e03</p> <p>\u6240\u4ee5\u540e\u9700\u624d\u53ef\u4ee5\u76f4\u63a5\u4e22\u6389Encoder\uff0c\u76f4\u63a5\u91c7\u6837x\u6620\u5c04\u7684\u9ad8\u65af\u5206\u5e03z\uff0c\u76f4\u63a5\u91c7\u6837\uff0c\u7136\u540e\u91cd\u5efax</p> <p>\u4e5f\u5c31\u662f\u4fdd\u8bc1VAE\u80fd\u505a\u751f\u6210\u4efb\u52a1\u7684\u524d\u63d0\uff1a<code>\u628a\u771f\u5b9e\u5206\u5e03\u6620\u5c04\u5230 \u9ad8\u65af\u5206\u5e03\u4e0a</code></p> <ul> <li> \u4f18\u5316VAE\uff0c\u5c31\u662f\u4f18\u5316ELBO\uff0cELBO\u6709\u4e24\u9879\uff1a\u91cd\u5efa\u9879+KL\u6563\u5ea6\u9879</li> </ul> <p>\u6240\u4ee5\uff0c\u6700\u5927\u5316ELBO\uff0c\u5c31\u662f\u6700\u5927\u5316\u91cd\u5efa\u9879\uff0c\u4e5f\u5c31\u662f\u6700\u5c0f\u5316\u5148\u9a8c\u5339\u914d\u9879</p> <p>\u63a5\u4e0b\u6765\u89e3\u91ca\uff0c\u6700\u5927\u5316\u91cd\u5efa\u9879\u662f\u4ec0\u4e48\u610f\u601d\uff1f\u6700\u5c0f\u5316\u5148\u9a8c\u5339\u914d\u9879\u53c8\u662f\u4ec0\u4e48\u610f\u601d\uff1f</p> <ul> <li> \u6700\u5927\u5316\u91cd\u5efa\u9879\u600e\u4e48\u7406\u89e3\uff1a  \\(\\mathbb{E}_{q_{\\phi}}(z|x)[logp_{\\theta}(x|z)]\\)</li> </ul> <p>\u6700\u5927\u5316\u91cd\u5efa\u9879\u4e5f\u662f\u6a21\u578bdecoder\u51fa\u6765\u7684\\(x\\)\uff0c\u548c\u771f\u5b9e\u7684\\(x\\) \u8d8a\u76f8\u8fd1\u8d8a\u597d\uff0c\u4e5f\u5c31\u662f\u91cd\u5efa\u8bef\u5dee\u8d8a\u5c0f\u8d8a\u597d\uff0c\u4e5f\u5c31\u662f\u91cd\u5efa\u51fa\u6765\u7684\u6982\u7387\u8d8a\u5927\u8d8a\u597d\uff0c\u4e5f\u5c31\u662f \\(p_{\\theta}(x|z) \\rightarrow 1\\) \uff0c\u4e5f\u5c31\u662f  \\(logp_{\\theta}(x|z) \\rightarrow 0\\)</p> <ul> <li> \u6700\u5c0f\u5316 \u5148\u9a8c\u5339\u914d\u9879 \\(D_{KL}(q_{\\phi}(z|x)||p(z))\\)</li> </ul> <p>\\(q_{\\phi}(z|x)\\)  \u901a\u8fc7Encoder\u6620\u5c04\u5230\u9690\u7a7a\u95f4\u4e4b\u540e\uff0c\u5c3d\u91cf\u6ee1\u8db3\u5206\u5e03\uff0c\u8d8a\u63a5\u8fd1\u8d8a\u597d\uff0c\u4e5f\u5c31\u662f<code>\u6700\u5c0f\u5316KL\u6563\u5ea6\u21920</code></p> <p>\u8fd9\u9879KL\u6563\u5ea6\uff0c\u76f8\u5f53\u4e8e\u8bad\u7ec3Encoder\u7684loss\uff0c\u8fd9\u9879loss\u964d\u5230\u6700\u5c0f\uff0c\u4e5f\u5c31\u662fEncoder\u5b66\u5230\u6700\u597d</p> <p>\u5982\u679c<code>\u8fd9\u4e00\u9879\uff1d0</code>\uff0c\u4e5f\u5c31\u662f\u5b8c\u5168\u628a\u771f\u5b9e\u4e16\u754c\u7684\\(x\\)\u6620\u5c04\u5230\u4e86\u6807\u51c6\u7684\u6b63\u6001\u5206\u5e03\uff0c\u6b64\u65f6Encoder\u662f\u5b66\u4e60\u7684\u975e\u5e38\u5b8c\u7f8e\u7684</p> <p>\u4e5f\u5c31\u662f<code>KL=0</code></p> <ul> <li> ADD\uff1a\u5982\u679c\u628a\u540e\u9762\u9879\u53bb\u6389\uff0c\u5c31\u662f\u53ea\u7559\u4e0b \u91cd\u5efa\u9879\uff0c\u5c31\u53d8\u6210\u7684AE\uff08AutoEncoder\uff09\u6a21\u578b\uff0cAutoEncoder\u6ca1\u6709\u751f\u6210\u80fd\u529b\uff0c\u56e0\u4e3a\u6ca1\u6709\u628a\u539f\u59cb\u6570\u636e\u6620\u5c04\u5230\u9ad8\u65af\u5206\u5e03\u4e0a\uff0c\u4e5f\u5c31\u662f\u5728\u751f\u6210\u7684\u65f6\u5019\uff0c\u6ca1\u6709\u529e\u6cd5\u8fdb\u884c\u91c7\u6837\uff0c\u4e5f\u5c31\u662f\u6ca1\u6709\u529e\u6cd5\u901a\u8fc7decoder\u5f97\u5230\u56fe\u50cf\uff0c\uff08\u6ca1\u6709\u4e86\u91c7\u6837\uff0c\u5c31\u6ca1\u6709\u4e86\u751f\u6210\uff09</li> </ul> <p>AE\u6a21\u578b\u6ca1\u6709\u4e86\u91c7\u6837\u80fd\u529b\uff0cdecoder\u51fa\u6765\u7684\u4e1c\u897f\u4e5f\u5c31\u4e0d\u662f\u591a\u6837\u7684\uff0c\u4e0d\u662f\u4e00\u4e2a\u5168\u65b0\u7684\uff0c\u6b64\u65f6\\(z\\)\u7684\u5206\u5e03\uff0c\u5c31\u662fAutoEncoder\u7684z\u5206\u5e03\u662f\u672a\u77e5\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u628a\\(x\\)\u5f3a\u5236\u7684\u5f52\u4e8e\u67d0\u4e2a\u5206\u5e03\uff0c\u4e5f\u5c31\u662f\\(z\\)\u662f\u672a\u77e5\u7684\uff0c\u6b64\u65f6\u662f\u6ca1\u6709\u529e\u6cd5\u8fdb\u884c\u6709\u6548\u91c7\u6837\u7684\uff0c\u4e5f\u5c31\u6ca1\u6709\u529e\u6cd5\u751f\u6210\u66f4\u591a\u7684\u56fe\u7247</p>"},{"location":"learning/6_Diffusion1/#16-vae","title":"1.6 VAE\u7684\u7ed3\u6784","text":"<ul> <li> \u63cf\u8ff0\u56fe\u7247</li> </ul> <p>\u9996\u5148\u6709\u4e00\u5f20\u8bad\u7ec3\u96c6\u56fe\u7247x\uff0c\u901a\u8fc7Encoder\uff0cEncoder\u5c31\u662f\u4f1a\u628a\u8bad\u7ec3\u7684x\u6620\u5c04\u5230z\u4e0a\u9762\uff0cz\u6240\u5728\u7684\u662f\u4e00\u4e2a\u9690\u7a7a\u95f4\uff0c\u9690\u7a7a\u95f4\u600e\u4e48\u8868\u8fbe\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u5c31\u662f\u8981\u628ax\u6620\u5c04\u5230\u6b63\u6001\u5206\u5e03\u7a7a\u95f4\u4e0a\uff0c\u6b63\u6001\u5206\u5e03\u662f\u901a\u8fc7\u5747\u503c\u548c\u65b9\u5dee\u63cf\u8ff0\u7684\uff0c\u4e5f\u5c31\u662f\u4f1a\u5f97\u5230\u4e00\u4e2a\u5747\u503c\u5411\u91cf \\(\\mu_{\\phi}(x)\\) \u548c\u4e00\u4e2a\u65b9\u5dee\u5411\u91cf  \\(\\sigma^2_{\\phi}(x)\\) \uff0c\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u7684\u662f\u6807\u51c6\u9ad8\u65af\u5206\u5e03\uff0c\u6240\u4ee5\u6211\u4eec\u4f18\u5316\u7684\u76ee\u6807\u5c31\u662f \\(\\mu_{\\phi}(x) \u2192 0\\) \u548c \\(\\sigma^2_{\\phi}(x) \u2192 \\mathrm{1}\\) <code>\u51680\u5411\u91cf</code> \u548c <code>\u51681\u5411\u91cf</code></p> <p></p> <p>\u4e0a\u9762\u63cf\u8ff0\u7684\u8fc7\u7a0b \u4e5f\u5c31\u662f \u516c\u5f0f\u4e2d\u7684 \u5148\u9a8c\u5339\u914d\u9879\uff0c\u4e5f\u5c31\u662f\u7b2c\u4e8c\u9879</p> <p>\u63a5\u4e0b\u6765\uff1a</p> <p></p> <p>\u6211\u4eec\u5f97\u5230\u4e86\u5c3d\u53ef\u80fd\u5f97<code>\u51680\u5411\u91cf</code> \\(\\mu_{\\phi}(x)\\)  \u3001<code>\u51681\u5411\u91cf</code> \\(\\sigma^2_{\\phi}(x)\\) \uff0c</p> <p>\\(z'\\)\u662f\u6839\u636eEncoder\u5f97\u5230\u7684 \\(\\mu\\) \u548c\\(\\sigma\\) \u91cd\u53c2\u6570\u5316\u5f97\u5230\u7684 \\(z'\\)</p> <p>\u6b63\u5f0f\u91c7\u6837\u7684\u65f6\u5019\uff0c\u4e0d\u9700\u8981Encoder\uff0c\u800c\u662f\u76f4\u63a5\u4ece \"\u6807\u51c6\"\u6b63\u6001\u5206\u5e03\\(z'\\)\u91c7\u6837\u51fa\u6765\u7684\uff0c\u7136\u540e\u901a\u8fc7decoder\u5f97\u5230\u4e00\u5f20\u65b0\u7684\u56fe\u7247</p> <p>\u6ce8\u610f\uff1a</p> <p>Encoder\u51fa\u6765\u7684\\(z\\)\uff0c\u5c31\u662f\\(z'\\)\u662f\u5e0c\u671b\u5c3d\u53ef\u80fd\u4e3a\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684</p>"},{"location":"learning/6_Diffusion1/#17","title":"1.7 \u53c2\u6570\u91cd\u6574\u5316","text":"<ul> <li> \u4e3a\u4ec0\u4e48\uff1f\u91cd\u53c2\u6570\u5316\u6280\u5de7</li> </ul> <p>\u5982\u679c\u4e0d\u8fdb\u884c\u53c2\u6570\u91cd\u6574\u5316\u7684\u8bdd\uff0c\\(z'\\)\u662f\u6839\u636e\u5747\u503c\u548c\u65b9\u5dee\u91c7\u6837\u51fa\u6765\u7684\uff0c\u4f46\u662f\u6211\u4eec\u7684\u5747\u503c \\(\\mu_{\\phi}(x)\\) \u548c\u65b9\u5dee \\(\\sigma^2_{\\phi}(x)\\) \u91cc\u9762\u90fd\u662f\u5305\u542b\u53c2\u6570\u7684<code>\u300a====\u300b</code>\u4e5f\u5c31\u662f\u6211\u4eec\u91c7\u6837\u7684\u968f\u673a\u566a\u58f0\u91cc\u9762\u662f\u5305\u542b\u53c2\u6570\u7684<code>\u300a=====\u300b</code> \u968f\u673a\u8fc7\u7a0b\u4e2d\u5305\u542b\u4e86\u5f85\u4f18\u5316\u7684\u53c2\u6570 \\(\\phi\\) \u6b64\u65f6\uff0c\u5bf9\u53c2\u6570\u7684 \\(\\phi\\) \u662f\u4e0d\u53ef\u5bfc\u7684<code>\u300a=====\u300b</code> \u56e0\u4e3a\u8fd9\u662f\u4e00\u4e2a\u968f\u673a\u8fc7\u7a0b\uff0c\u968f\u673a\u8fc7\u7a0b\u4e2d\u5305\u542b\u53c2\u6570\uff0c\u600e\u4e48\u6c42\u8fd9\u4e2a\u53c2\u6570\u7684\u5bfc\u6570\u5462\uff1f<code>\u300a=====\u300b</code>\u9274\u4e8e\u6b64\uff0c\u5f15\u5165\u4e86\u91cd\u53c2\u6570\u5316\u6280\u5de7\u3002\u8ba9\u8fd9\u4e2a\u8fc7\u7a0b\u53d8\u5f97\u53ef\u5bfc</p> <ul> <li> \u4ec0\u4e48\u662f\uff1f\u91cd\u53c2\u6570\u5316\u6280\u5de7</li> </ul> <p>\u9996\u5148\uff0c\u91cd\u53c2\u6570\u5316\u6280\u5de7\u662f\u5728\u4e00\u4e2a\u6807\u51c6\u6b63\u6001\u5206\u5e03\u91cc\u9762\uff0c\u5148\u968f\u673a\u53d6\u4e00\u4e2a\u503c\uff0c\u4e5f\u5c31\u662f \\(\\epsilon \\sim N(\\epsilon;0,I)\\)</p> <p>\u4e5f\u5c31\u662f\u91c7\u6837\u51fa\u6765\u7684 <code>\u5355\u4f4d\u566a\u58f0 \u00d7 \u65b9\u5dee + \u5747\u503c</code>  \u7b49\u4ef7\u4e8e \u4ece  \\(\\mu_{\\phi}(x)\\)  \u548c \\(\\sigma^2_{\\phi}(x)\\) \u4e2d\u76f4\u63a5\u91c7\u6837</p> <p>\u4e5f\u5c31\u662f\u8bf4\u8fd9\u6837\u91c7\u6837\u51fa\u6765\u7684\u53c2\u6570\uff0c\u5747\u503c\u4e5f\u7b49\u4e8e \\(\\mu_{\\phi}(x)\\) \uff0c\u65b9\u5dee\u4e5f\u7b49\u4e8e \\(\\sigma^2_{\\phi}(x)\\)</p> <ul> <li> <code>\u5355\u4f4d\u566a\u58f0 \u00d7 \u65b9\u5dee + \u5747\u503c</code>  \u7b49\u4ef7\u4e8e \u4ece  \\(\\mu_{\\phi}(x)\\)  \u548c \\(\\sigma^2_{\\phi}(x)\\) \u4e2d\u76f4\u63a5\u91c7\u6837 ||\u597d\u5904\u662f\u662f\u4ec0\u4e48\uff1f</li> </ul> <p>\u9996\u5148\uff0c\u6211\u4eec\u7684\u968f\u673a\u8fc7\u7a0b\u662f\u5728\u4e00\u4e2a\u6ca1\u6709\u53c2\u6570\u7684\u566a\u58f0\u4e2d\u91c7\u6837\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\u8fd9\u4e2a\u968f\u673a\u8fc7\u7a0b\u662f\u4e0d\u5305\u542b\u53c2\u6570\u7684\uff0c\u90a3\u540e\u9762\u5bf9\u53c2\u6570 \\(\\phi\\) \u4f18\u5316\u7684\u65f6\u5019\uff0c\u5c31\u53ef\u4ee5\u76f4\u63a5\u6c42\u5bfc\u4e86\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u53c2\u6570\u662f \u5265\u79bb\u51fa\u8fd9\u4e2a\u968f\u673a\u8fc7\u7a0b\u7684\uff0c\u4ee5\u4e0a\u5c31\u662f\u91cd\u53c2\u6570\u5316\u7684\u8fc7\u7a0b\uff1b</p> <p>\u603b\u4e4b\u91cd\u53c2\u6570\u5316\u8fc7\u7a0b \u4fdd\u8bc1\u4e86 \u91c7\u6837\u7684\u5206\u5e03\u6ca1\u6709\u53d8\uff0c\u7136\u540e\u53c8\u8ba9\u6240\u6709\u53c2\u6570\u662f\u53ef\u5bfc\u7684\uff0c\u800c\u6ca1\u6709\u5728\u4e00\u4e2a\u968f\u673a\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8fd9\u6837\u6a21\u578b\u624d\u80fd\u8bad\u7ec3\uff0c\u4e5f\u5c31\u662f\u91cd\u53c2\u6570\u5316\u6280\u5de7\u7684\u597d\u5904</p>"},{"location":"learning/6_Diffusion1/#18-vae","title":"1.8 VAE\u6a21\u578b\u5c0f\u7ed3","text":"<p>\u9996\u5148\u662f \u6a21\u578b\u7684\u6574\u4f53\u67b6\u6784\uff0c\u5e76\u5305\u542b\u4e86 \u91cd\u53c2\u6570\u5316\u6280\u5de7</p> <p>\u7b2c\u4e8c\u70b9\uff0c\u4f18\u5316VAE \u7b49\u4ef7\u4e8e \u4f18\u5316 ELBO\uff0c\u66f4\u5177\u4f53\u6765\u8bf4\u5c31\u662f \u6700\u5927\u5316 ELBO</p> <p>\u7b2c\u4e09\u70b9\uff0c\u7ee7\u7eed\u62c6\u89e3ELBO\uff0cELBO\u5305\u542b\u4e24\u90e8\u5206\uff081\uff09\u91cd\u5efa\u9879\uff082\uff09\u5148\u9a8c\u5339\u914d\u9879</p> <p>\uff081\uff09\u91cd\u5efa\u9879\uff1a\u53cd\u6620\u7684\u662fdecoder\u4ece\u9690\u53d8\u91cf\u91cd\u5efa\u56fe\u7247\u7684\u80fd\u529b</p> <p>\uff082\uff09\u5148\u9a8c\u5339\u914d\u9879\uff1a\u53cd\u6620\u7684\u662fEncoder\u5c06\u56fe\u7247\u6620\u5c04\u5230\u6307\u5b9a\u9690\u53d8\u91cf\u5206\u5e03\u7684\u80fd\u529b\uff0c\u4e5f\u5c31\u662f\u628a\u56fe\u7247\u6620\u5c04\u5230\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u80fd\u529b</p> <p>\u7b2c\u56db\u70b9\uff0c\u91cd\u53c2\u6570\u5316\u6280\u5de7\u7684\u597d\u5904</p> <p>\u7b2c\u4e94\u70b9\uff0c\u751f\u6210\u7684\u65f6\u5019\u5c31\u4e0d\u9700\u8981Encoder\u4e86\uff0c\u6bcf\u6b21\u53ea\u9700\u8981\u5728Encoder\u51fa\u6765\u7684\u53d8\u91cf\u91c7\u6837\u4e00\u4e2az\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u901a\u8fc7decoder\u751f\u6210\u4e00\u5f20\u65b0\u7684\u56fe\u7247\u4e86\uff0c\u4ee5\u4e0a\u662f\u4e00\u4e2aVAE\u7684\u8fc7\u7a0b</p>"},{"location":"learning/6_Diffusion1/#2-mhvae","title":"2 MHVAE\u7684\u63a8\u5bfc","text":"<p>\u4e2d\u6587\uff1a\u9a6c\u5c14\u79d1\u592b\u94fe\u7684\u7ea7\u8054\u7684VAE\uff08\u7ea7\u8054\u9a6c\u5c14\u79d1\u592b\u94feVAE\uff09</p> <p></p> <ul> <li>\u5b9e\u9645\u4e0a\u5c31\u662f\u628a\u5f88\u591aVAE\u5806\u53e0\u8d77\u6765\uff0c\u800c\u4e14\u6bcf\u4e00\u4e2a\u72b6\u6001\u503c\u4f9d\u8d56\u524d\u4e00\u4e2a\u548c\u5b83\u76f8\u90bb\u7684\u524d\u4e00\u4e2a\u72b6\u6001\uff0c\u800c\u4e0e\u4e4b\u524d\u7684\u66f4\u957f\u66f4\u8fdc\u7684\u72b6\u6001\u662f\u65e0\u5173\u7684\uff0c\u4e5f\u5c31\u662f\u9a6c\u5c14\u79d1\u592b\u94fe</li> <li>\u516c\u5f0f\u63a8\u5bfc\u7684\u53d8\u5316\uff1a</li> </ul> <p>\uff081\uff09\u516c\u5f0f\u63a8\u5bfc\u7684\u539f\u7406\u662f\u4e00\u6a21\u4e00\u6837\u7684</p> <p>\uff082\uff09\u533a\u522b\u5728\u4e8e\uff0c\u4e4b\u524d\u5355\u4e2aVAE\u53ea\u9700\u8981\u4e00\u4e2a\u8f85\u52a9\u53d8\u91cf\\(z\\)\uff0c\u73b0\u5728\u7ea7\u8054\u9a6c\u5c14\u79d1\u592bVAE\uff0c\u6709\u591a\u4e2a\u8f85\u52a9\u53d8\u91cf\\(z\\)\uff0c\u6240\u4ee5\u4ece\\(z\u2192z_{1:T}\\)</p>"},{"location":"learning/6_Diffusion1/#3-vdm","title":"3 VDM\u7406\u8bba\u63a8\u5bfc","text":"<p>\u4e2d\u6587\u7ffb\u8bd1\uff1aVariational Diffusion Models \u53d8\u5206\u6269\u6563\u6a21\u578b</p> <p>\u533a\u5206\uff1a\uff08 Denoising Diffusion Probabilistic Models\uff0cDDPM\uff09\u53bb\u566a\u6982\u7387\u6269\u6563\u6a21\u578b</p> <p></p> <p>\u4ece \u7ea7\u8054\u9a6c\u5c14\u79d1\u592bVAE \\(MHVAE\\)  \u2192  \\(VDM\\)</p> <p>MHVAE+3\u4e2a\u9650\u5236\u6761\u4ef6 \u5c31\u4f1a\u53d8\u6210 VDM</p> <p>\u9650\u52361\uff1a\u6570\u636e\\(x\\)\u548c\u6240\u6709\u7684\u9690\u53d8\u91cf\\(z_t\\)\u7ef4\u5ea6\u76f8\u540c</p> <p>\u4e4b\u524d\u7684VAE\uff0c\\(x\\)\u548c\\(z\\)\u7684\u7ef4\u5ea6\u53ef\u4ee5\u662f\u4e0d\u540c\uff0c\u4e14\u4e00\u822c\u4e5f\u662f\u4e0d\u540c\u7684</p> <p>\u4f46\u5982\u679c\u662f \\(VDM\\)\u7684\u8bdd\uff0c\u6620\u5c04\u7684\u7ef4\u5ea6\u5c31\u662f\u76f8\u540c\u7684\uff0c\u4e5f\u5c31\u662f\\(x\\)\u548c\u6240\u6709\u7684\u9690\u53d8\u91cf\\(z\\)\u7684\u7ef4\u5ea6\u90fd\u662f\u76f8\u540c\u7684</p> <p>\u9650\u52362\uff1a\u6240\u4ee5Encoder\u7684\u8fc7\u7a0b\u662f\u4e0d\u9700\u8981\u5b66\u4e60\u7684</p> <p>\u4e5f\u5c31\u662f\u4ece \\(z_1 \u2192 z_2\\) \u3001\u4ece \\(z_2\\) \u5230 \\(z_3\\) \u3001\u4ece \\(z_3\\)\u5230\\(z_4\\) \u3001\u4ece\\(z_{t-1}\\)\u5230\\(z_t\\) \u662f\u4e0d\u9700\u8981\u5b66\u4e60\u7684\uff0c\u662f\u4eba\u4e3a\u9884\u5b9a\u4e49\u597d\u7684\uff0c\u662f\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6bcf\u4e0b\u4e00\u4e2a\u72b6\u6001 \u90fd\u662f\u4ee5 \u4e0a\u4e00\u4e2a\u72b6\u6001\u4e3a\u5747\u503c\u7684\u9ad8\u65af\u5206\u5e03  \\(q(z_t|z_{t-1})\\)\uff0c\u8fd9\u4e2a\u5747\u503c\u548c\u65b9\u5dee\u90fd\u53ef\u4ee5\u5b9a\u4e49\u597d\uff0c\u5f53\u7136\u4e5f\u53ef\u4ee5\u8bbe\u7f6e\u4e3a\u9700\u8981\u5b66\u4e60\u7684\uff0c\u4f46\u662f\u8bba\u6587\u4e2d\u8bbe\u7f6e\u7684\u662f\u5148\u9884\u5b9a\u4e49\u597d\u7684</p> <p>\u9650\u52363\uff1a\u6700\u540e\u7684 \\(z_t\\)\u662f\u6ee1\u8db3\u6807\u51c6\u7684\u9ad8\u65af\u5206\u5e03\u7684</p> <p>\u7c7b\u6bd4VAE\u6700\u540e\u4e5f\u662f\u8981\u62df\u5408\u4e00\u4e2a\u6807\u51c6 \u9ad8\u65af\u5206\u5e03</p> <p>\u4f46 VAE\u4e2d\u7684Encoder\u8fc7\u7a0b\u662f\u5b66\u4e60\u5230\u7684\uff0c\u5b66\u4e60\u76ee\u6807\u5c31\u662f\u5c06\u539f\u59cb\u56fe\u50cf \\(x\\) \u53d8\u6210\u4e00\u4e2a\u6807\u51c6\u9ad8\u65af\u5206\u5e03 \\(z\\)</p> <p>\u4f46\u73b0\u5728\u662f\u4eba\u5de5\u5b9a\u4e49\u7684Encoder\u7684\u8fc7\u7a0b\uff0c\u8fd9\u5c31\u8981\u6c42\u6211\u4eec\u81ea\u5df1\u5b9a\u4e49\u7684\u8fd9\u4e2a\u8fc7\u7a0b \u6ee1\u8db3 \\(z_t\\) \u4e3a\u6807\u51c6\u9ad8\u65af\u5206\u5e03</p> <p>\u4e5f\u5c31\u662fEncoder\u6620\u5c04\\(x\\)\u5230\\(z_t\\)\u7684\u8fc7\u7a0b\u672c\u6765\u662f\u901a\u8fc7\u5b66\u4e60\u4f7f\u5176\u80fd\u591f\u6620\u5c04\u5230\u4e00\u4e2a\u6807\u51c6\u9ad8\u65af\u5206\u5e03\uff0c\u800c\u73b0\u5728\u662f\u901a\u8fc7\u8ba4\u4e3a\u5b9a\u4e49\u8ba9\u5b83\u53bb\u6620\u5c04\u5230\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\uff0c\u6240\u4ee5\u8981\u6c42\uff0c\u6211\u4eec\u5b9a\u4e49\u7684\u9ad8\u65af\u5206\u5e03\\(q\\)\uff0c\u4e00\u5b9a\u8981\u4f7f\u5f97\u6700\u540e\u7684 \\(z_t\\) \u662f\u6807\u51c6\u9ad8\u65af\u5206\u5e03\uff0c\u4e5f\u5c31\u6ee1\u8db3\u4e86\u4e4b\u524dVAE\u63a8\u5bfc\u7684\u8fc7\u7a0b</p> <p><code>\u603b\u7ed3\uff1aMHVAE+3\u4e2a\u9650\u5236=VDM</code></p>"},{"location":"learning/6_Diffusion1/#31","title":"3.1 \u9650\u5236\u662f\u600e\u4e48\u52a0\u7684\uff1f","text":"<ul> <li>\u9650\u52361\u548c\u9650\u52362\uff0c\u8981\u6c42\u7ef4\u5ea6\u76f8\u540c\uff0c\u5b9e\u9645\u4e0a\u4e3b\u8981\u5728\u9650\u52362\uff0c\u9650\u52361\u8981\u6c42\u7ef4\u5ea6\u76f8\u540c\uff0c\u9650\u52361\u662f\u9650\u52362\u7684\u524d\u63d0\uff0c\u56e0\u4e3a\u5982\u679c\u9650\u52361\u90fd\u4e0d\u6ee1\u8db3\uff0c\u4e5f\u5c31\u662f\u7ef4\u5ea6\u90fd\u4e0d\u76f8\u540c\u7684\u8bdd\uff0c\u90a3\u4e48\u662f\u6ca1\u6709\u529e\u6cd5\u76f4\u63a5\u4ee5\u524d\u4e00\u72b6\u6001\uff08\u4e58\u4ee5\u67d0\u4e2a\u7cfb\u6570\uff09\u4e3a\u5747\u503c\u7684\uff0c\u56e0\u4e3a\u7ef4\u5ea6\u90fd\u4e0d\u76f8\u540c\uff0c\u662f\u4e0d\u53ef\u4ee5\u76f4\u63a5\u4ee5\u524d\u4e00\u72b6\u6001\u4e3a\u5747\u503c\u7684\uff0c\u56e0\u6b64\uff0c\u9996\u5148\u7b2c\u4e00\u70b9\uff0c\u9650\u52361\u548c\u9650\u52362\u8981\u4e00\u8d77\u770b</li> <li>\u56e0\u6b64\uff0c\u5177\u4f53\u5730\u505a\u6cd5\u662f\uff1a\u4eba\u4e3a\u7684\u5b9a\u4e49\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\uff0c\u505aEncoder\uff0c\u5728\u6587\u7ae0\u4e2d\u7684\u5b9a\u4e49\u662f\uff1a</li> </ul> <p>\\(q(x_{t}|x_{t-1}) \\sim N(x_t;\\sqrt{\\alpha_t}x_{t-1},(1-\\alpha_t)t)\\)</p> <p>\u8bfb\uff1a\u5df2\u77e5 \\(x_{t-1}\\)  \u7684\u524d\u63d0\u4e0b\uff0c\\(x_t\\) \u670d\u4ece \u5747\u503c\u4e3a  \\(\\sqrt{\\alpha_t}x_{t-1}\\) \u65b9\u5dee\u4e3a \\((1-\\alpha_t)t\\)  \u7684\u9ad8\u65af\u5206\u5e03</p> <p>\u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u5b9a\u4e49\u7684\u5462\uff1f\u600e\u4e48\u7406\u89e3\uff1f</p> <ul> <li>\u9996\u5148\uff0c\\(\\sqrt{\\alpha_t}x_{t-1} \u2192 0\u3001(1-\\alpha_t)t \u2192 1\\)  \u4e5f\u5c31\u662f\u8bf4 \\(\\alpha_t \u2192 0\\)</li> <li>\u5176\u6b21\uff0c\u8981\u77e5\u9053 \u539f\u59cb\u56fe\u7247\u968f\u7740t\u7684\u589e\u5927\uff0c\u6240\u542b\u7684\u4fe1\u606f\u6d53\u5ea6\u5c31\u4f1a\u964d\u4f4e\uff0c\u56e0\u4e3a\u4e00\u76f4\u5728\u00d7  \\(\\sqrt{\\alpha_t}\\) \uff0c\u5e76\u4e14  \\(\\alpha_t \uff1c 1\\) </li> <li>\u6240\u4ee5\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5982\u679c\u521d\u59cb\u503c\u662f \\(x_0\\) \u7684\u8bdd\uff0c \u968f\u7740 \\(t\\) \u8d8a\u5927\uff0c\\(x_t\\) \u6240\u5305\u542b\u7684\u4fe1\u606f\u5c31\u4f1a\u8d8a\u5c11\uff0c\u566a\u58f0\u5c31\u4f1a\u8d8a\u591a\uff0c\u56e0\u4e3aEncoder\u5c31\u662f\u5728\u6bcf\u4e00\u6b65\u90fd\u4f1a\u52a0\u4e00\u4e9b\u566a\u58f0\uff0c\u6240\u4ee5\u566a\u58f0\u4f1a\u8d8a\u6765\u8d8a\u5927\uff0c\u6700\u540e\u52a0\u4e86\u5f88\u591a\u6b65\uff0c\u76f4\u5230 \\(x_t\\)\uff0c\u57fa\u672c\u4e0a\u8fd1\u4f3c\u8d8b\u8fd1\u4e8e \u4e00\u4e2a \u9ad8\u65af\u5206\u5e03\u3002\u5f53\u7136 \u4e0d\u53ef\u80fd \u5b8c\u5168 \u7b49\u4e8e \u4e00\u4e2a \u9ad8\u65af\u5206\u5e03\uff0c\u53ea\u9700\u8981\u8fd1\u4f3c\u63a5\u8fd1\u5373\u53ef \u9ad8\u65af\u5206\u5e03\u5373\u53ef</li> </ul> <p>\u8fd9\u91cc\u7684\u4fe1\u606f\u6307\u7684\u662f\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u968f\u7740t\u53d8\u5927\uff0c\u786e\u5b9a\u6027\u4fe1\u606f\u8d8a\u5c11</p> <ul> <li>\\(\\alpha\\)\u662f\u4ec0\u4e48\u5462\uff1f \\(\\alpha\\)  \u662f\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u662f\u4eba\u4e3a\u5b9a\u4e49\u7684\uff0c\u7c7b\u4f3c\u4e8e\u5b66\u4e60\u7387\uff0c\u53ef\u4ee5\u4eba\u4e3a\u5b9a\u4e49\u4e00\u4e2a\u516c\u5f0f\uff0c\u4e0et\u6709\u5173\u7684\u516c\u5f0f\uff0c\u6216\u8005\u901a\u8fc7\u5b66\u4e60\u5f97\u5230\uff0c\u603b\u4e4b\u5b9a\u4e49\u51fa\u6765\u7684 \\(\\alpha_t\\)\u8981\u6ee1\u8db3 \\(x_t\\)  \u65e0\u9650\u63a5\u8fd1\u4e8e \u9ad8\u65af\u5206\u5e03\uff0c\u4e00\u822c\u662f\u8ba4\u4e3a\u5b9a\u4e49\u516c\u5f0f\uff0c\u5f53\u4e00\u4e2a\u8d85\u53c2\u6570</li> <li>\u9650\u52363\uff0c\u6700\u540e\u4e00\u4e2a\u65f6\u523b \\(x_t\\)\u8981\u6ee1\u8db3\u9ad8\u65af\u5206\u5e03\uff0c\u6570\u5b66\u8868\u8fbe\u5c31\u662f \\(p(x_t)\\sim N(x_T;\\mathrm{0,I})\\)</li> </ul> <p></p> <ul> <li>\u4ee5\u4e0a\u662fVDM\u7684\u56fe\u793a</li> <li>\u5982\u679c\u53ea\u662fMHVAE\u7684\u8bdd\uff0c\u6bcf\u4e00\u4e2ap\u548cq\u90fd\u662f\u8981\u5b66\u4e60\u7684\uff0c\u73b0\u5728VDM\u7684\u8bdd\uff0c\u53ea\u6709p\u662f\u8981\u5b66\u4e60\u7684\uff0c\u901a\u8fc7 \\(\\theta\\) \u6765\u62df\u5408\uff0c\u4f46\u662f \\(q\\) \uff08q\u8868\u793a\u4ece\u4e0a\u4e00\u4e2a\u72b6\u6001\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001\uff09\u6bcf\u4e00\u4e2a\u72b6\u6001\u90fd\u662f\u5df2\u77e5\u7684\uff08\u9012\u63a8\u5df2\u77e5\uff09\uff0c\u90fd\u662f\u4eba\u4e3a\u5b9a\u4e49\u7684\uff0c\u5305\u62ec \\(z_t\\)  \u4e5f\u662f\u5df2\u77e5\u7684\uff0c\u5e76\u4e14\u6ee1\u8db3 \u6807\u51c6\u9ad8\u65af\u5206\u5e03\uff0c\u56e0\u6b64 VDM\u8981\u5b66\u4e60\u7684\u53ea\u662f \u4e0a\u9762\u4e00\u90e8\u5206 p \u8fc7\u7a0b\uff0c\u964d\u4f4e\u6a21\u578b\u5b66\u4e60\u96be\u5ea6</li> </ul>"},{"location":"learning/6_Diffusion1/#32-vdmelbo","title":"3.2 VDM\u7684ELBO\u63a8\u5bfc","text":"<ul> <li> <p>VAE \u2192 MHVAE \u2192 VDM \u63a8\u5bfc\u90fd\u662f\u5f88\u7c7b\u4f3c\u7684\uff0c\u533a\u522b\u5c31\u662f\u4e00\u4e9b\u7b26\u53f7\u7684\u7ec6\u8282</p> </li> <li> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u4e4b\u524d\u7528x \u8868\u793a\u7b2c\u4e00\u4e2a\u72b6\u6001\uff0c\u73b0\u5728\u7528 \\(x_0\\) \u8868\u793a\u7b2c\u4e00\u4e2a\u72b6\u6001</p> </li> <li>\u4e4b\u524d\u7528 \\(z_{1:T}\\) \u8868\u793a\u4e00\u4e9b\u9690\u53d8\u91cf\uff0c\u73b0\u5728VDM\u4e2d\u7ef4\u5ea6\u90fd\u4e00\u6837\uff0c\u6240\u4ee5\u76f4\u63a5\u4f5c\u7528 \\(x_{1:T}\\) \u6765\u8868\u793a \\(z_{1:T}\\) </li> <li>\u4e5f\u5c31\u662f \u7ea2\u5b57\u6807\u51fa\u7684 \\(x\\) \u66ff\u6362\u6210 \\(x_0\\)\uff0c\\(z_t\\) \u6362\u6210 \\(x_t\\)</li> <li>\u7ecf\u8fc7\u4ee5\u4e0a\u7b26\u53f7\u7684\u66ff\u6362\uff0c\u53ef\u4ee5\u770b\u5230 \\(VDM\\) \u4e2d\uff0c\u662f\u6ca1\u6709 \\(z\\) \u8fd9\u4e2a\u53d8\u91cf\u4e86\uff0c\u5168\u662f \\(x_0\\) \u5230 \\(x_T\\)\u4e86\uff0c\u63a8\u5bfc\u5c31\u662f\u4e00\u6a21\u4e00\u6837\u7684\uff0c\u5c31\u662f\u628a\u7b26\u53f7\u53d8\u4e00\u4e0b</li> <li>\u6700\u540e\u53ef\u4ee5\u8bc1\u660e\u51fa\u6765 \\(ELBO\\)</li> </ul>"},{"location":"learning/6_Diffusion1/#33-elbo","title":"3.3  ELBO\u7684\u62c6\u89e3","text":"<p>\u7ee7\u7eed\u6cbf\u7528\u4e4b\u524d\u7684\u7814\u7a76\u6b65\u9aa4\uff0c\u62c6\u89e3VDM\u7684ELBO\uff0c\u89c2\u5bdf\u4e0eVAE\u7684\u62c6\u89e3\u6709\u4ec0\u4e48\u4e0d\u540c</p> <p>\\(VAE\u7684ELBO\u62c6\u89e3=\u91cd\u5efa\u9879-KL\u6563\u5ea6\uff08\u5148\u9a8c\u5339\u914d\u9879\uff09\\)</p> <p></p> <ul> <li>\u9a6c\u5c14\u79d1\u592b\u6027\u8d28\uff1a\u53ea \u4e0e\u524d\u4e00\u4e2a\u72b6\u6001\u6709\u5173</li> <li>p\u8fc7\u7a0b\u662f\u540e\u5411 decoder\u8fc7\u7a0b\uff0c\u6240\u4ee5\u662f\u4ee5 \\(x_t\\) \u4e3a\u6761\u4ef6</li> <li>q\u8fc7\u7a0b\u662f\u524d\u5411 Encoder\u8fc7\u7a0b\uff0c\u6240\u4ee5\u662f\u4ee5 \\(x_{t-1}\\) \u4e3a\u6761\u4ef6</li> </ul> <p>\u7ee7\u7eed\u63a8\u5bfc\uff1a</p> <p></p> <p></p> <ul> <li>\u7740\u91cd\u7406\u89e3 \u4ec0\u4e48\u53eb \u5220\u6389\u65e0\u5173\u53d8\u91cf</li> </ul> <p>\u5c31\u770b\u7eff\u6846\u90e8\u5206 <code>\u5220\u6389\u65e0\u5173\u53d8\u91cf</code> \u8fd9\u4e00\u884c</p> <p>\u5bf9\u4ec0\u4e48\u6c42\u671f\u671b\uff1f\u5bf9 \\(x_t\\)\u6c42\u671f\u671b\uff0c\u540c\u65f6\u4e5f\u9700\u8981\u77e5\u9053 \\(x_{t+1}\\) \u548c \\(x_{t-1}\\) \u7684\u72b6\u6001\u4e0b\uff0c\u5bf9 \\(x_t\\) \u6c42\u671f\u671b\uff0c\u6240\u6d89\u53ca\u7684\u53d8\u91cf \u5c31\u662f \\(x_{t-1}\\) \u3001\\(x_{t+1}\\)</p> <p>\u800c\u524d\u9762\u7684\u5206\u5e03\u662f \\(q(x_{1:T}|x_0)\\) \u6709\u5f88\u591a\u65e0\u5173\u53d8\u91cf\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u5212\u6389\uff0c\u53ea\u4fdd\u7559 \u4e0e \\(x_{t}\u3001x_{t-1}\u3001x_{t+1}\\)\u6709\u5173\u7684\u90e8\u5206</p> <p>\u56e0\u4e3a\u6211\u4eec\u53ea\u5173\u5fc3 \u8981\u6c42\u671f\u671b\u91cc\u9762 \u6709\u5173 \u53d8\u91cf\u7684 \u5206\u5e03\u5373\u53ef\uff0c\u4e0e  \u90a3\u4e9b\u65e0\u5173 \u53d8\u91cf\u7684\u5206\u5e03\u662f \u6ca1\u6709\u5173\u7cfb\u7684</p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u8981\u505a\u7684\u5c31\u662f\u628a \u62ec\u53f7\u91cc\u9762 \u65e0\u5173\u7684\u53d8\u91cf\u5220\u6389\uff0c\u5b9e\u9645\u4e0a\u5c31\u662f\u4ec0\u4e48\u5206\u5e03\u6c42\u671f\u671b\uff0c\u4e0e\u4ec0\u4e48\u53d8\u91cf\u6709\u5173\uff0c\u5c31\u4fdd\u7559\u5206\u5e03\u4e2d\u7684\u53d8\u91cf\u5373\u53ef</p> <p>\u84dd\u8272\u6846\u540c\u7406</p> <p>\u671f\u671b\u91cc\u9762\u53ea\u4e0e \\(x_T\\) \u548c \\(x_{T-1}\\)\u6709\u5173\uff0c\u671f\u671b\u4e0b\u6807\u5c31\u53ea\u4fdd\u7559 \\(x_{T}\\) \u548c \\(x_{T-1}\\) \u6709\u5173\u7684\u5206\u5e03\u5373\u53ef\uff0c\u5220\u6389\u65e0\u5173\u53d8\u91cf\u5373\u65e0\u5173\u7684\u5206\u5e03</p> <p>\u6700\u540e\u4e00\u6b65\u7684\u63a8\u5bfc\uff0c\u5148\u770b\u84dd\u8272\u6846\u90e8\u5206\uff1a</p> <p></p> <ul> <li>\u9996\u5148\u6c42\u671f\u671b\uff0c\u5e76\u4e14\u662f\u5bf9\u4e24\u4e2a\u53d8\u91cf\u6c42\u671f\u671b\uff0c\u6240\u4ee5\u7528\u591a\u5143\u51fd\u6570\u671f\u671b\u7684\u5b9a\u4e49</li> <li>\u7531\u9a6c\u5c14\u79d1\u592b\u6027\u8d28\uff1a\\(q(x_T|x_{T-1},x_0)\\)  \u53ef\u4ee5\u5212\u6389 \\(x_0\\) \u53d8\u6210 \\(q(x_T|x_{T-1})\\) \uff0c\u4f46\u662f \\(q(x_{T-1}|x_0)\\)\u5212\u4e0d\u6389</li> <li>\u62fd\u51fa \\(x_T\\)\uff0c\\(dx_{T}\\) \u79ef\u5206\u53f7\uff0c\u8981\u79ef\u5206\u7684\u5206\u5e03\uff0c\u548c\u8981\u79ef\u5206\u7684\u51fd\u6570\uff0c\u6784\u6210\u4e00\u4e2a\u671f\u671b</li> </ul> <p></p> <ul> <li>\u63a5\u7740 \u5c31\u53d8\u6210\u4e86\u4e00\u4e2a KL\u6563\u5ea6 \\(KL(A||B)=\\int P(A)\\log\\frac{P(A)}{P(B)}?\\)\u8fd9\u516c\u5f0f\u771f\u8bb0\u4e0d\u4f4f</li> </ul> <p>Okay,\u84dd\u8272\u6846\u63a8\u5bfc\u5b8c\u4e86</p> <p></p> <p>\u63a8\u5bfc\u7eff\u8272\u6846</p> <p></p> <ul> <li>\u4e00\u4e2a\u79ef\u5206\u53d8\u91cf+\u4e00\u4e2a\u79ef\u5206\u53f7 \u2192 1\u5143\u671f\u671b</li> <li>2\u4e2a\u79ef\u5206\u53d8\u91cf+2\u4e2a\u79ef\u5206\u53f7 \u2192 2\u5143\u671f\u671b</li> <li> <p>\u6709\u4e00\u6b65\uff1a\u7531\u9a6c\u5c14\u79d1\u592b\u6027\u8d28 \\(q(x_t|x_{t-1},x_{t+1},x_0)\\) \u53ea\u7559\u4e0b \\(q(x_t|x_{t-1})\\)</p> </li> <li> <p>\\(dx\\) \u8bfb\u6210\u79ef\u5206\u7b26\u53f7</p> </li> </ul> <p>ELBO\u62c6\u89e3\u7684\u8bc1\u660e\uff0c\u73b0\u5728\u770b\u6bcf\u4e00\u9879\u5177\u4f53\u5730\u542b\u4e49</p>"},{"location":"learning/6_Diffusion1/#34-elbo","title":"3.4 ELBO\u62c6\u89e3\u51fa\u6765\u7684\u542b\u4e49","text":"<p>\u9996\u5148\uff0c\u7b2c\u4e00\u9879\uff1a\\(\\mathbb{E}_{q(x_1|x_0)}[\\log p_{\\theta}(x_0|x_1)]\\)</p> <p>\u91cd\u5efa\u9879</p> <ul> <li>\u628a\\(x_0\\)\u770b\u505a\u4e4b\u524d VAE\u7684\\(x\\) </li> <li>\\(x_1\\) \u770b\u505a\u4e4b\u524d\u7684 \\(z\\)</li> </ul> <p>\u5c31\u662f\u4e4b\u524d\u7684decoderye\u4e5f\u5c31\u662f\uff08\u91cd\u5efa\u9879\uff0creconstruction term\uff09</p> <p>\u63a5\u7740\uff0c\u7b2c\u4e8c\u9879\uff1a\\(\\mathbb{E}_{q(x_{T-1}|x_0)}[D_{KL}(q(x_T|x_{T-1})||p(x_T))]\\)</p> <ul> <li>\\(x_{T-1}\\) \u770b\u6210 \\(x\\)</li> <li>\\(x_T\\) \u770b\u6210\u4e4b\u524d\u7684 \\(z\\)</li> </ul> <p>\u4e5f\u5c31\u662f\u5148\u9a8c\u5339\u914d\u9879\uff0cEncoder\uff0c\uff08prior matching term\uff09</p> <p>\u8fd9\u4e24\u9879\u4e0eVAE\u4e2d\u4e00\u6a21\u4e00\u6837\uff0c\u591a\u4e86\u6700\u540e\u4e00\u9879\uff1a</p> <p>\u6700\u540e\uff0c\u7b2c\u4e09\u9879 \\(\\sum_{t=1}^{T-1} \\mathbb{E}_{q(x_{t-1},x_{t+1 }|x_0)}[D_{KL}(q(x_t|x_{t-1})||p_{\\theta}(x_t|x_{t+1}))]\\)</p> <ul> <li>\u53eb\u505a consistency term</li> <li>\u7b2c\u4e09\u9879\u524d\u9762\u662f\u6709\u4e00\u4e2a \u6c42\u548c\u7b26\u53f7\u7684\uff0c\u800c\u524d\u9762\u4e24\u9879\u662f\u6ca1\u6709\u6c42\u548c\u7b26\u53f7\u7684\uff0c\u524d\u9762\u4e24\u9879\u90fd\u662f\u53ea\u6709\u4e00\u9879\uff0c\u8fd9\u91cc\u6709\u6c42\u548c\u53f7\uff0c\u5e76\u4e14\u6709 \\(T-1\\) \u9879</li> <li>\u56e0\u4e3a\u6709\u8fd9\u4e2a\u6c42\u548c\u9879\uff0c\u6240\u4ee5\u7b2c\u4e09\u9879\u5728\u6574\u4e2a loss function \u91cc\u9762\uff0c\u5360\u5f97\u6743\u91cd\u662f\u6bd4\u8f83\u5927\u7684</li> <li>\u6240\u4ee5\u5728\u4f18\u5316 Diffusion \u65f6\uff0c\u7b2c\u4e09\u9879\uff0cconsistency term \u662f\u5360\u4e3b\u5bfc\u7684\u9879</li> </ul>"},{"location":"learning/6_Diffusion1/#35-elbo","title":"3.5 ELBO \u62c6\u89e3\u9879\u56fe\u793a","text":"<ul> <li>\u501f\u7528 MHVAE \u7684\u56fe\uff0c\u6765\u53ef\u89c6\u5316</li> <li>\u9996\u5148\uff0creconstruction term \u8868\u793a\u4ece x1 \u5230 x0 \u7684\u5206\u5e03\uff0c\u5728\u56fe\u4e0a\u5c31\u662f\uff1a</li> </ul> <ul> <li>\u5176\u6b21\uff0cprior matching term\uff0c\u7bad\u5934\u6765\u8bf4\u5c31\u662f\u6700\u540e\u4e00\u9879\uff0c\u8868\u793a \\(x_{T-1}\\) \u5230 \\(x_T\\)\u7684\u5206\u5e03\uff1a</li> </ul> <ul> <li>\u800c\u6700\u540e\u7684 consistency term \u5c31\u662f\u5269\u4e0b\u7684\u6240\u6709\u7bad\u5934</li> </ul> <p>\u5177\u4f53\u6765\u8bf4\uff0c</p> <ul> <li>\\(q(x_t|x_{t-1})\\) \u5c31\u662f\u7c89\u7ea2\u8272\u7684\u7bad\u5934</li> <li>\u5bf9 \\(p_{\\theta}(x_t|x_{t+1})\\) \u7684 KL \u6563\u5ea6</li> <li>\u800c\uff0c\\(p_{\\theta}(x_t|x_{t+1})\\) \u5c31\u662f\u7eff\u8272\u7684\u7bad\u5934</li> <li>\u8868\u793a\u7684\u662f \u4e24\u4e2a\u7bad\u5934\u5f97\u5230\u7684 \\(x_t\\) \u4e4b\u95f4\u7684 <code>KL \u6563\u5ea6</code></li> <li>\u7136\u540e\uff0c\u8fd9\u4e24\u7ec4\u7bad\u5934\u770b\u505a\u4e00\u4e2a\u6574\u4f53\uff0c\u5411\u53f3\u6216\u8005\u5411\u5de6\u79fb\uff0c\u56e0\u4e3a\u4e00\u5171\u6709\u5927 T \u4e2a\u72b6\u6001\uff0c\u6240\u4ee5\u6574\u4f53\u4e0a\u5c31\u662f \\(T-1\\) \u4e2a\u6c42\u548c</li> </ul> <p>\u4ee5\u4e0a\u662f\u4e09\u4e2a loss \u5728 MHVAE \u4e2d\u76f4\u89c2\u5730\u8868\u8fbe</p>"},{"location":"learning/6_Diffusion1/#36-consistency-term","title":"3.6 consistency term\uff1a\u53bb\u566a\u65b9\u5411\u9884\u4f30","text":"<p>\u8bf4\u660e\u7b2c\u4e09\u9879  </p> <p></p> <p>\\(consistency\\_term  = \\sum_{t=1}^{T-1} \\mathbb{E}_{q(x_{t-1},x_{t+1}|x_0)}[D_{KL}(q(x_t|x_{t-1})||p_{\\theta}(x_t|x_{t+1}))]\\)</p> <p>\u8fd9\u4e2a\u8868\u8fbe\u662f\u6709\u95ee\u9898\u7684\u3002</p> <p>\u8fd9\u4e2a consistency term \u662f\u4e00\u4e2a\u6c42\u548c\u9879\uff0c\u6709\u5f88\u591a\u9879</p> <p>\u4e14\u5728\u6a21\u578b\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5360\u4e3b\u5bfc</p> <p>\u4f46\u662f\u8fd9\u4e00\u9879\u91cc\u9762\u6709\u95ee\u9898\uff0c\u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u4e00\u9879\u662f\u901a\u8fc7\u4e24\u4e2a\u968f\u673a\u53d8\u91cf\u5f97\u5230\u7684</p> <p>\u4e5f\u5c31\u662f\u8bf4 \\(x_t\\) \u662f\u901a\u8fc7\u4e24\u4e2a\u968f\u673a\u53d8\u91cf \\(x_{t-1}\\) \u548c \\(x_{t+1}\\) \u6765\u9884\u4f30</p> <p>\\(\\mathbb{E}_{q(x_{t-1},x_{t+1}|x_0)}\\) \u6c42\u671f\u671b\u662f\u5bf9\u4e24\u4e2a\u53d8\u91cf \\(x_{t-1}\\)\u548c \\(x_{t+1}\\)\uff0c\u5bf9\u4e24\u4e2a\u591a\u5143\u53d8\u91cf\u5bf9\u5e94\u7684\u591a\u5143\u5206\u5e03\u6c42\u5f97\u671f\u671b</p> <p>\u53c8\u56e0\u4e3a\uff0c\u8981\u901a\u8fc7 \\(x_{t-1}\\) \u548c \\(x_{t+1}\\) \u6765\u9884\u4f30 \\(x_t\\)</p> <p>\u800c\u5728\u5b9e\u9645\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e76\u4e0d\u662f\u76f4\u63a5\u6c42\u5f97\u671f\u671b\u7684\uff0c\u800c\u662f\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u7684\u65b9\u6cd5\uff0c\u8499\u7279\u5361\u6d1b\u5c31\u662f\u7528\u5f88\u591a\u6837\u4f8b\u7684\u5e73\u5747\u7b49\u4e8e\u671f\u671b\uff0c\u5728\u8ba1\u7b97\u8fd9\u9879\u671f\u671b\u7684\u65f6\u5019\uff0c\u56e0\u4e3a\u6709\u4e24\u4e2a\u968f\u673a\u53d8\u91cf\u4f30\u8ba1 \\(x_t\\)\uff0c\u6b64\u65f6\u5f97\u5230\u7684 \\(x_t\\) \u7684\u65b9\u5dee\u662f\u6bd4\u8f83\u5927\u7684</p> <p>\u56e0\u4e3a\u4e24\u4e2a\u53d8\u91cf\u672c\u8eab\u662f\u6709\u65b9\u5dee\u7684\uff0c\u7528\u4e24\u4e2a\u5177\u6709\u65b9\u5dee\u7684\u53d8\u91cf\u53bb\u9884\u4f30\u53e6\u4e00\u4e2a\u53d8\u91cf\uff0c\u65b9\u5dee\u662f\u4f1a\u7d2f\u79ef\u7684\uff0c\u5c31\u4f1a\u5bfc\u81f4\u8fd9\u4e2a loss\uff08\u7b2c\u4e09\u9879\uff0cconsistency term\uff09\u4e0d\u592a\u51c6\u4e86</p> <ul> <li> \u63a5\u4e0b\u6765\uff0c\u8ba8\u8bba\u80fd\u4e0d\u80fd\u964d\u4f4e\u8fd9\u4e2a\u65b9\u5dee</li> </ul> <p></p> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u5c31\u662f\u628a 2 \u4e2a\u968f\u673a\u53d8\u91cf\u53d8\u6210 1 \u4e2a</p> <p>\u5206\u6790\u601d\u8def\uff1a\u65e2\u7136\u90fd\u662f\u8981\u6c42 \\(x_t\\) \uff0c\u601d\u8003\u628a \\(x_{t-1}\\) \u5230 \\(x_t\\) \u53d8\u6210 \\(x_{t+1}\\) \u5230 \\(x_t\\)\uff0c\u8bb0\u4f4f\u8d1d\u53f6\u65af\u516c\u5f0f</p> <p>\u4ed4\u7ec6\u770b ppt \u4e0a\u7684\u8d1d\u53f6\u65af\u516c\u5f0f\uff0c\u5728\u4e0d\u770b \\(x_0\\)\u7684\u60c5\u51b5\u4e0b\uff0c\u4f59\u4e0b\u7684\u90e8\u5206\u5c31\u662f\u6807\u51c6\u7684\u8d1d\u53f6\u65af\u516c\u5f0f</p> <p></p> <ul> <li> \u63a5\u4e0b\u6765\u8ba8\u8bba\uff0c\u5982\u4f55\u901a\u8fc7\u8d1d\u53f6\u65af\u516c\u5f0f\uff0c\u628a\u7b2c\u4e09\u9879\u53d8\u6210\u53ea\u6709\u4e00\u4e2a\u968f\u673a\u53d8\u91cf\uff0c\u6765\u9884\u4f30 \\(x_t\\)</li> </ul> <p></p> <p></p> <p>\uff081\uff09\u770b\u7b2c\u4e00\u4e2a\u5212\u6389\u7684\uff0c\u62ff\u51fa\u6765\uff1a</p> <p>\\(\\log \\prod_{t=2}^T\\frac{1}{\\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}}\\)</p> <p>\\(=\\log \\prod_{t=2}^{T}\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\\)</p> <p>\\(=\\sum_{t=2}^T (\\log q(x_{t-1}|x_0) - \\log q(x_t|x_0))\\)</p> <p>\\(=\\sum_{t=2}^T \\log q(x_{t-1}|x_0) - \\sum_{t=2}^T \\log q(x_t|x_0)\\)</p> <p>\\(=\\sum_{t=1}^{T-1} \\log q(x_{t}|x_0) - \\sum_{t=2}^T \\log q(x_t|x_0)\\)</p> <p>\\(=\\log q(x_1|x_0) - \\log q(x_T|x_0)\\)</p> <p>\\(=\\log \\frac{q(x_1|x_0)}{q(x_T|x_0)}\\)</p> <p>\uff082\uff09<code>denoising matching term</code> \u7684\u63a8\u5bfc</p> <p></p> <ul> <li>\u8fd9\u4e2a\u8fc7\u7a0b\u548c\u524d\u9762 prior matching term \u7684\u63a8\u5bfc\u662f\u4e00\u6837\u7684</li> <li>\u5177\u4f53\u6765\u8bf4\uff1a\u8981\u6c42\u671f\u671b\uff0c\u7528\u5b9a\u4e49\u5c55\u5f00\uff0c\u53d8\u6210\u79ef\u5206\u7684\u5f62\u5f0f</li> <li>\u56e0\u4e3a\u662f\u5bf9\u4e24\u4e2a\u53d8\u91cf\u7684\u5206\u5e03\u6c42\u671f\u671b\uff0c\u6240\u4ee5\u662f\u4e24\u4e2a\u79ef\u5206\u7b26\u53f7 \\(dx_t\\) \u548c \\(dx_{t-1}\\)\uff0c\u8868\u793a\u5bf9\u4e24\u4e2a\u53d8\u91cf\u6c42\u79ef\u5206</li> <li>\u63a5\u7740\u94fe\u5f0f\u6cd5\u5219\uff0c\u62c6 \\(q(x_t,x_{t-1}|x_0)\\) \u62c6\u6210 \\(=q(x_t|x_0)q(x_{t-1}|x_t,x_0)\\)</li> <li>\u94fe\u63a5\u6cd5\u5219\u4ee5\u540e\uff0c\u5408\u5e76\uff0c\u51fa\u73b0\u4e00\u4e2a\u671f\u671b\uff1a</li> </ul> <p></p> <p>\u4e5f\u5c31\u662f KL \u6563\u5ea6\uff08\u6ce8\u610f\u662f\u8d1f\u7684\uff09</p> <p></p> <ul> <li>\u6700\u540e\u5c31\u662f\u5bf9 KL \u6563\u5ea6\u6c42\u671f\u671b\uff0c\u5f97\u5230\u6700\u540e\uff1a</li> </ul> <p></p> <p><code>\u5206\u5e03\u3001\u51fd\u6570\u3001\u79ef\u5206\u53f7\u3001\u79ef\u5206\u7b26\u53f7\u90fd\u6709 \u2192 \u671f\u671b</code></p> <ul> <li> KL \u6563\u5ea6\u7684\u6587\u5b57\u63cf\u8ff0\uff1a</li> </ul> <p>\u4e00\u4e2a\u5206\u5e03\u9664\u4ee5\u53e6\u4e00\u4e2a\u5206\u5e03\u7684 log\uff0c\u7136\u540e\u5bf9\u5206\u5b50\u7684\u5206\u5e03\u6c42\u4e00\u4e2a\u671f\u671b</p> <ul> <li> \u671f\u671b\u7684\u63cf\u8ff0</li> </ul> <p>\u6709\u4e00\u4e2a \\(dx_t\\)\uff0c\u53c8\u6709 \\(x_t\\)\u7684\u5206\u5e03\uff0c\u53c8\u6709\u79ef\u5206\u53f7 \\(\\int\\)\uff0c\u8fd9\u4e09\u9879\u63d0\u51fa\u6765\u5c31\u662f\u5bf9 \\(q(x_t)\\) \u6c42\u671f\u671b\uff0c\u5269\u4e0b\u7684\u90e8\u5206\u7167\u6284</p>"},{"location":"learning/6_Diffusion1/#37-vdm-e-lbo-2","title":"3.7 VDM \u7684E LBO \u62c6\u89e32","text":"<p>\u6700\u540e\u5f97\u5230\u7684\u8868\u8fbe\u5f0f\uff1a</p> <p></p> <ul> <li>\u89c2\u5bdf\u7b2c\u4e09\u9879\uff0c\u6b64\u65f6\u5bf9\u4e8e \\(x_{t-1}\\)\u4e0d\u9700\u8981\u65e2\u77e5\u9053\u524d\u4e00\u4e2a\u72b6\u6001\u53c8\u77e5\u9053\u540e\u4e00\u4e2a\u72b6\u6001\uff0c\u5982\u679c\u8981\u9884\u4f30 \\(x_{t-1}\\)\uff0c\u53ea\u9700\u8981\u77e5\u9053 \\(x_t\\) \u5373\u53ef</li> <li>\u8fd9\u6837\u5c31\u628a\u4e4b\u524d\u9700\u8981\u4e24\u4e2a\u53d8\u91cf\u9884\u4f30\u7b2c\u4e09\u9879\uff08denosing matching term\uff09\u8868\u8fbe\u5f0f \u53d8\u6210\u4e86 \u4e00\u4e2a\u53d8\u91cf \u5c31\u662f \u7531 \\(x_t\\) \u5f97\u5230 \\(x_{t-1}\\) \uff0c\u56e0\u6b64\u51cf\u5c0f\u4e86\u8fd9\u9879\u8bef\u5dee</li> <li>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e4b\u524d\u7b2c\u4e09\u9879\u53eb\u505a <code>consistency term</code>\uff0c\u7406\u89e3\u5c31\u662f\u4e00\u81f4\u6027\uff0c\u524d\u5411\u8fc7\u7a0b\u548c\u540e\u5411\u8fc7\u7a0b\u5f97\u5230 \\(x_t\\) \uff0c\u4e5f\u5c31\u662f \u524d\u5411\u548c\u540e\u5411\u7684\u4e00\u81f4\u6027\uff0c\u6240\u4ee5\u53eb\u505a <code>consistency term</code></li> <li>\u800c\u73b0\u5728\uff0c\u662f\u4e00\u4e2a\u65b9\u5411\u7684\uff0c\u5c31\u662f\u53bb\u566a\u7684\u65b9\u5411\uff0c\u4e5f\u5c31\u662f\u4ece \\(x_t\\) \u5230 \\(x_0\\)\u7684\u65b9\u5411\uff0c\u6240\u4ee5\u628a\u8fd9\u9879\u547d\u540d\u6210 <code>denoising matching term</code></li> <li>\u5176\u5b9e\u662f\u4e00\u4e2a\u4e1c\u897f\uff0c\u53ea\u662f\u901a\u8fc7\u63a8\u5bfc\uff0c\u4f7f\u7528\u4e00\u4e2a\u65b9\u5411\u7684\u9884\u4f30\uff0c\u53ef\u4ee5\u4f7f\u5f97 \u65b9\u5dee\u53d8\u5c0f\uff0c\u9884\u4f30\u66f4\u51c6\u786e</li> </ul>"},{"location":"learning/6_Diffusion1/#38-vdm-elbo-vae-elbo","title":"3.8 VDM \u7684 ELBO \u62c6\u89e3&amp;VAE \u7684ELBO \u62c6\u89e3","text":"<p>\u5bf9\u6bd4 VAE \u7684 ELBO \u62c6\u89e3</p> <p>\uff081\uff09VAE \u7684 ELBO \u62c6\u89e3</p> <p>\uff082\uff09VDM \u7684 ELBO \u62c6\u89e3</p> <p>\u2460 \u53ef\u4ee5\u770b\u5230\u524d\u4e24\u9879\u662f\u4e00\u6837\u7684\uff0c\u90fd\u6709 reconstruction term \u548c prior matching term</p> <p>\u2461 \u76f8\u6bd4\u4e8e VAE\uff0cVDM \u591a\u4e86\u6700\u540e\u4e24\u9879\uff0cdenoising matching term\uff0c\u800c\u4e14\u5bf9\u4e8e\u8fd9\u4e2a\u6c42\u548c\u9879\uff0c\u5982\u679c \\(T=1\\) \u7684\u8bdd\uff0c\u8fd9\u9879\u5c31\u662f\u7b49\u4e8e \\(0\\) \u7684\uff0c\u56e0\u4e3a \\(t\\) \u662f\u4ece \\(2\\) \u5f00\u59cb\u7684\uff0c\u6240\u4ee5 \\(T=1\\)\uff0c\u8fd9\u9879\u5c31\u662f\u6ca1\u6709\u7684</p> <p></p> <p>\u6b64\u65f6 <code>VAE \u7684 ELBO \u62c6\u89e3=VDM \u7684 ELBO \u62c6\u89e3</code></p> <p>\u5f53\u7136\u5566\uff0c\u8981\u6ce8\u610f\u7b26\u53f7\u7684\u5bf9\u5e94\u5466~</p> <p></p> <p>\\(VDM \uff1ax_0 \u2192 VAE\uff1ax\\)</p> <p>\\(VDM \uff1ax_1 \u2192 VAE\uff1az\\)</p> <p>\u8fd9\u4e5f\u8bf4\u660e\u4e86\uff0cVAE \u548c VDM\uff08Diffusion model\uff09\u5e95\u5c42\u7684 loss \u8868\u8fbe\u662f\u4e00\u81f4\u7684\uff0c\u7406\u8bba\u57fa\u7840\u7684\u4e00\u8109\u76f8\u627f\u7684</p>"},{"location":"learning/6_Diffusion1/#39-vdm-elbo","title":"3.9 VDM \u7684 ELBO \u4f18\u5316","text":"<p>\uff081\uff09\u7b2c\u4e00\u9879\uff1areconstruction term  \\(\\mathbb{E}_{\\log p_{\\theta}(x_0|x_1)}[\\log p_{\\theta}(x_0|x_1)]\\)</p> <p>\u53ea\u6709 1 \u9879\uff0c\u5e76\u4e0d\u662f\u4f18\u5316\u7684\u91cd\u70b9</p> <p>\uff082\uff09\u7b2c\u4e8c\u9879\uff1aprior matching term  \\(D_{KL}(q(x_T|x_0)||p(x_T))\\)</p> <p>\\(q(x_T|x_0)\\)  \u56e0\u4e3a\u4ece \\(x_0\\) \u5230 \\(x_T\\) \u662f\u6211\u4eec\u4eba\u4e3a\u5b9a\u4e49\u7684\uff0c\u662f\u6ca1\u6709\u4efb\u4f55\u6a21\u578b\u53c2\u6570\u7684</p> <p>\\(p(x_T)\\) \u662f\u5b9a\u4e49\u7684\u4e00\u4e2a\u9ad8\u65af\u6b63\u6001\u5206\u5e03</p> <p>\u56e0\u6b64\uff0c\u8fd9\u4e2a KL \u6563\u5ea6\u662f\u5df2\u77e5\u7684\uff0c\u6ca1\u6709\u4efb\u4f55\u53c2\u6570\u53ef\u4ee5\u4f18\u5316\u7684</p> <p>\uff083\uff09\u91cd\u70b9\u4f18\u5316\u7b2c\u4e09\u9879\uff0c\u6709\u6c42\u548c\u53f7  \uff08\u53bb\u566a\u9879\uff09</p> <p>\\(\\sum_{t=2}^{T}\\mathbb{E}_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\\theta}(x_{t-1}|x_t))]\\)</p> <ul> <li>\u9996\u5148\u9700\u8981\u77e5\u9053 \\(q(x_{t-1}|x_t,x_0)\\) \u7684\u5206\u5e03\u662f\u4ec0\u4e48\u6837\u7684\uff0c\u9700\u8981\u77e5\u9053\u5b83\u7684\u771f\u5b9e\u5206\u5e03\uff0c\u7136\u540e\u624d\u80fd\u53bb\u62df\u5408\uff0c\u901a\u8fc7 \\(x_t\\) \u9884\u6d4b \\(x_{t-1}\\)</li> <li>VDM \u8fd9\u4e2a\u6a21\u578b\u5c31\u662f\u8981\u6839\u636e \\(x_t\\) \u5f97\u5230 \\(x_{t-1}\\)</li> </ul> <p>\u5176\u5b9e q \u662f\u52a0\u566a\u8fc7\u7a0b\uff0cp \u662f\u4ece \\(x_0\\) \u5230 \\(x_T\\) \u7684\uff0c\u4f46\u662f\u7ecf\u8fc7\u8f6c\u6362\u8fd9\u91cc\u7684 q \u4ece \\(x_t\\) \u5230 \\(x_0\\)</p>"},{"location":"learning/6_Diffusion1/#310-qx_t-1x_tx_0-ground-truth","title":"3.10 \\(q(x_{t-1}|x_t,x_0)\\) \u7684 ground truth","text":"<p>\u601d\u8def\uff1a\u5df2\u77e5 \\(q(x_t|x_{t-1})\\) \u7684\u5206\u5e03\u662f\u5df2\u77e5\u7684\uff0c\u73b0\u5728\u8981\u6c42 \\(q(x_{t-1}|x_t)\\) \u7684\u5206\u5e03\uff0c\u65b9\u6cd5\u5c31\u662f\u501f\u52a9\u8d1d\u53f6\u65af\u516c\u5f0f</p> <ul> <li>ppt \u4e0a\u5199\u7684\uff0c\u53ef\u4ee5\u5148\u4e0d\u7528\u770b \\(x_0\\)\uff0c\u4f59\u4e0b\u7684\u90e8\u5206\u5c31\u662f\u6807\u51c6\u7684\u8d1d\u53f6\u65af\u516c\u5f0f</li> <li>\u91cd\u53c2\u6570\u5316\u6280\u5de7\uff1a\\(x_t = \u5747\u503c + \u6807\u51c6\u5dee\u00d7\u6807\u51c6\u9ad8\u65af\u5206\u5e03\\)</li> <li>\u89c2\u5bdf\u8d1d\u53f6\u65af\u516c\u5f0f\uff1a</li> </ul> <p>\\(q(x_{t-1}|x_t,x_0) = \\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0}\\)</p> <ul> <li> <p>for  \\(q(x_t|x_{t-1},x_0)\\) \u662f\u6ee1\u8db3\u4e0e \\(x_{t-1}\\)\u6709\u5173\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u4e5f\u5c31\u662f\u53ef\u4ee5\u901a\u8fc7 \\(x_{t-1}\\) \u53ef\u4ee5\u5f97\u5230 \\(x_t\\)</p> </li> <li> <p>\uff08for \\(q(x_t|x_0)\\)\uff09\u73b0\u5728\u8981\u901a\u8fc7 \\(x_0\\) \u5f97\u5230 \\(x_{t-1}\\) \u548c \\(x_t\\)\uff0c\u65b9\u6cd5\uff1a\u501f\u52a9\u91cd\u53c2\u6570\u5316\u516c\u5f0f\uff0c\u9012\u63a8\uff0c\u5177\u4f53\u7684\u6b65\u9aa4\uff1a</p> </li> </ul> <p></p> <p>\u8bf4\u660e</p> <p>\uff081\uff09\u4e0d\u7ba1\u662f \\(\\epsilon_{t-1}^*\\)\u3001\\(\\epsilon_{t-2}^*\\)\u3001\\(\\epsilon_{t-2}\\) \\(\\sim N(0,1)\\) \uff0c\u90fd\u662f\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u7684\u968f\u673a\u566a\u58f0\uff0c\u53ea\u662f\u8868\u793a\u7b26\u53f7\u4e0d\u540c</p> <p>\uff082\uff09\u91cd\u70b9\u8bf4\u4e0b\uff1a</p> <p></p> <p>\u8fd9\u91cc\u7528\u7684\u77e5\u8bc6\u70b9\uff1a\u4e24\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\u548c \u8fd8\u662f \u9ad8\u65af\u5206\u5e03\uff0c\u4e14\u5747\u503c\u548c\u65b9\u5dee\u7684\u516c\u5f0f\u6709</p> <p>\\(a\\epsilon_{t-2}^*+b\\epsilon_{t-1}^* \\sim N(0,a^2+b^2)\\)</p> <p>\u8fd8\u6709\uff0c\\(\u4efb\u610f\u6b63\u6001\u5206\u5e03 = \\sigma N(0,1) + \\mu\\)</p> <p>\u6240\u4ee5\u6709\u4e0a\u56fe</p> <p>(3)\u6700\u540e\u9012\u63a8\u5f97\u5230\uff1a\\(x_t = \\sqrt{\\bar \\alpha_t}x_0 + \\sqrt{1-\\bar \\alpha_t} \\epsilon_0\\)</p> <p>\u5176\u4e2d\uff0c\\(x_0\\)\u4e3a\u5df2\u77e5\uff0c\\(\\epsilon_0 \\sim N(0,1)\\)</p> <p>\u4ee5\u4e0a\u5f97\u5230\u4e86 \\(q(x_t|x_0)\\)  \u4e5f\u5c31\u662f\u5df2\u77e5 \\(x_0\\)  \u5f97\u5230 \\(x_t\\)</p> <ul> <li>for \\(q(x_{t-1}|x_0)\\)  \u53ea\u9700\u8981\u628a \u4e0a\u9762\u5f97\u5230 \\(q(x_t|x_0)\\)\u76f8\u5173\u7684\uff0ct \u90fd\u6362\u6210 \\(t-1\\) \u5373\u53ef</li> </ul> <p>\u5982\u56fe\uff1a</p> <p></p> <ul> <li>\u4ee5\u4e0a\uff0c\u5c31\u5f97\u5230\u4e86\u4e09\u9879\u7684\u8868\u8fbe\u5f0f</li> <li>\u63a5\u4e0b\u6765\uff0c\u628a\u5f97\u5230\u7684\u8868\u8fbe\u5f0f\uff0c\u4ee3\u5165\u5230\u516c\u5f0f\u4e2d\uff0c\u4e5f\u5c31\u662f\u4e24\u4e2a\u5206\u5e03\u76f8\u4e58 \u9664\u4ee5 \u53e6\u5916\u4e24\u4e2a\u5206\u5e03</li> <li>\u7701\u7565\u7cfb\u6570</li> <li>\u628a \\(x_{t-1}\\)\u89c6\u4e3a\u672a\u77e5\u6570\uff0c\u5408\u5e76\u540c\u7c7b\u9879\uff0c\u56e0\u4e3a\u8981\u6c42\u7684\u5c31\u662f \\(x_{t-1}\\)\u7684\u5206\u5e03\u3002\u522b\u5fd8\u4e86\u662f\u5728\u5229\u7528\u8d1d\u53f6\u65af\u516c\u5f0f \u6c42 \\(q(x_{t-1}|x_t)\\)</li> </ul> <p></p> <p>\u5408\u5e76 x_{t-1} \u7684\u5e73\u65b9\u9879\u3001\u4e00\u6b21\u9879\u3001\u5e38\u6570\u9879\uff0c\u5f97\u5230\u6700\u540e\u7684\u8868\u8fbe\u5f0f</p> <ul> <li>\u5ffd\u7565\u5e38\u6570\u9879 \u6216\u8005 \u7cfb\u6570 \u90fd\u662f\u52a0\u4e86 <code>\u6b63\u6bd4</code> \u7b26\u53f7</li> </ul> <p></p> <p>\u6ce8\u610f\u770b\u7bad\u5934\u6307\u5411&amp;\u5bf9\u5e94\u5173\u7cfb\uff1a</p> <p></p> <p>\u5f97\u5230\u4e86 \u4ee5\u7ea2\u6846\u4e3a\u65b9\u5dee\uff0c\u9ec4\u6846\u4e3a\u5747\u503c\u7684 \u9ad8\u65af\u5206\u5e03\uff0c\u4e5f\u5c31\u662f\u63a8\u51fa\u4e86 \\(q(x_t|x_{t-1})\\)</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>\u7528\u7684\u57fa\u7840\u77e5\u8bc6\uff1a</p> <ul> <li>\u671f\u671b\u7684\u5b9a\u4e49</li> <li>KL \u6563\u5ea6\u7684\u516c\u5f0f</li> <li>\u8d1d\u53f6\u65af\u516c\u5f0f</li> </ul> <p></p>"},{"location":"learning/7_Clip/","title":"Clip","text":""},{"location":"learning/8_WeightNorm/","title":"WeightNorm","text":"<p>\u5feb\u901f\u590d\u73b0PyTorch\u7684Weight Normalization</p>"},{"location":"learning/8_WeightNorm/#1-api","title":"1 \u5b98\u65b9api\u89e3\u8bfb","text":"<p>\u6743\u91cd\u5f52\u4e00\u5316\u7684api</p> <p>\u751f\u6210\u5f0f\u7f51\u7edc\u6bd4\u5982GAN\uff0c\u4f7f\u7528\u6743\u91cd\u5f52\u4e00\u5316\u4f7f\u5f97\u7f51\u7edc\u8bad\u7ec3\u66f4\u52a0\u7a33\u5b9a</p> <p>\u6743\u91cd\u5f52\u4e00\u5316\u7684api\u5728<code>torch.nn.utils</code>\u4e0b\u7684\u4e00\u4e2a\u51fd\u6570\uff0c\u4e0d\u662fclass</p> <p>\u8fd9\u4e2a\u51fd\u6570\u7684\u4f20\u5165\u53c2\u6570\uff1a</p> <ul> <li>module\uff1apytorch\u4e2d\u5f88\u591aclass\u90fd\u662fnn.module\u7684\u5b50\u7c7b\uff0c\u6240\u4ee5\u53ea\u9700\u8981\u4f20\u5165module\u7684\u5bf9\u8c61\u5373\u53ef\uff0c\u5982\uff1ann.Linear\u3001nn.ReLu\u3001nn.Conv\u3001nn.RNN</li> <li>name\uff1a\u4e00\u822c\u4e0d\u4f1a\u6539</li> <li>dim\uff1a\u4e5f\u4e00\u822c\u662f\u9ed8\u8ba4\u7684</li> </ul> <p>\u6743\u91cd\u5f52\u4e00\u5316\u7684\u8bba\u6587\uff1a</p> <p></p> <p>\u6807\u9898\uff1a\u6743\u91cd\u5f52\u4e00\u5316\uff1a\u7b80\u5355\u7684\u53c2\u6570\u91cd\u6574\u5316\u65b9\u6cd5\uff1a\u52a0\u901f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3</p> <p>\u4eca\u5929\u8bb2\u89e3\uff1a\u6743\u91cd\u5f52\u4e00\u5316\u4e3b\u8981\u505a\u4e86\u4ec0\u4e48\u4e8b</p> <p>WeightNorm\u5bf9module\u8fdb\u884c\u4e00\u5c42\u5305\u88f9</p> <p>\u4f8b\u5982 \u628a\u4e00\u4e2ann.Linear\u653e\u5165WeightNorm\u4e2d\uff0cWeightNorm\u4ecd\u7136\u8fd4\u56de\u7684\u662f\u4e00\u4e2amodule\uff0c\u8fd9\u4e2a\u8fd4\u56de\u7684module\u5305\u542b\u4e24\u4e2a\u53c2\u6570\uff1aWeight_g \u548c Weight_v</p> <p>\u672c\u6765\u7684Linear\u5c42\uff0c\u53ea\u6709\u4e00\u4e2a\u53c2\u6570w\uff0c\u5ffd\u7565bias\uff0c\u653e\u5165WeightNorm\u4e2d\u5904\u7406\uff0c\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684module\uff0c\u65b0\u7684module\u6709\u4e24\u4e2a\u53c2\u6570\uff1aWeight_g \u548c Weight_v</p> <p>Weight_g \u548c Weight_v\u5c31\u662fWeightNorm\u516c\u5f0f\u4e2d\u7684g\u548cv</p> <p>\\(w = g\\frac{v}{||v||}\\)</p> <p></p> <p>\u770b\u516c\u5f0f</p> <p>\u76f8\u5f53\u4e8e\u628a\u539f\u6765\u7684module\u4e2d\u7684 \u6743\u91cdw \u5206\u89e3\u4e86\uff0c\u6240\u4ee5\u4e0e\u5176\u53eb\u6743\u91cd\u5f52\u4e00\u5316\uff0c\u4e5f\u53ef\u4ee5\u53eb\u6743\u91cd\u5206\u89e3</p> <p>\u76f8\u5f53\u4e8e\u628a\u539f\u6765module\u4e2d\u7684w\u5206\u89e3\u6210\u4e24\u9879\uff1a</p> <ul> <li>g\uff1ag\u8868\u793aw\u7684\u5e45\u5ea6\uff0c\u76f8\u5f53\u4e8ew\u6bcf\u9636\u5411\u91cf\u7684\u4e8c\u9636\u6a21\u3001\u8303\u6570</li> <li>\\(\\frac{v}{||v||}\\)\uff1a\u5355\u4f4d\u5411\u91cf\uff0c\u65b9\u5411\u9664\u4ee5\u6a21\u957f\u5f97\u5230\u65b9\u5411\u5411\u91cf</li> </ul> <p>\u4e5f\u5c31\u662f\u8bf4\u672c\u6765 module\u53ea\u6709\u4e00\u4e2a\u53c2\u6570 w\uff0c\u73b0\u5728\u53d8\u6210\u4e24\u4e2a\u53c2\u6570\uff0c\u5206\u522b\u662f g \u548c v \uff0c\u4e5f\u5c31\u662f\u6b64\u65f6\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u53c2\u6570\u65f6\uff0c\u4e0d\u662f\u5bf9 w \u8fdb\u884c\u66f4\u65b0\u4e86\uff0c\u800c\u662f\u5bf9 g \u548c v \u8fdb\u884c\u66f4\u65b0\uff08\u597d\u5904\u81ea\u5df1\u53bb\u770b\u8bba\u6587\uff09</p>"},{"location":"learning/8_WeightNorm/#2","title":"2 \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"learning/8_WeightNorm/#21","title":"2.1 \u4e24\u4e2a\u5b9e\u4f8b\u6f14\u793a","text":"<ol> <li>Linear\u5c42</li> <li>\u4e00\u7ef4Conv\u7684\u4f8b\u5b50</li> </ol> <p>\u4ee5\u4e0a \u4e24\u4e2a\u4f8b\u5b50 \u89e3\u91ca WeightNorm\u5b98\u65b9api\u505a\u4e86\u4ec0\u4e48\u4e8b</p> <pre><code>import torch\nimport torch.nn as nn\n\n# \u5173\u4e8e\u6743\u91cd\u5f52\u4e00\u5316\u7684\u518d\u6b21\u8bf4\u660e\n# WeightNorm W = Magnitude * UnitDirection = Magnitude * (W/Norm(W))\n\n# step1:define constant\nbatch_size = 2\nfeat_dim = 3\nhid_dim = 4\ninputx = torch.randn(batch_size,feat_dim)\nlinear = nn.Linear(feat_dim,hid_dim,bias=False)\nwn_linear = torch.nn.utils.weight_norm(linear)\n\n# step2:Linear Layer:calculate g and v\nweight_magnitude = torch.tensor([linear.weight[i:,].norm() for i in torch.arange(linear.weight.shape[0])],dtype =torch.float32).unsqueeze(-1)\nweight_direction = linear.weight / weight_magnitude\n\nprint(\"linear.weight:\",linear.weight)\nprint(\"weight_magnitude:\",weight_magnitude)\nprint(\"weight_direction:\",weight_direction)\nprint(\"magnitude of weight_direction:\",(weight_direction**2).sum(dim=-1))\n\n\n'''\nlinear.weight: tensor([[-0.2701, -0.0754,  0.3812],\n        [-0.1806,  0.1814,  0.4922],\n        [-0.2900, -0.5321, -0.4400],\n        [-0.5492,  0.0195, -0.5189]], grad_fn=&lt;WeightNormInterfaceBackward0&gt;)\nweight_magnitude: tensor([[1.2899],\n        [1.2000],\n        [1.0640],\n        [0.7558]])\nweight_direction: tensor([[-0.2094, -0.0584,  0.2956],\n        [-0.1505,  0.1512,  0.4102],\n        [-0.2726, -0.5001, -0.4135],\n        [-0.7267,  0.0259, -0.6865]], grad_fn=&lt;DivBackward0&gt;)\nmagnitude of weight_direction: tensor([0.1346, 0.2138, 0.4954, 1.0000], grad_fn=&lt;SumBackward1&gt;)\n'''\n</code></pre>"},{"location":"learning/8_WeightNorm/#22","title":"2.2 \u6ce8\u91ca","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# \u5173\u4e8e\u6743\u91cd\u5f52\u4e00\u5316\u7684\u518d\u6b21\u8bf4\u660e\n# WeightNorm W = Magnitude * UnitDirection = Magnitude * (W/Norm(W))\n\n# step1:define constant\nbatch_size = 2\nfeat_dim = 3\nhid_dim = 4\ninputx = torch.randn(batch_size,feat_dim) # 2\u00d73\nlinear = nn.Linear(feat_dim,hid_dim,bias=False) # linear.weight=4\u00d73\nwn_linear = torch.nn.utils.weight_norm(linear)\n\n# step2:Linear Layer:calculate g and v\nweight_magnitude = torch.tensor([linear.weight[i:,].norm() \n                                 for i in torch.arange(linear.weight.shape[0])],\n                                dtype =torch.float32).unsqueeze(-1)\n# weight_magnitude\uff1a4\u00d71\nweight_direction = linear.weight / weight_magnitude\n# weight_direction\uff1a4\u00d73\nprint(\"linear.weight:\",linear.weight) # linear.weight=4\u00d73\nprint(\"weight_magnitude:\",weight_magnitude) # weight_magnitude\uff1a4\u00d71\nprint(\"weight_direction:\",weight_direction) # weight_direction\uff1a4\u00d73\nprint(\"magnitude of weight_direction:\",(weight_direction**2).sum(dim=-1))\n\n\n'''\nlinear.weight: tensor([[-0.2701, -0.0754,  0.3812],\n        [-0.1806,  0.1814,  0.4922],\n        [-0.2900, -0.5321, -0.4400],\n        [-0.5492,  0.0195, -0.5189]], grad_fn=&lt;WeightNormInterfaceBackward0&gt;)\nweight_magnitude: tensor([[1.2899],\n        [1.2000],\n        [1.0640],\n        [0.7558]])\nweight_direction: tensor([[-0.2094, -0.0584,  0.2956],\n        [-0.1505,  0.1512,  0.4102],\n        [-0.2726, -0.5001, -0.4135],\n        [-0.7267,  0.0259, -0.6865]], grad_fn=&lt;DivBackward0&gt;)\nmagnitude of weight_direction: tensor([0.1346, 0.2138, 0.4954, 1.0000], grad_fn=&lt;SumBackward1&gt;)\n'''\n</code></pre>"},{"location":"learning/8_WeightNorm/#23","title":"2.3 \u8be6\u89e3","text":""},{"location":"learning/8_WeightNorm/#1","title":"\uff081\uff09\u5b9a\u4e49\u5e38\u91cf","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# \u5173\u4e8e\u6743\u91cd\u5f52\u4e00\u5316\u7684\u518d\u6b21\u8bf4\u660e\n# WeightNorm W = Magnitude * UnitDirection = Magnitude * (W/Norm(W))\n\nbatch_size = 2\nfeat_dim = 3\nhid_dim = 4\ninputx = torch.randn(batch_size,feat_dim)\nlinear = nn.Linear(feat_dim,hid_dim,bias=False)\nwn_linear = torch.nn.utils.weight_norm(linear)\n</code></pre> <ul> <li> feat_dim\uff1a\u6570\u636e\u7ef4\u5ea6</li> <li> hid_dim\uff1a\u9690\u542b\u5c42\u7ef4\u5ea6\uff0c\u6307\u7684\u662f\u7ebf\u6027\u5c42\u7684\u7ef4\u5ea6\uff0c\u7ebf\u6027\u5c42\u7684\u9690\u542b\u5c42\u6216\u8005Conv\u7684\u8f93\u51fa\u901a\u9053\u6570</li> <li> <code>inputx = torch.randn(batch_size,feat_dim)</code></li> </ul> <p>torch.randn\u521d\u59cb\u5316inputx\uff0c\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\uff0c\u7b2c\u4e00\u7ef4\u5ea6\u662f batch_size \uff0c\u7b2c\u4e8c\u7ef4\u5ea6\u662f \u8f93\u5165\u6570\u636e\u7684\u7279\u5f81\u7ef4\u5ea6</p> <ul> <li> <code>linear = nn.Linear(feat_dim,hid_dim,bias=False)</code></li> </ul> <p>\u5b9e\u4f8b\u5316\u4e00\u4e2alinear\u5c42\uff0clinear\u5c42\u7684api\uff1a</p> <ul> <li>\u7b2c\u4e00\u4e2a\u53c2\u6570 \u8f93\u5165\u6570\u636e\u7684\u7279\u5f81\u7ef4\u5ea6</li> <li>\u7b2c\u4e8c\u4e2a\u53c2\u6570 \u9690\u542b\u5c42\u7684\u7279\u5f81\u7ef4\u5ea6</li> <li> <p>bias\u8bbe\u7f6eFalse\uff0c\u7ed9\u5173\u6389</p> </li> <li> <p> <code>wn_linear = torch.nn.utils.weight_norm(linear)</code></p> </li> </ul> <p>\u63a5\u4e0b\u6765\uff0c\u628alinear\u4f5c\u4e3a\u4e00\u4e2a\u53c2\u6570\uff0c\u4f20\u5165 torch \u7684 weight norm\u51fd\u6570\u4e2d\uff0c\u5f97\u5230\u65b0\u7684\u6a21\u5757 weightnorm linear\uff1a<code>wn_linear</code>\uff0c\u4ecd\u7136\u662f\u4e00\u4e2amodule</p>"},{"location":"learning/8_WeightNorm/#2-linear-wn_linear-modulegv","title":"\uff082\uff09\u63a2\u8ba8 <code>linear</code> \u548c <code>wn_linear</code> \u4e24\u4e2amodule\u7684\u5173\u7cfb\uff08\u8ba1\u7b97g&amp;v\uff09","text":"<p>\u6839\u636e\u516c\u5f0f\uff0c\u53ef\u4ee5\u7b97\u51fa g \u548c \\(\\frac{v}{||v||}\\)\uff0c\u63a5\u4e0b\u6765\u7814\u7a76\u600e\u4e48\u7b97\u8fd9\u4e24\u4e2a\u5411\u91cf\uff1a</p> <pre><code>weight_magnitude = torch.tensor([linear.weight[i:,].norm() \n                                 for i in torch.arange(linear.weight.shape[0])],\n                                dtype =torch.float32).unsqueeze(-1)\n\nweight_direction = linear.weight / weight_magnitude\n\nprint(\"linear.weight:\",linear.weight)\nprint(\"weight_magnitude:\",weight_magnitude)\nprint(\"weight_direction:\",weight_direction)\nprint(\"magnitude of weight_direction:\",(weight_direction**2).sum(dim=-1))\n</code></pre> <ul> <li>\u9996\u5148 \u8ba1\u7b97 g\uff0cg\u8868\u793a\u5e45\u5ea6</li> </ul> <pre><code>weight_magnitude = torch.tensor([linear.weight[i:,].norm() \n                                 for i in torch.arange(linear.weight.shape[0])],\n                                dtype =torch.float32).unsqueeze(-1)\n</code></pre> <p>\u5e45\u5ea6\u6307\u7684\u662f \u8ddf\u8f93\u5165\u7684\u6bcf\u4e00\u4e2asample \u8fdb\u884c\u5185\u79ef\u7684\u5411\u91cf\u7684\u5e45\u5ea6</p> <p>\u9996\u5148\u62ff\u51falinear\u5c42\u7684\u6743\u91cd\u77e9\u9635\uff0c<code>linear.weight</code></p> <p>\u7136\u540e \u627e\u5230\u6bcf\u4e00\u4e2a \u8ddfsample\u8fdb\u884c\u5185\u79ef\u7684 \u5411\u91cf\uff0c\u4e5f\u5c31\u662f weight\u7684\u6bcf\u4e00\u884c <code>linear.weight[i:,]</code></p> <p>\u5bf9 <code>linear.weight[i:,]</code> \u7684\u6bcf\u4e00\u884c\u8fdb\u884c\u904d\u5386\uff0c\u8ba1\u7b97norm\uff1a<code>linear.weight[i:,].norm()</code></p> <p><code>.norm()</code> \u662f torch\u4e2d\u7684\u51fd\u6570\uff0c\u8ba1\u7b97L2\u8303\u6570\uff0c\u8c03\u7528norm() \u51fd\u6570\u4ee5\u540e\uff0c\u5f97\u5230\u6bcf\u4e00\u884c\u7684\u8303\u6570</p> <p><code>.unsqueeze(-1)</code> \u6269\u4e00\u7ef4</p> <p>\u8ba1\u7b97\u51fa linear\u5c42\uff0c\u539f\u6765\u6743\u91cd\u77e9\u9635\u7684 \u5e45\u5ea6\u503c</p> <pre><code>weight_magnitude: tensor([[1.2899],\n        [1.2000],\n        [1.0640],\n        [0.7558]])\n</code></pre> <p>\u6743\u91cd\u77e9\u9635\u662f 4\u884c\u7684\uff0c\u6bcf\u4e00\u884c\u90fd\u80fd\u8ba1\u7b97\u5e45\u5ea6\u503c</p> inputx = 2\u00d73\uff0c\u4e3a\u4ec0\u4e48 linear.weight=4\u00d73? <ul> <li>\u8ba1\u7b97\u5355\u4f4d\u5411\u91cf</li> </ul> <pre><code>weight_direction = linear.weight / weight_magnitude\n</code></pre> <ul> <li>\u5355\u4f4d\u5411\u91cf \u5c31\u662f v \u9664\u4ee5 v\u7684\u6a21</li> <li>v\u5176\u5b9e\u5c31\u662fw\uff0c\u6240\u4ee5\u5728\u8ba1\u7b97w\u7684\u65f6\u5019\uff0c\u5c31\u662f\u628a\u539f\u6765\u7684\u6743\u91cd\u77e9\u9635 <code>linear.weight</code> \u9664\u4ee5 \u6211\u4eec\u521a\u521a\u7b97\u51fa\u6765\u7684 \u5e45\u5ea6\u503c <code>weight_magnitude</code></li> <li> <p>\u6bcf\u4e00\u4e2a\u6743\u91cd\u5411\u91cf \u9664\u4ee5 \u5411\u91cf\u7684\u6a21\uff0c\u5f97\u5230 <code>weight_direction</code></p> </li> <li> <p><code>weight_direction</code> \u8ddf weight \u77e9\u9635\u7684\u5f62\u72b6\u662f\u4e00\u6837\u7684\uff0c\u8fd9\u4e2a\u77e9\u9635\u53eb\u505a\u5355\u4f4d\u5411\u91cf\u77e9\u9635\uff0c\u6bcf\u4e00\u884c\u90fd\u662f\u5355\u4f4d\u5411\u91cf</p> </li> <li>\u90a3\u4e3a\u4ec0\u4e48 \u662f \u5355\u4f4d\u5411\u91cf\u77e9\u9635\u5462\uff1f\u9a8c\u8bc1\uff1a</li> </ul> <pre><code>print(\"magnitude of weight_direction:\",(weight_direction**2).sum(dim=-1))\n</code></pre> <ul> <li>\u628a <code>weight_direction</code> \u9996\u5148\uff0c\u6bcf\u4e2a\u5143\u7d20\u53d6\u5e73\u65b9\uff0c\u7136\u540e\u518d\u5bf9\u6bcf\u4e00\u884c\u6c42\u548c</li> </ul> <p>\u8fd9\u91cc\u6709\u95ee\u9898\uff1aup\u4e3b\u7ed3\u679c\u4e3a1\uff0c\u6211\u7684\u7ed3\u679c\u4e0d\u4f1a\u662f1\uff0c\u4f46\u7ed3\u679c\u5e94\u8be5\u662f1</p> <p></p> <p>\u4e5f\u4e0d\u77e5\u9053\u54ea\u91cc\u51fa\u95ee\u9898\u4e86\uff0c\u603b\u4e4b\u6211\u7684\u6709\u95ee\u9898\uff0c\u6240\u4ee5\u79f0\u4e4b\u4e3a \u5355\u4f4d\u5411\u91cf</p> <ul> <li> <p>\u53eb\u505a \u5355\u4f4d\u5411\u91cf\u7684\u539f\u56e0\u662f weight\u6bcf\u4e00\u884c\u7684\u5e73\u65b9\u548c \u90fd\u662f1</p> </li> <li> <p>\u4e5f\u5c31\u662f\u8bf4 \u6bcf\u4e00\u884c\u5411\u91cf\u957f\u5ea6\u90fd\u662f 1\uff0c\u4e5f\u5c31\u662f\u5355\u4f4d\u5411\u91cf\uff0c\u53cd\u6620\u7684\u662f \u6bcf\u4e2a\u5411\u91cf\u7684\u65b9\u5411\u7684\uff0c\u5e76\u4e14\u7528 \u957f\u5ea6\u4e3a1 \u7684\u5411\u91cf \u53cd\u6620\u65b9\u5411</p> </li> <li> <p>\u4e0a\u9762\u5df2\u7ecf\u7b97\u51fa\u6765\u4e86 \u539f\u6765 linear\u5c42 \u7684\u6743\u91cd\u7684\u5e45\u5ea6\u548c\u65b9\u5411\uff1a</p> </li> </ul> <pre><code>weight_magnitude = torch.tensor([linear.weight[i:,].norm() for i in torch.arange(linear.weight.shape[0])],dtype =torch.float32).unsqueeze(-1)\nweight_direction = linear.weight / weight_magnitude\n</code></pre> <p>\u4e0b\u9762\u5c06\u65b9\u5411\u4e0e\u5e45\u5ea6\u76f8\u4e58\uff1a</p> <pre><code>print(\"weight_direction * weight_magnitude:\")\nprint(weight_direction * weight_magnitude)\n\nprint(\"inputx @ (weight_direction * weight_magnitude).T:\")\nprint(inputx @ (weight_direction * weight_magnitude).T)\n\nprint(\"linear(inputx):\")\nprint(linear(inputx))\n\nprint(\"wn_linear(inputx):\")\nprint(wn_linear(inputx))\n</code></pre> <pre><code>weight_direction * weight_magnitude:\ntensor([[ 0.0999, -0.1095,  0.0053],\n        [ 0.4107, -0.1039, -0.5627],\n        [-0.0347, -0.1121,  0.0211],\n        [ 0.1116,  0.0381, -0.0633]], grad_fn=&lt;MulBackward0&gt;)\ninputx @ (weight_direction * weight_magnitude).T:\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\nlinear(inputx):\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\nwn_linear(inputx):\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\n</code></pre> <p>\uff081\uff09\u65b9\u5411\u4e0e\u5e45\u5ea6\u76f8\u4e58\uff0c\u5f97\u5230\uff1a</p> <pre><code>print(\"weight_direction * weight_magnitude:\")\nprint(weight_direction * weight_magnitude)\n</code></pre> <pre><code># weight_direction * weight_magnitude:\n# tensor([[ 0.4900, -0.5379,  0.5541],\n#         [-0.5104,  0.3061,  0.4884],\n#         [-0.0247, -0.0822,  0.2893],\n#         [-0.1487, -0.4578, -0.4388]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>\u89c2\u5bdf\uff0clinear.weight\u7684\u7ed3\u679c\u548c \u65b9\u5411\u4e0e\u5e45\u5ea6\u76f8\u4e58 \u5f97\u5230\u7684\u4e58\u79ef  \u7ed3\u679c\u76f8\u540c</p> <p>\u53ef\u4ee5\u628a lienar.weight \u7684\u7ed3\u679c \u5206\u89e3\u4e3a \u5e45\u5ea6\u548c\u65b9\u5411\u7684\u4e58\u79ef</p> <p>\uff082\uff09\u628ainputx\u5206\u522b\u653e\u5165linear\u5c42\u548cweight_linear\u5c42\u3001inputx\u4e0eweight_direction \u65b9\u5411\u548c\u5e45\u5ea6 weight_magnitude\u505a\u77e9\u9635\u4e58\u6cd5\uff1a</p> <pre><code>print(\"inputx @ (weight_direction * weight_magnitude).T:\")\nprint(inputx @ (weight_direction * weight_magnitude).T)\nprint(\"linear(inputx):\")\nprint(linear(inputx))\nprint(\"wn_linear(inputx):\")\nprint(wn_linear(inputx))\n</code></pre> <p>\u6253\u5370\u7ed3\u679c\uff1a</p> <pre><code>inputx @ (weight_direction * weight_magnitude).T:\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\nlinear(inputx):\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\nwn_linear(inputx):\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\n</code></pre> <p>\u53d1\u73b0 \u4e09\u4e2a\u7ed3\u679c\u90fd\u662f\u4e00\u6837\u7684</p> <ul> <li>wn_linear \u8fd9\u4e2a\u65b0\u751f\u6210\u7684\u5c42\u5e76\u4e0d\u4f1a\u6539\u53d8\u6a21\u5757\u7684\u8f93\u51fa\u503c\uff0c\u4e4b\u524dlinear\u7684\u8f93\u51fa\u662f\u4ec0\u4e48\uff0c\u52a0\u4e86WeightNorm\u8f93\u51fa\u4f9d\u65e7\u4e0d\u53d8</li> <li>\u533a\u522b\uff1a\uff081\uff09\u539f\u59cblinear\u5c42\u7684\u53c2\u6570\u53ea\u6709weight\uff082\uff09weightNorm\u4e4b\u540e\u7684\u5c42\u6709g\u548cv</li> </ul> <p>\u67e5\u770bwn_lienar\u7684\u53c2\u6570</p> <pre><code>print(\"paramter of wn_linear:\")\nfor n,p in wn_linear.named_parameters():\n    print(n,p)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>weight_g Parameter containing:\ntensor([[0.1483],\n        [0.7043],\n        [0.1192],\n        [0.1339]], requires_grad=True)\nweight_v Parameter containing:\ntensor([[ 0.0999, -0.1095,  0.0053],\n        [ 0.4107, -0.1039, -0.5627],\n        [-0.0347, -0.1121,  0.0211],\n        [ 0.1116,  0.0381, -0.0633]], requires_grad=True)\n</code></pre> <p>\u89e3\u91ca\u8f93\u51fa\u7ed3\u679c\uff1a</p> <p>wn_linear\u53ea\u6709\u4e24\u4e2a\u8f93\u51fa\u7ed3\u679c\uff1a</p> <ul> <li>weight g\uff1alinear\u6743\u91cd\u7684\u5e45\u5ea6</li> <li>weight v\uff1aweight\u7684\u65b9\u5411\u5f52\u4e00\u5316</li> </ul> <pre><code>print(\"paramter of wn_linear:\")\nfor n,p in wn_linear.named_parameters():\n    print(n,p)\n\nprint(\"lienar.weight\")\nprint(linear.weight)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>paramter of wn_linear:\nweight_g Parameter containing:\ntensor([[0.1483],\n        [0.7043],\n        [0.1192],\n        [0.1339]], requires_grad=True)\nweight_v Parameter containing:\ntensor([[ 0.0999, -0.1095,  0.0053],\n        [ 0.4107, -0.1039, -0.5627],\n        [-0.0347, -0.1121,  0.0211],\n        [ 0.1116,  0.0381, -0.0633]], requires_grad=True)\nlienar.weight\ntensor([[ 0.0999, -0.1095,  0.0053],\n        [ 0.4107, -0.1039, -0.5627],\n        [-0.0347, -0.1121,  0.0211],\n        [ 0.1116,  0.0381, -0.0633]], grad_fn=&lt;WeightNormInterfaceBackward0&gt;)\n</code></pre> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff1a</p> <p>weight_v \u548c lienar.weight\u7684\u7ed3\u679c\u662f\u76f8\u7b49\u7684\uff0c\u5176\u5b9eweight_v \u5c31\u662flinear.weight\u4f46\u662f\u516c\u5f0f\u4e2d \u5bf9 weight_v\u8fdb\u884c\u4e86\u5f52\u4e00\u5316 \u5e76\u4e58\u4ee5 \u5e45\u5ea6\uff0c\u6240\u4ee5 linear.weight\u53ef\u4ee5\u62c6\u6210\u5e45\u5ea6\u5411\u91cf\u548c\u65b9\u5411\u5411\u91cf\uff0c\u5982\u679c\u4e0d\u5bf9weight_v \u8fdb\u884c\u53d8\u6362\uff0c\u90a3\u7b49\u5f0f\u4e24\u8fb9\u5c31\u4e0d\u4f1a\u76f8\u7b49\u7684</p> \\[ w = g \\frac{v}{||v||}\\] <p>v\u5c31\u662fw\uff0c\u5bf9v\u505a\u53d8\u6362\uff0c\u4f7f\u5f97\u7b49\u5f0f\u4e24\u8fb9\u76f8\u7b49</p> <pre><code>print(\"construct weight of linear:\")\nprint(wn_linear.weight_g*(wn_linear.weight_v)/torch.tensor([wn_linear.weight_v[i,:].norm() for i in torch.arange(wn_linear.weight_v.shape[-1])]))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>construct weight of linear:\ntensor([[ 0.0999, -0.0231,  0.0066],\n        [ 1.9504, -0.1039, -3.3245],\n        [-0.0279, -0.0190,  0.0211],\n        [ 0.1008,  0.0072, -0.0711]], grad_fn=&lt;DivBackward0&gt;)\n</code></pre> <p>\u8fd9\u4e2a\u7ed3\u679c\u548c weight_v \u4ee5\u53calinear.weight\u7684\u7ed3\u679c\u90fd\u662f\u76f8\u7b49\u7684</p>"},{"location":"learning/8_WeightNorm/#24-linear","title":"2.4 linear\u5c42\u6f14\u793a\u4ee3\u7801","text":"<p>\u6240\u6709linear\u5c42\u7684\u4ee3\u7801\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\n\n# \u5173\u4e8e\u6743\u91cd\u5f52\u4e00\u5316\u7684\u518d\u6b21\u8bf4\u660e\n# WeightNorm W = Magnitude * UnitDirection = Magnitude * (W/Norm(W))\n\n# step1:define constant\nbatch_size = 2\nfeat_dim = 3\nhid_dim = 4\ninputx = torch.randn(batch_size,feat_dim)\n\n# x\uff1a2\u00d73 3\u6620\u5c04\u52304\u7ef4 w^T 4\u00d73 w 3\u00d74 torch\u4e2d\u5b58\u7684\u5c31\u662f 4\u00d73\u7684 \u76f4\u63a5\u8fdb\u884c w\uff084 3\uff09\u53d8\u6210 2\u00d74\nlinear = nn.Linear(feat_dim,hid_dim,bias=False)\nwn_linear = torch.nn.utils.weight_norm(linear)\n</code></pre> <p><pre><code># step2:Linear Layer:calculate g and v\nweight_magnitude = torch.tensor([linear.weight[i:,].norm() for i in torch.arange(linear.weight.shape[0])],dtype =torch.float32).unsqueeze(-1)\nweight_direction = linear.weight / weight_magnitude\nprint(weight_direction)\n# print(\"inputx:\",inputx.shape) # inputx: torch.Size([2, 3])\n# print(\"linear.weight:\",linear.weight.shape) # linear.weight: torch.Size([4, 3])\n# print(\"linear(inputx)\",linear(inputx).shape)  # linear(inputx) torch.Size([2, 4])\n\n# print(\"weight_magnitude:\",weight_magnitude.shape)  # torch.Size([4, 1])\n# print(\"weight_direction:\",weight_direction.shape)  # weight_direction: torch.Size([4, 3])\nprint(\"magnitude of weight_direction:\",(weight_direction**2).sum(dim=-1))\n</code></pre> \u552f\u4e00\u6709\u7684\u4e00\u4e2a\u95ee\u9898\uff1amagnitude of weight_direction\u7ed3\u679c\u4e0d\u662f1</p> <pre><code>tensor([[ 0.1347, -0.1476,  0.0071],\n        [ 0.5651, -0.1429, -0.7742],\n        [-0.1938, -0.6252,  0.1177],\n        [ 0.8338,  0.2848, -0.4729]], grad_fn=&lt;DivBackward0&gt;)\nmagnitude of weight_direction: tensor([0.0400, 0.9392, 0.4423, 1.0000], grad_fn=&lt;SumBackward1&gt;)\n</code></pre> <pre><code>print(\"weight_direction * weight_magnitude:\")\nprint(weight_direction * weight_magnitude)\n\nprint(\"inputx @ (weight_direction * weight_magnitude).T:\")\nprint(inputx @ (weight_direction * weight_magnitude).T)\n\nprint(\"linear(inputx):\")\nprint(linear(inputx))\n\nprint(\"wn_linear(inputx):\")\nprint(wn_linear(inputx))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>weight_direction * weight_magnitude:\ntensor([[ 0.0999, -0.1095,  0.0053],\n        [ 0.4107, -0.1039, -0.5627],\n        [-0.0347, -0.1121,  0.0211],\n        [ 0.1116,  0.0381, -0.0633]], grad_fn=&lt;MulBackward0&gt;)\ninputx @ (weight_direction * weight_magnitude).T:\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\nlinear(inputx):\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\nwn_linear(inputx):\ntensor([[-0.2544, -0.1274, -0.3263,  0.1558],\n        [-0.0517,  1.6473, -0.2235,  0.3099]], grad_fn=&lt;MmBackward0&gt;)\n</code></pre> <pre><code>print(\"paramter of wn_linear:\")\nfor n,p in wn_linear.named_parameters():\n    print(n,p)\n\nprint(\"lienar.weight\")\nprint(linear.weight)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>paramter of wn_linear:\nweight_g Parameter containing:\ntensor([[0.1483],\n        [0.7043],\n        [0.1192],\n        [0.1339]], requires_grad=True)\nweight_v Parameter containing:\ntensor([[ 0.0999, -0.1095,  0.0053],\n        [ 0.4107, -0.1039, -0.5627],\n        [-0.0347, -0.1121,  0.0211],\n        [ 0.1116,  0.0381, -0.0633]], requires_grad=True)\nlienar.weight\ntensor([[ 0.0999, -0.1095,  0.0053],\n        [ 0.4107, -0.1039, -0.5627],\n        [-0.0347, -0.1121,  0.0211],\n        [ 0.1116,  0.0381, -0.0633]], grad_fn=&lt;WeightNormInterfaceBackward0&gt;)\n</code></pre> <pre><code>print(\"construct weight of linear:\")\nprint(wn_linear.weight_g*(wn_linear.weight_v)/torch.tensor([wn_linear.weight_v[i,:].norm() for i in torch.arange(wn_linear.weight_v.shape[-1])]))\n</code></pre> <pre><code>construct weight of linear:\ntensor([[ 0.0999, -0.0231,  0.0066],\n        [ 1.9504, -0.1039, -3.3245],\n        [-0.0279, -0.0190,  0.0211],\n        [ 0.1008,  0.0072, -0.0711]], grad_fn=&lt;DivBackward0&gt;)\n</code></pre>"},{"location":"learning/8_WeightNorm/#25-conv","title":"2.5 conv\u5c42\u7684\u4ee3\u7801","text":"<p>(\u5168\u90e8\u4ee3\u7801)</p> <pre><code>conv1d = nn.Conv1d(feat_dim,hid_dim,kernel_size=1,bias=False) # input:[B,C,T],weight:[oc,ic,1]\nwn_conv1d = torch.nn.utils.weight_norm(conv1d)\n</code></pre> <pre><code>covn1d_weight_magnitude = torch.tensor([conv1d.weight[i,:,:].norm() \n                                        for i in torch.arange(conv1d.weight.shape[0])],dtype=torch.float32).reshape(\n                                            conv1d.weight.shape[0],1,1\n                                        ).tile(1,feat_dim,1)\n\ncovn1d_weight_direction = conv1d.weight / covn1d_weight_magnitude\n\nprint(\"parameters of wn_conv1d:\")\nfor n,p in wn_conv1d.named_parameters():\n    print(n,p,p.shape)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>parameters of wn_conv1d:\nweight_g Parameter containing:\ntensor([[[0.4729]],\n\n        [[0.7834]],\n\n        [[0.5456]],\n\n        [[0.5718]]], requires_grad=True) torch.Size([4, 1, 1])\nweight_v Parameter containing:\ntensor([[[ 0.0987],\n         [-0.1757],\n         [ 0.4279]],\n\n        [[ 0.4962],\n         [-0.4026],\n         [ 0.4533]],\n\n        [[ 0.4717],\n         [-0.1782],\n         [-0.2082]],\n\n\n        [[-0.5708],\n         [ 0.0271],\n         [-0.0208]]], requires_grad=True) torch.Size([4, 3, 1])         \n</code></pre> <pre><code>print(\"construct weight of conv1d:\")\nprint(wn_conv1d.weight_g * (wn_conv1d.weight_v / torch.tensor(\n    [wn_conv1d.weight_v[i,:,:].norm() \n     for i in torch.arange(wn_conv1d.weight_v.shape[0])],\n                            dtype=torch.float32).unsqueeze(-1).unsqueeze(-1)))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>construct weight of conv1d:\ntensor([[[ 0.0987],\n         [-0.1757],\n         [ 0.4279]],\n\n        [[ 0.4962],\n         [-0.4026],\n         [ 0.4533]],\n\n        [[ 0.4717],\n         [-0.1782],\n         [-0.2082]],\n\n        [[-0.5708],\n         [ 0.0271],\n         [-0.0208]]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <pre><code>print(\"conv1d.weight:\")\nprint(conv1d.weight)\n\nprint(\"covn1d_weight_magnitude:\")\nprint(covn1d_weight_magnitude)\n\nprint(\"covn1d_weight_direction:\")\nprint(covn1d_weight_direction)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>conv1d.weight:\ntensor([[[ 0.0987],\n         [-0.1757],\n         [ 0.4279]],\n\n        [[ 0.4962],\n         [-0.4026],\n         [ 0.4533]],\n\n        [[ 0.4717],\n         [-0.1782],\n         [-0.2082]],\n\n        [[-0.5708],\n         [ 0.0271],\n         [-0.0208]]], grad_fn=&lt;WeightNormInterfaceBackward0&gt;)\ncovn1d_weight_magnitude:\ntensor([[[0.4729],\n         [0.4729],\n         [0.4729]],\n\n        [[0.7834],\n         [0.7834],\n         [0.7834]],\n\n        [[0.5456],\n         [0.5456],\n         [0.5456]],\n        [[0.5718],\n         [0.5718],\n         [0.5718]]])\ncovn1d_weight_direction:\ntensor([[[ 0.2086],\n         [-0.3715],\n         [ 0.9047]],\n\n        [[ 0.6334],\n         [-0.5139],\n         [ 0.5785]],\n\n        [[ 0.8647],\n         [-0.3267],\n         [-0.3816]],\n\n        [[-0.9982],\n         [ 0.0475],\n         [-0.0364]]], grad_fn=&lt;DivBackward0&gt;)         \n</code></pre> <p>\u89e3\u8bfb\uff1a</p> <p>\uff081\uff09\u5b9e\u4f8b\u5316\u4e00\u4e2a 1\u00d71 \u7684\u5377\u79ef\u5c42</p> <pre><code>conv1d = nn.Conv1d(feat_dim,hid_dim,kernel_size=1,bias=False)\n# input:[B,C,T],weight:[oc,ic,1]\nwn_conv1d = torch.nn.utils.weight_norm(conv1d)\n</code></pre> <ul> <li>1\u00d71 \u7684\u5377\u79ef\u5c42 \u7c7b\u4f3c MLP</li> <li>\u8f93\u5165\u901a\u9053\u6570 \u8bbe\u7f6e\u4e3a feat_dim\uff0c\u8f93\u51fa\u901a\u9053\u6570\u662f  hid_dim\uff0ckernel_size\u8bbe\u4e3a1\uff0c\u4e0d\u8981bias</li> <li>1\u00d71\u7684\u5377\u79ef\u5c42\uff0c\u8f93\u5165\u7684\u6570\u636e\u683c\u5f0f\uff1abatchsize \u00d7 channel\u00d7length</li> <li>\u6743\u91cd\u7684\u7ef4\u5ea6\u662f output channel\u00d7input channel \u00d7kernel size\uff0c\u8fd9\u91cckernel size=1</li> <li>\u628a\u4e00\u7ef4\u5377\u79ef conv1d \u4f5c\u4e3a module\uff0c\u9001\u5165\u5230weight Norm\u51fd\u6570\u4e2d\uff0c\u5f97\u5230WeightNorm\u5305\u88f9\u540e\u7684convolution\uff1awn_conv1d\uff08\u5373\u52a0\u4e86\u6743\u91cd\u5f52\u4e00\u5316\u7684\u6a21\u5757\uff09</li> </ul> <p>\uff082\uff09\u8ba1\u7b97 covn1d_weight_magnitude</p> <pre><code>covn1d_weight_magnitude = torch.tensor([conv1d.weight[i,:,:].norm() \n                                        for i in torch.arange(conv1d.weight.shape[0])],dtype=torch.float32).reshape(\n                                            conv1d.weight.shape[0],1,1\n                                        ).tile(1,feat_dim,1)\n\ncovn1d_weight_direction = conv1d.weight / covn1d_weight_magnitude\n\nprint(\"parameters of wn_conv1d:\")\nfor n,p in wn_conv1d.named_parameters():\n    print(n,p,p.shape)\n</code></pre> <ul> <li>conv1d.weight\u62ff\u51fa\u5377\u79ef\u5c42\u7684\u6743\u91cd\uff0c\u8ba1\u7b97\u6743\u91cd\u77e9\u9635\u7684\u5e45\u5ea6\uff0c\u6743\u91cd\u662f\u4e00\u4e2a\u4e09\u7ef4\u77e9\u9635\uff0c\u683c\u5f0f\u662fweight:[oc,ic,1]\uff0c\u5bf9\u6bcf\u4e00\u884c\u8ba1\u7b97\u6a21\uff0c\u6bcf\u4e00\u884c\u662f\u4e00\u4e2a\u4e8c\u9636\u5f20\u91cf\uff0c\u6bcf\u4e00\u4e2a\u4e8c\u9636\u5f20\u91cf \u8ddf input \u8fdb\u884c\u4e58\u6cd5\u64cd\u4f5c\uff0c\u6240\u4ee5\u6211\u4eec\u5bf9\u6bcf\u4e00\u884c \u8ba1\u7b97\u8303\u6570\uff0c\u7136\u540e\u62fc\u8d77\u6765\uff0c\u7136\u540e\u8fdb\u884c\u6269\u7ef4\uff0c\u5f97\u5230\u5e45\u5ea6 covn1d_weight_magnitude</li> <li>\u5355\u4f4d\u5411\u91cf \u540c\u6837\u662f weight \u9664\u4ee5 \u5e45\u5ea6\uff0c\u5f97\u5230\u5355\u4f4d\u5411\u91cf\uff0c\u4e5f\u5c31\u662f\u65b9\u5411\u5411\u91cf</li> </ul> <p>\uff083\uff09\u6253\u5370\u5e45\u5ea6\u3001\u5355\u4f4d\u5411\u91cf\u3001\u539f\u6765\u7684\u6743\u91cd\uff0c\u4ee5\u53cawn_conv1d\u4e2d\u7684\u4e24\u4e2a\u53c2\u6570\uff1a</p> <pre><code>print(\"parameters of wn_conv1d:\")\nfor n,p in wn_conv1d.named_parameters():\n    print(n,p,p.shape)\n\n\nprint(\"construct weight of conv1d:\")\nprint(wn_conv1d.weight_g * (wn_conv1d.weight_v / torch.tensor(\n    [wn_conv1d.weight_v[i,:,:].norm() \n     for i in torch.arange(wn_conv1d.weight_v.shape[0])],\n                            dtype=torch.float32).unsqueeze(-1).unsqueeze(-1)))\n\nprint(\"conv1d.weight:\")\nprint(conv1d.weight)\n\nprint(\"covn1d_weight_magnitude:\")\nprint(covn1d_weight_magnitude)\n\nprint(\"covn1d_weight_direction:\")\nprint(covn1d_weight_direction)\n</code></pre> <p>\u2460 wn_conv1d\u4e2d\u7684\u4e24\u4e2a\u53c2\u6570\uff1a\u6709weight_g  \u548c weight_v</p> <p>weight_g  \u5377\u79ef\u7684\u6743\u91cd\u7684\u5e45\u5ea6</p> <p>weight_v  \u5c31\u662f\u5377\u79ef\u7684\u6743\u91cd</p> <p>\u533a\u5206\uff1a</p> <p>weight_v \u5c31\u662f\u6743\u91cd</p> <p>\u9664\u4ee5v\u7684\u8303\u6570 \u624d\u662f\u65b9\u5411</p> <p>\u9664\u4ee5v\u7684\u8303\u6570 \u518d\u4e58\u4ee5 \u5e45\u5ea6 \u8fd8\u662f\u6743\u91cd \u8fd8\u662f weight_v</p> <p>\u2461 </p> <p>\u628a \u5e45\u5ea6 \u4e0e\u5355\u4f4d\u5411\u91cf \u76f8\u4e58</p> <p>\u5e45\u5ea6\uff1awn_conv1d.weight_g</p> <p>\u5355\u4f4d\u5411\u91cf\uff1a</p> <pre><code>wn_conv1d.weight_v / torch.tensor(\n    [wn_conv1d.weight_v[i,:,:].norm() \n     for i in torch.arange(wn_conv1d.weight_v.shape[0])],\n                            dtype=torch.float32)\n</code></pre> <p>\u672c\u8d28\u662f weight\u5411\u91cf \u9664\u4ee5 weight \u5411\u91cf\u7684\u6a21\uff0c\u5f97\u5230\u5355\u4f4d\u65b9\u5411\u5411\u91cf\uff0c\u6216\u8005\u53eb \u5355\u4f4d\u957f\u5ea6\u7684\u65b9\u5411\u5411\u91cf</p> <p>\u5e45\u5ea6 \u00d7 \u5355\u4f4d\u957f\u5ea6\u7684\u65b9\u5411\u5411\u91cf \u5f97\u5230  <code>construct weight of conv1d</code></p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff1a<code>construct weight of conv1d</code> \u4e0e <code>weight_v</code> \u4e0e <code>conv1d.weight</code> \u662f\u4e00\u6837\u7684\uff0c\u5c31\u662f\u5377\u79ef\u7684\u6743\u91cd</p> <p>\u2462 \u5bf9\u6bd4  <code>weight_v</code> <code>construct weight of conv1d</code> <code>conv1d.weight</code>  \u90fd\u662f\u4e00\u6837\u7684</p> <p>\u5728\u4e00\u7ef4\u5377\u79ef\u4e2d \u4e5f\u662f\u4e00\u6837\u7684\uff0c\u52a0\u4e86\u6743\u91cd\u5f52\u4e00\u5316\uff0c\u5c31\u662f\u628a\u5e45\u5ea6\u4e0e \u6743\u91cd\u7684\u5355\u4f4d\u957f\u5ea6\u7684\u65b9\u5411\u5411\u91cf \u89e3\u8026\u5f00\u6765\uff0c\u6253\u5370\u6743\u91cd\u7684\u5e45\u5ea6 \u548c \u6743\u91cd\u7684\u5355\u4f4d\u957f\u5ea6\u7684\u65b9\u5411\u5411\u91cf</p> <pre><code>print(\"covn1d_weight_magnitude:\")\nprint(covn1d_weight_magnitude)\n\nprint(\"covn1d_weight_direction:\")\nprint(covn1d_weight_direction)\n</code></pre> <p>\u4ee5\u4e0a\u7684WeightNorm\u7684\u539f\u7406\uff0c\u6700\u91cd\u8981\u7684\u5c31\u662f \u516c\u5f0f\uff1a</p> <p></p> <p>\uff081\uff09\u5982\u679c\u4e0d\u52a0 WeightNorm\uff0c\u53ea\u9700\u8981\u4f18\u5316\u4e00\u4e2a\u53c2\u6570\uff0c\u52a0\u4e86WeightNorm\u4e4b\u540e\uff0c\u9700\u8981\u4f18\u5316\u4e24\u4e2a\u53c2\u6570\uff0c\u628aloss\u540c\u65f6\u5bf9g\u6c42\u68af\u5ea6\uff0c\u5bf9v\u6c42\u4e00\u4e2a\u68af\u5ea6</p> <p>\uff082\uff09\u5e76\u4e14WeightNorm\u5e76\u6ca1\u6709\u5e26\u6765\u989d\u5916\u7684\u53c2\u6570\uff0c\u5e76\u6ca1\u6709\u5e26\u6765\u5b9e\u8d28\u610f\u4e49\u4e0a\u7684\u989d\u5916\u53c2\u6570\uff0c\u5bf9\u6bd4BatchNorm\u4f1a\u9700\u8981\u8ba1\u7b97\u989d\u5916\u7684\u7edf\u8ba1\u91cf\uff0c\u800cWeightNorm\u5e76\u6ca1\u6709\u589e\u52a0\u989d\u5916\u7684\u53c2\u6570</p> <p>\uff083\uff09\u7ecf\u8fc7\u4ee5\u4e0a\u7684\u5b9e\u9a8c\uff0c\u53ef\u4ee5\u770b\u5230\u505a\u4e86WeightNorm\u4e4b\u540e\uff0c\u8f93\u51fa\u503c\u662f\u6ca1\u6709\u53d8\u5316\u7684\uff0c\u6bd4\u5982\uff1a</p> <p></p> <p>\u5c55\u793a\u4e86\u4e09\u79cd\u8ba1\u7b97\uff0c</p> <p>\u2460 inputx\u8ddf\u65b9\u5411\u5411\u91cf\u76f8\u4e58</p> <p>\u2461inputx\u653e\u5165linear\u5c42</p> <p>\u2462inputx\u653e\u5165WeightNorm linear\u5c42</p> <p>\u8f93\u51fa\u503c\u90fd\u662f\u4e00\u6837\u7684\uff0c\u6743\u91cd\u5f52\u4e00\u5316\u4e0d\u4f1a\u6539\u53d8\u6a21\u5757\u7684\u8f93\u51fa\u503c\uff0c\u53ea\u662f\u6539\u53d8\u4e86\u53c2\u6570\u5185\u90e8\u7684\u4e58\u6cd5\u64cd\u4f5c\u7684\u6a21\u5f0f\uff0c\u76f8\u5f53\u4e8e\u77e9\u9635\u5206\u89e3\u7684\u8fc7\u7a0b</p> <p>\u6240\u4ee5\u6743\u91cd\u5f52\u4e00\u5316 \u4e5f\u53ef\u4ee5 \u7406\u89e3\u4e3a\u6743\u91cd\u5206\u89e3</p> <p>\u628a\u6743\u91cd\u7684\u5e45\u5ea6 \u8ddf \u5355\u4f4d\u957f\u5ea6\u7684 \u65b9\u5411\u5411\u91cf \u89e3\u8026\u5f00\u6765\uff0c\u5728pytorch\u4e2d\u901a\u8fc7torch.nn.utils.weight_norm\u7684\u51fd\u6570\uff0c\u628a\u4e00\u4e2amodule\u5305\u88f9\u8d77\u6765\uff0c\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684module\uff0c\u65b0\u7684module\u53c2\u6570\u662f\u4e24\u4e2a\uff0c\u4e00\u4e2a\u662fweight_g \u4e00\u4e2a\u662f weight_v</p> <p>weight_g \u662f\u539f\u6765\u6743\u91cd\u7684\u5e45\u5ea6\uff0cweight_v \u5c31\u662f\u539f\u6765\u7684\u6743\u91cd</p> <p>\u95ee\u9898\uff1a\u4e3a\u4ec0\u4e48\u8981\u9664\u4ee5 v\u7684\u6a21\uff1f</p> <p>\u7b54\uff1a\u56e0\u4e3a\u9664\u4ee5 v\u7684\u6a21\uff0c\u5f97\u5230\u5355\u4f4d\u957f\u5ea6\u7684\u65b9\u5411\u5411\u91cf\uff0c\u518d\u8ddf\u5e45\u5ea6\u76f8\u4e58\uff0c\u5f97\u5230\u539f\u6765\u7684\u6743\u91cd\uff0c\u5982\u679c\u4e0d\u9664\u4ee5\u6a21\u7684\u8bdd\uff0c\u6240\u6709\u4e24\u8fb9\u662f\u4e0d\u4f1a\u76f8\u7b49\u7684</p> <p></p>"},{"location":"learning/9_cGAN/","title":"GAN \u53d8\u4f53","text":"<p>cGAN\u53caLSGAN\u7684\u539f\u7406\u4e0ePyTorch\u624b\u5199\u9010\u884c\u8bb2\u89e3</p> <p></p> <p></p> <ul> <li>\u6761\u4ef6GAN</li> <li>\u6700\u5c0f\u5e73\u65b9GAN or  \u6700\u5c0f\u4e8c\u4e58GAN</li> </ul> <p>topic\uff1a\u539f\u7406 &amp; \u4ee3\u7801\u5b9e\u73b0</p> <p>minist\u6570\u636e\u96c6\uff1a6w\u5f20\u624b\u5199\u6570\u5b57\u56fe\u7247</p> <p>GAN\u57fa\u4e8eminist\u6570\u636e\u96c6\u8fdb\u884c\u65e0\u76d1\u7763\u7684\u56fe\u7247\u751f\u6210\u4efb\u52a1</p>"},{"location":"learning/9_cGAN/#1-recall","title":"1 Recall","text":"<p>GAN\u7684\u4ee3\u7801\u5b9e\u73b0\u903b\u8f91\uff0c\u9996\u5148\u6784\u5efaGenerator\uff0cGenerator\u4ee5\u4e00\u4e2a\u9690\u53d8\u91cf\uff0c\u4ece\u9ad8\u65af\u5206\u5e03\u751f\u6210\u7684\u968f\u673a\u9690\u53d8\u91cfz\uff0c\u4f5c\u4e3a\u8f93\u5165\uff0c\u7136\u540e\u628az\u653e\u5165\u5230\u5f88\u591a\u5c42DNN\u4e2d\uff0cDNN\u6700\u540e\u751f\u6210\u56fe\u7247\u5927\u5c0f\u7684\u751f\u6210\u56fe\u7247\uff0c\u7136\u540e\u901a\u8fc7\u6fc0\u6d3b\u51fd\u6570\u7ea6\u675f\u5230\u4e00\u5b9a\u7684\u503c\u57df\u5185\uff0c\u901a\u8fc7nn.Sigmoid() \u6216\u8005 nn.tanh()\u90fd\u53ef\u4ee5\uff1a</p> <p></p> <p></p> <p>\u7ecf\u8fc7 \u751f\u6210\u5668\uff0c\u751f\u6210\u4e00\u5f20\u56fe\u7247</p> <p>\u57fa\u4e8e \u968f\u673a\u9ad8\u65af\u53d8\u91cfz\uff0c\u751f\u6210\u4e00\u5f20\u7167\u7247\uff0cz\u7684\u7ef4\u5ea6\u53ef\u4ee5\u8bbe\u7f6e\u4e00\u4e2alatent_dim\uff0c\u6bd4\u598296\uff0c\u5e76\u4e14\u4ee4batchsize=64\uff0c\u6b64\u65f6\u6bcf\u4e00\u6b21\u8bad\u7ec3\u7684\u5927\u5c0f \u5c31\u662f 64\u00d796\u7684\u4e8c\u7ef4\u5f20\u91cf</p> <p>\u4ee5\u4e0a\u662f\u751f\u6210\u5668\uff0c\u63a5\u4e0b\u6765\u770b\u5224\u522b\u5668\uff1a</p> <p></p> <p>\u5224\u522b\u5668\u7684\u4f5c\u7528\uff1a</p> <p>\uff081\uff09\u51c6\u786e\u7684\u533a\u5206\u51fa \u4ec0\u4e48\u662f\u771f\u5b9e\u6837\u672c \u4ec0\u4e48\u662f \u9884\u6d4b\u6837\u672c</p> <p>\uff082\uff09\u7ed9\u51fa\u4fe1\u53f7\uff0c\u4f7f\u5f97\u751f\u6210\u5668 \u66f4\u597d\u7684\u751f\u6210 \u66f4\u52a0\u903c\u8fd1\u771f\u5b9e\u7684\u6837\u672c</p> <p>\u5224\u522b\u5668\u4ee5 \u56fe\u7247\u4f5c\u4e3a\u8f93\u5165\uff0c\u63a5\u7740\u7531 \u4e00\u7cfb\u5217\u7684  nn\u5c42\uff0c\u8c31\u5f52\u4e00\u5316 \u662f\u540e\u9762\u52a0\u7684\uff0c\u6700\u540e\u8f93\u51fa\u4e00\u4e2a\u6807\u91cf\u503c\uff0c\u6807\u91cf\u503c \u901a\u8fc7nn.Sigmoid() \u8f93\u51fa\u7684\uff0c\u539f\u59cb\u7684GAN\u4f7f\u7528\u7684\u662f \u4e8c\u5206\u7c7b \u4ea4\u53c9\u71b5 \u7684\u635f\u5931\u51fd\u6570\uff0c\u6240\u4ee5\u4f7f\u7528\u7684\u662fnn.Sigmoid</p> <p></p> <p>\u6784\u5efa dataset\uff0cdataloader</p> <p>dataset\u4f7f\u7528\u7684torchvision\u7684\u5e93\uff0c\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u4e0b\u8f7d</p> <p>\u89e3\u91ca torchvision.transforms.Compose\u4e2d   torchvision.transformsNormalize \u4e3a\u4ec0\u4e48\u4f7f\u7528\u7684 \u5747\u503c=0.5\uff0c\u65b9\u5dee=0.5\uff0c\u5982\u679c\u6211\u4eec\u53bb\u8ba1\u7b97minist\u6570\u636e\u96c6\u5747\u503c\u548c\u65b9\u5dee \u7684\u8bdd\uff0c\u5747\u503c \u5927\u6982\u7b49\u4e8e 0.1\uff0c\u6807\u51c6\u5dee\u7ea6\u7b49\u4e8e0.3\uff0c\u800c\u8fd9\u91cc\u4f7f\u7528\u7684\u5747\u503c\u548c\u65b9\u5dee\u4e3a0.5\uff0c\u662f\u56e0\u4e3a\u672c\u6765\u662f\u4f7f\u7528ToTensor\u7684\u8bed\u53e5\uff0c\u5df2\u7ecf\u628a\u56fe\u7247\u7684\u503c\u57df\u7ea6\u675f\u5230\u4e860~1\u4e4b\u95f4\uff0c0~1\u4e4b\u95f4 \u51cf\u53bb 0.5\uff0c\u53d8\u6210-0.5\u52300.5\u4e4b\u95f4\uff0c-0.5\u52300.5\u518d\u9664\u4ee50.5\uff0c\u53d8\u6210-1\u52301\u4e4b\u95f4\uff0c\u6240\u4ee5\u8fd9\u884c\u7684\u8bed\u53e5\u5e76\u4e0d\u662f\u5f52\u4e00\u5230\u6b63\u6001\u5206\u5e03\uff0c\u800c\u662f\u628a\u503c\u57df\u4ece0~1\u53d8\u5316\u5230-1~1\u4e4b\u95f4\uff0c\u8fd9\u662f\u5199\u4ee3\u7801\u7684\u5c0f\u6280\u5de7\uff1a\u5728transforms\u4e2d\u7684\u7ec4\u5408\u91cc\u9762\uff0c\u600e\u4e48\u628a\u4e0a\u4e00\u6b65\u76840~1\u7684\u6d6e\u70b9\u6570\u600e\u4e48\u53d8\u6210-1~1\u4e4b\u95f4\u7684\u503c\u57df\uff0c\u53ef\u4ee5\u901a\u8fc7\u5747\u503c\u548c\u6807\u51c6\u5dee\u5f52\u4e00\u5316\u5b9e\u73b0\uff0c\u6b64\u65f6\u6211\u4eec\u8bbe\u7f6e\u5747\u503c=0.5\uff0c\u6807\u51c6\u5dee=0.5\uff0c\u4ece\u539f\u6765\u76840~1\u8303\u56f4\u5185\uff0c\u53d8\u6210-1~1\u8303\u56f4\u5185\uff0c\u53d8\u6362\u5230-1~1\u8303\u56f4\u5185\u3002\u53d8\u6210-1~1\u8303\u56f4\u5185\uff0c\u5c31\u53ef\u4ee5\u5728Generator\u4e2d\uff0c\u4f7f\u7528tanh\u51fd\u6570\u9884\u6d4b\u6700\u7ec8\u7684\u50cf\u7d20\u503c</p> <p>\u6700\u7ec8tanh\u51fd\u6570\u867d\u7136\u8f93\u51fa\u7684\u662f-1~1\uff0c\u4f46\u662f\u6211\u4eec\u5728\u4fdd\u5b58\u7167\u7247\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u4e00\u4e2aNormalize=True\uff0c\u5c31\u53ef\u4ee5\u4f7f\u5f97\u56fe\u7247\u4ece-1~1\uff0c\u518d\u6b21\u53d8\u62100 ~1\u4e4b\u95f4</p> <p>\u5728\u8bad\u7ec3\u65f6\uff0c</p> <p></p> <p>\u8ba1\u7b97g\u7684\u65f6\u5019\uff0c\u628a\u9884\u6d4b\u7684\u7167\u7247 \u9001\u5165\u5230\u5224\u522b\u5668\u4e2d\uff0c\u628a\u51681\u7684\u6807\u7b7e\uff0c\u4f5c\u4e3a\u5f53\u524d\u7684\u6807\u7b7e\uff0c\u6765\u5f97\u5230loss\u503c\uff0c\u6765\u66f4\u65b0Generator</p> <p>\u5bf9\u4e8e\u5224\u522b\u5668\u800c\u8a00\uff0c\u6709\u4e24\u4e2aloss\uff0c</p> <p></p> <p><code>real_loss</code> \u548c <code>fake_loss</code> </p> <p><code>real_loss</code> \u628a\u771f\u5b9e\u7684\u7167\u7247\u9001\u5165\u5230\u5224\u522b\u5668\u4e2d\uff0c\u6807\u7b7e\u662f\u51681 \u7684</p> <p><code>fake_loss</code>  \u628a\u9884\u6d4b\u7684\u7167\u7247 \u9001\u5165\u5230\u5224\u522b\u5668\u4e2d\uff0c\u6807\u7b7e\u662f\u51680 \u7684</p> <p>\u6211\u4eec\u5e0c\u671b\u5224\u522b\u5668\u662f\u80fd\u591f\u533a\u5206\u771f\u5b9e\u7167\u7247\u548c\u9884\u6d4b\u7167\u7247\u7684</p> <p></p> <p>\u4e4b\u540e\uff0c\u4f9d\u6b21\u66f4\u65b0 \u751f\u6210\u5668\u548c\u5224\u522b\u5668\u5373\u53ef</p> <p>\u4ee5\u4e0a\u662f\u539f\u59cbGAN\uff0c\u901a\u8fc7\u4e8c\u5206\u7c7b\u7684\u3001\u4ea4\u53c9\u71b5loss\u6765\u4f5c\u4e3a\u5224\u522b\u5668\u7684loss function</p>"},{"location":"learning/9_cGAN/#2-gan","title":"2 \u6761\u4ef6GAN","text":"<ul> <li>\u6761\u4ef6GAN</li> <li>\u5e94\u7528\u5f88\u5e7f\u6cdb</li> <li>\u5f15\u7528\u6b21\u6570 4k \u591a\u6b21</li> </ul>"},{"location":"learning/9_cGAN/#21-cgan","title":"2.1 cGAN\u7684\u521b\u65b0\u70b9","text":"<ul> <li>\u4e3a\u4ec0\u4e48\u4f1a\u6709 \u6761\u4ef6GAN\uff1f</li> </ul> <p>\u9996\u5148\u8ba8\u8bba\u539f\u59cbGAN\u7684\u751f\u6210\u6709\u4ec0\u4e48\u95ee\u9898\uff1f</p> <p></p> <p>\u539f\u59cbGAN\u7684\u56fe\u7247\u751f\u6210\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u770b\u539f\u6587\u7684\u7b97\u6cd51</p> <p>\uff081\uff09\u770b\u5224\u522b\u5668\u7684\u8f93\u5165\uff0c\u65e0\u8bba\u662f\u771f\u5b9e\u7684\u6837\u672c\uff0c\u8fd8\u662f\u9884\u6d4b\u7684\u6837\u672c\uff0c\u8f93\u5165\u90fd\u53ea\u6709\u4e00\u4e2a\uff0c\\(x^{(i)}\\) \u6216\u8005 \\(G(z{(i)})\\)\uff0c\u53ea\u662f\u628a\u7167\u7247\u9001\u5165\u5230\u5224\u522b\u5668\u4e4b\u4e2d</p> <p></p> <p>\uff0c\u4f46\u662f\u5728minist\u6570\u636e\u5e93\u4e2d\uff0c\u7167\u7247\u670910\u4e2a\u7c7b\u522b\uff0c0~9\uff0c10\u4e2a\u624b\u5199\u5b57\u8bc6\u522b\uff0c10\u7c7b\u7684\u65f6\u5019\uff0c\u4ec5\u4ec5\u8f93\u5165\u4e00\u4e2a\u968f\u673a\u7684\u9ad8\u65af\u53d8\u91cfz\u7684\u8bdd\uff0c\u6ca1\u6709\u8f93\u5165\u4efb\u4f55\u5176\u4ed6\u7684\u4fe1\u606f\uff0c\u5e76\u4e14\u5e0c\u671b\u751f\u6210\u5668\u80fd\u591f\u751f\u6210\u5f53\u524d\u6837\u672c\uff0c\u5f53\u524d\u4eceminibatch\u4e2d\uff0c\u53d6\u5f97\u7684\u662f0\uff0c\u5e0c\u671b\u5728z\u7684\u6307\u5bfc\u4e0b\uff0c\u751f\u62100\u7684\u7167\u7247\uff0c\u5982\u679c\u5f53\u524d\u771f\u5b9e\u7167\u7247\u62ff\u5230\u7684\u662f1\uff0c\u6211\u4eec\u6307\u671b\u968f\u673a\u53d8\u91cfz\uff0c\u751f\u6210\u4e3a1\u7684\u7167\u7247\uff0c\u8fd9\u6837\u4e5f\u662f\u53ef\u4ee5\u7684\uff0c\u4f46\u662f\u6709\u70b9\u96be\uff0c\u5c31\u662f\u7ed9\u7684\u4fe1\u606f\u91cf\u592a\u5c11\u4e86\uff0cz\u5c31\u662f\u4e00\u4e2a\u968f\u673a\u7684\u9ad8\u65af\u53d8\u91cf\uff0c\u4e0d\u786e\u5b9a\u6027\u5f88\u5927\uff0c\u56e0\u6b64\u6709\u52a9\u4e8e\u6211\u4eec\u9884\u6d4b\u76ee\u6807\u7167\u7247\u7684\u4fe1\u606f\u5c31\u5f88\u5c11\uff0c\u8fd9\u65f6\uff0c\u601d\u8003\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u63d0\u4f9b\u4ec0\u4e48\u91cf\u5462\uff1f\u6211\u4eec\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2ac\uff0c\u4e00\u4e2acondition\uff0c\u4e5f\u5c31\u662f\u8bf4\u5f53\u6211\u4eec\u7684G\u7684\u8f93\u5165\uff0c\u63a5\u6536\u7684\u8f93\u5165\u4e0d\u4ec5\u4ec5\u662fz\uff0c\u800c\u662f\u4ee5\u968f\u673a\u9ad8\u65af\u53d8\u91cfz\u548c\u6761\u4ef6c\u4e00\u8d77\u4f5c\u4e3a\u8f93\u5165\u7684\u65f6\u5019\uff0cc\u5c31\u662fcondition\u6761\u4ef6\uff0c\u53ef\u4ee5\u662f\u6807\u7b7e\uff0c\u6bd4\u5982\u6211\u4eec\u5f53\u524d\u9884\u6d4b\u624b\u5199\u5b571\u7684\u7167\u7247\uff0c\u5c31\u53ef\u4ee5\u5c061 \u7684 class\u4fe1\u606f\u4f20\u5165\u5230G\u4e4b\u4e2d\uff0c\u8fd9\u65f6\u5019G\u7684\u8f93\u5165\u4e0d\u4ec5\u662fz\u8fd8\u6709\u7c7b\u522b\u6807\u7b7e1\uff0c\u5f53\u505a\u6761\u4ef6c\uff0c\u8fd9\u65f6\u751f\u6210\u5668\u80fd\u66f4\u597d\u7684\u77e5\u9053 \u8981\u751f\u6210\u7684 \u56fe\u7247\u662f 1\uff0c\u4e0d\u662f2\u4e5f\u4e0d\u662f3\uff0c\u53ef\u4ee5\u4f7f\u5f97\u751f\u6210\u5668\u6709\u76ee\u6807\u7684\u751f\u6210\uff0c\u4ee5\u4e0a\u5c31\u662fcGAN\u7684\u70b9</p> <p>\u539f\u59cbGAN\u516c\u5f0f\uff1a</p> <p></p> <p>\u4ee5 \\(x\\) \u6216\u8005 \\(G(z)\\) \u4f5c\u4e3a\u8f93\u5165\uff0c\u4e5f\u5c31\u662f\u539f\u59cbGAN\u4e2d\u4ee5\u7167\u7247\u4f5c\u4e3a\u8f93\u5165</p> <p>\u73b0\u5728\u5f15\u5165\\(y\\)</p> <p></p> <p>\\(y\\)\u8868\u793a \u6761\u4ef6\u4fe1\u606f</p> <p>\u6bd4\u5982\u5728MNIST\u6570\u636e\u96c6\u4e2d\uff0c\\(y\\)\u53ef\u4ee5\u8868\u793a\u6bcf\u5f20\u7167\u7247\u7684\u6807\u7b7e\u4fe1\u606f\uff0c\u6bd4\u5982\u5f53\u524d\u624b\u5199\u6570\u5b57\u7167\u7247\u662f1\u7684\u8bdd\uff0c\u90a3\u8fd9\u4e2a\u6807\u7b7e\u5c31\u7b97\u662f1\uff0c\u5982\u679c\u5f53\u524d\u751f\u6210\u624b\u5199\u6570\u5b57\u7167\u7247\u662f2\u7684\u8bdd\uff0c\u90a3\u4e48\u8fd9\u4e2a\u6807\u7b7e\u5c31\u662f2\uff0c\u4e5f\u5c31\u662f\u628a\\(y\\)\u7684\u4fe1\u606f\uff0c\u4e5f\u4f5c\u4e3a\u751f\u6210\u5668\u7684\u8f93\u5165\uff0c\u6b64\u65f6\u53ef\u4ee5\u66f4\u597d\u5730\u5b66\u4e60\u76ee\u6807\u7167\u7247\u7684\u751f\u6210\uff0c\u56e0\u4e3a\u6211\u4eec\u6307\u5b9a\u5f53\u524d\u751f\u6210\u5668\u751f\u6210\"1\"\u7684\u7167\u7247\uff0c\u4e8e\u662f\u6211\u4eec\u63d0\u4f9b\u6807\u7b7e\u7b49\u4e8e1\uff0c\u8fd9\u4e2a\u4fe1\u606f\uff1b\u5982\u679c\u5f53\u524d\u6307\u5b9a\u751f\u6210\"2\"\u7684\u7167\u7247\uff0c\u6211\u4eec\u5c31\u4f20\u5165 \"y=2\" \u7684\u4fe1\u606f \u4f20\u5165\u7f51\u7edc\uff0c\u4ee5\u4e0a\u5c31\u662fcGAN\u7684\u8bba\u6587 \u63d0\u51fa\u7684\u6539\u8fdb\u70b9\uff0c\u521b\u65b0\u601d\u60f3\u6bd4\u8f83\u7b80\u5355\uff0c\u4f46\u662f\u5e94\u7528\u5374\u5f88\u5e7f\u6cdb</p> <p>\u540e\u9762\u7684\u5e94\u7528\u4e2d\uff0c\u57fa\u672c\u4e0a\u90fd\u662f\u6761\u4ef6GAN\u7684\uff0c\u800c\u4e0d\u662f\u5b8c\u5168\u7684\u4e00\u4e2a\u9ad8\u65af\u53d8\u91cf\u4f5c\u4e3a\u751f\u6210\u5668\u7684\u8f93\u5165\u4fe1\u53f7</p>"},{"location":"learning/9_cGAN/#22","title":"2.2 \u7f51\u7edc\u7ed3\u6784","text":"<p>\u56fe\uff1a</p> <ul> <li>\u4e0a\uff1a\u751f\u6210\u5668</li> <li>\u4e0b\uff1a\u5224\u522b\u5668</li> </ul> <p>\uff081\uff09\u751f\u6210\u5668</p> <p>\u751f\u6210\u5668\u7684\u8f93\u5165\u9664\u6807\u51c6\u7684z\u4ee5\u5916\uff0c\u8fd8\u6709\u7eff\u8272\u90e8\u5206\uff0c\u7eff\u8272\u90e8\u5206\u5c31\u662f\u6761\u4ef6\u4fe1\u606f</p> <p>\u6761\u4ef6\u4fe1\u606f\u53ef\u4ee5\u662f\u8fde\u7eed\u7684\u53d8\u91cf\uff0c\u4e5f\u53ef\u4ee5\u662f\u79bb\u6563\u7684\u53d8\u91cf</p> <p>\u6bd4\u5982\u8bf4 \u624b\u5199\u5b57\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u63d0\u4f9b\u7684\u6761\u4ef6\u4fe1\u606f\u5c31\u662f \u6bcf\u4e00\u6b21 \u624b\u5199\u5b57\u7684\u7167\u7247\uff0c\u91cc\u9762\u6570\u5b57\u7684\u7c7b\u522b\uff0c\u63d0\u4f9b\u7684\u4e00\u4e2aclass\u4fe1\u606f\uff0c\u4e14\u8fd9\u4e2aclass\u4fe1\u606f\u662f\u4e00\u4e2aone hot\u7684\u53d8\u91cf\uff0c\u5982\u679c\u628aone hot\u53d8\u91cf\u76f4\u63a5\u4f20\u5165\u8fdb\u53bb\uff0c\u4f1a\u6bd4\u8f83\u7a00\u758f\uff0c\u6700\u6807\u51c6\u7684\u505a\u6cd5\uff0c\u5c31\u662f\u6309\u7167\u4e4b\u524d\u7684word embedding\u4e00\u6837\uff0c\u628aclass\u4fe1\u606f\u8f6c\u5316\u6210\u4e00\u4e2a class embedding\uff0c\u7136\u540e\u518d\u8ddfz\u62fc\u8d77\u6765\uff0c\u7136\u540e\u518d\u8f93\u5165\u5230\u7f51\u7edc\u4e4b\u4e2d\uff0c\u8fd9\u662f\u6807\u51c6\u7684\u4f5c\u6cd5\u3002\uff08class \\(\\rightarrow\\) class embedding\uff09</p> <p>\uff082\uff09\u5224\u522b\u5668</p> <p>\u7c7b\u4f3c\u7684\uff0c\u5728\u5224\u522b\u5668\u4e4b\u4e2d\uff0c\u4e5f\u53ef\u4ee5\u52a0\u5165\u6761\u4ef6\u4fe1\u606f\uff0c\u600e\u4e48\u7406\u89e3\uff1f</p> <p>\u5224\u522b\u5668\u6bcf\u6b21\u63a5\u6536\u7684\u7167\u7247\uff0c\u7c7b\u522b\u4e0d\u592a\u4e00\u6837\uff0c\u6bd4\u5982\u4e0a\u6b21\u63a5\u6536\u7167\u7247\u7684\u662f\"1\"\uff0c\u5224\u522b\u5668\u6839\u636e\u81ea\u5df1\u7684\u5224\u65ad\uff0c\u5224\u65ad1\u662f\u771f\u7684\u8fd8\u662f\u5047\u7684\uff0c\u7b2c\u4e8c\u6b21\u7ed9\u5224\u522b\u5668\u4e00\u5f20\"2\"\u7684\u7167\u7247\uff0c\u53c8\u53bb\u5224\u65ad2\u662f\u771f\u7684\u8fd8\u662f\u5047\u7684\uff0c\u5982\u679c\u4e0d\u544a\u8bc9\u5224\u522b\u5668\u8fd9\u5f20 \"1\" \u7684\u7167\u7247 \u662f 1\uff0c\"2\" \u7684\u7167\u7247\u662f2\uff0c\u4e24\u5f20\u7167\u7247\u5c5e\u4e8e\u4e0d\u540c\u7c7b\u522b\u7684\u8bdd\uff0c\u5224\u522b\u5668\u53ef\u80fd\u5f88\u96be\u53bb\u5224\u65ad\u3002\u4f46\u662f\u5982\u679c\u544a\u8bc9\u4e86\u5224\u522b\u5668\u4e24\u5f20\u7167\u7247\u662f\u4e0d\u540c\u7c7b\u522b\u7684\u8bdd\uff0c\u544a\u8bc9\u5224\u522b\u5668\u5f53\u524d \u7167\u7247 \u5c5e\u4e8e 1 \u8fd9\u4e2a\u7c7b\u522b\uff0c\u5224\u522b\u5668\u5224\u65ad\u5f53\u524d\u7167\u7247\u662f\u4e0d\u662f\u771f\u7684\u662f1\uff0c\u7b2c\u4e8c\u6b21\u7ed9\u5224\u522b\u5668 2 \u8fd9\u5f20\u7167\u7247\uff0c\u7136\u540e\u5224\u65ad\u5f53\u524d\u7167\u7247\u662f\u4e0d\u662f\u771f\u7684\u662f2\uff0c\u6240\u4ee5\u5728\u5224\u522b\u5668\u5f53\u4e2d \u4e5f\u53ef\u4ee5\u5f15\u5165class\u4fe1\u606f\uff0c\u5f15\u5165\u6761\u4ef6\uff0c\u5728minist\u624b\u5199\u5b57\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u5224\u522b\u5668\u7684\u8f93\u5165\u53ef\u4ee5\u901a\u8fc7 one hot\u7684class label\u8f6c\u6362\u6210 class embedding\uff0c\u7136\u540e\u8ddf\u56fe\u50cf\u62fc\u8d77\u6765\uff0c\u6216\u8005\u628aclass embedding\u7ecf\u8fc7\u51e0\u5c42DNN\u518d\u62fc\u8d77\u6765</p> <p>\u4ee5\u4e0a\u662fcGAN\u6838\u5fc3\u7684\u70b9</p>"},{"location":"learning/9_cGAN/#3-gan","title":"3 \u6761\u4ef6GAN \u4ee3\u7801\u5b9e\u73b0","text":"<pre><code>import  torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\nimport os\n\nimage_size = [1,28,28]\nlatent_dim = 96\nlabel_emb_dim = 32\nbatch_size = 64\nuse_gpu = torch.cuda.is_available()\nsave_dir = \"cgan_images\"\nos.makedirs(save_dir,exist_ok=True)\n\nclass Generator(nn.Moudle):\n    def __init__(self):\n        super(Generator,self).__init__()\n\n        self.embedding = nn.Embedding(10,label_emb_dim)\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim+label_emb_dim,128),\n            torch.nn.BatchNorm1d(128),\n            torch.nn.GELU(),\n\n            nn.Linear(128,256),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.GELU(),\n            nn.Linear(256,512),\n            torch.nn.BatchNorm1d(512),\n            torch.nn.GELU(),\n            nn.Linear(512,1024),\n            torch.nn.BatchNorm1d(1024),\n            torch.nn.GELU(),\n            nn.Linear(1024,np.prod(image_size,dtype=np.int32)),\n            nn.Sigmoid(),\n        )\n    def forward(self,z,labels):\n        # shape of z:[batchszie,latent_dim]\n        label_embedding = self.embedding(labels)\n        z = torch.cat([z,label_embedding],axis=-1)\n\n        output = self.model(z)\n        image = output.reshape(z.shape[0],*image_size)\n\n        return image\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator,self).__init__()\n\n        self.embedding = nn.Embedding(10,label_emb_dim)\n        self.model = nn.Sequential(\n            nn.Linear(np.prod(image_size,dtype=np.int32)+label_emb_dim,512),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(512,256)),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(256,128)),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(128,64)),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(64,32)),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(32,1)),\n            nn.Sigmoid(),\n        )\n    def forward(self,image,labels):\n        # shape of image:[batchsize,1,28,28]\n\n        label_embedding = self.embedding(labels)\n        prob = self.model(torch.cat([image.reshape(image.shape[0],-1),label_embedding],axis=-1))\n        return prob\n\n\n# Training\ndataset = torchvision.datasets.MNIST(\"mnist_data\", train=True, download=True,\n                                     transform=torchvision.transforms.Compose(\n                                         [\n                                             torchvision.transforms.Resize(28),\n                                             torchvision.transforms.ToTensor(),\n                                             #  torchvision.transforms.Normalize([0.5], [0.5]),\n                                         ]\n                                                                             )\n                                     )\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\ngenerator = Generator()\ndiscriminator = Discriminator()\n\n\ng_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.4, 0.8), weight_decay=0.0001)\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.4, 0.8), weight_decay=0.0001)\n\nloss_fn = nn.BCELoss()\nlabels_one = torch.ones(batch_size, 1)\nlabels_zero = torch.zeros(batch_size, 1)\n\nif use_gpu:\n    print(\"use gpu for training\")\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    loss_fn = loss_fn.cuda()\n    labels_one = labels_one.to(\"cuda\")\n    labels_zero = labels_zero.to(\"cuda\")\n\nnum_epoch = 200\nfor epoch in range(num_epoch):\n    for i, mini_batch in enumerate(dataloader):\n        gt_images, labels = mini_batch\n\n\n        z = torch.randn(batch_size, latent_dim)\n\n        if use_gpu:\n            gt_images = gt_images.to(\"cuda\")\n            z = z.to(\"cuda\")\n\n        pred_images = generator(z,labels)\n        g_optimizer.zero_grad()\n\n        recons_loss = torch.abs(pred_images-gt_images).mean()\n\n        g_loss = recons_loss*0.05 + loss_fn(discriminator(pred_images,labels), labels_one)\n\n        g_loss.backward()\n        g_optimizer.step()\n\n        d_optimizer.zero_grad()\n\n        real_loss = loss_fn(discriminator(gt_images,labels), labels_one)\n        fake_loss = loss_fn(discriminator(pred_images.detach(),labels), labels_zero)\n        d_loss = (real_loss + fake_loss)\n\n        # \u89c2\u5bdfreal_loss\u4e0efake_loss\uff0c\u540c\u65f6\u4e0b\u964d\u540c\u65f6\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u5e76\u4e14\u5dee\u4e0d\u591a\u5927\uff0c\u8bf4\u660eD\u5df2\u7ecf\u7a33\u5b9a\u4e86\n\n        d_loss.backward()\n        d_optimizer.step()\n\n        if i % 50 == 0:\n            print(f\"step:{len(dataloader)*epoch+i}, recons_loss:{recons_loss.item()}, g_loss:{g_loss.item()}, d_loss:{d_loss.item()}, real_loss:{real_loss.item()}, fake_loss:{fake_loss.item()}\")\n\n        if i % 800 == 0:\n            image = pred_images[:16].data\n            torchvision.utils.save_image(image, f\"{save_dir}/image_{len(dataloader)*epoch+i}.png\", nrow=4)\n</code></pre>"},{"location":"learning/9_cGAN/#31","title":"3.1 \u751f\u6210\u5668\u4ee3\u7801\u89e3\u8bfb","text":"<pre><code>class Generator(nn.Moudle):\n    def __init__(self):\n        super(Generator,self).__init__()\n\n        self.embedding = nn.Embedding(10,label_emb_dim)\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim+label_emb_dim,128),\n            torch.nn.BatchNorm1d(128),\n            torch.nn.GELU(),\n\n            nn.Linear(128,256),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.GELU(),\n            nn.Linear(256,512),\n            torch.nn.BatchNorm1d(512),\n            torch.nn.GELU(),\n            nn.Linear(512,1024),\n            torch.nn.BatchNorm1d(1024),\n            torch.nn.GELU(),\n            nn.Linear(1024,np.prod(image_size,dtype=np.int32)),\n            nn.Sigmoid(),\n        )\n    def forward(self,z,labels):\n        # shape of z:[batchszie,latent_dim]\n        label_embedding = self.embedding(labels)\n        z = torch.cat([z,label_embedding],axis=-1)\n\n        output = self.model(z)\n        image = output.reshape(z.shape[0],*image_size)\n\n        return image\n</code></pre> <p>cGAN\u8fdb\u884c\u624b\u5199\u5b57\u751f\u6210\u4efb\u52a1</p> <ul> <li>\u751f\u6210\u5668\u7684forward\u51fd\u6570\u4e2d\uff0c\u52a0\u5165labels\u4fe1\u606f</li> <li>labels\u8868\u793a\u5e0c\u671b\u751f\u6210\u5668\u8981\u751f\u6210\u6307\u5b9a\u7684\u76ee\u6807\uff0c\u800c\u4e0d\u662f\u968f\u4fbf\u751f\u6210\u7684\uff0c\u6bd4\u5982\u670910\u4e2a\u7c7b\u522b\uff0c0~9\u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u7167\u7247\uff0c\u901a\u8fc7\u6307\u5b9alabel\uff0c\u6bd4\u5982\u6307\u5b9a1\u5c31\u751f\u62101\u7684\u56fe\u50cf\uff0c\u6307\u5b9a2\u5c31\u751f\u62102\u7684\u7167\u7247\uff0c\u8fd9\u5c31\u662f\u6761\u4ef6\u4fe1\u606f\uff0c\u901a\u8fc7labels\u4f20\u5165\uff0clabels\u5c31\u662f\u79bb\u6563\u7684\u6807\u7b7e\u53d8\u91cf\uff0c\u65e2\u7136\u662f\u79bb\u6563\u7684</li> </ul> <pre><code>    def forward(self,z,labels):\n        # shape of z:[batchszie,latent_dim]\n        label_embedding = self.embedding(labels)\n        z = torch.cat([z,label_embedding],axis=-1)\n\n        output = self.model(z)\n        image = output.reshape(z.shape[0],*image_size)\n\n        return image\n</code></pre> <p>\uff081\uff09\u7b2c\u4e00\u6b65\uff0c\u901a\u8fc7embedding table\u628alabel\u4f20\u5165\u5230embedding table\u53bb\u627e\u5230\u5bf9\u5e94\u7684embedding vector\uff0c\u5f97\u5230label embedding\uff0c\u8fd9\u662f\u7b2c\u4e00\u6b65\uff0c\u628a\u79bb\u6563\u7684label \u7c7b\u522b\u4fe1\u606f \u8f6c\u5316\u6210\u8fde\u7eed\u7684 \u6d6e\u70b9\u5411\u91cf\uff0c</p> <p>\uff082\uff09\u7b2c\u4e8c\u6b65\uff0c\u5f97\u5230embedding\u5411\u91cf\u4e4b\u540e\uff0c\u628a\u8fd9\u4e2a\u5411\u91cf\u6700\u7b80\u5355\u7684\u662f\uff0c\u8ddfz\u62fc\u8d77\u6765\uff0c\u5f53\u7136\u8fd9\u4e2a\u6548\u679c\u4e0d\u4e00\u5b9a\u6700\u597d\uff0c\u53ef\u4ee5\u5e38\u8bc6\u4e0d\u540c\u7684\u7279\u5f81\u6784\u9020\u65b9\u6cd5</p> <p>\u4ee5\u4e0a\u6b65\u9aa4\u5f97\u5230\u4e86\u65b0\u7684z\uff0c\u8fd9\u4e2az\u5305\u542b\u4e86\u6761\u4ef6\u4fe1\u606f\u7684\u91cf\uff0c\u8fd9\u65f6\u628az\u7ee7\u7eed\u9001\u5165\u5230\u4e4b\u524d\u7684\u751f\u6210\u5668\u4e3b\u5e72\u7f51\u7edc\u4e2d\uff0c\u7531DNN\u548c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u6784\u6210\u7684\u4e3b\u5e72\u7f51\u7edc\u4e4b\u4e2d\uff0c\u7684\u90fd\u6700\u540e\u7684image</p> <p>\u4ee5\u4e0a\u5728\u751f\u6210\u5668\u4e4b\u4e2d\uff0c\u5f15\u5165\u4e86 \u6761\u4ef6\u4fe1\u606f\uff0c\u8fd9\u91cc\u7684\u6539\u53d8\uff1a</p> <ul> <li><code>self.embedding = nn.Embedding(10,label_emb_dim)</code>  init\u4e2d \u6dfb\u52a0\u4e86embedding\u7684\u91cf\uff0c\u5b9e\u4f8b\u5316\u662f10\u884c\uff0c\u56e0\u4e3a\u670910\u7c7b\u624b\u5199\u6570\u5b57\uff0c\u7b2c\u4e8c\u7ef4 <code>label_emb_dim</code> \u4e5f\u5c31\u662f <code>label_embedding</code> \u7684\u7ef4\u5ea6\uff0c\u8bbe\u7f6e\u4e3a32\uff0c\u5728forward\u51fd\u6570\u4e2d\uff0c\u9700\u8981\u628a labels \u4f5c\u4e3a\u53c2\u6570\uff0c\u4f5c\u4e3a\u4e00\u90e8\u5206\u8f93\u5165\u4f20\u5165  <code>self.model</code></li> </ul>"},{"location":"learning/9_cGAN/#32","title":"3.2 \u5224\u522b\u5668\u4ee3\u7801\u89e3\u8bfb","text":"<pre><code>class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator,self).__init__()\n\n        self.embedding = nn.Embedding(10,label_emb_dim)\n        self.model = nn.Sequential(\n            nn.Linear(np.prod(image_size,dtype=np.int32)+label_emb_dim,512),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(512,256)),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(256,128)),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(128,64)),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(64,32)),\n            torch.nn.GELU(),\n            torch.nn.utils.spectral_norm(nn.Linear(32,1)),\n            nn.Sigmoid(),\n        )\n    def forward(self,image,labels):\n        # shape of image:[batchsize,1,28,28]\n\n        label_embedding = self.embedding(labels)\n        prob = self.model(torch.cat([image.reshape(image.shape[0],-1),label_embedding],axis=-1))\n        return prob\n</code></pre> <ul> <li>\u5224\u522b\u5668\u7684 <code>forward</code> \u51fd\u6570\u4e2d\uff0c\u4e5f\u9700\u8981\u4f20\u5165labels\u544a\u8bc9\u5224\u522b\u5668\u5f53\u524d\u7c7b\u522b\u662f \u7c7b\u522b1 \u8fd8\u662f\u7c7b\u522b2 \uff0c\u5e2e\u52a9\u5224\u522b\u5668\u66f4\u597d\u7684\u505a\u51fa\u5224\u65ad</li> <li>\u4fee\u65391\uff1a\u9996\u5148 <code>def forward(self,image,labels)</code>  forward\u51fd\u6570\u4e2d\u52a0\u5165 labels</li> <li>\u4fee\u65392\uff1a <code>label_embedding = self.embedding(labels)</code> \u79bb\u6563\u53d8\u91cf\u8f6c\u5316\u6210\u8fde\u7eed\u53d8\u91cf\uff0c\u901a\u8fc7embedding table\u5f97\u5230<code>label embedding</code>\u7684\u5411\u91cf</li> <li>\u4fee\u65393\uff1a\u63a5\u7740\u628a <code>label embedding</code>\u7684\u5411\u91cf \uff0c\u8ddf image \u8fdb\u884c\u62fc\u63a5 \uff1a</li> </ul> <p><code>torch.cat([image.reshape(image.shape[0],-1),label_embedding],axis=-1)</code></p> <p>\u8fd9\u662f\u6700\u7b80\u5355\u7684\u5904\u7406\uff0c\u5982\u679c\u6548\u679c\u4e0d\u597d\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u66f4\u6539\uff0c\u501f\u9274\u5176\u4ed6\u7684\u7f51\u7edc\u7684\u505a\u6cd5 \u628a <code>label embedding</code> \u5f15\u5165\u7f51\u7edc\uff0c \u4f5c\u4e3a\u4e3b\u5e72\u7f51\u7edc\u7684\u8f93\u5165\uff0c\u6700\u540e\u5f97\u5230 0~1\u4e4b\u95f4\u7684\u6982\u7387\u503c\uff1a<code>nn.Sigmoid()</code></p>"},{"location":"learning/9_cGAN/#33-train","title":"3.3 train\u51fd\u6570\u7684\u4fee\u6539","text":"<p>\uff081\uff09<code>pred_images = generator(z,labels)</code></p> <p>\u5728\u8bad\u7ec3\u7f51\u7edc\u7684\u65f6\u5019\u9700\u8981\u628a <code>labels</code>\u4e5f\u4f20\u5165\u7f51\u7edc</p> <p><code>labels</code>\u600e\u4e48\u6765\u7684\u5462\uff1f <code>gt_images, labels = mini_batch</code> \u5bf9<code>mini_batch</code>\u89e3\u6790\u51fa\u6765\u7684</p> <p>\uff082\uff09<code>g_loss = recons_loss*0.05 + loss_fn(discriminator(pred_images,labels), labels_one)</code></p> <p>\u628a\u9884\u6d4b\u7684\u7167\u7247\u4f20\u5165\u5230  discriminator \u4e2d\uff0c\u540c\u6837\u4e5f\u9700\u8981\u5e26\u4e0a \u6761\u4ef6\u4fe1\u606flabels</p> <p>\u4e5f\u5c31\u662f\u8bf4\u9700\u8981\u5bf9\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u90fd\u9700\u8981\u52a0\u5165condition\u4fe1\u606f</p> <p>\uff083\uff09\u5728\u8ba1\u7b97 real loss\u548cfake loss\u65f6\uff0c\u90fd\u9700\u8981\u628alabels\u4f20\u5165discriminator\u4e4b\u4e2d</p> <pre><code>        real_loss = loss_fn(discriminator(gt_images,labels), labels_one)\n        fake_loss = loss_fn(discriminator(pred_images.detach(),labels), labels_zero)\n</code></pre> <p>\u4ee5\u4e0a\uff0c\u6240\u6709\u6761\u4ef6GAN\u7684\u4ee3\u7801\u5b9e\u73b0</p>"},{"location":"learning/9_cGAN/#4-gan","title":"4 \u6700\u5c0f\u5e73\u65b9GAN","text":"<ul> <li>\u6700\u5c0f\u5e73\u65b9GAN or \u6700\u5c0f\u4e8c\u4e58GAN or LSGAN</li> <li>cGAN\uff1a\u6761\u4ef6GAN</li> <li>LSGAN\uff1a\u6700\u5c0f\u5e73\u65b9GAN</li> <li>\u5f15\u7528\u91cf\uff1a3k\u5de6\u53f3\uff0c\u5f88\u9ad8</li> <li>\u73b0\u5728\u7528\u7684GAN\u5df2\u7ecf\u5f88\u5c11\u662f\u539f\u59cbGAN\uff0c\u4e0d\u7528\u4e8c\u5206\u7c7b\u4ea4\u53c9\u71b5\u7684\u635f\u5931\u51fd\u6570\uff0c\u73b0\u5728\u5f88\u591a\u635f\u5931\u51fd\u6570\u90fd\u662f\u7528\u7684LSGAN\u7684\u635f\u5931\u51fd\u6570\uff0c\u7c7b\u4f3c\u56de\u5f52\u4efb\u52a1\u800c\u4e0d\u662f\u5206\u7c7b\u4efb\u52a1</li> </ul>"},{"location":"learning/9_cGAN/#41","title":"4.1 \u6458\u8981","text":"<ul> <li>\u57fa\u4e8eGAN\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u5927\u83b7\u6210\u529f</li> <li>\u6807\u51c6\u7684GAN\u628a\u5224\u522b\u5668\u5f53\u6210\u5206\u7c7b\u5668\uff0c\u91c7\u7528sigmoid\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\uff0c\u4f46\u662fsigmoid\u635f\u5931\u51fd\u6570\u4f1a\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898</li> <li>\u4e3a\u4e86\u514b\u670d\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faLSGAN</li> <li>LSGAN\u91c7\u7528\u6700\u5c0f\u5e73\u65b9\u8bef\u5dee\u51fd\u6570\uff0c\u5bf9\u4e8e\u5224\u522b\u5668\u800c\u8a00\u91c7\u7528\u6700\u5c0f\u5e73\u65b9\u8bef\u5dee\u51fd\u6570</li> <li>\u5f53\u91c7\u7528 \u6700\u5c0f\u5e73\u65b9\u8bef\u5dee\u635f\u5931\u51fd\u6570\u65f6\uff0c\u76f8\u5f53\u4e8e\u4f18\u5316 <code>\u76ae\u5c14\u900a\u5f00\u65b9\u6563\u5ea6</code>  \uff1a $\\mathrm{Pearson    \\mathcal{X}^2   divergence} $ </li> <li>\u5728GAN\u7684\u8bba\u6587\u4e2d\uff0c\u540c\u6837\u4e5f\u662f\u5728\u4f18\u5316\u4e00\u4e2a\u6563\u5ea6\uff1a</li> </ul> <p>\u2460 KL\u6563\u5ea6</p> <p>\u2461 \u8a79\u68ee-\u9999\u519c \u6563\u5ea6</p> <ul> <li>LSGAN\u76f8\u6bd4\u539f\u59cbGAN\u7684\u4e24\u4e2a\u4f18\u70b9\uff1a</li> </ul> <p>\uff081\uff09LSGAN\u53ef\u4ee5\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u7247</p> <p>\uff082\uff09LSGAN\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u66f4\u52a0\u7a33\u5b9a</p>"},{"location":"learning/9_cGAN/#42","title":"4.2 \u672c\u6587\u7684\u8d21\u732e","text":"<p>\u672c\u6587\u7684\u4e09\u4e2a\u8d21\u732e\u70b9\uff08\u539f\u6587Intro\u6700\u540e\u4e00\u6bb5\uff09</p> <p>\uff081\uff09</p> <ul> <li> <p>\u63d0\u51fa\u4e86LSGAN\uff0c\u91c7\u7528\u6700\u5c0f\u5e73\u65b9\u7684\u8bef\u5dee\u51fd\u6570\uff0c\u5728\u539f\u59cbGAN\u4e2d\u91c7\u7528\u7684\u4e8c\u5206\u7c7b\u4ea4\u53c9\u71b5\u8bef\u5dee\u51fd\u6570\uff08BCE\u3001\u57fa\u4e8eSigmoid\u3001\u4e8c\u5206\u7c7b\u4ea7\u751f0  ~1\u7684\u6982\u7387\uff09 </p> </li> <li> <p>\u5728\u6700\u5c0f\u5316 LSGAN\u76ee\u6807\u51fd\u6570\u7684\u65f6\u5019\uff0c\u76f8\u5f53\u4e8e\u5728\u6700\u5c0f\u5316 \u76ae\u5c14\u900a\u5f00\u65b9\u6563\u5ea6\uff08\u6709\u63a8\u5bfc\uff09</p> </li> </ul> <p>\uff082\uff09</p> <p>\u7ecf\u8fc7\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u5bf9\u6bd4\uff0cLSGAN\u80fd\u4ea7\u751f\u66f4\u52a0\u903c\u771f\u7684\u7167\u7247</p> <p>\uff083\uff09</p> <p>\u5c06LSGAN\u5e94\u7528\u5230\u4e2d\u6587\u7684\u624b\u5199\u5b57\u751f\u6210\u4e0a\uff0c\u53d1\u73b0LSGAN\u4e5f\u80fd\u4ea7\u751f\u53ef\u61c2\u7684\u4e2d\u6587\u6c49\u5b57</p>"},{"location":"learning/9_cGAN/#43-3-method","title":"4.3 \u539f\u65873 Method","text":"<ul> <li>\u56de\u987eGAN\u7684\u516c\u5f0f</li> <li>\u5c55\u793aLSGAN\u7684\u516c\u5f0f</li> <li>\u4ecb\u7ecd\u4e24\u4e2aLSGAN\u7684\u6a21\u578b</li> </ul>"},{"location":"learning/9_cGAN/#31-gan","title":"3.1 GAN\u7684\u5b66\u4e60\u8fc7\u7a0b","text":"<p>GAN\u7684\u5b66\u4e60\u8fc7\u7a0b\u5b66\u4e60\u4e00\u4e2a \u5224\u522b\u5668\\(D\\)\u548c\u4e00\u4e2a\u751f\u6210\u5668\\(G\\)\uff0c\u8fd9\u4e24\u4e2a\u662f\u540c\u65f6\u8bad\u7ec3\u7684\uff1b</p> <p>\u751f\u6210\u5668G</p> <p>\\(G\\)\u7684\u76ee\u6807\u662f\u5b66\u5230\u6570\u636e\\(x\\)\u7684\u5206\u5e03\\(p_g\\)</p> <p>\\(G\\)\u4ece\u4e00\u4e2a\u5747\u5300\u5206\u5e03 \u6216\u8005 \u9ad8\u65af\u5206\u5e03\u4e2d\uff0c\u91c7\u6837\u4e00\u4e2a\u8f93\u5165\u53d8\u91cf \\(z\\)\uff0c\u88ab\u91c7\u6837\u7684\u5206\u5e03\u8bb0\u4e3a \\(p_z(z)\\)</p> <p>\u901a\u8fc7\u751f\u6210\u5668\\(G\\)\uff0c\u5c06\u8f93\u5165\u53d8\u91cf\\(z\\)\uff0c\u901a\u8fc7\u53ef\u5fae\uff08differentiable netwark\uff09\u7684\u7f51\u7edc \u6620\u5c04\u5230\u4e00\u4e2a\u65b0\u7684\u7a7a\u95f4\u4e0a\uff0c\u8bb0\u4f5c \\(G(z;\\theta_g)\\) \uff0c\u4e5f\u5c31\u662f\u8bf4 \u901a\u8fc7 \\(\u751f\u6210\u5668\u751f\u6210\u7684\u6570\u636e \\sim p_G\\)</p> <p>\u5224\u522b\u5668D</p> <p>\u5224\u522b\u5668D\u662f\u4e00\u4e2a\u5206\u7c7b\u5668</p> <p>\u5224\u522b\u5668D\u7684\u76ee\u6807\u662f\u51c6\u786e\u7684\u8bc6\u522b\u51fa\u4e00\u5f20\u7167\u7247\u662f\u6765\u81ea\u8bad\u7ec3\u96c6\u8fd8\u662f\u6765\u81ea\u751f\u6210\u5668\u6240\u751f\u6210\u7684\u6570\u636e</p> <p>\u6807\u51c6GAN\u7684\u76ee\u6807\u51fd\u6570</p> <p></p> <ul> <li>\u662f\u4e00\u4e2a minmax\u7684\u516c\u5f0f</li> <li>GAN\u9700\u8981\u540c\u65f6\u4f18\u5316\u4e24\u4e2a\u7f51\u7edc\uff1aG&amp;D</li> </ul> <p>\uff081\uff09\u5f53\u4f18\u5316\\(D\\)\u7684\u65f6\u5019\uff0c\u5e0c\u671b\\(V\\)\u8fbe\u5230\u6700\u5927 \\(\\mathrm{max_d}\\)</p> <p>\u7b49\u4ef7\u4e8e\u5f53\\(x\\)\u670d\u4ece\\(p_{data}\\)\u5206\u5e03\u7684\u65f6\u5019\uff0c\\(logD(x)\\)\u8fbe\u5230\u6700\u5927\uff0c\u540c\u65f6\u5f53\\(z\\)\u670d\u4ece\\(p_z\\)\u5206\u5e03\u7684\u65f6\u5019\uff0c\u5e0c\u671b\\(log1-D(G(z))\\)\u4e5f\u8fbe\u5230\u6700\u5927\uff0c\u8fd9\u662f\u5f53\u4f18\u5316\\(D\\)\u7684\u65f6\u5019</p> <p>\uff082\uff09\u5f53\u4f18\u5316\\(G\\)\u7684\u65f6\u5019\uff0c\u662f\u4e00\u4e2a\\(\\mathrm{min_G}\\)\u51fd\u6570\uff0c\u5e0c\u671b\u8fd9\u4e2a\u51fd\u6570\u8fbe\u5230\u6700\u5c0f</p> <p>\u5f53\u4f18\u5316\\(G\\)\u7684\u65f6\u5019\uff0c\u5e0c\u671b\\(V\\)\u8fbe\u5230\u6700\u5c0f\uff0c\u9996\u5148\u5ffd\u7565\u7b2c\u4e00\u9879\uff0c\u56e0\u4e3a\u7b2c\u4e00\u9879\u4e0d\u5305\u542b\\(G\\)\uff0c\u53ea\u770b\u7b2c\u4e8c\u9879\uff0c\u5f53\u4f18\u5316G\u7684\u65f6\u5019\uff0c\u5f53\\(z\\)\u670d\u4ece\\(p_z\\)\u5206\u5e03\u7684\u65f6\u5019\uff0c\u5e0c\u671b \\(log(1-D(G(z)))\\) \u8fbe\u5230\u6700\u5c0f</p> <p>\u4ee5\u4e0a\u662f\u6807\u51c6GAN\uff0cminmax</p> <p>\\(\\mathbb{E}\\) \u8868\u793a\u671f\u671b\u503c\uff0c\u671f\u671b\u503c\u7684\u610f\u601d\u662f\u9700\u8981\u8003\u8651\u5230\u6574\u4e2a\u5206\u5e03\uff0c\u901a\u8fc7\u5c0f\u6279\u6b21\u8bad\u7ec3\u4e0d\u65ad\u903c\u8fd1\u671f\u671b\u503c</p>"},{"location":"learning/9_cGAN/#32-lsgan","title":"3.2 LSGAN","text":"<p>\u68af\u5ea6\u6d88\u5931\u95ee\u9898</p> <ul> <li>\u5f53\u628a\u5224\u522b\u5668\u5f53\u505a\u5206\u7c7b\u5668\u7684\u65f6\u5019\uff0c\u6807\u51c6\u7684GAN\u91c7\u7528\u57fa\u4e8eSigmoid\u7684\u4ea4\u53c9\u71b5\u8bef\u5dee\u51fd\u6570</li> <li>\u5f53\u66f4\u65b0\u751f\u6210\u5668\u7684\u65f6\u5019\uff0c\u6807\u51c6\u7684GAN\u4f1a\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\uff0c\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51faLSGAN</li> </ul> <p>\u53ef\u89c6\u5316 \u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff1a</p> <p></p> <p>(a)sigmoid\u7684\u4ea4\u53c9\u71b5\u8bef\u5dee\u51fd\u6570\u56fe\uff0c\u4eceloss\u66f2\u7ebf\u53ef\u4ee5\u770b\u5230\uff0c\u5f53x&gt;2\u65f6\uff0closs\u7684\u659c\u7387\u5f00\u59cb\u63a5\u8fd1\u4e8e0\uff0c\u800c\u4e14\u4e0d\u600e\u4e48\u53d8\u5316\uff0c\u4e5f\u5c31\u662f\u8bf4\u5f53x\u9010\u6e10\u53d8\u5927\u65f6\uff0closs\u63a5\u8fd1\u4e8e\u9971\u548c\uff0c\u68af\u5ea6\u4e00\u76f4\u5904\u57280\u7684\u4f4d\u7f6e\u4e0a\uff0c\u8fd9\u5e76\u4e0d\u5229\u4e8e\u53c2\u6570\u7684\u66f4\u65b0</p> <p>\u6f14\u793a\u8fd9\u4e2a\u56fe\u50cf\u662f\u600e\u4e48\u6765\u7684\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nlogits = torch.linspace(-10,10,2000)\nloss = []\nloss_fn = nn.BCELoss()\nfor lgs in logits:\n    loss.append(loss_fn(torch.sigmoid(lgs),torch.ones_like(lgs)))\n\nplt.plot(logits,loss)\nplt.show()\n</code></pre> <p></p> <ul> <li>\u523b\u5ea6\u4e0d\u4e00\u6837\uff0c\u5f62\u72b6\u548c\u539f\u6587\u662f\u4e00\u6837\u7684</li> <li>sigmoid ce\u7684\u56fe\u50cf\uff1a\u8868\u793asigmoid\u4ea4\u53c9\u71b5\u7684\u56fe\u50cf</li> <li>logits\u4ece-10\u53d8\u5316\u523010\uff0closs\u4ece10\u53d8\u5316\u52300\uff0c\u4f46\u662f0\u662f\u5927\u6982\u52302\u7684\u65f6\u5019\uff0c\u5c31\u5df2\u7ecf\u52300\u4e86\uff0c\u5f53logits\u4ece2\u523010\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u8ba4\u4e3a logits\u5c31\u662f\u4e0d\u53d8\u4e86\uff0c\u68af\u5ea6\u5c31\u6d88\u5931\u4e86</li> <li>x\u8f74\u662f \u9884\u6d4b\u6982\u7387 logits\uff0cy\u8f74\u662f\u5bf9\u5e94\u7684loss\uff0c\u628a\u771f\u5b9e\u6807\u7b7e\u8bbe\u7f6e\u4e3a1 \u7684loss</li> </ul> <p>\u2460 <code>logits = torch.linspace(-10,10,2000)</code></p> <p>\u9996\u5148\u968f\u673a\u751f\u6210logits\uff0c\u8fdb\u5165\u5224\u522b\u5668\u4e4b\u524d\uff0c\u751f\u6210\u5668\u9884\u6d4b\u7684\u503c</p> <p>linspace\u751f\u6210-10\u523010\u4e4b\u95f4\uff0c2000\u4e2a\u70b9\u7684logits</p> <p>\u2461 </p> <pre><code>loss = []\nloss_fn = nn.BCELoss()\nfor lgs in logits:\n    loss.append(loss_fn(torch.sigmoid(lgs),torch.ones_like(lgs)))\n</code></pre> <p>\u751f\u6210loss\u51fd\u6570\uff0c\u5b9e\u4f8b\u5316loss function\uff0cnn.BCELoss()</p> <p>\u8ba1\u7b97\u6bcf\u4e00\u4e2alogits\uff0c\u5bf9\u5e94\u7684BCELoss\u662f\u4ec0\u4e48</p> <p>\u5bf9logits\u8fdb\u884c\u904d\u5386\u5f97\u5230lgs\uff0c\u628algs\u4f20\u5165\u5230sigmoid\u4e4b\u4e2d\uff0c\u5f97\u5230\u9884\u6d4b\u6982\u7387\u503c</p> <p>\u63a5\u4e0b\u6765\u5c06 \u9884\u6d4b\u6982\u7387\u503c <code>torch.sigmoid(lgs)</code> \u548c\u771f\u5b9e\u503c <code>torch.ones_like(lgs)</code> \u4f20\u5165\u5230 BCELoss\u5b9e\u4f8b\u5316\u7684 <code>loss_fn</code> \u5f97\u5230loss\uff0c\u5c06loss\u6dfb\u52a0\u5230\u5217\u8868\u4e2d\uff0c\u5f97\u5230\u6bcf\u4e00\u4e2algs\u5bf9\u5e94\u7684loss</p> <p>\u2462</p> <p>\u53ef\u89c6\u5316</p> <p>\u4ee5logits\u4f5c\u4e3ax\u8f74\uff0closs\u4f5c\u4e3ay\u8f74 plt.plot(logits,loss)</p> <p>plt.show() \u53ef\u89c6\u5316</p> <p>\u4ee5\u4e0a\u901a\u8fc7\u56fe\u50cf\uff0c\u76f4\u89c2\u5730\u7ed9\u51fa\u4e86\u539f\u59cbGAN\u9047\u5230\u7684\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898</p> <p>LSGAN</p> <p></p> <ul> <li>\u4e3a\u4e86\u5f25\u8865\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86LSGAN</li> <li>\u5047\u8bbe\u7528\u5224\u522b\u5668 a,b\u7684\u7f16\u7801\u65b9\u6848\uff0c\u6765\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\u548c\u865a\u5047\u6807\u7b7e\uff0c\u4e5f\u5c31\u662f\u662f\u8bf4\u628afake data\u7684\u6807\u7b7e\u5b9a\u4e49\u4e3aa\uff0creal data\u7684\u6807\u7b7e\u5b9a\u4e49\u4e3ab\uff0c\u6b64\u65f6\u5f97\u5230LSGAN\u7684\u76ee\u6807\u51fd\u6570\uff1a</li> </ul> <p>\\(\\mathrm{min}_DV_{LSGAN}(D) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}}}[(D(x)-b)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-a)^2]\\)</p> <p>\\(\\min_GV_{LSGAN}(G) = \\frac{1}{2}\\mathbb{E}_{z\\sim{p_z(z)}}[(D(G(z))-c)^2]\\)</p> <p>\uff081\uff09\u4f18\u5316D\uff0c\u5206\u4e3a\u4e24\u6b65\uff0creal loss\uff08\u771f\u5b9e\u6570\u636e\u5bf9\u5e94\u7684loss\uff09\u548cfake loss\uff08\u751f\u6210\u5668\u751f\u6210\u7684\u865a\u5047\u6570\u636e\u5bf9\u5e94\u7684loss\uff09</p> <ul> <li>real loss\u8f93\u5165\u7684\u662fx\uff08\u771f\u5b9e\u6570\u636e\uff09\uff0c\u5e0c\u671b\u6a21\u578b\u9884\u6d4b\u51fa\u6765\u7684\u6807\u7b7e\u662fb</li> </ul> <p>\uff08\u5148\u8bbe\u7f6ea\u3001b\u3001c\u7684\u7b26\u53f7\u8868\u793a\uff0c\u5173\u4e8eabc\u5177\u4f53\u5730\u8bbe\u7f6e\uff0c\u7b49\u4e0b\u4f1a\u8bf4\uff09</p> <ul> <li>\u628a\u751f\u6210\u5668\u7684\u8f93\u51fa G(z) \u8f93\u5165\u5230\u5224\u522b\u5668\u4e2d\uff0c\u5e0c\u671b\u9884\u6d4b\u7684\u6807\u7b7e\u662fa</li> </ul> <p>\u4ee5\u4e0a\u662f\u4f18\u5316\u5224\u522b\u5668</p> <p>\uff082\uff09\u4f18\u5316\u751f\u6210\u5668G\uff0c\u7528\u7684\u6807\u7b7e\u662fc</p> <p>\uff083\uff09</p> <p>\u8fd9\u91cc\u6ca1\u6709log\u51fd\u6570\uff0c\u7528\u7684\u662f\u4ea4\u53c9\u71b5\u51fd\u6570</p> <p>\u628a\u5224\u522b\u5668\u7528\u56de\u5f52\u7684\u503c\u8868\u793a\uff0c\u4f18\u5316\u7684\u662f\u5224\u522b\u5668\u4e0e\u6807\u7b7e\u503c\uff0c\u6240\u4ee5\u53eb\u6700\u5c0f\u5e73\u65b9GAN</p> <p>\u4ece\u6807\u51c6GAN\u8fc7\u6e21\u5230\u6700\u5c0f\u5e73\u65b9GAN\uff0c\u5c31\u662f\u5c06\u539f\u6765\u7684\u4e8c\u5206\u7c7b\u5206\u7c7b\u4efb\u52a1\u8f6c\u5316\u6210\u4e00\u4e2a\u56de\u5f52\u4efb\u52a1\uff0c\u5e76\u4e14\u5224\u522b\u5668\u662f\u8bbe\u7f6e\u7684\u4e0d\u540c\u7684\u56de\u5f52\u6807\u7b7e\uff0c\u4f18\u5316\u7684\u662f\u6700\u5c0f\u5e73\u65b9\u5dee\uff0c\u53eb\u505a\u6700\u5c0f\u5e73\u65b9GAN</p> <p>\u524d\u9762\u6709\u4e00\u4e2a \\(\\frac{1}{2}\\) \u662f\u4e3a\u4e86\u6c42\u5bfc\u4ee5\u540e\uff0c2\u00d7 \\(\\frac{1}{2}\\) \u7ea6\u6389</p> <p>\uff084\uff09\u518d\u6b21\u5f3a\u8c03 LSGAN\u7684\u76ee\u6807\u51fd\u6570</p> <p>\u5728LSGAN\u4e2d\uff0c\u4f18\u5316D\u7684\u76ee\u6807\u51fd\u6570\u662f </p> <p>\\(\\mathrm{min}_DV_{LSGAN}(D) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}}}[(D(x)-b)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-a)^2]\\)</p> <p>\\(\\frac{1}{2}\\) \u7684 \u5f53 \\(x\\)\u670d\u4ece \\(p_{data}\\) \u7684\u5206\u5e03\u7684\u65f6\u5019\uff0c\\(D(x)-b \u7684\u5e73\u65b9\\) \u7684\u671f\u671b \u52a0\u4e0a \\(\\frac{1}{2}\\) \u7684 \u5f53z \u670d\u4ece \\(p_z\\) \u7684\u65f6\u5019\uff0c\\(D(G(z))-a\u7684\u5e73\u65b9\\) \u7684\u671f\u671b </p> <p>\u4f18\u5316\u751f\u6210\u5668G\u7684\u65f6\u5019</p> <p>\\(\\min_GV_{LSGAN}(G) = \\frac{1}{2}\\mathbb{E}_{z\\sim{p_z(z)}}[(D(G(z))-c)^2]\\)</p> <p>\u4f18\u5316\u7684\u662f \\(\\frac{1}{2}\\) \u7684 \u5f53z\u670d\u4ece \\(p_{z(z)}\\) \u5206\u5e03\u7684\u65f6\u5019\uff0c\\(D(G(z)) - c \u5e73\u65b9\\)\u7684\u671f\u671b</p> <p>\u901a\u8fc7 \u4f18\u5316\u4e24\u4e2a\u671f\u671b\uff0c\u6765\u4f18\u5316\u5224\u522b\u5668\u548c\u751f\u6210\u5668</p> <p>\u4ee5\u4e0a\u662fLSGAN\u7684\u5b9a\u4e49</p>"},{"location":"learning/9_cGAN/#33-lsgan","title":"3.3 LSGAN \u7684\u63a8\u5bfc","text":"<p>\u4f18\u5316 LSGAN \u7b49\u4ef7\u4e8e \u4f18\u5316 \u76ae\u5c14\u900a\u5f00\u65b9\u6563\u5ea6</p> <p></p> <ul> <li>\u5728\u539f\u59cbGAN\u4e2d\uff0c\u4f18\u5316 \u4ef7\u503c\u51fd\u6570 \u7b49\u4ef7\u4e8e \u4f18\u5316 \u8a79\u68ee-\u9999\u519c\u6563\u5ea6</li> </ul> <p>LSGAN\uff1a</p> <p></p> <p>\uff081\uff09\u9996\u5148 \u5bf9\u4e8e \\(\\min_G\\)  \u52a0\u4e0a\u4e00\u4e2a   \\(\\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}}}[(D(x)-c)^2]\\)</p> <p>\u5f97\u5230\uff1a</p> <p>\\(\\mathrm{min}_GV_{LSGAN}(G) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}}}[(D(x)-c)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-c)^2]\\)</p> <p>\u56e0\u6b64\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u90fd\u5199\u6210\u4e86\u4e24\u90e8\u5206\uff1a</p> <p>\\(\\mathrm{min}_DV_{LSGAN}(D) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}(x)}}[(D(x)-b)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-a)^2]\\)</p> <p>\\(\\mathrm{min}_GV_{LSGAN}(G) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}(x)}}[(D(x)-c)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-c)^2]\\)</p> <p>\u7b2c\u4e00\u90e8\u5206\uff1a\u5f53 \\(x\\sim p_{data}(x)\\) \u5206\u5e03\uff08real\u771f\u5b9e\u5206\u5e03\uff09\u7684\u65f6\u5019\uff0c\\(D(x)\\)\u8ddf\u6807\u7b7eb\u6216\u8005\u6807\u7b7ec\u7684\u5e73\u65b9\u5dee</p> <p>\u7b2c\u4e8c\u90e8\u5206\uff1a\u5f53 \\(x\\sim p_z(z)\\) \u5206\u5e03\uff08fake\u751f\u6210\u5206\u5e03\uff09\u7684\u65f6\u5019\uff0c\\(D(G(x))\\)\u8ddf\u6807\u7b7ea\u6216\u8005\u6807\u7b7ec\u7684\u5e73\u65b9\u5dee</p> <p>\u539f\u6587\uff1a</p> <p></p> <p>\u4e5f\u63d0\u5230\u4e86\uff0c\u5bf9\u4e8e \\(V_{LSGAN}(G)\\) \u4e2d\u6dfb\u52a0\u4e00\u9879 \\(\\mathbb{E}_{x \\sim p_{data}(x)}[(D(x)-c)^2]\\) \u5e76\u4e0d\u6539\u53d8\u6700\u4f18\u503c\uff0c\u56e0\u4e3a\u6dfb\u52a0\u7684\u4e00\u9879\u5e76\u4e0d\u5305\u542b \u53c2\u6570G</p> <p>\u4f46\u662f\u6dfb\u52a0\u4e00\u9879\uff0c\u4f1a\u65b9\u4fbf\u63a8\u5bfc</p> <p>\u518d\u6b21\u516c\u5f0f\uff1a</p> <p>\\(\\mathrm{min}_DV_{LSGAN}(D) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}(x)}}[(D(x)-b)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-a)^2]\\)</p> <p>\\(\\mathrm{min}_GV_{LSGAN}(G) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}(x)}}[(D(x)-c)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-c)^2]\\)</p> <p></p> <p>\u5f53\u5bf9\u4e8e\u56fa\u5b9a\u7684G\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u5224\u522b\u5668D\uff0c\u5177\u4f53\u7684\u63a8\u5bfc\u8fc7\u7a0b\uff0c\u6362\u5143 \u4ee4 \\(x = G(z)\\) \uff1a</p> <p>\\(z \\sim p_{z}(z))  \\rightarrow  x \\sim p_{g}\\)</p> <p>\u56e0\u6b64</p> <p>\\(\\mathrm{min}_DV_{LSGAN}(D) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}(x)}}[(D(x)-b)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-a)^2]\\)</p> <p>\u63a8\u5bfc\u51fa\uff1a</p> <p>\u671f\u671b\u5199\u6210\u79ef\u5206</p> <p>$\\min_DV_{LSGAN}(D) $</p> <p>\\(= \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}(x)}}[(D(x)-b)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-a)^2]\\)</p> <p>$= \\int_x p_{data}(x)(D(x)-b)^2dx + \\int_z p_z(z)(D(G(z))-a)^2dz $</p> <p>$= \\int_x p_{data}(x)(D(x)-b)^2dx +  p_g(x)(D(x)-a)^2dx $</p> <p>\u79ef\u5206\u53f7\u91cc\u9762\uff0c\u5c55\u5f00\uff0c\u662f\u4e00\u4e2a\u4e8c\u6b21\u65b9\u9879\uff0c\u5173\u4e8e\\(D(x)\\)\u7684\u4e00\u5143\u4e8c\u6b21\u65b9\u7a0b\uff1a</p> <p>\\(p_{data} (D^2(x)-2bD(x)+b^2) + p_g((D^2(x)-2aD(x)+a^2))\\)</p> <p>\\(= (p_{data}+p_g)D^2(x)-2(ap_g+bp_{data})D(x)+(b^2p_{data}+a^2p_g)\\)</p> <p>\u6700\u5c0f\u503c \\(D^*(x)=-\\frac{b}{2a}=-\\frac{-2(ap_g+bp_{data})}{2(p_{data}+p_g)}=\\frac{ap_g+bp_{data}}{p_{data}+p_g}=\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}\\) </p> <p>\u4e5f\u5c31\u662f \u56fa\u5b9a\u751f\u6210\u5668\uff0c\u5f97\u5230\u6700\u4f18\u7684\u5224\u522b\u5668\uff1a</p> <p>\\(D^*(x)= \\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}\\)</p> <p>\uff08\u4ee5\u4e0a\u63a8\u5bfc \u6a21\u4eff\u539f\u59cbGAN\uff1a</p> <p></p> <p>\uff09</p> <p>\u63a5\u4e0b\u6765\uff0c</p> <p></p> <p>\u7528 \\(p_d\\) \u8868\u793a \\(p_{data}\\)</p> <p>\u7136\u540e\u628a \u516c\u5f0f4 \u7684 \\(V_{LSGAN}(G)\\) \u5199\u6210\uff1a</p> <p></p> <p>\u4e0a\u9762 \u6211\u4eec\u4f18\u5316\u5b8c\u4e86 \u5224\u522b\u5668\uff0c\u5904\u7406\u7684\u662f\u7b2c\u4e00\u4e2a\u5f0f\u5b50\uff1a</p> <p>\\(\\mathrm{min}_DV_{LSGAN}(D) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}(x)}}[(D(x)-b)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-a)^2]\\)</p> <p>\u63a5\u4e0b\u6765 \u4f18\u5316\u751f\u6210\u5668\uff0c\u7b2c\u4e8c\u4e2a\u5f0f\u5b50\uff1a</p> <p>\\(\\mathrm{min}_GV_{LSGAN}(G) = \\frac{1}{2}\\mathbb{E}_{x\\sim {p_{data}(x)}}[(D(x)-c)^2] + \\frac{1}{2}\\mathbb{E}_{z\\sim {p_z(z)}}[(D(G(z))-c)^2]\\)</p> <p>\uff081\uff09\u9996\u5148\uff0c\u6309\u7167\u8bba\u6587\u6240\u8bf4\uff0c\\(p_{data}\\) \u5199\u6210 \\(p_d\\)</p> <p>\u8fd8\u6709\u51e0\u70b9\u6ce8\u610f\uff1a</p> <ul> <li>\\(V_{LSGAN}\\) \u6539\u5199\u6210 \\(C(G)\\)  \u8868\u793a \u4ee3\u4ef7 \u662f\u4e00\u4e2a\u610f\u601d</li> <li>\u7b49\u5f0f\u5de6\u53f3\u4e24\u8fb9\u540c\u65f6\\(\u00d72\\)</li> <li>\\(D(x)\\) \u5168\u90e8\u66ff\u6362\u4e3a \\(D^*(x)\\)</li> </ul> <p>\\(\u2234\\)</p> <p>\\(2C(G) = \\mathbb{E}_{x \\sim p_d}[(D^*(x)-c)^2]+\\mathbb{E}_{x \\sim p_z}(D^*(G(z))-c)^2\\)</p> <p>\\(z\\)\u6362\u5143\u6210 \\(G(z)\\)</p> <p>\\(x\u670d\u4ecep_z\\) \u6362\u5143\u6210 \\(x\u670d\u4ecep_g\\) </p> <p>\\(= \\mathbb{E}_{x \\sim p_d}[(D^*(x)-c)^2]+\\mathbb{E}_{x \\sim p_g}(D^*(x)-c)^2\\)</p> <p>\u63a5\u4e0b\u6765 \u4ee3\u5165 \\(D^*(x)=\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}\\)</p> <p>\\(= \\mathbb{E}_{x \\sim p_d}[(\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}-c)^2]+\\mathbb{E}_{x \\sim p_g}(\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}-c)^2\\)</p> <p>\u63a5\u4e0b\u6765 \u5c06 \u671f\u671b\u6539\u5199\u6210\u79ef\u5206\u7684\u5f62\u5f0f\uff1a</p> <p>\\(= \\int_x(\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}-c)^2 p_d(x)dx+\\int_x(\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}-c)^2p_g(x)dx\\)</p> <p>c\u901a\u9879 \u5408\u5e76  \u4ee5 \\(p_g\\) \\(p_{data}\\) \u4e3a\u57fa\u51c6(\\(p_{data}\\)\u548c\\(p_d\\) \u662f\u4e00\u4e2a\u4e1c\u897f\uff0c\u4e0d\u540c\u8bb0\u53f7\uff0c\u540e\u9762\u5c31\u7edf\u4e00)</p> <p>\\(= \\int_x(\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}-c)^2 p_d(x)dx+\\int_x(\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}-c)^2p_g(x)dx\\)</p> <p>\\(= \\int_x(\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}-c\\frac{p_g(x)+p_{data}(x)}{p_{data}(x)+p_g(x)})^2 p_d(x)+(\\frac{ap_g(x)+bp_{data}(x)}{p_{data}(x)+p_g(x)}-c\\frac{p_g(x)+p_{data}(x)}{p_{data}(x)+p_g(x)})^2p_g(x)dx\\)</p> <p>\\(= \\int_x p_d (\\frac{(a-c)p_g+(b-c)p_d}{p_{d}+p_g})^2+p_g(\\frac{(a-c)p_g+(b-c)p_d}{p_d+p_g})^2dx\\)</p> <p>\u5e73\u65b9\u9879 \u76f8\u7b49\uff0c\u5408\u5e76 \uff0c\u540c\u65f6\u548c\u5206\u6bcd\u6d88\u53bb\u4e00\u4e2a \\((p_d + p_g)\\)</p> <p>\\(=\\int_x  \\frac{((a-c)p_g + (b-c)p_d)^2}{p_d+p_g} dx\\)</p> <p>\u5206\u6bcd\u662f\u4e00\u6b21\u9879\uff0c\u5206\u5b50\u662f\u4e8c\u6b21\u9879\uff0c\u5bf9\u5206\u5b50\u5e73\u65b9\u91cc\u9762\u505a\u53d8\u6362\uff0c\u5bf9\\(p_g\\) \u505a \\(-b+b\\) \u7684\u64cd\u4f5c\uff1a</p> <p>\\(=\\int_x  \\frac{((a-b+b-c)p_g + (b-c)p_d)^2}{p_d+p_g} dx\\)</p> <p>\\(=\\int_x  \\frac{((b-c)(p_d+p_g)-(b-a)p_g)^2}{p_d+p_g} dx\\)</p> <p>\u5199\u6210 \u8fd9\u4e2a\u5f62\u5f0f\u4ee5\u540e\uff0c\u4ee4 \\(b-c=1\uff0cb-a=2\\)\uff0c\u521a\u597d\u662f\u76ae\u5c14\u900a\u5f00\u65b9\u6563\u5ea6\u7684\u516c\u5f0f\uff1a</p> <p>\\(2C(G) = \\int_x \\frac{((p_d+p_g)-2p_g)^2}{p_d+p_g}dx\\)</p> <p>\u5e73\u65b9\u91cc\u9762\uff0c\u6539\u53d8 \u9879\u7684\u6b21\u5e8f\uff1a</p> <p>\\(2C(G) = \\int_x \\frac{(2p_g-(p_d+p_g))^2}{p_d+p_g}dx\\)</p> <p>\\(=\\mathcal{X}^2_{Pearson}(p_d+p_g||2p_g)\\)</p> <p></p> <p>\u8868\u793a \\(2p_g\\) \u4e0e \\(p_d + p_g\\) \u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684 \u76ae\u5c14\u900a\u5f00\u65b9\u6563\u5ea6\u516c\u5f0f\uff0c\u4e5f\u5c31\u662f\u8bf4 \u5f53\u6211\u4eec\u5728\u4f18\u5316 \u516c\u5f0f</p> <p></p> <p>\u5f53\u6211\u4eec\u5728\u4f18\u5316\u516c\u5f0f(4)\u7684\u65f6\u5019\uff0c\u76f8\u5f53\u4e8e\u5728\u4f18\u5316\u516c\u5f0f(7)\uff1a\\(2p_g\\) \u548c \\(p_d + p_g\\)\u4e24\u4e2a\u5206\u5e03\u7684 \u76ae\u5c14\u900a\u5f00\u65b9\u6563\u5ea6</p> <p>\u9700\u8981\u6ee1\u8db3\u7684\u6761\u4ef6</p> <p></p> <ul> <li>b-c=1\uff08\u6807\u7b7eb \u51cf\u53bb \u6807\u7b7ec \u7684\u503c=1\uff09</li> <li>b-a=1\uff08\u6807\u7b7eb \u51cf\u53bb \u6807\u7b7ea\u7684\u503c=2\uff09</li> </ul> <p>\u6563\u5ea6 \u4e00\u822c \u5927\u4e8e\u7b49\u4e8e 0\uff0c\u4e5f\u5c31\u662f \\(p_g + p_d =2p_g\\) \u6563\u5ea6\u8fbe\u5230\u6700\u5c0f\uff0c\u7b49\u4ef7\u4e8e \\(p_g = p_d\\) \uff0c\u4e5f\u5c31\u662f\\(\u751f\u6210\u5668\u7684\u6570\u636e\u5206\u5e03=\u8bad\u7ec3\u96c6\u771f\u5b9e\u7684\u6570\u636e\u5206\u5e03\\)\uff0c\u4e5f\u5c31\u662f\u6b64\u65f6 \u751f\u6210\u5668G \u8fbe\u5230\u6700\u4f18</p>"},{"location":"learning/9_cGAN/#34","title":"3.4 \u5b9e\u9645\u7684\u53c2\u6570\u8bbe\u7f6e","text":"<ul> <li>\u786e\u5b9a \u65b9\u7a0b2 \u4e2da\u3001b\u3001c\u4e09\u4e2a\u503c\uff0c\u9700\u8981\u6ee1\u8db3\u6761\u4ef6 \\(b-c=1\\)\uff0c\u5e76\u4e14 \\(b-a=2\\) \uff0c\u6b64\u65f6 \u76f8\u5f53\u4e8e\u4f18\u5316 \\(2p_g\\) \u548c \\(p_d+p_g\\)  \u4e24\u4e2a\u5206\u5e03 \u7684\u76ae\u5c14\u900a\u5f00\u65b9\u6563\u5ea6</li> </ul> <ul> <li>\u6bd4\u5982\u8bbe\u7f6e \\(a=-1\u3001b=1\u3001c=0\\)\uff0c\u6b64\u65f6\u76ee\u6807\u51fd\u6570 \u5199\u6210 \u516c\u5f0f8\u7684\u5f62\u5f0f</li> </ul> <p>\u6b64\u65f6 \u76f8\u5f53\u4e8e\u4f18\u5316\u76ae\u5c14\u900a\u5f00\u65b9\u6563\u5ea6</p> <ul> <li>\u53e6\u5916\u4e00\u79cd\u65b9\u5f0f</li> </ul> <p></p> <p>\u8ba9\u751f\u6210\u5668\u5c3d\u53ef\u80fd\u8ddf\u771f\u5b9e\u6837\u672c\u4e00\u6837\uff0c\u4e5f\u5c31\u662f\u4ee4\\(c=b\\)</p> <p>\u5982\u516c\u5f0f9\u6240\u793a\uff0c\u91c7\u752801\u7f16\u7801\uff1a</p> <p>\uff081\uff09\u4f18\u5316\u751f\u6210\u5668\u7684\u65f6\u5019\uff0c\u4ee4\\(\u771f\u5b9e\u503c=1\\) \u7b49\u4ef7\u4e8e D(x)=1\uff0c\u628aD(G(z))\u4e5f\u5c3d\u53ef\u80fd\u63a5\u8fd1\u771f\u5b9e\u6837\u672c\uff0c\u6240\u4ee5\u6807\u7b7e\u4e5f\u7b49\u4e8e1\uff1b</p> <p>\uff082\uff09\u540c\u65f6\u5728 \u4f18\u5316\u5224\u522b\u5668\u7684\u65f6\u5019\uff0c\u4ee4\\(D(G(Z))=0\u3001D(x)=1\\)</p> <p>\u4ee5\u4e0a \u516c\u5f0f\uff088 \uff09\u548c\u516c\u5f0f\uff089\uff09\uff0c\u90fd\u662fLSGAN\u76ee\u6807\u51fd\u6570\u7684\u5199\u6cd5\uff1a</p> <p></p> <p>\u867d\u7136\u516c\u5f0f\uff088\uff09\u548c\u516c\u5f0f\uff089\uff09\u5b9e\u9a8c\u6548\u679c\u662f\u76f8\u4f3c\u7684\uff0c\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\uff0c\u516c\u5f0f\uff089\uff09\u7684\u5199\u6cd5\u66f4\u5e38\u89c1\uff0c\u5728\u539f\u672c\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4e5f\u662f\u4f7f\u7528\u7684\u516c\u5f0f\uff089\uff09\u8bad\u7ec3\u6a21\u578b</p> <p>\u4ee5\u4e0a\u662fLSGAN\u7684\u539f\u7406</p>"},{"location":"learning/9_cGAN/#5-gan","title":"5 \u6700\u5c0f\u5e73\u65b9GAN\u4ee3\u7801\u5b9e\u73b0","text":"<p>\u4ee3\u7801\u4fee\u6539\uff1a</p> <p>\u539f\u59cbGAN\u7528\u7684BCELoss\uff0cLSGAN\u628aBCELoss\u6539\u4e86\uff0c\u7528MSELoss\uff0c\u5177\u4f53\u5230\u516c\u5f0f\uff089\uff09\uff0c\u5c31\u662f\u9047\u5230\u771f\u5b9e\u6837\u672c\u7528\u6807\u7b7e1\uff0c\u9047\u5230\u751f\u6210\u5668\u751f\u6210\u7684\u6837\u672c\uff0c\u5224\u522b\u5668\u4f18\u5316\\(D(G(z))\\)\u7684\u76ee\u6807\u662f0</p> <p>ls_cgan_minist.py</p> <p>\u5728cgan\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4fee\u6539\uff0c\u53ea\u9700\u8981\u4fee\u6539\u635f\u5931\u51fd\u6570\u5373\u53ef</p> <pre><code>\"\"\" \u57fa\u4e8eMNIST \u5b9e\u73b0\u5bf9\u6297\u751f\u6210\u7f51\u7edc (GAN) \"\"\"\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\n\nimage_size = [1, 28, 28]\nlatent_dim = 96\nbatch_size = 64\nuse_gpu = torch.cuda.is_available()\n\nclass Generator(nn.Module):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            torch.nn.BatchNorm1d(128),\n            torch.nn.GELU(),\n\n            nn.Linear(128, 256),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.GELU(),\n            nn.Linear(256, 512),\n            torch.nn.BatchNorm1d(512),\n            torch.nn.GELU(),\n            nn.Linear(512, 1024),\n            torch.nn.BatchNorm1d(1024),\n            torch.nn.GELU(),\n            nn.Linear(1024, np.prod(image_size, dtype=np.int32)),\n            #  nn.Tanh(),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, z):\n        # shape of z: [batchsize, latent_dim]\n\n        output = self.model(z)\n        image = output.reshape(z.shape[0], *image_size)\n\n        return image\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(np.prod(image_size, dtype=np.int32), 512),\n            torch.nn.GELU(),\n            nn.Linear(512, 256),\n            torch.nn.GELU(),\n            nn.Linear(256, 128),\n            torch.nn.GELU(),\n            nn.Linear(128, 64),\n            torch.nn.GELU(),\n            nn.Linear(64, 32),\n            torch.nn.GELU(),\n            nn.Linear(32, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, image):\n        # shape of image: [batchsize, 1, 28, 28]\n\n        prob = self.model(image.reshape(image.shape[0], -1))\n\n        return prob\n\n# Training\ndataset = torchvision.datasets.MNIST(\"mnist_data\", train=True, download=True,\n                                     transform=torchvision.transforms.Compose(\n                                         [\n                                             torchvision.transforms.Resize(28),\n                                             torchvision.transforms.ToTensor(),\n                                             #  torchvision.transforms.Normalize([0.5], [0.5]),\n                                         ]\n                                                                             )\n                                     )\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\ngenerator = Generator()\ndiscriminator = Discriminator()\n\n\ng_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.4, 0.8), weight_decay=0.0001)\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.4, 0.8), weight_decay=0.0001)\n\n# loss_fn = nn.BCELoss()\nloss_fn = nn.MSELoss()\n\nlabels_one = torch.ones(batch_size, 1)\nlabels_zero = torch.zeros(batch_size, 1)\n\nif use_gpu:\n    print(\"use gpu for training\")\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    loss_fn = loss_fn.cuda()\n    labels_one = labels_one.to(\"cuda\")\n    labels_zero = labels_zero.to(\"cuda\")\n\nnum_epoch = 200\nfor epoch in range(num_epoch):\n    for i, mini_batch in enumerate(dataloader):\n        gt_images, _ = mini_batch\n\n\n        z = torch.randn(batch_size, latent_dim)\n\n        if use_gpu:\n            gt_images = gt_images.to(\"cuda\")\n            z = z.to(\"cuda\")\n\n        pred_images = generator(z)\n        g_optimizer.zero_grad()\n\n        recons_loss = torch.abs(pred_images-gt_images).mean()\n\n        g_loss = recons_loss*0.05 + loss_fn(discriminator(pred_images), labels_one)\n\n        g_loss.backward()\n        g_optimizer.step()\n\n        d_optimizer.zero_grad()\n\n        real_loss = loss_fn(discriminator(gt_images), labels_one)\n        fake_loss = loss_fn(discriminator(pred_images.detach()), labels_zero)\n        d_loss = (real_loss + fake_loss)\n\n        # \u89c2\u5bdfreal_loss\u4e0efake_loss\uff0c\u540c\u65f6\u4e0b\u964d\u540c\u65f6\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u5e76\u4e14\u5dee\u4e0d\u591a\u5927\uff0c\u8bf4\u660eD\u5df2\u7ecf\u7a33\u5b9a\u4e86\n\n        d_loss.backward()\n        d_optimizer.step()\n\n        if i % 50 == 0:\n            print(f\"step:{len(dataloader)*epoch+i}, recons_loss:{recons_loss.item()}, g_loss:{g_loss.item()}, d_loss:{d_loss.item()}, real_loss:{real_loss.item()}, fake_loss:{fake_loss.item()}\")\n\n        if i % 400 == 0:\n            image = pred_images[:16].data\n            torchvision.utils.save_image(image, f\"image_{len(dataloader)*epoch+i}.png\", nrow=4)\n</code></pre> <p>\u76f4\u63a5\u770bloss function\uff0c\u7531BCELoss\u6362\u6210MSELoss()</p> <p></p> <ul> <li>loss function\u4eceBCELoss\u4fee\u6539\u6210MSELoss</li> <li>\u5177\u4f53\u5230\u4f18\u5316\u751f\u6210\u5668\u7684\u65f6\u5019\uff0c\u4f20\u5165\u7684\u4e5f\u662f labels_one</li> </ul> <p></p> <p>\u53ea\u4e0d\u8fc7\u662f\u628a\u4ece\u524d\u7684\u5206\u7c7b\u4efb\u52a1\u53d8\u6210\u4e86\u56de\u5f52\u4efb\u52a1</p> <ul> <li>\u540c\u6837\u5728\u8ba1\u7b97\u5224\u522b\u5668\u7684 real loss\u548cfake loss\u7684\u65f6\u5019\uff0c</li> </ul> <p></p> <p>\u5728\u8ba1\u7b97 real loss\u7684\u65f6\u5019\uff0c\u4e5f\u662f\u4f20\u5165\u51681 \u7684 \u6d6e\u70b9\u578b</p> <p>\u5728\u8ba1\u7b97fake loss\u7684\u65f6\u5019\uff0c\u4f20\u5165\u7684\u662f\u51680\u7684\u6d6e\u70b9\u578b</p> <p>\u4e5f\u5c31\u662f\u628a\u4ece\u524d\u7684\u4e24\u4e2a\u5206\u7c7bloss BCELoss\uff0c\u6539\u6210\u4e24\u4e2a \u5e73\u65b9\u5dee\u7684 \u56de\u5f52loss</p>"},{"location":"learning/convs/","title":"4\u79cd\u5377\u79ef","text":"<ul> <li> \u8f6c\u7f6e\u5377\u79ef\u3001\u53cd\u5377\u79ef</li> <li> \u5206\u7ec4\u5377\u79ef\u3001\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef</li> <li> 1\u00d71\u5377\u79ef\u3001\u9010\u70b9\u5377\u79ef</li> <li> \u81a8\u80c0\u5377\u79ef\u3001\u7a7a\u6d1e\u5377\u79ef\u5377\u79ef</li> <li> \u53ef\u53d8\u5f62\u5377\u79ef</li> <li> \u5927\u6838\u5377\u79ef</li> </ul>"},{"location":"learning/convs/#1","title":"1 \u5e93\u51fd\u6570\u5b9e\u73b0\u5377\u79ef","text":"<ul> <li>\u7c7b\uff1a<code>torch.nn.Conv2d</code></li> <li>\u51fd\u6570\uff1a<code>F.conv2d</code>  or <code>torch.nn.functional.conv2d</code></li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nin_channels = 1\nout_channels = 1\nkernel_size = 3\nbatch_size = 1\nbias = False\n\ninput_size = [batch_size,in_channels,4,4]\n\n# \u7b2c\u4e00\u79cd\u5b9e\u73b0\nconv_layer = torch.nn.Conv2d(in_channels,out_channels,kernel_size,bias=bias)\n\ninput_feature_map = torch.randn(input_size)\nout_feature_map = conv_layer(input_feature_map)\n# print(input_feature_map)\n# print(conv_layer.weight)  # 1*1*3*3=out_channels*in_channels*height*width\n\nprint(out_feature_map)\n\nout_feature_map1 = F.conv2d(input_feature_map,conv_layer.weight)\n\nprint(out_feature_map1)\n</code></pre>"},{"location":"learning/convs/#api","title":"\u9996\u5148\u770b\u4e00\u4e0b \u4e8c\u7ef4\u5377\u79ef\u7684api","text":"<p>\u8c37\u6b4c\u641c\u7d22 pytorch conv2d\uff0c\u51fa\u73b0\u4e24\u4e2aapi \uff1a</p> <ul> <li>\u4e00\u4e2a\u662f\u5927\u5199\u7684\u4e8c\u7ef4\u5377\u79ef\u3001 class</li> <li>\u4e00\u4e2a\u662f torch.nn.functional.conv2d\u5c0f\u5199\u7684\u4e8c\u7ef4\u5377\u79ef\u3001\u51fd\u6570</li> </ul> <p>\u533a\u522b\uff1a</p> <ul> <li> <p>\uff08\u7b2c\u4e00\u4e2a\u533a\u522b\uff09</p> </li> <li> <p>\u7b2c\u4e00\u4e2a\u5927\u5199\u7684\u662f\u4e00\u4e2aclass\uff0c\u5982\u679c\u6211\u4eec\u8981\u7528\u7b2c\u4e00\u4e2a\u7684\u8bdd\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u5bf9\u8fd9\u4e2aclass\u8fdb\u884c\u4e00\u4e2a\u5b9e\u4f8b\u5316\uff0c\u7136\u540e\u5bf9\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\uff0c\u518d\u5bf9\u8f93\u5165\u7279\u5f81\u56fe\u8fdb\u884c\u4e00\u4e2a\u5377\u79ef \u64cd\u4f5c\uff1b  </p> </li> <li> <p>\u7b2c\u4e8c\u4e2a\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e0d\u9700\u8981\u5b9e\u4f8b\u5316\uff0c\u5c31\u76f4\u63a5\u63a5\u6536\u4e00\u4e2a\u8f93\u5165\u7279\u5f81\u56fe\uff0c\u76f4\u63a5\u8fdb\u884c\u4e00\u4e2a\u5377\u79ef\u64cd\u4f5c\uff1b\u4ee5\u4e0a\u662f\u7b2c\u4e00\u4e2a\u533a\u522b\uff1b </p> </li> <li> <p>\uff08\u7b2c\u4e8c\u4e2a\u533a\u522b\uff09</p> </li> <li>class\u53ef\u4ee5\u81ea\u5df1\u53bb\u521b\u5efa\u64cd\u4f5c\uff0c\u5305\u62ecweight\u548cbias\uff0c\u53ef\u4ee5\u81ea\u52a8\u53bb\u521b\u5efa\uff0c\u5c31\u4e0d\u9700\u8981\u624b\u52a8\u521b\u5efa\uff1b</li> <li>\u5bf9\u4e8e\u51fd\u6570\u6765\u8bf4\uff0c \u9700\u8981\u624b\u52a8\u7684\u4f20\u5165weight\u548cbias\uff1b</li> </ul>"},{"location":"learning/convs/#conv2d","title":"CONV2D","text":"<ul> <li>\u8c03\u7528\uff1atorch.nn.Conv2d</li> <li>\u9700\u8981\u4f20\u5165\u7684\u53c2\u6570\uff1a</li> <li>\u8f93\u5165\u901a\u9053</li> <li>\u8f93\u51fa\u901a\u9053</li> <li>kernel\u7684\u5927\u5c0f</li> <li>\u6b65\u957f</li> <li>padding\u586b\u5145</li> <li>\u81a8\u80c0dilation</li> <li> <p>group</p> </li> <li> <p>\u533a\u5206 \u5377\u79ef &amp; \u5168\u8fde\u63a5\uff1a</p> </li> </ul> <p>\u795e\u7ecf\u7f51\u7edc\u6700\u6838\u5fc3\u7684\u4e00\u4e2a\u64cd\u4f5c\uff1a\u4eff\u5c04\u53d8\u6362\uff1a\u5c06\u4e00\u4e2a\u77e9\u9635 \u4e58\u4ee5 \u8f93\u5165\u5411\u91cf \u5f97\u5230 \u53e6\u5916\u4e00\u4e2a\u5411\u91cf\u3002\u8fd9\u662f\u5168\u8fde\u63a5\u7f51\u7edc\u7684\u4e00\u4e2a\u505a\u6cd5\uff0c \u6240\u4ee5\u6211\u4eec\u4e00\u822c\u4f1a\u5bf9\u4e00\u4e2a\u5411\u91cf \u505a\u5168\u8fde\u63a5\u7684\u7f51\u7edc \u7684\u8f93\u5165\uff1b\u6bd4\u65b9\u8bf4\uff1a\u4e00\u4e2aword embedding\u5411\u91cf\uff1b\u6bd4\u65b9\u8bf4 \u8981\u9884\u6d4b\u623f\u4ef7\uff0c\u57ce\u5e02\u7684\u4eba\u53e3\u8fd8\u6709\u7269\u4ef7\u7b49\uff0c\u4e0d\u540c\u7684\u6d6e\u70b9\u6570 \u7ec4\u6210\u7684\u5411\u91cf\uff0c\u8fd9\u4e9b\u90fd\u53ef\u4ee5\u9001\u5165 \u5168\u8fde\u63a5\u7f51\u7edc\u3002</p> <p>\u6240\u4ee5\u5168\u8fde\u63a5\u7f51\u7edc \u662f\u628a \u8f93\u5165\u5f53\u6210\u4e00\u4e2a\u5411\u91cf\uff0c\u7136\u540e\u7edf\u4e00\u7684\u53bb\u4e58 \u4e00\u4e2a\u77e9\u9635\uff0c\u8fdb\u884c\u64cd\u4f5c\u3002\u4f46\u662f\uff0c\u8fd8\u6709\u5f88\u591a\u5176\u4ed6\u4e1c\u897f\uff0c\u4e0d\u80fd\u4ec5\u4ec5\u4f7f\u7528\u4e00\u4e2a\u5411\u91cf\u6765\u8fdb\u884c\u523b\u753b\uff0c\u6bd4\u5982\u56fe\u50cf\u6709\u957f\u5ea6\u548c\u5bbd\u5ea6\uff0c\u662f\u4e00\u4e2a\u4e8c\u7ef4\u7684\uff0c\u8fd8\u6709RGB\u4e09\u4e2a\u901a\u9053\uff0c\u8fd9\u4e9b \u6211\u4eec\u4e0d\u80fd\u4ec5\u4ec5\u53ea\u662f\u628a\u56fe\u7247\u62c9\u76f4\u5904\u7406\uff0c\u8fd9\u6837\u7834\u574f\u4e86\u56fe\u7247\u7684\u7a7a\u95f4\u7ed3\u6784\uff1b</p> <p>\u7c7b\u4f3c\u7684\u8fd8\u6709\u8bed\u97f3\uff0c\u8bed\u8a00\u6709\u65f6\u95f4\u7ef4\u8fd8\u6709\u9891\u7387\u7ef4\uff0c\u6211\u4eec\u6bcf\u4e2a\u65f6\u523b\u53d1\u51fa\u7684\u58f0\u97f3\uff0c \u662f\u7531\u4e0d\u540c\u7684\u9891\u7387\u7ec4\u5408\u7684\uff0c\u540c\u6837\u5bf9\u4e8e\u8bed\u97f3\u8fd9\u79cd\u4fe1\u53f7\uff0c\u6211\u4eec\u4e5f\u4e0d\u80fd\u4ec5\u4ec5\u662f \u5f53\u6210 \u4e00\u7ef4\u4fe1\u53f7\u5904\u7406\uff0c\u751a\u81f3\u66f4\u590d\u6742\u7684\u662f \u56fe\u50cf\u548c\u8bed\u97f3\u4fe1\u53f7\u7684\u7ed3\u5408\uff0c\u6bd4\u5982\u89c6\u9891\u3002\u6240\u4ee5\u5bf9\u4e8e\u8fd9\u4e9b\u6211\u4eec\u4e0d\u80fd\u4ec5\u4ec5\u53ea\u662f\u5f53\u6210\u4e00\u4e2a\u5411\u91cf\u5904\u7406\uff0c\u8fd9\u6837\u7684\u8bdd\uff0c\u5168\u8fde\u63a5\u7f51\u7edc\u4e5f\u5c31\u65e0\u6cd5\u523b\u753b\u5b83\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u5377\u79ef\u7f51\u7edc\u523b\u753b\uff0c\u5bf9\u4e8e\u5377\u79ef\u7f51\u7edc \u548c \u54ea\u4e9b\u64cd\u4f5c \u6bd4\u8f83\u76f8\u5173\u5462\uff1f\u5c31\u662f\u4e92\u76f8\u5173\uff0c\u5982\u679c\u5b66\u8fc7\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u7684\u8bdd\uff0c\u4e92\u76f8\u5173\u5c31\u662f \u5bf9\u4e8e\u4e24\u4e2a\u4e00\u7ef4\u5411\u91cf\uff0c\u6211\u4eec\u628a\u4e00\u4e2a\u4e00\u7ef4\u4fe1\u53f7 \u6cbf\u7740 \u53e6\u5916\u4e00\u4e2a\u4e00\u7ef4\u4fe1\u53f7\uff0c\u4e0d\u65ad\u5730\u8fdb\u884c \u6ed1\u52a8\u76f8\u4e58\u7684\u64cd\u4f5c\uff0c\u7136\u540e\u8ba1\u7b97 \u4e00\u4e2a\u76f8\u5173\u7cfb\u6570\u3002\u5377\u79ef\u4e5f\u662f\u7c7b\u4f3c\u7684\uff0c\u5bf9\u4e8e\u4e00\u5f20\u56fe\u7247\uff0c\u5982\u679c\u6211\u4eec\u6709\u4e00\u4e2a\u5377\u79ef\u6838\u7684\u8bdd\uff0c\u53eb\u505akernel\uff0c\u6211\u4eec\u4f1a\u628a kernel \u6cbf\u7740 \u56fe\u7247\u7684\u4e0d\u540c\u533a\u57df \u8fdb\u884c\u4e00\u4e2a\u6ed1\u52a8\u76f8\u4e58\uff0c\u6765\u5f97\u5230\u4e00\u4e2a\u7279\u5f81\u7684\u8868\u793a</p> <ul> <li>\u6570\u5b66\u4f8b\u5b50\uff1a</li> </ul> <p></p> <ul> <li>\u5047\u8bbe\u6211\u4eec\u7684input feature map=4\u00d74\uff0ckernel=3\u00d73\uff0c\u5377\u79ef\u64cd\u4f5c\u5c31\u662f\u5c06kernel\u5728\u56fe\u7247\u4e0a \u4e0d\u540c\u4f4d\u7f6e\u5143\u7d20\u76f8\u4e58 element-wise\uff0c\u4e0d\u540c\u4f4d\u7f6e\u5143\u7d20\u76f8\u4e58\u518d\u76f8\u52a0\uff0c\u5f97\u5230\u8f93\u51fa\uff1b</li> <li>k=3\uff0cp=0\uff0cs=1</li> <li>kernel\u7684\u79fb\u52a8\u8f68\u8ff9\u662fZ\u5b57\u578b\u7684\uff0c\u4ece\u5de6\u5230\u53f3\uff0c\u4ece\u4e0a\u5230\u4e0b</li> <li>\u8f93\u5165input future map\u7684\u5927\u5c0f\u662f4\u00d74\u7684\uff0c\u800c\u4e14 channel=1\uff0c\u518d\u7528\u4e00\u4e2a3\u00d73\u7684kernel\uff0c\u4e0e\u8f93\u5165\u7279\u5f81\u56fe \u8fdb\u884c\u5377\u79ef\u64cd\u4f5c\uff0c\u5f97\u5230output\uff0c\u5e76\u4e14output\u5927\u5c0f 2\u00d72\uff0cchannel=1\uff0c\u540c\u65f6\u8fd9\u91cc\u6211\u4eec\u8bbe\u7f6e\u7684bias=False\uff0c\u4e0d\u52a0 bias\uff1b</li> <li>\u5982\u679c\u6211\u4eec\u52a0\u5165 bias\u5462\uff1f</li> <li>\u5982\u679c channel=1\uff0c\u90a3\u4e48 bias\u5c31\u662f\u4e00\u4e2a\u6807\u91cf\uff0c\u76f4\u63a5\u76f8\u52a0\u5c31\u597d\u4e86\uff0c\u8fd9\u5c31\u662f\u4e00\u4e2a bias\u7684\u64cd\u4f5c</li> <li>\u5982\u679c \u8f93\u5165\u7684\u901a\u9053\u6570\u4e0d\u6b62\u662f1\u5462\uff1f\u6bd4\u5982\u4e24\u4e2a\u901a\u9053\uff0c\u8fd9\u4e2a\u65f6\u5019 \u5c31\u4f1a\u6709\u4e24\u4e2akernel\uff0c\u7b2c\u4e00\u4e2akernel\u5f97\u5230y1 y2 y3 y4\uff1b\u7b2c\u4e8c\u4e2akernel\u53c8\u4f1a\u5f97\u5230\u4e00\u4e2ay1\uff0cy2,y3,y4,\u7136\u540e\u6211\u4eec\u518d\u628a\u4e24\u4e2akernel\u5f97\u5230\u7684\u8f93\u51fa \u518d\u8fdb\u884c\u4e00\u4e2a\u70b9\u5bf9\u70b9\u7684\u8f93\u51fa\uff0c\u8fd9\u6837\u5f97\u5230 \u6700\u7ec8\u7684output\uff0c\u8fd9\u662f\u5bf9\u8f93\u5165\u7279\u5f81\u56fe\u6709\u591a\u4e2a\u901a\u9053\u7684\u60c5\u51b5\u3002\uff08\u6362\u4e00\u79cd\u8bf4\u6cd5\uff1a\u8f93\u5165\u901a\u9053\u7684channel\u6709\u51e0\u4e2a\uff0ckernel\u7684channel\u5c31\u6709\u51e0\u4e2a\uff09</li> <li>\u90a3\u5982\u679c\u6211\u4eec \u8f93\u51fa \u7279\u5f81\u56fe \u4e5f\u6709\u591a\u4e2a\u901a\u9053\u7684\u60c5\u51b5 \u4f1a\u600e\u4e48\u5904\u7406\u5462\uff1f \u521a\u521a \u6211\u4eec\u5f97\u5230\u4e86\u7b2c\u4e00\u4e2a\u901a\u9053\uff0c\u5bf9\u4e8e\u7b2c\u4e8c\u4e2a\u901a\u9053\uff0c\u6211\u4eec\u540c\u6837 \u5728\u53e6\u5916\u521b\u9020 \u4e0d\u540c\u7684kernel\uff0c\u5bf9\u8f93\u5165\u8fdb\u884c\u4e00\u4e2a\u5377\u79ef\u64cd\u4f5c\uff0c\u6700\u540e\u628a \u8f93\u5165\u7684\u901a\u9053 \u52a0\u8d77\u6765\uff0c\u53d8\u6210 \u8f93\u51fa \u901a\u9053\u7684\u7b2c\u4e8c\u4e2a\u8f93\u51fa\uff08\u8fd8\u662f\u7406\u89e3\u4e3a\uff1a\u6709\u51e0\u4e2akernel\u5c31\u6709\u51e0\u4e2a\u8f93\u51fa\uff1bkernel\u7684\u901a\u9053\u6570\u7531\u8f93\u5165\u7684\u901a\u9053\u6570\u51b3\u5b9a\uff09</li> </ul> <p>\u4ee5\u4e0a\u662f\u6240\u6709 \u5377\u79ef\u7684\u8fc7\u7a0b\uff1a</p> <ul> <li>\u6709\u51e0\u4e2a\u5377\u79ef\u6838 \u5c31\u6709\u51e0\u4e2a \u8f93\u51fa\u901a\u9053\uff1b</li> <li> <p>\u5355\u4e2a\u5377\u79ef\u6838\u7684\u901a\u9053\u6570 \u53d6\u51b3\u4e8e \u8f93\u5165\u7279\u5f81\u56fe\u7684\u901a\u9053\u6570</p> </li> <li> <p>\u6211\u4eec\u5c06 3\u00d73\u7684kernel\uff0c\u5728\u8f93\u5165\u7684\u7279\u5f81\u56fe\u4e0a \u8fdb\u884c\u4e00\u4e2aZ\u5b57\u578b\u7684\u6ed1\u52a8\u76f8\u4e58\u7684\u64cd\u4f5c</p> </li> <li>==\uff08\u62c9\u76f4\u6ed1\u52a8\u8f93\u5165\u533a\u57df\uff09==\u5176\u5b9e\u8fd9\u91cc\u7684\u6ed1\u52a8\u76f8\u4e58 \u53ef\u4ee5\u7406\u89e3\u4e3a \u5982\u679c\u628a\u8f93\u5165\u7684\u7279\u5f81\u56fe\uff08\u88ab\u5377\u79ef\u6838\u8986\u76d6\u7684\u533a\u57df\uff093\u00d73\u7684\u533a\u57df \u62c9\u6210\u4e00\u4e2a\u5411\u91cf\u7684\u8bdd \u7136\u540e\u6211\u4eec\u628akernel\u4e5f\u62c9\u6210\u4e00\u4e2a\u5411\u91cf\uff0c\u5176\u5b9e\u5c31\u662f\u8ba1\u7b97 \u4e24\u4e2a\u5411\u91cf\u7684 \u4e00\u4e2a\u5185\u79ef\u3002\u5185\u79ef\u8d8a\u5927 \u4e24\u4e2a\u5411\u91cf \u8d8a\u76f8\u4f3c\u3002</li> <li>\u6240\u4ee5\u5377\u79ef\u7f51\u7edc \u5b66\u4e60\u7684\u662f\u4ec0\u4e48\u5462\uff1f\u5377\u79ef\u7f51\u7edc \u4f1a \u4e0d\u65ad\u7684\u66f4\u65b0 kernel\u548c bias\u3002\u5c31\u662f\u4e3a\u4e86\u5b66\u5230\uff1a</li> <li>\u6bd4\u65b9\u8bf4 \u4eba\u8138\u8bc6\u522b\uff0c\u5c31\u5e0c\u671bkernel\u80fd\u591f\u5b66\u5230 \u80fd\u591f\u53cd\u6620\u4eba\u8138\u7684 \u7279\u5f81\uff0c\u7136\u540e\u628akernel\u5bf9\u56fe\u7247\u7684\u4e0d\u540c\u533a\u57df\uff0c\u8fdb\u884c\u6bd4\u5bf9\uff0c\u5982\u679c\u521a\u597d\u53d1\u73b0\uff0c\u56fe\u7247\u7684\u67d0\u4e00\u4e2a\u533a\u57df\u521a\u597d\u4e0e\u4eba\u8138\u7684kernel\u5f88\u76f8\u4f3c\u7684\u8bdd\uff0c\u90a3\u5c31\u8bf4\u660e\u4f60\u7ed9\u6211\u4eec\u5df2\u7ecf\u627e\u5230\u4eba\u8138\u4e86\uff0c\u603b\u4e4b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u662f \u7ed9\u5b9a\u4e00\u4e2a\u76ee\u6807 \u4e0d\u65ad\u7684\u5b66\u4e60kernel\uff0c\u6700\u7ec8\u5e0c\u671bkernel\uff0c\u80fd\u591f\u8ddf\u56fe\u7247\u7684\u67d0\u4e00\u4e2a\u533a\u57df\uff0c\u76f8\u4f3c\u5ea6\u8fbe\u5230\u4e00\u4e2a\u6bd4\u8f83\u9ad8\u7684\u503c\uff0c\u5f97\u5230\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684\u7279\u5f81\uff0c\u7136\u540e\u518d\u4e0d\u65ad\u7684\u5f80 \u6df1\u5c42\u53bb\u4f20</li> </ul> <p>\u4f7f\u7528api\u7684\u65f6\u5019\uff0c\u9700\u8981\u6ce8\u610f\ud83d\udce2</p> <ul> <li> <p>Conv2d\u9ed8\u8ba4\u8f93\u5165\u662f4\u7ef4\u7684\uff0c\u7b2c\u4e00\u7ef4\u662fbatch size\u7ef4\uff0c\u6211\u4eec\u8bbe\u7f6ebatch size=1\uff0c\u5e76\u6dfb\u52a0\u5230input_size\u5373\u53ef;</p> </li> <li> <p>input feature map\u7684\u5f62\u72b6\uff1abatch size \u00d7 \u901a\u9053\u6570 \u00d7 \u9ad8 \u00d7 \u5bbd \u53ef\u4ee5\u67e5\u770b\u5b98\u7f51 \u627e\u5230\u9700\u8981\u7684\u8f93\u5165\u5f62\u72b6</p> </li> </ul> <p></p> <ul> <li>\u5e76\u4e14\u6253\u5370 \u5377\u79ef\u5c42\u7684 weight\uff0c\u4e5f\u5c31\u662fkernel\uff0c\u8fd8\u53ef\u4ee5\u6253\u5370\u8f93\u5165\u548c\u8f93\u51fa</li> </ul> <p></p> <ul> <li> <p>\u8f93\u51fa\u4e09\u4e2a\u5f20\u91cf \u7b2c\u4e00\u4e2a\u662f \u8f93\u5165\u7279\u5f81\u56fe\u3001\u7b2c\u4e8c\u4e2a\u662f\u5377\u79ef\u7684weight\u3001\u6216\u8005kernel\uff0c\u7b2c\u4e09\u4e2a\u662f \u5377\u79ef\u7684\u8f93\u51fa</p> </li> <li> <p>\u8f93\u51fa\u7684\u5927\u5c0f\u662f 1\u00d71\u00d74\u7684\uff1b</p> </li> <li> <p>kernel\u662f1\u00d71\u00d73\u00d73 \u6743\u91cd\u5c31\u662fout channel\u00d7 input channel\u00d7height\u00d7width</p> </li> </ul> <p>\u4e5f\u5c31\u662f\u8bf4 \u5bf9\u4e8e \u4e8c\u7ef4\u5377\u79ef\uff0cweight\u662f4\u7ef4\u7684\uff0c\u90a3\u4e48\u603b\u7684\u6570\u76ee \u7b49\u4e8e \u8f93\u51fa\u901a\u9053\u6570\u00d7\u8f93\u5165\u901a\u9053\u6570\u00d7\u5377\u79ef\u6838\u7684\u9ad8\u5ea6\u00d7\u5377\u79ef\u6838\u7684\u5bbd\u5ea6\uff0c\u5982\u679c\u6211\u4eec\u8ba4\u4e3a \u5377\u79ef\u6838\u662f\u4e00\u4e2a\u4e8c\u7ef4\u7684\u56fe\u7247\u7684\u8bdd\uff0c\u90a3\u4e48\u4e00\u5171\u6709 \u8f93\u5165\u901a\u9053\u6570 \u00d7 \u8f93\u51fa\u901a\u9053\u6570 \u8fd9\u4e48\u591a\u4e2a  \u5377\u79ef\u6838\u56fe\u7247</p> <ul> <li> <p>torch.nn.Conv2d(class \u7684api)</p> </li> <li> <p>functional\u7684api(\u51fd\u6570\u7684api)</p> </li> </ul> <p></p> <p>\u5bf9\u4e8e\u8fd9\u4e2aapi \u6211\u4eec\u9700\u8981\u624b\u52a8\u7684\u6307\u5b9a weight \u548c bias\uff0c\u4e3a\u4e86\u9a8c\u8bc1\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u628a\u521a\u521a\u7684weight\u4f20\u5165\uff0c\u53ef\u4ee5\u770b\u5230 \u7ed3\u679c\u662f\u4e00\u6837\u7684:</p> <pre><code>output_feature_map1 = F.conv2d(input_feature_map,conv_layer.weight)\n</code></pre> <ul> <li>kernel\u5c31\u662f\u5728\u8bad\u7ec3\u4e2d\uff0c\u4e0d\u65ad\u66f4\u65b0\u7684</li> </ul>"},{"location":"learning/convs/#2","title":"2 \u624b\u6495\u666e\u901a\u5377\u79ef","text":"<p>\u4ece\u4e24\u79cd\u89d2\u5ea6\u770b\u5377\u79ef\uff1a</p> <ul> <li>\u628a\u5377\u79ef\u770b\u6210\u662f \u9996\u5148\u5bf9\u8f93\u5165\u7279\u5f81\u56fe\u8fdb\u884c\u5c55\u5f00\uff0c\u7136\u540e\u518d\u8fdb\u884c\u77e9\u9635\u7684\u76f8\u4e58\uff1b</li> <li>\u5bf9kernel\u6216\u8005filter\u8fdb\u884c\u5c55\u5f00\uff0c\u7136\u540e\u518d\u8fdb\u884c\u77e9\u9635\u76f8\u4e58\uff1b</li> </ul> <ul> <li>\u6709\u4e86\u8fd9\u79cd\u65b9\u6cd5 \u53ef\u4ee5\u987a\u5176\u81ea\u7136\u7684\u5f15\u51fa \u8f6c\u7f6e\u5377\u79ef\uff1b\u4e4b\u540e\u4f1a\u8bb2 \u8f6c\u7f6e\u5377\u79ef \u4e5f\u79f0\u4e3a\u53cd\u5377\u79ef\uff0c\u4f46\u662f\u53cd\u5377\u79ef\u7684\u8bf4\u6cd5\u4e0d\u592a\u51c6\u786e\uff0c\u56e0\u4e3a \u8f6c\u7f6e\u5377\u79ef\u867d\u7136\u8bf4\u662f\u4e0a\u91c7\u6837\uff0c\u4f46\u662f\u4e0d\u80fd\u4eceoutput\u53bb\u6062\u590dinput\uff0c\u8f6c\u7f6e\u5377\u79ef \u6062\u590d\u7684\u53ea\u662f input\u7684\u5f62\u72b6\uff0c\u4e0d\u662finput\u7684\u5143\u7d20\u503c</li> <li>\u66f4\u51c6\u786e\u7684\u5b9a\u4e49 \u5c31\u662f\u8f6c\u7f6e\u5377\u79ef\uff1b\u4e3a\u4ec0\u4e48\u53eb\u8f6c\u7f6e\u5377\u79ef\u5462\uff1f\u518d\u8bf4\u5b8c \u5bf9kernel \u8fdb\u884c\u5c55\u5f00\uff0c\u518d\u8fdb\u884c\u77e9\u9635\u76f8\u4e58 \u5c31\u660e\u767d\u4e86</li> <li>\u5f53\u6211\u4eec\u628a\u5e38\u89c4\u7684\u5377\u79ef \u770b\u6210\u662f\u5bf9kernel\u7684\u5c55\u5f00\uff0c\u7136\u540e\u518d\u77e9\u9635\u76f8\u4e58\u7684\u8bdd\uff0c\u90a3\u4e48\u8f6c\u7f6e\u5377\u79ef\u53ef\u4ee5\u770b\u6210 \u5c06kernel\u8fdb\u884c\u4e00\u4e2a \u8f6c\u7f6e\u64cd\u4f5c\uff0c\u7136\u540e\u518d\u8fdb\u884c\u77e9\u9635\u76f8\u4e58\uff0c\u5c31\u80fd\u5f97\u5230\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa</li> </ul> <pre><code>input = torch.randn(5,5) # \u5377\u79ef \u8f93\u5165\u7279\u5f81\u56fe\nkernel = torch.randn(3,3) # \u5377\u79ef\u6838\nbias = torch.randn(1) # \u5377\u79ef\u504f\u7f6e\uff0c\u9ed8\u8ba4\u8f93\u51fa\u901a\u9053\u6570\u76ee\u7b49\u4e8e1\n\n# step1 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5148\u4e0d\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6\ndef matrix_multiplication_for_conv2d(input,kernel,bias=0,stride=1,padding=0):\n\n  if padding &gt;0:\n    input = F.pad(input,(padding,padding,padding,padding))\n\n\n  input_h,input_w = input.shape\n  kernel_h,kernel_w = kernel.shape\n\n  output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u9ad8\u5ea6\n  output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u5bbd\u5ea6 \n  output = torch.zeros(output_h,output_w) # \u521d\u59cb\u5316 \u8f93\u51fa\u77e9\u9635\n\n  for i in range(0,input_h - kernel_h + 1,stride): # \u5bf9\u9ad8\u5ea6\u8fdb\u884c\u904d\u5386\n    for j in range(0,input_w - kernel_w +1,stride):  # \u5bf9\u5bbd\u5ea6\u7ef4\u8fdb\u884c\u904d\u5386\n      region = input[i:i+kernel_h, j:j+kernel_w]  # \u53d6\u51fa\u88ab\u6838\u6ed1\u52a8\u5230\u7684\u533a\u57df\n      output[int(i/stride),int(j/stride)] = torch.sum(region * kernel) + bias # \u70b9\u4e58 \u5e76\u8d4b\u503c\u7ed9\u8f93\u51fa\u4f4d\u7f6e\u7684\u5143\u7d20 \n\n  return output\n\n\n# step2 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5148\u4e0d\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6\uff0cflatten\u7248\u672c\ndef matrix_multiplication_for_conv2d_flatten(input,kernel,bias=0,stride=1,padding=0):\n\n  if padding &gt;0:\n    input = F.pad(input,(padding,padding,padding,padding))\n\n\n  input_h,input_w = input.shape\n  kernel_h,kernel_w = kernel.shape\n\n  output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u9ad8\u5ea6\n  output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u5bbd\u5ea6 \n  output = torch.zeros(output_h,output_w) # \u521d\u59cb\u5316 \u8f93\u51fa\u77e9\u9635\n\n  region_matrix = torch.zeros(output.numel(),kernel.numel()) #\u5b58\u50a8\u7740\u6240\u6709\u62c9\u5e73\u540e\u7279\u5f81\u533a\u57df\n  kernel_matrix = kernel.reshape(kernel.numel(),1) # \u5b58\u50a8\u7740kernel\u7684 \u5217\u5411\u91cf\uff08\u77e9\u9635\uff09\u5f62\u5f0f\n  row_index = 0\n\n  for i in range(0,input_h - kernel_h + 1,stride): # \u5bf9\u9ad8\u5ea6\u8fdb\u884c\u904d\u5386\n    for j in range(0,input_w - kernel_w +1,stride):  # \u5bf9\u5bbd\u5ea6\u7ef4\u8fdb\u884c\u904d\u5386\n      region = input[i:i+kernel_h, j:j+kernel_w]  # \u53d6\u51fa\u88ab\u6838\u6ed1\u52a8\u5230\u7684\u533a\u57df\n      region_vector = torch.flatten(region)\n      region_matrix[row_index] = region_vector\n      row_index +=1\n\n  output_matrix = region_matrix @ kernel_matrix\n  output = output_matrix.reshape((output_h,output_w))+bias\n\n  return output\n\n\n# \u77e9\u9635\u8fd0\u7b97\u5b9e\u73b0\u5377\u79ef\u7684\u7ed3\u679c\nmat_mul_conv_output = matrix_multiplication_for_conv2d(input,kernel,bias = bias,stride=2,padding=1)\n# print(mat_mul_conv_output)\n\n# \u8c03\u7528pytorch api\u5377\u79ef\u7684\u7ed3\u679c\npytorch_api_conv_output = F.conv2d(input.reshape((1,1,input.shape[0],input.shape[1])),\n                                   kernel.reshape((1,1,kernel.shape[0],kernel.shape[1])),\n                                   padding=1,bias=bias,stride=2).squeeze(0).squeeze(0)\n\n# \u77e9\u9635\u8fd0\u7b97\u5b9e\u73b0\u5377\u79ef\u7684\u7ed3\u679c flatten input\u7248\u672c\nmat_mul_conv_output_flatten = matrix_multiplication_for_conv2d_flatten(input,kernel,bias = bias,stride=2,padding=1)\n# \u9a8c\u8bc1\u4e86 flatten\u7248\u672c\u5377\u79ef \u4e0e pytorch \u5b98\u65b9\u5377\u79ef\u7684\u7ed3\u679c\uff0c\u6b63\u786e\nflag1 = torch.allclose(mat_mul_conv_output,pytorch_api_conv_output)\nflag2 = torch.allclose(mat_mul_conv_output_flatten,pytorch_api_conv_output)\nprint(flag1)\nprint(flag2)\n</code></pre> <pre><code># step3 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6\ndef matrix_multiplication_for_conv2d_full(input,kernel,bias=0,stride=1,padding=0):\n\n  # input kernel \u90fd\u662f4\u7ef4\u5f20\u91cf\n  if padding &gt;0:\n    input = F.pad(input,(padding,padding,padding,padding,0,0,0,0))\n\n  bs,in_channel,input_h,input_w = input.shape\n  out_channel,in_channel,kernel_h,kernel_w = kernel.shape\n\n  if bias is None:\n    bias = torch.zeros(out_channel)\n\n\n  output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u9ad8\u5ea6\n  output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u5bbd\u5ea6 \n  output = torch.zeros(bs,out_channel,output_h,output_w) # \u521d\u59cb\u5316 \u8f93\u51fa\u77e9\u9635\n\n\n  for ind in range(bs):\n    for oc in range(out_channel):\n      for ic in range(in_channel):\n        for i in range(0,input_h - kernel_h + 1,stride): # \u5bf9\u9ad8\u5ea6\u8fdb\u884c\u904d\u5386\n          for j in range(0,input_w - kernel_w +1,stride):  # \u5bf9\u5bbd\u5ea6\u7ef4\u8fdb\u884c\u904d\u5386\n            region = input[ind,ic,i:i+kernel_h, j:j+kernel_w]  # \u53d6\u51fa\u88ab\u6838\u6ed1\u52a8\u5230\u7684\u533a\u57df\n            output[ind,oc,int(i/stride),int(j/stride)] += torch.sum(region * kernel[oc,ic]) # \u70b9\u4e58 \u5e76\u8d4b\u503c\u7ed9\u8f93\u51fa\u4f4d\u7f6e\u7684\u5143\u7d20 \n      output[ind,oc] += bias[oc]\n  return output\n\ninput = torch.randn(2,2,5,5)  # bs*in_channel*in_h*in_w\nkernel = torch.randn(3,2,3,3) # out_channel*in_channel*kernel_h*kernel_w\nbias = torch.randn(3)\n\n# \u9a8c\u8bc1matrxi_multiplication_for_conv2d_full\u4e0e\u5b98\u65b9API\u7ed3\u679c\u662f\u5426\u4e00\u81f4\npytorch_api_conv_output = F.conv2d(input,kernel,bias=bias,padding=1,stride=2)\nmm_conv2d_full_output = matrix_multiplication_for_conv2d_full(input,kernel,bias=bias,padding=1,stride=2)\nflag = torch.allclose(pytorch_api_conv_output,mm_conv2d_full_output)\nprint(\"all close:\",flag)\n</code></pre>"},{"location":"learning/convs/#3","title":"3 \u8f6c\u7f6e\u5377\u79ef","text":"<p>\u4ee3\u7801\u5b9e\u73b0\uff1a</p> <pre><code># step4 \u901a\u8fc7\u5bf9kernel\u8fdb\u884c\u5c55\u5f00\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5e76\u63a8\u5bfc\u51fa\u8f6c\u7f6e\u5377\u79ef\uff0c\u4e0d\u8003\u8651batch\u3001channel\u5927\u5c0f\uff0c\u4e0d\u8003\u8651padding\uff0c\u5047\u8bbestride=1\ndef get_kernel_matrix(kernel,input_size):\n    # \u57fa\u4e8ekernel\u548c\u8f93\u5165\u7279\u5f81\u56fe\u7684\u5927\u5c0f\u6765\u5f97\u5230\u586b\u5145\u62c9\u76f4\u540e\u7684kernel\u5806\u53e0\u540e\u7684\u77e9\u9635\n    kernel_h,kernel_w = kernel.shape\n    input_h,input_w = input.shape\n    num_out_fea_map = (input_h-kernel_h+1)*(input_w-kernel_w+1)  # \u5377\u79ef\u516c\u5f0f\n    result = torch.zeros((num_out_fea_map,input_h*input_w)) #\u521d\u59cb\u5316\u7ed3\u679c\u77e9\u9635\uff0c\u8f93\u51fa\u7279\u5f81\u56fe\u5143\u7d20\u4e2a\u6570*\u8f93\u5165\u7279\u5f81\u56fe\u5143\u7d20\u4e2a\u6570\n    count = 0\n    for i in range(0,input_h-kernel_h+1,1):\n        for j in range(0,input_w - kernel_w +1,1):\n            # \u586b\u5145\u6210 \u8ddf \u8f93\u5165\u7279\u5f81\u56fe\u4e00\u6837\u5927\u5c0f\n            # padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j))\n            padded_kernel = F.pad(kernel,(j,input_h-kernel_h-j,i,input_w-kernel_w-i))\n            result[count] = padded_kernel.flatten()\n            count +=1\n    return result  \n\n\n\n# \u6d4b\u8bd51\uff1a\u9a8c\u8bc1 \u4e8c\u7ef4\u5377\u79ef\nkernel = torch.randn(3,3)\ninput = torch.randn(4,4)\nkernel_matrix = get_kernel_matrix(kernel,input.shape)  # 4*16\n\n# \u901a\u8fc7\u77e9\u9635\u76f8\u4e58\u6765\u8ba1\u7b97\u5377\u79ef\nmm_conv2d_output = kernel_matrix @ input.reshape((-1,1))  \n\n# pytorch conv2d API\npytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0),kernel.unsqueeze(0).unsqueeze(0))\n# print(kernel)\n# print(kernel_matrix)\n# print(mm_conv2d_output)\n# print(pytorch_conv2d_output)\n\n# \u6d4b\u8bd52  \u901a\u8fc7\u77e9\u9635\u4e58\u79ef\u6765\u8ba1\u7b97\u8f6c\u7f6e\u5377\u79ef || \u9a8c\u8bc1\u4e8c\u7ef4\u8f6c\u7f6e\u5377\u79ef\nmm_transposed_conv2d_output = kernel_matrix.transpose(-1,-2) @ mm_conv2d_output\npytorch_transposed_conv2d_conv2d = F.conv_transpose2d(pytorch_conv2d_output,kernel.unsqueeze(0).unsqueeze(0))  #API\nprint(mm_transposed_conv2d_output.reshape(4,4))\nprint(pytorch_transposed_conv2d_conv2d)\n</code></pre> <pre><code>tensor([[ 0.9213, -4.1975, -2.0054,  1.9133],\n        [ 1.1103,  6.4068, -3.9560, -1.6305],\n        [-3.2193,  3.4451,  0.5374, -2.8065],\n        [ 0.5796, -3.2003,  3.8138,  0.9070]])\ntensor([[[[ 0.9213, -4.1975, -2.0054,  1.9133],\n          [ 1.1103,  6.4068, -3.9560, -1.6305],\n          [-3.2193,  3.4451,  0.5374, -2.8065],\n          [ 0.5796, -3.2003,  3.8138,  0.9070]]]])\n</code></pre>"},{"location":"learning/convs/#torchunfold-api","title":"torch.unfold api","text":"<p>\u67e5\u5b98\u7f51\uff0c\u770b\u5177\u4f53\u7528\u6cd5\uff1a</p> <p></p>"},{"location":"learning/convs/#_1","title":"\u5b9e\u4f8b\u8bb2\u89e3","text":"<p>\u9010\u884c\u89e3\u91ca\uff1a</p> <ul> <li>\u7b2c\u4e00\u884c\uff0c\u5b9e\u4f8b\u5316 Unfold\u64cd\u4f5c\uff0c\u8fd9\u91cc\u8c03\u7528\u7684\u662fnn.Unfold\uff0c\u7136\u540e\u4f20\u5165 kernel size\uff0ckernel size\u662f2\u00d73\u7684</li> <li>\u7b2c\u4e8c\u884c\uff0c\u7136\u540e\u5b9a\u4e49input\uff0c\u4f20\u5165 2\u00d75\u00d73\u00d74\u7684\u5f20\u91cf</li> <li>\u518d\u628ainput\u4f5c\u4e3aunfold\u7684\u8f93\u5165\uff0c\u4f20\u8fdb\u53bb\u5f97\u5230output</li> <li>\u5f97\u5230output\u7684\u5f62\u72b6\uff1a2\u00d730\u00d74</li> </ul> <p>\u89e3\u91caoutput\u7684\u5f62\u72b6\uff1a</p> <ul> <li> <p>\u6bcf\u4e2apatch\u5305\u542b\u4e8630\u4e2a\u6570\u503c\uff0c\u4e3a\u4ec0\u4e48\u662f30\u4e2a\u6570\u503c\uff1f\u5c31\u662f\u56e0\u4e3a\u8fd9\u91ccinput\u7684\u5f62\u72b62\u00d75\u00d73\u00d74</p> </li> <li> <p>2\u662fbatch size</p> </li> <li> <p>5\u662f input channel</p> </li> <li> <p>3\u548c4\u5206\u522b\u662f input\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6</p> </li> <li> <p>\u5982\u679c\u6211\u4eec\u5bf9input \u628a\u6bcf\u4e00\u6b21 \u5377\u79ef\u7684\u5757 \u62ff\u51fa\u6765\u7684\u7684\u8bdd\uff0c\u90a3\u4e48\u4e00\u5171\u662f 2\u00d73\u00d75 \u8fd9\u4e48\u591a\u4e2a\u503c</p> <p>\u4e3a\u4ec0\u4e48\u662f\u8fd9\u4e48\u591a\u4e2a\u503c\u5462\uff1f\u9996\u51482\u00d73\u662fkernel size\u7684\u9762\u79ef\uff0c\u7136\u540e\u7531\u4e8e input\u67095\u4e2achannel\uff0c\u5176\u5b9e\u8fd9\u4e2a\u662f\u628achannel\u4e00\u8d77\u8003\u8651\u8fdb\u6765\u4e86\uff0c\u90a3\u6bcf\u4e2apatch\u5c31\u670930\u4e2a\u503c\uff1b</p> </li> <li> <p>\u7136\u540e\u6211\u4eec\u8fd9\u91cc \u8f93\u5165\u5927\u5c0f\u662f 3\u00d74\uff0c\u800ckernel size\u662f2\u00d73\u7684\uff0c\u90a3\u4e48\u8fd9\u6837\u7684\u8bdd\uff0c\u5982\u679c\u9ed8\u8ba4stride=1\uff0cpadding=0\u7684\u8bdd\uff0c\u5c31\u4e00\u5171\u67094\u4e2ablocks\uff0c\u5c31\u662f2\u00d72\u7684\u4e00\u4e2a\u8f93\u51fa \\([3-2+1=2]\\)  \u00d7  \\([ 4-3 +1 =2]\\) </p> </li> </ul> <p>\u4e00\u53e5\u8bdd\u603b\u7ed3 torch.unfold api\u5377\u79ef\u6838\u6ed1\u52a8input\uff0c\u5f97\u5230\u5bf9\u5e94\u7684region\uff0c\u8ddf\u5377\u79ef\u6838\u4e00\u6837\u5927\uff0c\u62c9\u6210\u884c\u5411\u91cf\uff0c\u5f62\u72b6\u662f </p> <p>\uff08\u5bf9\u4e8e\u5355\u4e2a\u5377\u79ef\u6838\uff09</p> <p><code>batch size\u00d7input region\u7684\u5143\u7d20\u6570\uff08=kernel\u7684\u5143\u7d20\u6570 \u901a\u9053\u6570*h*w\uff09\u00d7\u6ed1\u52a8\u4e86\u51e0\u4e2a\u533a\u57df\uff08=\u8f93\u51fa\u7279\u5f81\u56fe\u7684\u9ad8 \u00d7 \u5bbd\uff09</code></p> <p>\uff08\u5bf9\u4e8e \u591a\u4e2a\u5377\u79ef\u6838 torch.unfold\u8f93\u51fa\u7684\u5f62\u72b6\u662f\u4ec0\u4e48\uff1f\uff09</p>"},{"location":"learning/convs/#_2","title":"\u4ec0\u4e48\u662f\u8f6c\u7f6e\u5377\u79ef\uff1f","text":"<p>\u5377\u79ef\u7684\u4e24\u79cd\u89d2\u5ea6\uff1a</p> <ul> <li>flatten input feature map region</li> </ul> <ol> <li> <p>\u6211\u4eec\u5c06input\u8fdb\u884c\u5c55\u5f00\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6211\u4eec\u662f\u628a\uff0c\u6bcf\u4e00\u4e2ainput\u533a\u57df\u62c9\u76f4\uff0c\u62c9\u6210\u4e00\u4e2a\u5411\u91cf \uff0c\u7136\u540e\u628a\u6240\u6709\u7684\u533a\u57df\u7ec4\u5408\u6210\u4e00\u4e2a\u77e9\u9635\uff0c\u7136\u540e\u518d\u8ddf kernel\uff0c\u4e5f\u628akernel\u62c9\u6210\u4e00\u4e2a\u5411\u91cf\uff0c\u7136\u540e\u628a\u4e24\u4e2a\u77e9\u9635 \u8fdb\u884c\u51e0\u4e2a\u76f8\u4e58\u3002\u8fd9\u6837\u5f97\u5230\u6700\u7ec8\u7684\u5377\u79ef\u7ed3\u679c\uff1b </p> </li> <li> <p>flatten input feature map region\u62c9\u6210\u884c\u5411\u91cf\uff0ckernel\u62c9\u6210\u5217\u5411\u91cf</p> </li> <li>\u628a\u6bcf\u6b21\u6ed1\u52a8\u76f8\u4e58 \u8fd9\u4e2ainput region\u62c9\u76f4\uff0c\u62c9\u6210\u4e00\u4e2a\u5411\u91cf\uff0c\u628a9\u4e2a\u5411\u91cf \u62fc\u6210\u4e00\u4e2a\u77e9\u9635\uff0c\u518d\u8ddfkernel\uff0c\u628akernel \u4e5f\u62c9\u6210\u4e00\u4e2a\u5217\u5411\u91cf\uff0c\u8fdb\u884c\u4e24\u4e2a\u77e9\u9635\u7684\u76f8\u4e58\uff1b</li> </ol> <ul> <li>pad &amp; flatten kernel</li> </ul> <ol> <li>\u9996\u5148\u662f\u628a\u6574\u4e2ainput\uff0cinput\u662f5\u00d75\uff0c\u628a\u6574\u4e2ainput\u62c9\u6210\u4e00\u4e2a25\u00d71\u7684\u5411\u91cf\uff0c\u518d\u628a\u6bcf\u4e00\u6b65\u7684kernel\uff0c\u4e5f\u628a\u5b83\u53d8\u6210\u4e00\u4e2a\u957f\u5ea6\u4e3a25\u7684\u5411\u91cf\uff0c\u65b9\u6cd5\u662f\u628a\u6bcf\u4e00\u6b65\u7684kernel\u586b\u5145\u62105\u00d75\u7684\u5927\u5c0f</li> </ol> <p></p> <ol> <li> <p>9\u4e2akernel \u8ddf \u540c\u4e00\u4e2a input \u8fdb\u884c\u5185\u79ef\u64cd\u4f5c</p> </li> <li> <p>\u628a9\u4e2akernel \u62fc\u6210\u4e00\u4e2a\u77e9\u9635\u7684\u8bdd\uff0c\u76f8\u5f53\u4e8e\u662f\u4e00\u4e2a 9\u00d725\u7684 kernel\u77e9\u9635\uff0c\u8ddf25\u00d71\u7684input feature map\u8fdb\u884c\u77e9\u9635\u76f8\u4e58\uff0c\u6700\u7ec8\u5f97\u5230 9\u00d71\uff0c\u6211\u4eec\u518d\u628a 9\u00d71\u7684\u8f93\u51fa reshape\u4e00\u4e0b\uff0c\u53d8\u6210 3\u00d73\uff1b</p> </li> <li> <p>kernel \u62c9\u6210\u884c\u5411\u91cf\uff0cinput\u62c9\u6210\u5217\u5411\u91cf</p> </li> <li> <p>again\uff1a\u628a\u5377\u79ef\u770b\u6210 \u6bcf\u4e00\u6b65 \u90fd\u662f 5\u00d75 \u7684kernel \u8ddf 5\u00d75 \u7684input \u8fdb\u884c\u5185\u79ef\uff0c\u7136\u540e\u6c42\u548c\u7684\u64cd\u4f5c\uff1b\u4e3a\u4ec0\u4e48\u662f5\u00d75\uff0c\u56e0\u4e3a\u6211\u4eec\u628a\u6bcf\u4e00\u6b65 kernel\u586b\u5145\u6210 5\u00d75\u7684\uff0c\u5177\u4f53\u600e\u4e48 \u586b\u5145  \u770bkernel\u7684\u4f4d\u7f6e\uff0c\u6309\u7167 input\u7684\u5f62\u72b6 \u8fdb\u884c\u586b</p> </li> </ol>"},{"location":"learning/convs/#kernel-flatten-convolution","title":"\u4ece kernel flatten convolution \u5f00\u59cb","text":"<pre><code># step4 \u901a\u8fc7\u5bf9kernel\u8fdb\u884c\u5c55\u5f00\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5e76\u63a8\u5bfc\u51fa\u8f6c\u7f6e\u5377\u79ef\uff0c\u4e0d\u8003\u8651batch\u3001channel\u5927\u5c0f\uff0c\u4e0d\u8003\u8651padding\uff0c\u5047\u8bbestride=1\ndef get_kernel_matrix(kernel,input_size):\n    # \u57fa\u4e8ekernel\u548c\u8f93\u5165\u7279\u5f81\u56fe\u7684\u5927\u5c0f\u6765\u5f97\u5230\u586b\u5145\u62c9\u76f4\u540e\u7684kernel\u5806\u53e0\u540e\u7684\u77e9\u9635\n    kernel_h,kernel_w = kernel.shape\n    input_h,input_w = input.shape\n    num_out_fea_map = (input_h-kernel_h+1)*(input_w-kernel_w+1)  # \u5377\u79ef\u516c\u5f0f\n    result = torch.zeros((num_out_fea_map,input_h*input_w)) #\u521d\u59cb\u5316\u7ed3\u679c\u77e9\u9635\uff0c\u8f93\u51fa\u7279\u5f81\u56fe\u5143\u7d20\u4e2a\u6570*\u8f93\u5165\u7279\u5f81\u56fe\u5143\u7d20\u4e2a\u6570\n    count = 0\n    for i in range(0,input_h-kernel_h+1,1):\n        for j in range(0,input_w - kernel_w +1,1):\n            # \u586b\u5145\u6210 \u8ddf \u8f93\u5165\u7279\u5f81\u56fe\u4e00\u6837\u5927\u5c0f\n            # padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j))\n            padded_kernel = F.pad(kernel,(j,input_h-kernel_h-j,i,input_w-kernel_w-i))\n            result[count] = padded_kernel.flatten()\n            count +=1\n    return result  \n\n\n\n# \u6d4b\u8bd51\uff1a\u9a8c\u8bc1 \u4e8c\u7ef4\u5377\u79ef\nkernel = torch.randn(3,3)\ninput = torch.randn(4,4)\nkernel_matrix = get_kernel_matrix(kernel,input.shape)  # 4*16\n\n# \u901a\u8fc7\u77e9\u9635\u76f8\u4e58\u6765\u8ba1\u7b97\u5377\u79ef\nmm_conv2d_output = kernel_matrix @ input.reshape((-1,1))  \n\n# pytorch conv2d API\npytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0),kernel.unsqueeze(0).unsqueeze(0))\nprint(kernel)\nprint(kernel_matrix)\nprint(mm_conv2d_output)\nprint(pytorch_conv2d_output)\n</code></pre> <pre><code>kernel\ntensor([[ 0.3170,  2.4005, -1.2991],\n        [ 1.1566, -0.3610, -0.7246],\n        [-0.5764, -0.7988,  1.5611]])\nkernel_matrix\ntensor([[ 0.3170,  2.4005, -1.2991,  0.0000,  1.1566, -0.3610, -0.7246,  0.0000,\n         -0.5764, -0.7988,  1.5611,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.3170,  2.4005, -1.2991,  0.0000,  1.1566, -0.3610, -0.7246,\n          0.0000, -0.5764, -0.7988,  1.5611,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3170,  2.4005, -1.2991,  0.0000,\n          1.1566, -0.3610, -0.7246,  0.0000, -0.5764, -0.7988,  1.5611,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3170,  2.4005, -1.2991,\n          0.0000,  1.1566, -0.3610, -0.7246,  0.0000, -0.5764, -0.7988,  1.5611]])\nmm_conv2d_output\ntensor([[ 5.3770],\n        [-2.0131],\n        [-5.9471],\n        [-2.7944]])\npytorch_conv2d_output\ntensor([[[[ 5.3770, -2.0131],\n          [-5.9471, -2.7944]]]])\n</code></pre>"},{"location":"learning/convs/#_3","title":"\u8f6c\u7f6e\u5377\u79ef","text":"<ul> <li>\u8f93\u5165\uff1a4\u00d74\uff0ckernel\uff1a3\u00d73\uff0coutput\uff1a2\u00d72   <ul> <li>flatten input feature map region\uff1a4\u00d79  @ 9\u00d71 = 4\u00d71</li> <li>padding &amp; flatten kernel \uff1a4\u00d716 @ 16\u00d71 = 4\u00d71</li> </ul> </li> <li>\u8f6c\u7f6e\u5377\u79ef\uff1a   <ul> <li>16\u00d74 @ 4\u00d71 = 16\u00d71  $ reshape \\rightarrow $ 4 \u00d7 4 </li> </ul> </li> </ul> <p>\u8f6c\u7f6e\u5377\u79ef\u662f\u600e\u4e48\u505a\u7684\u5462\uff1f</p> <p>\u5176\u5b9e\u505a\u6cd5\u5f88\u7b80\u5355\uff0c\u5c31\u662f\u628akernel matrix \u9996\u5148\u8f6c\u7f6e\u4e00\u4e0b\uff1b\u6bd4\u65b9\u8bf4\u672c\u6765\u662f4\u00d716\u7684 \u77e9\u9635\uff1b\u6211\u4eec\u8f6c\u7f6e\u4e00\u4e0b\uff1b\u8f6c\u7f6e\u621016\u00d74\u7684\u77e9\u9635\uff1b</p> <p>\u7136\u540e\u6211\u4eec\u4e5f\u8bb2\u4e86output\u662f\u4e00\u4e2a2\u00d72\u7684 \u77e9\u9635\uff0c\u6211\u4eec\u4e5f\u628a\u5b83\u62c9\u76f4\u4e00\u4e0b\uff0c\u53d8\u62104\u00d71\u7684\u77e9\u9635\uff1b\u4e8e\u662f16\u00d74\u7684\u77e9\u9635\uff0c\u8ddf4\u00d71\u7684\u77e9\u9635\uff0c\u76f8\u4e58\uff0c\u5c31\u53d8\u6210\u4e86\u4e00\u4e2a16\u00d71\u7684\u77e9\u9635\uff0c\u6211\u4eec\u5728reshape\u4e00\u4e0b\uff0c\u5c31\u53d8\u6210\u4e864\u00d74\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u628a\u4e00\u4e2a 2\u00d72\u7684\u7279\u5f81\u56fe\uff0c\u53d8\u6210\u4e86\u4e00\u4e2a4\u00d74\u7684\u7279\u5f81\u56fe\uff1b\u8fd9\u662f\u4ece\u539f\u7406\u4e0a\u7684\u89e3\u91ca</p> <p>\u53e6\u5916\u8fd8\u6709\u4e00\u79cd\uff0c\u6211\u4eec\u8fd9\u91cc\u5b9e\u73b0\u4e86\u4e8c\u7ef4\u5377\u79ef\uff0c\u5c31\u7c7b\u4f3c\u4e8e y=wx(w\u4e58\u4ee5x\u8fd9\u6837\u7684\u4e00\u4e2a\u8fc7\u7a0b)\uff1b</p> <p>w\u8ddfx\u4e4b\u95f4 \u662f\u4e00\u4e2a\u77e9\u9635\u4e58\u6cd5\uff1b\u7136\u540e\u6211\u4eec\u6c42\u540e\u5411\u68af\u5ea6\u7684\u65f6\u5019\uff0c\u504fy\uff0c\u504fx\uff0c\u521a\u597d\u5c31\u662fw\u7684\u4e00\u4e2a\u8f6c\u7f6e\uff0c\u6240\u4ee5\u8bf4\u5728pytorch\u4e2d\uff0c\u5b9e\u73b0\u8f6c\u7f6e\u5377\u79ef \u6216\u8005\u53eb deconvolution \u6216\u8005\u53ebtranspose convolution\uff0c\u90fd\u662f\u57fa\u4e8e\u540e\u5411\u4f20\u64ad \u6765\u5b9e\u73b0\u7684\uff1b</p> <p>y=wx</p> <p>dy dx\u5c31\u7b49\u4e8ew\u7684\u8f6c\u7f6e</p> <p>\u8fd9\u4e2a\u5c31\u662f\u8f6c\u7f6e\u5377\u79ef\u7684\u539f\u7406\u90e8\u5206</p> <ul> <li>\u4e09\u70b9\u9700\u8981\u7279\u522b\u6ce8\u610f\uff1a</li> </ul> <p>\u7b2c\u4e00\u70b9</p> <p>\u8f6c\u7f6e\u5377\u79ef\u4e00\u822c\u7528\u5728\u4e0a\u91c7\u6837\u7684\u8fc7\u7a0b\uff1b\u56e0\u4e3a\u666e\u901a\u7684\u5377\u79ef\u4f1a\u7528\u5728\u4e0b\u91c7\u6837\uff0c\u6bd4\u65b9\u8bf4\u8fd9\u91cc\u7684\u4f8b\u5b50\uff0c\u628a4\u00d74\u7684\u7279\u5f81\u56fe\uff0c\u901a\u8fc7\u5377\u79ef\u53d8\u6210\u4e86\u4e00\u4e2a2\u00d72\u7684\uff0c\u8fd9\u662f\u5e38\u89c4\u7684\u64cd\u4f5c\uff0c\u8fd9\u662f\u4e0b\u91c7\u6837</p> <p>\u90a3\u6709\u65f6\u5019\uff0c\u5728\u751f\u6210\u7684\u6a21\u578b\u4e2d\uff0c\u6211\u4eec\u53ef\u80fd\u9700\u8981\uff0c\u8f93\u5165\u662f2\u00d72\u7684\uff0c\u8f93\u51fa\u53d8\u62104\u00d74\u7684\uff0c\u8fd9\u4e2a\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u8f6c\u7f6e\u5377\u79ef\u5b9e\u73b0\uff0c\u8fd9\u662f\u7b2c\u4e00\u70b9\uff1b </p> <p>\u7b2c\u4e8c\u70b9</p> <p>\u8f6c\u7f6e\u5377\u79ef \u6216\u8005 \u540e\u5411 \u5377\u79ef \u68af\u5ea6\uff1b\u610f\u601d\u5c31\u662f\u8bf4 \u6211\u4eec\u901a\u8fc7\u540e\u5411\u4f20\u64ad \u6765\u5b9e\u73b0\u8f6c\u7f6e\u5377\u79ef\u7684</p> <p>\u7b2c\u4e09\u70b9</p> <p>\u8f6c\u7f6e\u5377\u79ef\u4e5f\u53ef\u4ee5\u901a\u8fc7 \u586b\u5145\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0\uff0c\u4ec0\u4e48\u610f\u601d\u5462\uff1f\u5c31\u662f\u53ef\u4ee5\u628a2\u00d72\u7684\u8f93\u5165 \u586b\u5145\u52306\u00d76\u7684\u5927\u5c0f\uff1b\u7136\u540e\u518d\u53bb\u75283\u00d73\u7684kernel \u8fdb\u884c\u4e00\u4e2a\u5377\u79ef\uff1b\u4e5f\u80fd\u5b9e\u73b0\u4e00\u4e2a\u4e0a\u91c7\u6837\u7684\u6548\u679c\uff1b\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5e76\u4e0d\u662f\u6846\u67b6\u4e2d\u4f7f\u7528\u7684\u65b9\u6cd5\uff1b\u6846\u67b6\u4e2d\u7684\u5b9e\u73b0 \u662f\u901a\u8fc7 \u540e\u5411\u4f20\u64ad\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0 \u8f6c\u7f6e\u5377\u79ef\u7684\uff1b</p> <p>\u4ee3\u7801\u5b9e\u73b0\uff1a</p> <p>\u9996\u5148\u5bf9kernel matrix\u8fdb\u884c\u4e00\u4e2a\u8f6c\u7f6e\uff0ctranspose\uff0c-1\u7ef4\uff0c-2\u7ef4\u8f6c\u7f6e\u4e00\u4e0b</p> <p>kernel_matrix.transpose(-1,-2)\uff0c\u8fd9\u6837\u5f97\u5230w\u7684\u4e00\u4e2a\u8f6c\u7f6e\uff0c\u6211\u4eec\u518d\u628a\u8fd9\u4e2a\u8f6c\u7f6e\u8ddf\u4e0a\u9762\u8fd9\u4e2aoutput <code>mm_conv2d_output</code> \u8fdb\u884c\u4e00\u4e2a\u77e9\u9635\u76f8\u4e58\u64cd\u4f5c</p> <pre><code>kernel_matrix.transpose(-1,-2) @ mm_conv2d_output\n</code></pre> <ul> <li> <p>mm_conv2d_output \u662f\u4e00\u4e2a 4\u00d71 \u7684\u77e9\u9635\uff0c\u524d\u9762\u8f6c\u7f6e\u540e\u662f\u4e00\u4e2a 16\u00d74\u7684\uff0c\u5f97\u5230\u4e00\u4e2a 16\u00d71\u7684\u7ed3\u679c </p> </li> <li> <p>\u5b9a\u4e49\u4e3a mm_transposed_conv2d_output  </p> </li> <li> <p>\u8fd9\u4e2a\u5c31\u662f\u901a\u8fc7\u77e9\u9635\u76f8\u4e58 \u5f97\u5230\u7684\u8f6c\u7f6e\u5377\u79ef\uff0c\u4e5f\u53eb\u505a\u53cd\u5377\u79ef\uff1b </p> </li> <li> <p>\u8fd9\u4e2a\u53cd\u5377\u79ef \u6216\u8005\u53eb \u8f6c\u7f6e\u5377\u79ef\uff0c\u5e76\u4e0d\u662f\u4e00\u4e2a\u53ef\u9006\u7684\uff0c\u4e0d\u662f\u4e00\u4e2a\u9006\u8ba1\u7b97\uff0c\u8fd9\u91cc\u7684output\u5e76\u4e0d\u662f\u5f53\u521d\u7684input\uff0c\u53ea\u662f\u5f62\u72b6\u8ddfinput\u4e00\u6837\u800c\u5df2</p> </li> </ul> <pre><code>mm_transposed_conv2d_output = kernel_matrix.transpose(-1,-2) @ mm_conv2d_output\n</code></pre> <p>\u4ee5\u4e0a\u662f\u77e9\u9635\u4e58\u79ef\u5f97\u5230\u8f6c\u7f6e\u5377\u79ef\u7684\uff1b</p> <p>\u4e3a\u4e86\u9a8c\u8bc1\uff0c\u6211\u4eec\u53ef\u4ee5\u8c03\u7528pytorch\u8f6c\u7f6e\u5377\u79ef\u7684api</p> <p></p> <ol> <li>\u7c7b\u5f62\u5f0f</li> <li>\u51fd\u6570\u5f62\u5f0f</li> </ol> <p></p> <ul> <li>\u5b9e\u4f8b\u5316class\uff0c\u8c03\u7528\u7684\u8fd8\u662f\u51fd\u6570\u5f62\u5f0f\uff1b\u73b0\u5728\u6211\u4eec\u6765\u8c03\u7528\u4e00\u4e0b\u8fd9\u4e2a\u51fd\u6570</li> <li>\u5c31\u662fF.conv_transpose2d()\u4e00\u6837\u7684\uff0c\u9996\u5148\u4f20\u5165\u4e0a\u9762\u7684output\uff0c\u5c31\u662f\u628a\u4e0a\u9762\u7684pytorch_conv2d_output\u4f5c\u4e3a\u8f93\u5165\uff0ckernel\u4e5f\u8981\u4f20\u8fdb\u53bb\uff0ckernel\u5c31\u662f\u4e4b\u524d\u5199\u7684kernel\uff0c\u540c\u6837\u4e5f\u8981\u5bf9\u5b83\u8fdb\u884c\u4e24\u6b21\u7684unsqueeze\u64cd\u4f5c\uff08batch size \u00d7 channel \u00d7 height \u00d7 width\uff09\uff0c\u8fd9\u6837\u5f97\u5230pytorch_transposed_conv2d_output API</li> </ul> <pre><code># \u6d4b\u8bd52  \u901a\u8fc7\u77e9\u9635\u6210\u7ee9\u6765\u8ba1\u7b97\u8f6c\u7f6e\u5377\u79ef\nmm_transposed_conv2d_output = kernel_matrix.transpose(-1,-2) @ mm_conv2d_output\npytorch_transposed_conv2d_conv2d = F.conv_transpose2d(pytorch_conv2d_output,kernel.unsqueeze(0).unsqueeze(0))  #API\nprint(mm_transposed_conv2d_output.reshape(4,4))\nprint(pytorch_transposed_conv2d_conv2d)\n</code></pre> <p>\u5173\u4e8e\u8f6c\u7f6e\u5377\u79ef\u8981\u8bf4\u660e\u7684\uff1a</p> <ul> <li>\u6211\u4eec\u628a\u5377\u79ef\u770b\u6210\u662f \u586b\u5145\u540e\u7684kernel\u8ddfinput\uff0c\u5f97\u5230 kernel_matrix\u4e4b\u540e\uff0c\u518d\u628akernel matrix\u8f6c\u7f6e\u4e00\u4e0b\uff0c\u8ddfconvolution output\u8fdb\u884c\u77e9\u9635\u76f8\u4e58\uff0c\u8fd9\u6837\u5f97\u5230\u4e86\u4e00\u4e2a\u65b0\u7684output\uff0c\u521a\u597doutput\u7684\u5927\u5c0f\u548cinput\u7684\u5927\u5c0f\u662f\u4e00\u6837\u7684\uff1b\u6210\u529f\u5b9e\u73b0\u4e86\u4e0a\u91c7\u6837\uff0c\u56e0\u4e3amm conv2d output\u662f 2\u00d72\u7684\uff0c\u5de6\u8fb9mm transposed conv2d output\u662f 4\u00d74\u7684\uff0c\u6211\u4eec\u5c31\u5b9e\u73b0\u4e86\u4e0a\u91c7\u6837\uff1b</li> <li>F.conv_transposed2d()\u7684\u8f93\u5165\uff0c\u5c31\u662f\u666e\u901a\u5377\u79ef\u7684\u8f93\u51fa\uff0ckernel\u8fd8\u662f\u90a3\u4e2akernel\uff0c\u628a\u5b83\u6269\u5145\u4e00\u4e0b</li> </ul> <p>\u5173\u4e8e\u4e0a\u91c7\u6837\u7684\u4e24\u4e2a\u89d2\u5ea6\uff1a </p> <ul> <li>\uff08\u7b2c\u4e00\u79cd\u5b9e\u73b0\uff1a\u628akernel\u8f6c\u7f6e 16 \u00d7 4  $ \\rightarrow $ 4\u00d716 \uff09  \u9996\u5148\u8981\u628a\u666e\u901a\u5377\u79ef\u7684kernel matrix\u5199\u51fa\u6765\uff0c\u7136\u540e\u518d\u628amatrix\u8f6c\u7f6e\u4e00\u4e0b\uff0c\u518d\u8ddf\u666e\u901a\u5377\u79ef\u7684\u8f93\u51fa \u76f8\u4e58\u4e00\u4e0b\uff1b\u5c31\u5b9e\u73b0\u4e86</li> <li>\uff08\u7b2c\u4e8c\u79cd\u5b9e\u73b0\uff1a\u628ainput\u53d8\u5927\uff09   \u76f4\u63a5\u628ainput\u8fdb\u884c\u586b\u5145\uff1b\u6bd4\u5982\u73b0\u5728input\u662f2\u00d72\uff0c\u6211\u4eec\u4e3a\u4e86\u5b9e\u73b04\u00d74\uff0c\u4e3a\u4e86\u7528\u666e\u901a\u7684\u5377\u79ef\uff0c\u6211\u4eec\u53ef\u4ee5\u628a2\u00d72\u7684\u586b\u5145\u62105\u00d75 \u6216\u8005 6\u00d76\u7684\uff1b\u5047\u5982\u8bf4\u662f6\u00d76\u7684\uff0c\u6211\u4eec\u5c31\u628a\u4e0a\u4e0b\u5de6\u53f3 \u586b\u5145\u4e24\u884c0\u5c31\u597d\u4e86\uff0c\u518d\u7528\u666e\u901a\u5377\u79ef\u5b9e\u73b0 \u4e5f\u662f\u53ef\u4ee5\u7684\uff1b\u56e0\u4e3a\u53cd\u6b63\u53c2\u6570\u90fd\u662f\u8981\u5b66\u4e60\u7684\uff0c\u6211\u4eec\u7684\u76ee\u7684\u5c31\u662f\u505a\u4e0a\u91c7\u6837\uff1b\u65e0\u8bba\u662f\u4ece\u540e\u5411\u4f20\u64ad\u7684\u89d2\u5ea6\uff0c\u8fd8\u662f\u76f4\u63a5\u5bf9input\u8fdb\u884c\u586b\u5145\uff0c\u628ainput\u53d8\u5927\uff0c\u90fd\u80fd\u5b9e\u73b0 \u4e0a\u91c7\u6837\uff0c\u4e0d\u8fc7\u6570\u503c\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u4e0d\u8fc7\u6ca1\u5173\u7cfb\uff0c\u53cd\u6b63\u90fd\u662f\u8981\u5b66\u4e60\u7684</li> </ul> <p>\u8f6c\u7f6e\u5377\u79ef \u53cd\u5377\u79ef=transpose conv2d</p>"},{"location":"learning/convs/#4_1","title":"4 \u81a8\u80c0\u5377\u79ef &amp; \u7a7a\u6d1e\u5377\u79ef","text":"<p>intro\uff0c\u5b98\u65b9api\uff1a</p> <p></p> <p>\u5728\u9ed8\u8ba4\u7684api\u4e2d dilation\u7684\u503c\u7b49\u4e8e1\uff0cgroups\u7684\u503c \u4e5f\u662f\u7b49\u4e8e1 \u7684\uff0c\u4e5f\u5c31\u662f\u6211\u4eec\u5e38\u7528\u7684\u5377\u79ef\u90fd\u6ca1\u6709\u6307\u5b9a\uff0c\u5e38\u7528\u7684\u503c\u90fd\u4e3a1</p> <p>\u4ec0\u4e48\u662fdilation\uff1f</p> <p>dilation\u7684\u610f\u601d\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u666e\u901a\u7684\u5377\u79ef\uff0c\u6bd4\u5982\u8bf43\u00d73\u7684\u5377\u79ef\u6838\uff0c\u5728\u4e00\u4e2a\u8f93\u5165\u7279\u5f81\u56fe\u4e0a \u8fdb\u884c \u5377\u79ef\u7684\u8bdd\uff0c\u6211\u4eec\u6bcf\u6b21\uff0c\u4ece\u8f93\u5165\u7279\u5f81\u56fe\u4e0a\u53d6\u4e00\u5757 3\u00d73\u7684 \u9762\u79ef\uff0c\u53d69\u4e2a\u5143\u7d20\uff0c\u5e76\u4e14\u8fd99\u4e2a\u5143\u7d20\uff0c\u90fd\u662f\u7d27\u6328\u7740\u5f7c\u6b64\u7684\uff0c\u5c31\u662f3\u00d73\u7684\u533a\u57df\uff0c\u4e00\u4e2a\u65b9\u5f62\u533a\u57df\uff0c\u8fd9\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u6210\u4e3adilation=1\uff0c\u4e5f\u5c31\u662f\u8bf4\u5f7c\u6b64\u4e4b\u95f4\u95f4\u9694\u4e3a1\uff0c\u53ef\u4ee5\u8fd9\u4e48\u7406\u89e3\uff0c\u5f7c\u6b64\u7684\u7d22\u5f15\uff0c\u5dee\u8ddd\u4e3a1\uff0c\u6bd4\u65b9\u8bf4\u7b2c\u4e00\u4e2a\u5143\u7d20 \u7d22\u5f15\u4e3a1\uff0c\u7b2c\u4e8c\u4e2a\u5143\u7d20 \u7d22\u5f15 \u5c31\u662f2\uff0c\u90a3\u5982\u679cdilation\u4e0d\u662f\u7b49\u4e8e1\uff0c\u800c\u662f2\u7684\u8bdd\u5462\uff0c\u8bf4\u660e\u7b2c\u4e00\u4e2a\u5143\u7d20\u548c\u7b2c\u4e8c\u4e2a\u5143\u7d20 \u7d22\u5f15\u76f8\u5dee\u4e862\uff0c\u90a3\u5c31\u8bf4\u660e \u4e2d\u95f4\u8fd8\u591a\u4e86\u4e00\u4e2a\u5143\u7d20\uff1b</p> <p>\u4e5f\u5c31\u662f\u8bf4 dilation \u662f\u63a7\u5236\u7740\u6211\u4eec\u8f93\u5165 \u7279\u5f81\u56fe \u8981\u53d6\u5f97\u90a3\u90e8\u5206\u9762\u79ef \u662f\u5426\u662f\u7d27\u51d1\u7684\uff0c\u5982\u679c\u5b83\u7684\u503c\u5927\u4e8e1\u7684\u8bdd\uff0c\u5b83\u5c31\u4e0d\u662f\u7d27\u51d1\u7684\uff0c\u5b83\u4e2d\u95f4\u662f\u6709\u4e00\u4e9b\uff0c\u8df3\u8fc7\u7684\u5143\u7d20\u7684\uff1b</p> <pre><code>a = torch.randn(7,7)\n</code></pre> <p></p> <ul> <li>dilation=2  a[0:5:2,0:5:2] \u7d22\u5f150\u5230\u7d22\u5f155\uff0c\u8df3\u8fc7\u4e00\u4e2a\u53d6\u4e00\u4e2a\uff0c\u6700\u540e\u4e00\u4e2a\u53d6\u4e0d\u5230</li> <li>dilation=3 \u7528\u7d22\u5f15\u8868\u793a\u7684\u8bdd \u5c31\u662f 0\u52307\uff0c\u7136\u540e\u95f4\u9694\u662f3\uff1ba[0:7:3,0:7:3] # dilation=3 \u540c\u6837\u5217\u6570\u4e5f\u662f\u4e00\u6837\u7684 0\u52307 \u95f4\u9694\u662f3\uff1b\u7d22\u5f15\u95f4\u9694\u4e3a3</li> </ul> <p></p> <p>\u4e00\u53e5\u8bdd\u8bf4\u6e05dilation\u662f\u4ec0\u4e48\uff1f\u5377\u79ef\u7684\u8986\u76d6\u533a\u57df \u7d22\u5f15\u95f4\u9694\u591a\u5c11</p> <p>\u5982\u679c input size=7\u00d77 kernel size=3\u00d73\uff0cdilation=3\uff0c\u6211\u4eec\u53ea\u9700\u8981 \u53d6\u4e00\u6b21\u5c31\u597d\u4e86\uff1b</p> <p>\u53d6\u4e00\u6b21 \u5c31\u521a\u597d \u5df2\u7ecf\u5230 \u8fb9\u754c\u4e86</p> <p>\u6240\u4ee57\u00d77\u7684input \u8ddf 3\u00d73\u7684kernel \u8fdb\u884c \u5377\u79ef\u7684\u8bdd\uff0c\u6211\u4eec\u4e0d\u505apadding stride=1\u7684\u8bdd\uff0c\u90a3\u4e48\u8f93\u51fa\u5c31\u662f\u4e00\u4e2a\u6570\uff0c\u5c31\u662f\u4e00\u4e2a\u6807\u91cf\uff1b\u8fd9\u5c31\u662fdilation \u53d6 \u4e0d\u540c\u503c \u5177\u4f53\u7684\u8fd0\u7b97\u89c4\u5219</p> <p>\u90a3\u4e3a\u4ec0\u4e48\u8981\u7528dilation\u5927\u4e8e1\u7684\u8fd9\u4e9b\u60c5\u51b5\u5462\uff1f\u5c31\u662f\u56e0\u4e3a\u6211\u4eec \u589e\u5927dilation \u4f46\u662f\u5e76\u6ca1\u6709\u589e\u5927\u8fd0\u7b97\u91cf\uff1b\u6211\u4eec\u8fd8\u662f3\u00d73\u7684\u77e9\u9635\uff0c\u8ddf3\u00d73\u7684\u77e9\u9635 \u8fdb\u884c\u5143\u7d20\u76f8\u4e58\uff1b\u5e76\u6ca1\u6709\u56e0\u4e3a \u611f\u53d7\u91ce\u53d8\u5927 \u8ba1\u7b97\u91cf \u53d8\u5927\uff1b\u6240\u4ee5\u4e00\u822c \u589e\u5927 dilation\u7684\u76ee\u7684 \u5c31\u662f\u6211\u4eec\u5728 \u4fdd\u6301\u8fd0\u7b97\u91cf\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\uff0c\u5e0c\u671b \u589e\u5927 \u611f\u53d7\u91ce\u7684\u9762\u79ef\uff1b\u8fd9\u5c31\u662fdilation</p> <p>\u4e00\u53e5\u8bdd\u4e3a\u4ec0\u4e48dilation\uff1a\u5728\u4e0d\u589e\u52a0\u8fd0\u7b97\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u589e\u5927\u611f\u53d7\u91ce</p>"},{"location":"learning/convs/#5","title":"5 \u5206\u7ec4\u5377\u79ef &amp; \u7fa4\u5377\u79ef","text":""},{"location":"learning/convs/#_4","title":"\u4ec0\u4e48\u662f\u5206\u7ec4\u5377\u79ef\uff1f","text":"<p>\u5206\u7ec4\u5377\u79ef group convolution\uff1b\u662f\u5bf9\u8f93\u5165\u901a\u9053\u8fdb\u884c\u5206\u7ec4\uff1b\u8f93\u51fa\u901a\u9053\u5e76\u4e0d\u662f\u7531\u6240\u6709\u7684\u8f93\u5165\u901a\u9053\u5171\u540c\u4f5c\u7528\u7684\uff1b\u4f1a\u6709\u4e00\u79cd\u60c5\u51b5\uff0c\u6bd4\u5982\u8f93\u5165\u901a\u9053\u662f4\uff0c\u8f93\u51fa\u901a\u9053\u662f2\uff0c\u8f93\u51fa\u901a\u9053\u7684\u7b2c\u4e00\u4e2a\u901a\u9053\u53ea\u8ddf\u8f93\u5165\u901a\u9053\u7684\u7b2c1\u30013\u4e2a\u901a\u9053\u6709\u5173\uff1b\u8f93\u51fa\u901a\u9053\u7684\u7b2c\u4e8c\u4e2a\u901a\u9053\u53ea\u8ddf\u8f93\u5165\u901a\u9053\u7684\u7b2c2\u30014\u4e2a\u901a\u9053\u6709\u5173\uff1b\u5982\u679c\u8f93\u5165\u901a\u9053\u6709\u8fd9\u6837\u7684\u5173\u7cfb\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u91c7\u7528\u5206\u7ec4\u5377\u79ef\uff0c\u8bbe\u7f6e\u7ec4\u6570group=2\uff0c\u8fd9\u65f6\u6709\u51e0\u4e2a\u7ec4\u5c31\u4f1a\u6709\u51e0\u4e2a\u8f93\u51fa\u901a\u9053\uff1b\u8fd9\u79cd\u60c5\u51b5\u662f\u6211\u4eec\u5bf9\u6bcf\u4e2a\u7ec4\u8fdb\u884c\u4e00\u6b21\u5377\u79ef\uff0c\u5982\u679c\u6211\u4eec\u5bf9\u6bcf\u4e2a\u7ec4\u8fdb\u884c\u591a\u6b21\u5377\u79ef\uff0c\u90a3\u4e48\u5377\u79ef\u6838\u7684\u4e2a\u6570\u5c31\u4f1a\u589e\u52a0\u4e86\uff1b\u8fd9\u6837\u4e5f\u6709\u4e00\u4e2a\u95ee\u9898\uff0c\u5c31\u662f\u8f93\u5165\u7279\u5f81\u56fe\u7684\u901a\u9053\u4e4b\u95f4\u6ca1\u6709\u4ea4\u4e92\uff0c\u6240\u4ee5\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5728\u540e\u9762\u7684\u5377\u79ef\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u6709\u901a\u9053\u4e4b\u95f4\u7684\u968f\u673a\u6df7\u5408\u6216\u8005\u75281\u00d71\u7684\u5377\u79ef\uff1bpoinwise convolution\uff1b</p>"},{"location":"learning/convs/#depthwise-pointwise","title":"\u8865\u5145\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef depthwise &amp; pointwise\uff1a","text":"<p>\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff0c\u662f\u7279\u6b8a\u7684\u5206\u7ec4\u5377\u79ef\uff0c\u6709\u51e0\u4e2a\u8f93\u5165\u901a\u9053\uff0c\u5c31\u5206\u6210\u51e0\u4e2a\u7ec4\uff0c\u8f93\u5165\u901a\u9053\u4e4b\u95f4\u5b8c\u5168\u76f8\u4e92\u72ec\u7acb\uff0cdeepwise convolution\uff1b\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u540e\u9762\u901a\u5e38\u4f1a\u8ddf\u7740 pointwise  convolution\uff1b</p> <p>\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef &amp; 1\u00d71\u5377\u79ef</p> <p>\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684Separable Convolution</p> <p>\u4e00\u5f20\u56fe\u770b\u61c2\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff1a</p> <p></p> <p>Depthwise Convolution\u5b8c\u6210\u540e\u7684Feature map\u6570\u91cf\u4e0e\u8f93\u5165\u5c42\u7684depth\u76f8\u540c\uff0c\u4f46\u662f\u8fd9\u79cd\u8fd0\u7b97\u5bf9\u8f93\u5165\u5c42\u7684\u6bcf\u4e2achannel\u72ec\u7acb\u8fdb\u884c\u5377\u79ef\u8fd0\u7b97\u540e\u5c31\u7ed3\u675f\u4e86\uff0c\u6ca1\u6709\u6709\u6548\u7684\u5229\u7528\u4e0d\u540cmap\u5728\u76f8\u540c\u7a7a\u95f4\u4f4d\u7f6e\u4e0a\u7684\u4fe1\u606f\u3002\u56e0\u6b64\u9700\u8981\u589e\u52a0\u53e6\u5916\u4e00\u6b65\u64cd\u4f5c\u6765\u5c06\u8fd9\u4e9bmap\u8fdb\u884c\u7ec4\u5408\u751f\u6210\u65b0\u7684Feature map\uff0c\u5373\u63a5\u4e0b\u6765\u7684Pointwise Convolution\u3002\uff08\u6458\u81ea\uff09</p> <p>\u4e00\u5f20\u56fe\u770b\u61c21\u00d71\u5377\u79ef\uff1a</p> <p></p> <p>Pointwise Convolution\u7684\u8fd0\u7b97\u4e0e\u5e38\u89c4\u5377\u79ef\u8fd0\u7b97\u975e\u5e38\u76f8\u4f3c\uff0c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u5377\u79ef\u6838\u7684\u5c3a\u5bf8\u4e3a 1\u00d71\u00d7M\uff0cM\u4e3a\u4e0a\u4e00\u5c42\u7684depth\u3002\u6240\u4ee5\u8fd9\u91cc\u7684\u5377\u79ef\u8fd0\u7b97\u4f1a\u5c06\u4e0a\u4e00\u6b65\u7684map\u5728\u6df1\u5ea6\u65b9\u5411\u4e0a\u8fdb\u884c\u52a0\u6743\u7ec4\u5408\uff0c\u751f\u6210\u65b0\u7684Feature map\u3002\u6709\u51e0\u4e2aFilter\u5c31\u6709\u51e0\u4e2aFeature map\u3002\uff08\u6458\u81ea\uff09</p> <p>\u8865\u5145\u666e\u901a\u5377\u79ef\uff1a</p> <p></p>"},{"location":"learning/convs/#_5","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u5206\u7ec4\u5377\u79ef\uff1f","text":"<p>\u5f52\u7eb3\u504f\u7f6e\uff1a</p> <p>\u6bcf\u4e00\u4e2a\u6a21\u578b \u90fd\u6709\u81ea\u5df1\u7684\u5047\u8bbe\uff0c\u6216\u8005\u53eb \u5f52\u7eb3\u504f\u7f6e inductive bias\uff1b</p> <ul> <li>CNN\u7684\u5f52\u7eb3\u504f\u7f6e\u5c31\u662f \u5c40\u90e8\u5efa\u6a21\u6027 \u548c \u5e73\u79fb\u4e0d\u53d8\u6027</li> <li>RNN\u5c31\u662f\u524d\u540e\u5173\u8054\u6027</li> <li>Transformer\u6ca1\u6709\u4ec0\u4e48\u5047\u8bbe\uff0c\u53ea\u662f\u5f15\u5165\u4e86\u4e00\u4e2aposition embedding\u800c\u5df2</li> </ul> <p>\u5728\u6211\u4eec\u8fd9\u91cc\u5f15\u5165\u7684 group&gt;1\u7684\u8bdd\uff0c\u5f15\u5165\u7684\u5047\u8bbe\u662f\u4ec0\u4e48\u5462\uff1f</p> <p>\u6211\u4eec\u53ea\u9700\u8981\u4e00\u5c0f\u90e8\u5206\uff0c\u53ea\u9700\u8981\u505a\u4e00\u5c0f\u90e8\u5206 \u901a\u9053\u4e4b\u95f4\u7684\u5efa\u6a21\u5c31\u597d\u4e86\uff0c\u4e0d\u9700\u8981\u8003\u8651 \u6bcf\u4e2a\u901a\u9053 \u8ddf\u6240\u6709\u901a\u9053\u7684 \u5173\u7cfb\uff1b\u5176\u5b9e\u672c\u8d28\u4e0a group=1\u7684\u8bdd\uff0c\u5c31\u662f\u8bf4 in channel\uff0c\u6bcf\u4e2a\u901a\u9053 \u90fd\u9700\u8981 \u8ddf \u5176\u4ed6 \u901a\u9053 \u8fdb\u884c\u4e00\u4e2a\u6df7\u5408\uff1b\u4f46\u662f\u5f53\u6211\u4eec\u628a groups\uff0c\u8bbe\u7f6e\u6210&gt;1\u7684\u8bdd\uff0c\u5c31\u662f\u628a\u5b83\u4eec\u5206\u7ec4\u6765\u8003\u8651\uff0c\u5c31\u662f\u6bcf\u6b21\u5462\uff0c\u53ea\u5728\u51e0\u4e2a\u901a\u9053\u505a\u4e00\u4e0b\u5377\u79ef\uff1b\u7136\u540e\u4e0b\u6b21 \u518d\u53e6\u5916\u7684\u901a\u9053 \u505a\u5377\u79ef\uff1b\u7136\u540e\u628a\u7ed3\u679c\u62fc\u8d77\u6765 \u5c31\u597d\u4e86\uff1b\u4e5f\u5c31\u662f\u8bf4 \u901a\u9053\u878d\u5408 \u5e76\u4e0d\u5145\u5206\uff1b\u7b80\u5355\u8bf4 \u5c31\u662f \u8fd9\u6837\u7684</p> <p>\u518d\u91cd\u590d\uff1agroups&gt;1\uff0c\u5c31\u662f\u8bf4 \u901a\u9053\u878d\u5408 \u4e0d\u9700\u8981 \u5b8c\u5168 \u5145\u5206\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5728\u4e00\u4e2a\u4e2agroup\u5185\u8fdb\u884c\u878d\u5408\uff0c\u6700\u540e\u62fc\u63a5\uff0c\u8fd9\u5c31\u662fgroup convolution \u5f15\u5165\u7684\u4e00\u4e2a\u504f\u7f6e</p> <p>\u5176\u5b9e\u8fd9\u4e2a\u504f\u7f6e\u4e5f\u5f88\u597d\u89e3\u51b3\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5728group convolution\u540e\u9762\uff0c\u518d\u52a0\u4e0a\u4e00\u4e2a 1\u00d71 point wise\u5377\u79ef\u5c31\u597d\u4e86 </p> <p>\u5c31\u662f\u8bf4 1\u00d71\u7684\u9010\u70b9\u5377\u79ef\uff0c\u867d\u7136\u6ca1\u6709\u8003\u8651 \u5c40\u90e8\u5efa\u6a21\uff0c\u4f46\u662f\u5b83\u80fd\u5bf9\u901a\u9053\u4e4b\u95f4 \u8fdb\u884c\u878d\u5408\uff1b\u6240\u4ee5\u6700\u540e \u6211\u4eec\u8fd8\u662f\u80fd\u591f\u628a \u901a\u9053\u4e4b\u95f4 \u8fdb\u884c\u878d\u5408\u7684</p> <p>\u5206\u7ec4\u5377\u79ef &amp; \u9010\u70b9\u5377\u79ef</p> <p>add \u5404\u79cdwise </p> <ul> <li>\u6211\u4eec\u518d\u8bf4\u4e00\u4e0b \u8fd9\u91cc\u7684wise\uff0c\u4e00\u65e6\u770b\u5230\u5404\u79cd wise\uff0c\u5c31\u662f\u8bf4 \u6211\u4eec\u53ea\u8003\u8651wise\u524d\u9762\u8fd9\u4e2a\u4e1c\u897f\uff1b</li> <li>\u6bd4\u65b9\u8bf4\uff1bpoint wise\u5c31\u662f\u8bf4 \u6211\u4eec\u53ea\u5bf9 \u4e00\u4e2a\u70b9 \u53bb\u7b97 \u76f8\u4e58\uff0c\u800c\u4e0d\u662f\u8bf4 \u50cf \u666e\u901a\u7684\u5377\u79ef\u4e00\u6837\uff0c\u53d6\u4e00\u4e2a3\u00d73\u7684\u533a\u57df\uff1b\u90a3\u5c31\u4e0d\u662f\u4e00\u4e2a\u70b9\uff1b</li> <li>\u8fd8\u6bd4\u5982\u8bf4 channel wise\uff0c\u6211\u4eec\u53ea\u5bf9\u4e00\u4e2a\u901a\u9053\uff1b\uff08\u6709\u70b9\u50cf \u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff09</li> <li>\u6bd4\u5982\u8bf4layer wise\uff0c\u6211\u4eec\u53ea\u5bf9\u4e00\u5c42\u8003\u8651\u7b49\u7b49\uff1b</li> <li>\u5404\u79cd wise\uff0c\u6bd4\u5982element wise \u53ea\u5bf9\u5143\u7d20\u8ddf\u5143\u7d20\u4e4b\u95f4\uff1b\u76f8\u540c\u4f4d\u7f6e\u7684\u5143\u7d20\u8fdb\u884c\u8003\u8651\uff1b</li> </ul>"},{"location":"learning/convs/#_6","title":"\u5206\u7ec4\u5377\u79ef\u4e2d\u7684\u53d8\u4e0e\u4e0d\u53d8","text":"<p>\u9898\u8bbe\uff1a in channel\u548cout channel\u5206\u522b\u7b49\u4e8e2\u548c4\uff0c\u5206\u6790group=1\u548cgroup=2</p> <p>case1 \uff1agroup=1\uff0c\u4e00\u5171\u662f8\u5f20kernel map\uff0c\uff084\u4e2a\u5377\u79ef\u6838\uff0c\u6bcf\u4e2akernel\u901a\u9053\u6570\u7b49\u4e8e2\uff09 </p> <p>\u9996\u5148\u6211\u4eec\u62ff\u51fa\u4e24\u5f20kernel map \u5206\u522b\u4e0einput\u8fdb\u884c\u5377\u79ef\uff0c\u7136\u540e\u52a0\u8d77\u6765\uff0c\u52a0\u8d77\u6765\u7684\u7ed3\u679c\u8d4b\u7ed9\u7b2c\u4e00\u4e2a\u901a\u9053\uff1b\u518d\u62ff\u4e24\u4e2a\u5377\u79ef\u6838\uff0c\u540c\u6837\u8ddf\u8f93\u5165\u7684\u4e24\u4e2a\u901a\u9053\u8fdb\u884c\u5377\u79ef\uff0c\u7136\u540e\u52a0\u8d77\u6765\uff0c\u8d4b\u503c\u7ed9\u7b2c\u4e8c\u4e2a\u901a\u9053\uff0c\u4ee5\u6b64\u7c7b\u63a8\uff0c\u76f4\u5230\u6211\u4eec\u62ff\u51fa\u6700\u540e\u7684\u4e24\u4e2a\u5377\u79ef\u6838 \u8ddf \u8f93\u5165\u4e24\u4e2a\u7279\u5f81\u56fe \u8fdb\u884c\u5377\u79ef\uff0c\u7136\u540e\u518d\u6c42\u548c \u8d4b\u503c\u7ed9 \u6700\u540e\u4e00\u4e2a\u901a\u9053\uff1b\u6240\u4ee5\u4e00\u5171\u662f8\u5f20kernel map</p> <p>case2 \uff1agroups=2\uff0c\u4e00\u5171\u662f4\u5f20kernel map\uff0c\uff084\u4e2a\u5377\u79ef\u6838\uff0c\u6bcf\u4e2akernel\u7684\u901a\u9053\u6570=1\uff09 </p> <p>\uff1ain channels=2\uff0cgroups=2\uff0c\u5982\u679c\u8fd8\u8ba9output channel=4\uff0c\u90a3\u4e48kernel map\u6709\u51e0\u5f20\uff1f\u5377\u79ef\u6838\u6709\u51e0\u4e2a\uff1f</p> <p>\u9996\u5148\uff0c<code>#\u5377\u79ef\u6838</code>   $ \\stackrel{\u51b3\u5b9a}{\\rightarrow} $ <code>#\u8f93\u51fa\u901a\u9053\u6570</code>  \u3001<code>#\u8f93\u5165\u901a\u9053\u6570</code>   $ \\stackrel{\u51b3\u5b9a}{\\rightarrow} $  <code>#\u5355\u4e2a\u5377\u79ef\u6838\u901a\u9053\u6570</code></p> <p>\u2234 \u67094\u4e2a\u5377\u79ef\u6838\uff0c\u6bcf\u4e2a\u5377\u79ef\u6838\u7684channels=1\uff0c\uff08\u2235\u628a\u8f93\u5165\u901a\u9053\u6570\u5206\u62102\u7ec4\uff0c\u6240\u4ee5\u8f93\u5165\u901a\u9053\u6570\u53d8\u6210 2\u00f72=1 \uff09</p> <p>\u2234\u67094\u5f20kernel map </p> <p>\u7efc\u4e0a\uff1a </p> <ol> <li>kernel map\u51cf\u5c11\u4e00\u534a || \u5728\u6bcf\u4e00\u7ec4\u4e2d\uff0c\u5176\u5b9e\u6709\u4e24\u4e2a\u5377\u79ef\u6838\uff0c\u6240\u4ee5\u4e24\u7ec4 \u4e00\u5171\u662f 4\u4e2a kernel map\uff0c\u76f8\u6bd4\u4e0a\u9762 8\u4e2akernel map \u5c31\u5c11\u4e86\u4e00\u534a\uff08kernel map\u3001\u53c2\u6570\u91cf\u3001\u8fd0\u7b97\u91cf\u51cf\u534a\uff09</li> <li>\u8f93\u51fa\u7279\u5f81\u56fe\u7684\u9ad8\u5ea6 &amp; \u5bbd\u5ea6 \u4e0d\u53d8\uff0cbatch size\u4e0d\u53d8</li> </ol>"},{"location":"learning/convs/#dilationgroups","title":"\u4ee3\u7801\u5b9e\u73b0 dilation&amp;groups \u624b\u6495 &amp; \u5e93\u51fd\u6570","text":"<pre><code>def matrix_multiplication_for_conv2d_finall(input,kernel,bias=None,stride=1,padding=0,dilation=1,groups=1):\n    if padding&gt;0:\n        input = F.pad(input,(padding,padding,padding,padding,0,0,0,0))\n\n    bs,in_channel,input_h,input_w = input.shape\n    out_channel,_,kernel_h,kernel_w = kernel.shape\n\n    assert out_channel % groups == 0 and in_channel % groups==0,\"groups\u5fc5\u987b\u8981\u540c\u65f6\u88ab\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u6574\u9664\uff01\"\n    input = input.reshape((bs,groups,in_channel//groups,input_h,input_w))\n    kernel = kernel.reshape((groups,out_channel//groups,in_channel//groups,kernel_h,kernel_w))\n\n    kernel_h = (kernel_h-1)*(dilation-1)+kernel_h\n    kernel_w = (kernel_w-1)*(dilation-1)+kernel_w\n\n    output_h = math.floor((input_h-kernel_h)/stride)+1\n    output_w = math.floor((input_w-kernel_w)/stride)+1\n\n    output_shape = (bs,groups,out_channel//groups,output_h,output_w)\n    output = torch.zeros(output_shape)\n\n    if bias is None:\n        bias = torch.zeros(out_channel)\n\n    for ind in range(bs): # \u5bf9batch size\u8fdb\u884c\u904d\u5386\n        for g in range(groups): # \u5bf9\u7fa4\u7ec4\u8fdb\u884c\u904d\u5386\n            for oc in range(out_channel//groups): # \u5bf9\u5206\u7ec4\u540e\u7684\u8f93\u51fa\u901a\u9053\u8fdb\u884c\u904d\u5386\n                for ic in range(in_channel//groups): # \u5bf9\u5206\u7ec4\u540e\u7684\u8f93\u5165\u901a\u9053\u8fdb\u884c\u904d\u5386\n                    for i in range(0,input_h-kernel_h+1,stride): #\u5bf9\u9ad8\u5ea6\u904d\u5386\n                        for j in range(0,input_w-kernel_w+1,stride): # \u5bf9\u5bbd\u5ea6\u904d\u5386\n                            region = input[ind,g,ic,i:i+kernel_h:dilation,j:j+kernel_w:dilation] #\u7279\u5f81\u533a\u57df\n                            output[ind,g,oc,int(i/stride),int(j/stride)] += torch.sum(region*kernel[g,oc,ic])\n\n                output[ind,g,oc] += bias[g*(out_channel//groups)+oc]  # \u8003\u8651\u504f\u7f6e\u9879\n    output = output.reshape((bs,out_channel,output_h,output_w))  # \u8fd8\u539f\u6210\u56db\u7ef4\u5f20\u91cf\n    return output\n\n# \u9a8c\u8bc1\u6d4b\u8bd5\u7684\u4ee3\u7801\nkernel_size=3\nbs,in_channel,input_h,input_w = 2,2,5,5\nout_channel=4\ngroups,dilation,stride,padding=2,2,2,1\n\ninput = torch.randn(bs,in_channel,input_h,input_w)\nkernel = torch.randn(out_channel,in_channel//groups,kernel_size,kernel_size)\nbias = torch.randn(out_channel)\n\n# pytorch API\u7684\u7ed3\u679c\npytorch_conv2d_api_output = F.conv2d(input,kernel,bias=bias,padding=padding,\n                                     stride=stride,dilation=dilation,groups=groups)\nmm_conv2d_finall_output = matrix_multiplication_for_conv2d_finall(input,kernel,bias=bias,padding=padding,\n                                                                  stride=stride,dilation=dilation,groups=groups)\n\nflag = torch.allclose(pytorch_conv2d_api_output,mm_conv2d_finall_output)\nprint(flag)\n</code></pre>"},{"location":"learning/convs/#6","title":"6 \u6c47\u603b\u4ee3\u7801","text":"<p>\u5e93\u51fd\u6570\u5b9e\u73b0\u5377\u79ef</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nin_channels = 1\nout_channels = 1\nkernel_size = 3\nbatch_size = 1\nbias = False\n\ninput_size = [batch_size,in_channels,4,4]\n\n# \u7b2c\u4e00\u79cd\u5b9e\u73b0\nconv_layer = torch.nn.Conv2d(in_channels,out_channels,kernel_size,bias=bias)\n\ninput_feature_map = torch.randn(input_size)\nout_feature_map = conv_layer(input_feature_map)\n# print(input_feature_map)\n# print(conv_layer.weight)  # 1*1*3*3=out_channels*in_channels*height*width\n\nprint(out_feature_map)\n\nout_feature_map1 = F.conv2d(input_feature_map,conv_layer.weight)\n\nprint(out_feature_map1)\n</code></pre> <p>step1 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5148\u4e0d\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6</p> <p>step2 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5148\u4e0d\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6\uff0cflatten\u7248\u672c</p> <pre><code>input = torch.randn(5,5) # \u5377\u79ef \u8f93\u5165\u7279\u5f81\u56fe\nkernel = torch.randn(3,3) # \u5377\u79ef\u6838\nbias = torch.randn(1) # \u5377\u79ef\u504f\u7f6e\uff0c\u9ed8\u8ba4\u8f93\u51fa\u901a\u9053\u6570\u76ee\u7b49\u4e8e1\n\n# step1 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5148\u4e0d\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6\ndef matrix_multiplication_for_conv2d(input,kernel,bias=0,stride=1,padding=0):\n\n  if padding &gt;0:\n    input = F.pad(input,(padding,padding,padding,padding))\n\n\n  input_h,input_w = input.shape\n  kernel_h,kernel_w = kernel.shape\n\n  output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u9ad8\u5ea6\n  output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u5bbd\u5ea6 \n  output = torch.zeros(output_h,output_w) # \u521d\u59cb\u5316 \u8f93\u51fa\u77e9\u9635\n\n  for i in range(0,input_h - kernel_h + 1,stride): # \u5bf9\u9ad8\u5ea6\u8fdb\u884c\u904d\u5386\n    for j in range(0,input_w - kernel_w +1,stride):  # \u5bf9\u5bbd\u5ea6\u7ef4\u8fdb\u884c\u904d\u5386\n      region = input[i:i+kernel_h, j:j+kernel_w]  # \u53d6\u51fa\u88ab\u6838\u6ed1\u52a8\u5230\u7684\u533a\u57df\n      output[int(i/stride),int(j/stride)] = torch.sum(region * kernel) + bias # \u70b9\u4e58 \u5e76\u8d4b\u503c\u7ed9\u8f93\u51fa\u4f4d\u7f6e\u7684\u5143\u7d20 \n\n  return output\n\n\n# step2 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5148\u4e0d\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6\uff0cflatten\u7248\u672c\ndef matrix_multiplication_for_conv2d_flatten(input,kernel,bias=0,stride=1,padding=0):\n\n  if padding &gt;0:\n    input = F.pad(input,(padding,padding,padding,padding))\n\n\n  input_h,input_w = input.shape\n  kernel_h,kernel_w = kernel.shape\n\n  output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u9ad8\u5ea6\n  output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u5bbd\u5ea6 \n  output = torch.zeros(output_h,output_w) # \u521d\u59cb\u5316 \u8f93\u51fa\u77e9\u9635\n\n  region_matrix = torch.zeros(output.numel(),kernel.numel()) #\u5b58\u50a8\u7740\u6240\u6709\u62c9\u5e73\u540e\u7279\u5f81\u533a\u57df\n  kernel_matrix = kernel.reshape(kernel.numel(),1) # \u5b58\u50a8\u7740kernel\u7684 \u5217\u5411\u91cf\uff08\u77e9\u9635\uff09\u5f62\u5f0f\n  row_index = 0\n\n  for i in range(0,input_h - kernel_h + 1,stride): # \u5bf9\u9ad8\u5ea6\u8fdb\u884c\u904d\u5386\n    for j in range(0,input_w - kernel_w +1,stride):  # \u5bf9\u5bbd\u5ea6\u7ef4\u8fdb\u884c\u904d\u5386\n      region = input[i:i+kernel_h, j:j+kernel_w]  # \u53d6\u51fa\u88ab\u6838\u6ed1\u52a8\u5230\u7684\u533a\u57df\n      region_vector = torch.flatten(region)\n      region_matrix[row_index] = region_vector\n      row_index +=1\n\n  output_matrix = region_matrix @ kernel_matrix\n  output = output_matrix.reshape((output_h,output_w))+bias\n\n  return output\n\n\n# \u77e9\u9635\u8fd0\u7b97\u5b9e\u73b0\u5377\u79ef\u7684\u7ed3\u679c\nmat_mul_conv_output = matrix_multiplication_for_conv2d(input,kernel,bias = bias,stride=2,padding=1)\n# print(mat_mul_conv_output)\n\n# \u8c03\u7528pytorch api\u5377\u79ef\u7684\u7ed3\u679c\npytorch_api_conv_output = F.conv2d(input.reshape((1,1,input.shape[0],input.shape[1])),\n                                   kernel.reshape((1,1,kernel.shape[0],kernel.shape[1])),\n                                   padding=1,bias=bias,stride=2).squeeze(0).squeeze(0)\n\n# \u77e9\u9635\u8fd0\u7b97\u5b9e\u73b0\u5377\u79ef\u7684\u7ed3\u679c flatten input\u7248\u672c\nmat_mul_conv_output_flatten = matrix_multiplication_for_conv2d_flatten(input,kernel,bias = bias,stride=2,padding=1)\n# \u9a8c\u8bc1\u4e86 flatten\u7248\u672c\u5377\u79ef \u4e0e pytorch \u5b98\u65b9\u5377\u79ef\u7684\u7ed3\u679c\uff0c\u6b63\u786e\nflag1 = torch.allclose(mat_mul_conv_output,pytorch_api_conv_output)\nflag2 = torch.allclose(mat_mul_conv_output_flatten,pytorch_api_conv_output)\nprint(flag1)\nprint(flag2)\n</code></pre> <p>step3 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6</p> <pre><code># step3 \u7528\u539f\u59cb\u7684\u77e9\u9635\u8fd0\u7b97\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u8003\u8651 batch size\u7ef4\u5ea6 \u548c channel\u7ef4\u5ea6\ndef matrix_multiplication_for_conv2d_full(input,kernel,bias=0,stride=1,padding=0):\n\n  # input kernel \u90fd\u662f4\u7ef4\u5f20\u91cf\n  if padding &gt;0:\n    input = F.pad(input,(padding,padding,padding,padding,0,0,0,0))\n\n  bs,in_channel,input_h,input_w = input.shape\n  out_channel,in_channel,kernel_h,kernel_w = kernel.shape\n\n  if bias is None:\n    bias = torch.zeros(out_channel)\n\n\n  output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u9ad8\u5ea6\n  output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # \u5377\u79ef\u8f93\u51fa\u7684\u5bbd\u5ea6 \n  output = torch.zeros(bs,out_channel,output_h,output_w) # \u521d\u59cb\u5316 \u8f93\u51fa\u77e9\u9635\n\n\n  for ind in range(bs):\n    for oc in range(out_channel):\n      for ic in range(in_channel):\n        for i in range(0,input_h - kernel_h + 1,stride): # \u5bf9\u9ad8\u5ea6\u8fdb\u884c\u904d\u5386\n          for j in range(0,input_w - kernel_w +1,stride):  # \u5bf9\u5bbd\u5ea6\u7ef4\u8fdb\u884c\u904d\u5386\n            region = input[ind,ic,i:i+kernel_h, j:j+kernel_w]  # \u53d6\u51fa\u88ab\u6838\u6ed1\u52a8\u5230\u7684\u533a\u57df\n            output[ind,oc,int(i/stride),int(j/stride)] += torch.sum(region * kernel[oc,ic]) # \u70b9\u4e58 \u5e76\u8d4b\u503c\u7ed9\u8f93\u51fa\u4f4d\u7f6e\u7684\u5143\u7d20 \n      output[ind,oc] += bias[oc]\n  return output\n\ninput = torch.randn(2,2,5,5)  # bs*in_channel*in_h*in_w\nkernel = torch.randn(3,2,3,3) # out_channel*in_channel*kernel_h*kernel_w\nbias = torch.randn(3)\n\n# \u9a8c\u8bc1matrxi_multiplication_for_conv2d_full\u4e0e\u5b98\u65b9API\u7ed3\u679c\u662f\u5426\u4e00\u81f4\npytorch_api_conv_output = F.conv2d(input,kernel,bias=bias,padding=1,stride=2)\nmm_conv2d_full_output = matrix_multiplication_for_conv2d_full(input,kernel,bias=bias,padding=1,stride=2)\nflag = torch.allclose(pytorch_api_conv_output,mm_conv2d_full_output)\nprint(\"all close:\",flag)\n</code></pre> <p>step4 \u901a\u8fc7\u5bf9kernel\u8fdb\u884c\u5c55\u5f00\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5e76\u63a8\u5bfc\u51fa\u8f6c\u7f6e\u5377\u79ef\uff0c\u4e0d\u8003\u8651batch\u3001channel\u5927\u5c0f\uff0c\u4e0d\u8003\u8651padding\uff0c\u5047\u8bbestride=1</p> <pre><code># step4 \u901a\u8fc7\u5bf9kernel\u8fdb\u884c\u5c55\u5f00\u6765\u5b9e\u73b0\u4e8c\u7ef4\u5377\u79ef\uff0c\u5e76\u63a8\u5bfc\u51fa\u8f6c\u7f6e\u5377\u79ef\uff0c\u4e0d\u8003\u8651batch\u3001channel\u5927\u5c0f\uff0c\u4e0d\u8003\u8651padding\uff0c\u5047\u8bbestride=1\ndef get_kernel_matrix(kernel,input_size):\n    # \u57fa\u4e8ekernel\u548c\u8f93\u5165\u7279\u5f81\u56fe\u7684\u5927\u5c0f\u6765\u5f97\u5230\u586b\u5145\u62c9\u76f4\u540e\u7684kernel\u5806\u53e0\u540e\u7684\u77e9\u9635\n    kernel_h,kernel_w = kernel.shape\n    input_h,input_w = input.shape\n    num_out_fea_map = (input_h-kernel_h+1)*(input_w-kernel_w+1)  # \u5377\u79ef\u516c\u5f0f\n    result = torch.zeros((num_out_fea_map,input_h*input_w)) #\u521d\u59cb\u5316\u7ed3\u679c\u77e9\u9635\uff0c\u8f93\u51fa\u7279\u5f81\u56fe\u5143\u7d20\u4e2a\u6570*\u8f93\u5165\u7279\u5f81\u56fe\u5143\u7d20\u4e2a\u6570\n    count = 0\n    for i in range(0,input_h-kernel_h+1,1):\n        for j in range(0,input_w - kernel_w +1,1):\n            # \u586b\u5145\u6210 \u8ddf \u8f93\u5165\u7279\u5f81\u56fe\u4e00\u6837\u5927\u5c0f\n            # padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j))\n            padded_kernel = F.pad(kernel,(j,input_h-kernel_h-j,i,input_w-kernel_w-i))\n            result[count] = padded_kernel.flatten()\n            count +=1\n    return result  \n\n\n\n# \u6d4b\u8bd51\uff1a\u9a8c\u8bc1 \u4e8c\u7ef4\u5377\u79ef\nkernel = torch.randn(3,3)\ninput = torch.randn(4,4)\nkernel_matrix = get_kernel_matrix(kernel,input.shape)  # 4*16\n\n# \u901a\u8fc7\u77e9\u9635\u76f8\u4e58\u6765\u8ba1\u7b97\u5377\u79ef\nmm_conv2d_output = kernel_matrix @ input.reshape((-1,1))  \n\n# pytorch conv2d API\npytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0),kernel.unsqueeze(0).unsqueeze(0))\n# print(kernel)\n# print(kernel_matrix)\n# print(mm_conv2d_output)\n# print(pytorch_conv2d_output)\n\n# \u6d4b\u8bd52  \u901a\u8fc7\u77e9\u9635\u4e58\u79ef\u6765\u8ba1\u7b97\u8f6c\u7f6e\u5377\u79ef || \u9a8c\u8bc1\u4e8c\u7ef4\u8f6c\u7f6e\u5377\u79ef\nmm_transposed_conv2d_output = kernel_matrix.transpose(-1,-2) @ mm_conv2d_output\npytorch_transposed_conv2d_conv2d = F.conv_transpose2d(pytorch_conv2d_output,kernel.unsqueeze(0).unsqueeze(0))  #API\nprint(mm_transposed_conv2d_output.reshape(4,4))\nprint(pytorch_transposed_conv2d_conv2d)\n</code></pre> <p>\u5206\u7ec4\u5377\u79ef&amp;\u81a8\u80c0\u5377\u79ef</p> <pre><code>def matrix_multiplication_for_conv2d_finall(input,kernel,bias=None,stride=1,\n                                            padding=0,dilation=1,groups=1):\n    if padding&gt;0:\n        input = F.pad(input,(padding,padding,padding,padding,0,0,0,0))\n\n    bs,in_channel,input_h,input_w = input.shape\n    out_channel,_,kernel_h,kernel_w = kernel.shape\n\n    assert out_channel % groups == 0 and in_channel % groups==0,\"groups\u5fc5\u987b\u8981\u540c\u65f6\u88ab\u8f93\u5165\u901a\u9053\u548c\u8f93\u51fa\u901a\u9053\u6570\u6574\u9664\uff01\"\n    input = input.reshape((bs,groups,in_channel//groups,input_h,input_w))\n    kernel = kernel.reshape((groups,out_channel//groups,in_channel//groups,kernel_h,kernel_w))\n\n    kernel_h = (kernel_h-1)*(dilation-1)+kernel_h\n    kernel_w = (kernel_w-1)*(dilation-1)+kernel_w\n\n    output_h = math.floor((input_h-kernel_h)/stride)+1\n    output_w = math.floor((input_w-kernel_w)/stride)+1\n\n    output_shape = (bs,groups,out_channel//groups,output_h,output_w)\n    output = torch.zeros(output_shape)\n\n    if bias is None:\n        bias = torch.zeros(out_channel)\n\n    for ind in range(bs): # \u5bf9batch size\u8fdb\u884c\u904d\u5386\n        for g in range(groups): # \u5bf9\u7fa4\u7ec4\u8fdb\u884c\u904d\u5386\n            for oc in range(out_channel//groups): # \u5bf9\u5206\u7ec4\u540e\u7684\u8f93\u51fa\u901a\u9053\u8fdb\u884c\u904d\u5386\n                for ic in range(in_channel//groups): # \u5bf9\u5206\u7ec4\u540e\u7684\u8f93\u5165\u901a\u9053\u8fdb\u884c\u904d\u5386\n                    for i in range(0,input_h-kernel_h+1,stride): #\u5bf9\u9ad8\u5ea6\u904d\u5386\n                        for j in range(0,input_w-kernel_w+1,stride): # \u5bf9\u5bbd\u5ea6\u904d\u5386\n                            region = input[ind,g,ic,i:i+kernel_h:dilation,j:j+kernel_w:dilation] #\u7279\u5f81\u533a\u57df\n                            output[ind,g,oc,int(i/stride),int(j/stride)] += torch.sum(region*kernel[g,oc,ic])\n\n                output[ind,g,oc] += bias[g*(out_channel//groups)+oc]  # \u8003\u8651\u504f\u7f6e\u9879\n    output = output.reshape((bs,out_channel,output_h,output_w))  # \u8fd8\u539f\u6210\u56db\u7ef4\u5f20\u91cf\n    return output\n\n# \u9a8c\u8bc1\u6d4b\u8bd5\u7684\u4ee3\u7801\nkernel_size=3\nbs,in_channel,input_h,input_w = 2,2,5,5\nout_channel=4\ngroups,dilation,stride,padding=2,2,2,1\n\ninput = torch.randn(bs,in_channel,input_h,input_w)\nkernel = torch.randn(out_channel,in_channel//groups,kernel_size,kernel_size)\nbias = torch.randn(out_channel)\n\n# pytorch API\u7684\u7ed3\u679c\npytorch_conv2d_api_output = F.conv2d(input,kernel,bias=bias,padding=padding,\n                                     stride=stride,dilation=dilation,groups=groups)\nmm_conv2d_finall_output = matrix_multiplication_for_conv2d_finall(input,kernel,bias=bias,padding=padding,\n                                                                  stride=stride,dilation=dilation,groups=groups)\n\nflag = torch.allclose(pytorch_conv2d_api_output,mm_conv2d_finall_output)\nprint(flag)\n</code></pre>"},{"location":"learning/pe/","title":"4\u79cd\u4f4d\u7f6e\u7f16\u7801","text":"<ul> <li> \u4e00\u7ef4\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801</li> <li> \u4e00\u7ef4\u53ef\u5b66\u4e60\u4f4d\u7f6e\u7f16\u7801</li> <li> \u4e8c\u7ef4\u76f8\u5bf9\u504f\u7f6e\u4f4d\u7f6e\u7f16\u7801</li> <li> \u4e8c\u7ef4\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801</li> <li> \u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801</li> </ul>"},{"location":"learning/pe/#_1","title":"\u4f4d\u7f6e\u7f16\u7801\u4e3a\u4ec0\u4e48\u662f\u4e09\u89d2\u51fd\u6570\u5f62\u5f0f\u7684\uff1f","text":"<ol> <li>\u6700\u76f4\u89c2\u7684\u7f16\u7801\u65b9\u5f0f\u662f\u4ece0\u5230sequence length\uff0c\u4f46\u662f\u65e0\u754c</li> <li>\u7528 \\(\\frac{1}{sequence\\_length}\\) \u6539\u53d8\u4e86\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u7f6e</li> <li>\u4e8c\u8fdb\u5236\u7f16\u7801\uff0cd model\u901a\u5e38\u8bbe\u7f6e\u4e3a512,2\u7684512\u6b21\u65b9\u80fd\u7f16\u7801\u5b8c max sequence length\u4e2a\u4f4d\u7f6e\uff0c\u4f46\u662f\u662f\u79bb\u6563\u7684</li> <li>\u8fde\u7eed\uff0c\u5e26\u6709\u5468\u671f\u6027\u7684\u4e09\u89d2\u51fd\u6570\u4f4d\u7f6e\u7f16\u7801\uff0c\u7c7b\u4f3c\u4e8c\u8fdb\u5236\uff0c\u4f4e\u4f4d\u53d8\u5316\u5feb\uff0c\u9ad8\u4f4d\u53d8\u5316\u6162</li> </ol> <p>\u301046\u3001\u56db\u79cdPosition Embedding\u7684\u539f\u7406\u4e0ePyTorch\u624b\u5199\u9010\u884c\u5b9e\u73b0\uff08Transformer/ViT/Swin-T/MAE\uff09-\u54d4\u54e9\u54d4\u54e9\u3011</p> <p></p>"},{"location":"learning/pe/#transformer","title":"\u539f\u59cbTransformer\u7684\u4f4d\u7f6e\u7f16\u7801 \uff1a\u4e00\u7ef4\u7edd\u5bf9\u3001\u5e38\u6570\u4f4d\u7f6e\u7f16\u7801","text":"<p>pos\uff1a\u53e5\u5b50\u4e2d\u8bcd\u7684\u4f4d\u7f6e\uff080-max sequence length\uff09</p> <p>i\uff1a\u8bcd\u5d4c\u5165\u4f4d\u7f6e\uff080\u2014255\uff09</p> <p>1D\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e38\u6570\u4e0d\u9700\u8981\u8bad\u7ec3</p> <p>\u4ee3\u7801\u5b9e\u73b0\uff1a</p> <p>(\u7c7b\u5199\u6cd5)</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SinCosPositionEmbedding(nn.Module):\n    def __init__(self, max_sequence_length,model_dim):\n        super().__init__()\n        self.max_sequence_length = max_sequence_length\n        self.model_dim = model_dim\n    def forward(self):\n        pe = torch.zeros(self.max_sequence_length,self.model_dim)\n        pos_mat = torch.arange(self.max_sequence_length).reshape(-1,1)\n        i_mat = torch.pow(10000,\n                          torch.arange(0,self.model_dim,2).reshape(1,-1)/self.model_dim\n                          )\n\n        pe[:,0::2] = torch.sin(pos_mat/i_mat)\n        pe[:,1::2] = torch.cos(pos_mat/i_mat)\n\n        return pe\nprint(SinCosPositionEmbedding(max_sequence_length=8,model_dim=4).forward())\n</code></pre> <p>\uff08\u51fd\u6570\u5199\u6cd5\uff09</p> <pre><code>def position_sincos_embedding(max_sequence_length,model_dim):\n    assert model_dim%2 == 0,\"wrong dimension\"\n    pe_table = torch.zeros(max_sequence_length,model_dim)\n    pos_mat = torch.arange(max_sequence_length).reshape(-1,1)\n    i_mat = torch.pow(\n        10000,\n        torch.arange(0,model_dim,2)/model_dim\n    )\n    pe_table[:,0::2]=torch.sin(pos_mat/i_mat)\n    pe_table[:,1::2]=torch.cos(pos_mat/i_mat)\n    return pe_table\n\n# Transformer\u8bba\u6587 \u4e00\u7ef4\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\nif __name__==\"__main__\":\n    max_sequence_length = 8\n    model_dim = 4\n    pe_table = position_sincos_embedding(max_sequence_length,model_dim)\n    print(pe_table)\n</code></pre>"},{"location":"learning/pe/#vit-1","title":"ViT  1\u7ef4\u7edd\u5bf9\u7684\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801","text":"<p>\u6807\u51c6\u7684\u3001\u53ef\u5b66\u4e60\u7684\u4e00\u7ef4\u4f4d\u7f6e\u7f16\u7801\uff1b\u4e8c\u7ef4\u7684\u4f4d\u7f6e\u7f16\u7801\u5e76\u6ca1\u6709\u5e26\u6765\u66f4\u597d\u7684\u6548\u679c</p> <pre><code>def create_1d_absolute_trainable_embeddings(max_sequence_length,model_dim):\n    pe = nn.Embedding(max_sequence_length,model_dim)\n    nn.init.constant_(pe.weight,0.)\n\n    return pe\n</code></pre>"},{"location":"learning/pe/#swintransformer-2","title":"SwinTransformer 2\u7ef4\u7684\u3001\u76f8\u5bf9\u7684\u3001\u57fa\u4e8e\u4f4d\u7f6e\u504f\u5dee\u53ef\u8bad\u7ec3\u7684\u4f4d\u7f6e\u7f16\u7801","text":"<ul> <li>\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u3001\u53ef\u5b66\u4e60\u7684\u3001\u76f8\u5bf9\u4f4d\u7f6e\u504f\u5dee\u52a0\u5230\u6bcf\u4e00\u4e2a\u5934\u4e0a</li> <li>\\(QK^T\\) \u7684\u7ef4\u5ea6\u662f \\(\u5e8f\u5217\u957f\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6\\)\uff0c\u6240\u4ee5B\u7684\u5f62\u72b6\u4e5f\u662f  \\(\u5e8f\u5217\u957f\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6\\)</li> <li>\u8003\u8651head\uff0c\u90a3\u4e48\u5f62\u72b6\u662f \\(num\\_head \\times L \\times L\\)</li> <li>\u7531\u4e8e\u662f\u53ef\u5b66\u4e60\u7684\uff0c\u8981\u8ba1\u7b97\u4e24\u4e24Patch\u7684\u504f\u5dee\uff0c\\(Position\\_bias\\)\uff0c\u628a\\(bias\\)\u5f53\u6210\u7d22\u5f15\uff0c\u4ece\\(bias\\_matrix\\)\u91cc\u67e5\u627e\u5230\\(learnable \\_ vector\\)\uff0c\u5373\u53ef\u5b66\u4e60\u7684\u5411\u91cf</li> <li>\u53ef\u4ee5\u770b\u5230 \u504f\u5dee\u77e9\u9635\u662f \\(\\hat{B} \\in \\mathbb{R}^{(2M-1) \\times (2M-1)}\\)</li> </ul> <p>\u4ee3\u7801\uff1a</p> <ul> <li> <p>\u9996\u5148\uff0c\u7531\u4e8e\u662f\u4e8c\u7ef4\u7684\uff0c\u6240\u4ee5\u65e2\u8981\u8003\u8651\u6a2a\u8f74\uff0c\u53c8\u8981\u8003\u8651\u7eb5\u8f74 </p> </li> <li> <p>\u4e8c\u7ef4\u3001\u76f8\u5bf9\u7684\u3001\u57fa\u4e8ebias\u7684\u3001\u53ef\u8bad\u7ec3\u7684\u4f4d\u7f6e\u7f16\u7801 <code>create_2d_relative_bias_trainable_embeddings</code></p> </li> </ul> <pre><code>def create_2d_relative_bias_trainable_embeddings(n_head,height,width,dim):\n    # width:5,[0,1,2,3,4],bias=[-width+1,width-1],2*width-1\n    # height:5,[0,1,2,3,4],bias=[-height+1,height-1],2*height-1\n\n    position_embedding = nn.Embedding((2*width-1)*(2*height-1),n_head)\n    nn.init.constant_(position_embedding.weight,0.)\n\n    def get_relative_position_index(height,width):\n        m1,m2 = torch.meshgrid(torch.arange(height),torch.arange(width))\n        coords = torch.stack(m1,m2) #[2,height,width]\n        coords_flatten = torch.flatten(coords,1) #[2,height*width]\n\n        # \u628a\u504f\u5dee\u53d8\u6210\u6b63\u6570\uff0c\u7136\u540e\u4eceposition_embedding\u4e2d\u6309\u7d22\u5f15\u53d6\u503c\n        relative_coords_bias = coords_flatten[:,:,None]-coords_flatten[:,None,:] # [2,height*width,height*width]\n\n        relative_coords_bias[0,:,:] += height-1\n        relative_coords_bias[1,:,:] += width-1\n\n        # A:2d,B:1d,B[[i*cols+j] = A[i,j]\n        relative_coords_bias[0,:,:] *= relative_coords_bias[1,:,:].max()+1\n\n        return relative_coords_bias.sum(0) # [height*width,height*width]\n    relative_position_bias = get_relative_position_index(height,width)\n    bias_embedding = position_embedding(torch.flatten(relative_position_bias)).reshape(height*width,height*width,n_head) #[height*width,height*width,n_head]\n\n    bias_embedding = position_embedding.permute(2,0,1).unsqueeze(0) # [1,n_head,height*width,height*width]\n\n    return bias_embedding\n</code></pre>"},{"location":"learning/pe/#mae","title":"MAE\u4e2d\u7684\u4f4d\u7f6e\u7f16\u7801","text":"<p>\u9644\u5f55\u90e8\u5206</p> <p></p> <ul> <li>sin\u3001cos\u4f4d\u7f6e\u7f16\u7801</li> <li>\u6ca1\u6709\u76f8\u5bf9\u4f4d\u7f6e\u6216\u8005Layer scaling</li> <li>\u4e8c\u7ef4\u7684\u3001\u7edd\u5bf9\u7684 sin cos embedding</li> </ul> <pre><code># 4.2d absolute constant sincos embedding\n# Masked AutoEncoder \u8bba\u6587\ndef create_2d_absolute_sincos_embeddings(height,width,dim):\n    assert dim%4 ==0,\"wrong dimension!\"\n    position_embedding = torch.zeros(height*width,dim)\n    m1,m2 = torch.meshgrid(torch.arrange(height,dtype=torch.float),torch.arrange(width,dtype=torch.float))\n    coords = torch.stack(m1,m2)  # [2,height*width]\n\n    height_embedding = create_1d_absolute_sincos_embeddings(torch.flatten(coords[0]),dim//2)  # [height*width,dim//2]\n    width_embedding = create_1d_absolute_sincos_embeddings(torch.flatten(coords[1]),dim//2)  # [height*width,dim//2]\n\n    position_embedding[:,:dim//2] = height_embedding\n    position_embedding[:,:dim//2] = width_embedding\n\n    return position_embedding\n</code></pre> <p>\u5168\u90e8\u4ee3\u7801</p> <p></p> <p></p> <p></p> <p></p> <ul> <li> \u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801</li> </ul>"},{"location":"learning/swintransformer/","title":"SwinTransformer \u5b66\u4e60\u7b14\u8bb0","text":"<ul> <li>SwinTransformer \u5b66\u4e60\u7b14\u8bb0<ul> <li>SwinTransformer\u6548\u679c\u6709\u591a\u597d</li> <li>\u5185\u5bb9\u76ee\u5f55</li> <li>\u5bf9\u6bd4SwinTransformer&amp;ViT</li> <li>\u7f51\u7edc\u6574\u4f53\u6846\u67b6</li> <li>\u4ec0\u4e48\u662fpatch partition?</li> <li>\u4e3a\u4ec0\u4e48SwinTransformer block\u90fd\u662f\u5076\u6570\u6b21\uff1f</li> <li>Patch merging\uff1f</li> <li>W-MSA\uff1f</li> <li>MSA\u548cW-MSA\u8ba1\u7b97\u91cf\u7684\u8ba8\u8bba</li> <li>shifted window multihead self attention\u6a21\u5757<ul> <li>\u4e3a\u4ec0\u4e48\u9700\u8981 shift window multihead self attention\uff1f</li> <li>\u600e\u4e48\u7406\u89e3shift \uff1f</li> <li>\u600e\u4e48\u7406\u89e3shift\u4ee5\u540e\u7684\u4fe1\u606f\u4ea4\u4e92\uff1f</li> <li>shift\u4e4b\u540e\uff0c\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u4f1a\u51fa\u73b0\u7684\u65b0\u95ee\u9898\uff1f</li> <li>\u89e3\u51b3\u7a97\u53e3\u53d8\u591a\u4e14\u4e0d\u4e00\u6837\u5927\u8ba1\u7b97 \u81ea\u6ce8\u610f\u529b\u7684\u95ee\u9898</li> <li>shift window\u548cmask</li> <li>\u533a\u57df3\u548c\u533a\u57df5 \u7684mask</li> </ul> </li> <li>\u7ee7\u7eed\u8ba8\u8bba SW-MSA</li> <li>\u76f8\u5bf9\u4f4d\u7f6e\u504f\u79fb\uff1frelative position bias<ul> <li>\u4ec0\u4e48\u662f\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e\uff1f</li> <li>\u4e8c\u5143\u5750\u6807\u600e\u4e48\u8f6c\u5316\u6210\u4e00\u5143\u5750\u6807</li> <li>\u4ece\u76f8\u5bf9\u4f4d\u7f6e\u7d22\u5f15\u5230\u76f8\u5bf9\u4f4d\u7f6e\u504f\u79fb</li> </ul> </li> <li>\u6a21\u578b\u8be6\u7ec6\u914d\u7f6e\u53c2\u6570</li> </ul> </li> </ul> <p>\u301012.1 Swin-Transformer\u7f51\u7edc\u7ed3\u6784\u8be6\u89e3-\u54d4\u54e9\u54d4\u54e9\u3011</p> <p>\u7b2c\u4e00\u6b21\u5b66\u7684\u65f6\u5019\uff0c\u6240\u6709\u7684swin\u5168\u90fd\u5199\u6210\u4e86swim\uff0c\u5446\uff0c\u53c8\u53cc\u53d2\u53d5\u4e22\u4eba\u663e\u773c\u4e86\uff0c\u7b11\uff09</p>"},{"location":"learning/swintransformer/#swintransformer_1","title":"SwinTransformer\u6548\u679c\u6709\u591a\u597d","text":"<p>2021\u5e743\u6708\u53d1\u8868</p> <p>COCO\u6570\u636e\u96c6 \u76ee\u6807\u68c0\u6d4b\u4e0b\u7684\u6a21\u578b\u6392\u540d \u90fd\u6709SwinTransformer\u7684\u5f71\u5b50</p> <p></p> <p></p>"},{"location":"learning/swintransformer/#_1","title":"\u5185\u5bb9\u76ee\u5f55","text":"<p>\u5b8c\u5168\u4e0d\u8bb0\u5f97\uff0c\u6211\u5c45\u7136\u5b66\u8fc7\u8fd9\u4e9b\uff0c\u4e5f\u7b97\u662f \u590d\u4e60\u4e86-.-</p> <p>\u7b2c\u56db\u70b9\u3001\u7b2c\u4e94\u70b9\u4e0d\u597d\u7406\u89e3</p>"},{"location":"learning/swintransformer/#swintransformervit","title":"\u5bf9\u6bd4SwinTransformer&amp;ViT","text":"<p>\u4e24\u70b9\u4e0d\u540c\uff1a</p> <p>\u7b2c\u4e00\u70b9\uff1a</p> <p>\uff08a\uff09swintransformer\u7684feature map\u5177\u6709\u5c42\u6b21\u6027\uff0c\u7c7b\u4f3cCNN\uff0c\u968f\u7740\u5c42\u7684\u52a0\u6df1\uff0cfeature map\u7684\u9ad8\u548c\u5bbd\u8d8a\u6765\u8d8a\u5c0f\uff0c\u5206\u522b\u662f4\u500d\u4e0b\u91c7\u6837\u30018\u500d\u4e0b\u91c7\u6837\u548c16\u500d\u4e0b\u91c7\u6837\uff0c\u56e0\u4e3a\u5c42\u6b21\u6027\u7684\u7279\u5f81\u56fe\uff0c\u6240\u4ee5\u5bf9\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0c\u5206\u5272\u4efb\u52a1\u6709\u66f4\u5927\u7684\u4f18\u52bf</p> <p>(b)ViT  \u4e00\u76f4\u662f16\u500d\u7684\u4e0b\u91c7\u6837\u7387</p> <p>\u7b2c\u4e8c\u70b9\uff1a</p> <p>SwinTransformer\u4f7f\u7528\u4e00\u4e2a\u4e00\u4e2a\u7a97\u53e3\u7684\u5f62\u5f0f\u628afeature map\u5206\u5f00\u4e86\uff0c\u7a97\u53e3\u4e0e\u7a97\u53e3\u4e4b\u95f4 \u4e0d\u91cd\u53e0</p> <p>ViT\u4e2d\uff0c\u7a97\u53e3\u662f\u4e00\u4e2a\u6574\u4f53\uff0c\u6ca1\u6709\u5206\u5272</p> <p></p> <p>\u770b\u6700\u540e\u4e00\u5217</p> <p></p>"},{"location":"learning/swintransformer/#_2","title":"\u7f51\u7edc\u6574\u4f53\u6846\u67b6","text":""},{"location":"learning/swintransformer/#patch-partition","title":"\u4ec0\u4e48\u662fpatch partition?","text":"<p>\u662f\u53d6\u539f\u59cb\u56fe\u50cf\u7684\u50cf\u7d20\u503c\uff0c\u7136\u540e\u5c55\u5e73\uff0c\u6211\u89c9\u5f97\u8fd9\u91cc\u6709\u70b9\u95ee\u9898\uff0c\\(4\u00d74\u00d73 \\rightarrow 16\u00d73\\) \u5e94\u8be5\u662f2\u00d72\u7684window\uff1f</p> <p></p> <p></p> <p>\u4e3a\u4ec0\u4e48\u8fd9\u91cc\u662f48\u4e2akernel\uff1f</p>"},{"location":"learning/swintransformer/#swintransformer-block","title":"\u4e3a\u4ec0\u4e48SwinTransformer block\u90fd\u662f\u5076\u6570\u6b21\uff1f","text":""},{"location":"learning/swintransformer/#patch-merging","title":"Patch merging\uff1f","text":""},{"location":"learning/swintransformer/#w-msa","title":"W-MSA\uff1f","text":"<p>\u7f3a\u70b9\uff1awindow\u4e4b\u95f4\u6ca1\u6709\u4fe1\u606f\u4ea4\u4e92\uff0c\u90a3\u4e48\u611f\u53d7\u91ce\u5c31\u4f1a\u53d8\u5c0f\uff0c\u4e5f\u5c31\u662f\u6ca1\u6709\u529e\u6cd5\u770b\u5230\u5168\u5c40\u7684\u89c6\u91ce\uff0c\u6700\u7ec8\u9884\u6d4b\u7ed3\u679c\u4f1a\u4e0d\u597d</p>"},{"location":"learning/swintransformer/#msaw-msa","title":"MSA\u548cW-MSA\u8ba1\u7b97\u91cf\u7684\u8ba8\u8bba","text":"<p>\u666e\u901a\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 \u548c \u5e26\u7a97\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236</p> <p></p> <p></p> <p></p> <p>\u4ee5\u4e0b\u6765\u81ea\uff1aup\u4e3b\u7684\u535a\u6587</p> <p></p> <p>\u8865\u5145\u77e9\u9635\u4e58\u6cd5\u5f53\u4e2d   FLOPS\u8ba1\u7b97\u65b9\u5f0f\uff1a</p> <p></p> <p>\u5982\u679c\u77e9\u9635A\u662fa\u00d7b\u7684\uff0c\u77e9\u9635B\u662fb\u00d7c\u7684\uff0c\u90a3\u4e48\u77e9\u9635\u76f8\u4e58\u4ee5\u540e\u7684flops\u8ba1\u7b97\u91cf\u662f a\u00d7b\u00d7c\u7684</p> <p></p> <p>\u5355\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u91cfflops</p> <p></p> <p>\u6240\u4ee5\u4f7f\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6700\u540e\u8fd8\u8981\u4f7f\u7528 \\(W^O\\)\u7684\u8ba1\u7b97\u91cf</p> <p></p> <p></p>"},{"location":"learning/swintransformer/#shifted-window-multihead-self-attention","title":"shifted window multihead self attention\u6a21\u5757","text":""},{"location":"learning/swintransformer/#shift-window-multihead-self-attention","title":"\u4e3a\u4ec0\u4e48\u9700\u8981 shift window multihead self attention\uff1f","text":"<p>\u4e3a\u4e86\u89e3\u51b3window\u4e0ewindow\u4e4b\u95f4\u6ca1\u6709\u4fe1\u606f\u4ea4\u4e92\u7684\u95ee\u9898</p> <p></p> <p></p>"},{"location":"learning/swintransformer/#shift","title":"\u600e\u4e48\u7406\u89e3shift \uff1f","text":"<p>\u4e0b\u9762\u56fe\uff0c\u9ec4\u8272\u6846 \u5411\u4e0b\u3001\u5411\u53f3\u79fb\u52a8</p> <p></p>"},{"location":"learning/swintransformer/#shift_1","title":"\u600e\u4e48\u7406\u89e3shift\u4ee5\u540e\u7684\u4fe1\u606f\u4ea4\u4e92\uff1f","text":""},{"location":"learning/swintransformer/#shift_2","title":"shift\u4e4b\u540e\uff0c\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u4f1a\u51fa\u73b0\u7684\u65b0\u95ee\u9898\uff1f","text":""},{"location":"learning/swintransformer/#_3","title":"\u89e3\u51b3\u7a97\u53e3\u53d8\u591a\u4e14\u4e0d\u4e00\u6837\u5927\u8ba1\u7b97 \u81ea\u6ce8\u610f\u529b\u7684\u95ee\u9898","text":"<p>\u539f\u6587\u4e2d\u7ed9\u7684\u89e3\u91ca\uff0c\u4e0d\u597d\u770b\u61c2\uff0cup\u4e3b\u7ed9\u51fa\u4e86\u81ea\u5df1\u7684\u89e3\u51b3\u65b9\u5f0f</p> <p></p> <p></p>"},{"location":"learning/swintransformer/#shift-windowmask","title":"shift window\u548cmask","text":""},{"location":"learning/swintransformer/#35-mask","title":"\u533a\u57df3\u548c\u533a\u57df5 \u7684mask","text":""},{"location":"learning/swintransformer/#sw-msa","title":"\u7ee7\u7eed\u8ba8\u8bba SW-MSA","text":"<p>\u79fb\u52a8\u7684\u89c4\u5f8b\uff0c\u5148\u628a\u4e0a\u9762\u79fb\u52a8\u5230\u4e0b\u9762\uff0c\u518d\u628a\u5de6\u8fb9\u79fb\u52a8\u5230\u53f3\u8fb9</p> <p></p> <p>\u6ce8\u610f\u770b \u8fd9\u91cc\u9ec4\u8272 \u6bcf\u4e2awindow\u90fd\u80fd\u878d\u5408\u56db\u4e2awindow\u7684\u4fe1\u606f</p> <p>\u5bf9\u4e8e\u7d2b\u8272\u7684\u533a\u57df\u5e76\u4e0d\u662f\u8fde\u7eed\u7684\uff0c\u9700\u8981\u4f7f\u7528mask-msa</p> <p>\u533a\u5206\u597d \u54ea\u91cc\u662f\u8fde\u7eed\u7684 \u54ea\u91cc\u4e0d\u662f\u8fde\u7eed\u7684</p> <p></p> <p>\u8bb0\u5f97\u770b\u4ee3\u7801\u600e\u4e48\u5b9e\u73b0\uff0cup\u4e3b\u4e5f\u8bb2\u4e86</p>"},{"location":"learning/swintransformer/#relative-position-bias","title":"\u76f8\u5bf9\u4f4d\u7f6e\u504f\u79fb\uff1frelative position bias","text":""},{"location":"learning/swintransformer/#_4","title":"\u4ec0\u4e48\u662f\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e\uff1f","text":"<p>\u533a\u5206 \u76f8\u5bf9\u4f4d\u7f6e\u7d22\u5f15\u548c\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e</p> <p></p>"},{"location":"learning/swintransformer/#_5","title":"\u4e8c\u5143\u5750\u6807\u600e\u4e48\u8f6c\u5316\u6210\u4e00\u5143\u5750\u6807","text":""},{"location":"learning/swintransformer/#_6","title":"\u4ece\u76f8\u5bf9\u4f4d\u7f6e\u7d22\u5f15\u5230\u76f8\u5bf9\u4f4d\u7f6e\u504f\u79fb","text":"<p>\u6211\u4eec\u8981\u8bad\u7ec3\u7684\u53c2\u6570\u662f relative Position bias table</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"learning/swintransformer/#_7","title":"\u6a21\u578b\u8be6\u7ec6\u914d\u7f6e\u53c2\u6570","text":""},{"location":"learning/vit/","title":"vision Transformer\u7684\u539f\u7406\u4e0e\u96be\u70b9\u6e90\u7801\u5b9e\u73b0","text":"<ul> <li> \u9739\u96f3\u5427\u5566z</li> </ul> <p>\u301028\u3001Vision Transformer(ViT)\u6a21\u578b\u539f\u7406\u53caPyTorch\u9010\u884c\u5b9e\u73b0-\u54d4\u54e9\u54d4\u54e9\u3011</p>"},{"location":"learning/vit/#1-transformer","title":"1 \u600e\u4e48\u4eceTransformer\u5e94\u7528\u5230\u56fe\u50cf\u8bc6\u522b","text":""},{"location":"learning/vit/#11-encoder","title":"1.1 encoder","text":"<p>\u9996\u5148\u4eceTransformer\u5f00\u59cb\uff0c\u9996\u5148Transformer\u662f\u4e00\u4e2asequence to sequence \u7684\u6846\u67b6\uff0c\u5b83\u5305\u542b encoder\u548cdecoder\u4e24\u4e2a\u90e8\u5206\uff0c\u65e0\u8bba\u662fencoder \u8fd8\u662f decoder\uff0c\u6838\u5fc3\u7684\u5efa\u6a21\u6a21\u5757 \u662f\u7531multihead self Attention\u548cfeed forward Neural network \u8fd9\u4e24\u4e2a\u90e8\u5206 \u6784\u6210\u7684\u3002</p> <p></p>"},{"location":"learning/vit/#111-mhsa-fnn","title":"1.1.1 MHSA\u505a\u7684\u662f \u7a7a\u95f4\u878d\u5408\uff1bFNN \u505a\u7684\u662f \u901a\u9053\u878d\u5408","text":"<p>\u90a3\u5177\u4f53\u6765\u770b\uff0cencoder \u90e8\u5206 \u5c31\u662f\u5c06\u6211\u4eec\u7684\u6e90\u5e8f\u5217\u9001\u5165\u5230 multihead self Attention\u548cFNN \u8fd9\u4e24\u4e2a\u6a21\u5757\u91cc\uff0c\u8fd9\u4e24\u4e2a\u6a21\u5757\u6784\u6210\u4e00\u5c42\uff0c\u90a3\u4e48encoder\u5462\uff0c\u4e00\u822c\u67096\u5c42\uff0c\u8fd9\u6837\u76846\u5c42 \u5806\u53e0\u8d77\u6765\uff0c\u90a3\u6211\u4eec\u9700\u8981\u6ce8\u610f\u4e00\u4e0b \u8fd9\u4e24\u4e2a\u6a21\u5757\u7684\u4f5c\u7528\u662f\u4e0d\u4e00\u6837\u7684[\u91cd\u70b9]\uff0c**\u90a3\u9996\u5148 multihead self Attention\u505a\u7684\u4e8b \u662f\u5bf9\u5404\u4e2a\u4f4d\u7f6e\u4e0a\u7684 embedding \u8fdb\u884c\u4e00\u4e2a\u878d\u5408\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u7406\u89e3\u4e3a \u5b83\u505a\u7684\u662f \u7a7a\u95f4\u878d\u5408\u90e8\u5206\uff0c\u800cFNN\u5462\uff0c\u662fposition-wise\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4 \u5bf9\u6bcf\u4e2a\u4f4d\u7f6e\u4e0a\u5355\u72ec\u8fdb\u884c \u4eff\u5c04\u53d8\u6362\uff0c\u90a3\u6211\u4eec\u53ef\u4ee5\u7406\u89e3\u4e3aFNN \u505a\u7684\u662f \u901a\u9053\u878d\u5408\uff0c**\u6240\u4ee5FNN\u548cMHA \u5e72\u7684\u662f \u4e24\u4e2a \u4e0d\u540c\u7684\u89d2\u5ea6\uff1b</p> <p>MHSA\u505a\u7684\u662f \u7a7a\u95f4\u878d\u5408\uff1bFNN \u505a\u7684\u662f \u901a\u9053\u878d\u5408\uff1b\u4e24\u4e2a\u7684\u4f5c\u7528\u662f\u4e0d\u4e00\u6837\u7684\uff1b</p>"},{"location":"learning/vit/#12-decoder","title":"1.2 decoder","text":"<p>\u7136\u540edecoder\u90e8\u5206 \u4e5f\u662f\u4e00\u4e2a\u7c7b\u4f3c\u7684\u7ed3\u6784\uff0c\u5e76\u4e14 encoder\u548cdecoder \u662f\u901a\u8fc7 cross-Attention \u8fdb\u884c\u4e00\u4e2a\u4ea4\u4e92\uff0c\u6765\u76f8\u4e92\u4f20\u9012\u4fe1\u606f\u7684</p>"},{"location":"learning/vit/#13","title":"1.3 \u5f52\u7eb3\u504f\u7f6e","text":"<p>\u8fd9\u6837\u7684\u7ed3\u6784 \u5c31\u6784\u6210\u4e86\u4e00\u4e2aTransformer\uff0c\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684CNN RNN\u7684\u533a\u522b\u5728\u4e8e\uff0cTransformer \u6ca1\u6709\u5c40\u90e8\u5efa\u6a21\u7684\u5047\u8bbe\uff0c\u4e5f\u6ca1\u6709\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u5047\u8bbe\uff0c\u662f\u4e00\u79cd\u5168\u5c40\u7684\u5047\u8bbe\uff0c\u5bf9\u5e8f\u5217\u8ba1\u7b97\u4e00\u4e2a\u8868\u5f81\uff0c\u4f46\u662f\u5b83\u5f15\u5165\u4e86 \u4e00\u4e2a\u5f52\u7eb3\u504f\u7f6e\uff1aposition embedding\uff0c\u5c31\u662f\u8bf4 \u8fd8\u662f\u6ce8\u5165\u4e86\u4f4d\u7f6e\u4fe1\u606f\uff1b</p>"},{"location":"learning/vit/#131-transformernlpcnn-rnn","title":"1.3.1 Transformer\u5728nlp\u4e2d\uff0c\u4e3a\u4ec0\u4e48\u4f1a\u6bd4CNN RNN\u6548\u679c\u8981\u597d\u5462","text":"<p>\u6362\u4e2a\u89d2\u5ea6\u6765\u8bf4\uff0cTransformer\u5728nlp\u4e2d\uff0c\u4e3a\u4ec0\u4e48\u4f1a\u6bd4CNN RNN\u6548\u679c\u8981\u597d\u5462\uff1f\u5c31\u662f\u56e0\u4e3aTransformer\uff0c\u6240\u5f15\u5165\u7684\u5f52\u7eb3\u504f\u7f6e \u662f\u6bd4\u8f83\u5c11\u7684\uff0c\u5982\u679c\u771f\u7684\u8981\u7b97\u5f52\u7eb3\u504f\u7f6e\u7684\u8bdd\uff0c\u90a3\u4e48\u5c31\u662f\u5f15\u5165\u4e86 position embedding\uff0c\u800c\u5176\u5b83\u4f4d\u7f6e\u5c31\u53ef\u4ee5\u8bf4 \u6ca1\u6709\u5f15\u5165 \u5148\u9a8c\u5047\u8bbe\u548c \u5f52\u7eb3\u504f\u7f6e\u7684\uff1b</p> <p>\u5bf9\u4e8e\u8fd9\u6837\u4e00\u4e2a\u6a21\u578b\uff0c\u5b83\u5b66\u4e60\u4e0a\u754c \u662f\u6bd4\u8f83\u597d\u7684\uff1b\u4e5f\u5c31\u662f\u8bf4 \u5b83\u7684performance\u662f\u6bd4\u8f83\u9ad8\u7684\uff0c\u552f\u4e00\u7684\u7f3a\u9677\u5c31\u8bf4 \u6570\u636e\u91cf\u7684\u8981\u6c42\u548c\u5f15\u5165\u7684\u5f52\u7eb3\u504f\u7f6e \u662f\u6210 \u53cd\u6bd4\u7684\uff0c\u6362\u53e5\u8bdd\u8bf4 \u5c31\u662f\u6211\u4eec\u5f15\u5165\u4e86\u8d8a\u591a\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6211\u4eec\u4eba\u4e3a\u5730\u6ce8\u5165\u4e86 \u4eba\u7c7b\u7684\u7ecf\u9a8c\u6027 \u77e5\u8bc6 \u5c31\u53ef\u4ee5\u5e2e\u52a9\u8fd9\u4e2a\u6a21\u578b \u66f4\u597d\u7684\u53bb\u5b66\u4e60\uff1b</p> <p>\u4e00\u65e6\u5f52\u7eb3\u504f\u7f6e \u5f15\u5165\u7684\u6bd4\u8f83\u5c11\uff0c\u6211\u4eec\u5c31\u671f\u671b\u8fd9\u4e2a\u6a21\u578b\u4ece\u5927\u91cf\u7684\u6570\u636e\u4e2d\uff0c\u5f52\u7eb3\u51fa \u6a21\u578b\u81ea\u5df1\u7684 \u7ecf\u9a8c\u6765\u505a \u8fd9\u4e2a\u4efb\u52a1\u3002\u6240\u4ee5Transformer\u7684\u4f18\u70b9\u662f\uff0c\u4e0a\u9650\u5f88\u9ad8\uff0c\u7f3a\u70b9\u5c31\u662f \u5bf9\u6570\u636e\u91cf\u7684\u8981\u6c42\u6bd4\u8f83\u9ad8\uff1b</p> <p>\u518d\u6b21 \u5f3a\u8c03 \u5f52\u7eb3\u504f\u7f6e\u3002\u5f52\u7eb3\u504f\u7f6e\u5c31\u662f \u6211\u4eec\u4eba\u7c7b \u7528\u5f52\u7eb3\u6cd5 \u6240\u603b\u7ed3\u51fa\u7684\u7ecf\u9a8c\uff0c\u7136\u540e\u6211\u4eec\u628a\u7ecf\u9a8c \u4ee3\u5165\u5230 \u6a21\u578b\u7684\u6784\u5efa\u4e4b\u4e2d\uff0c\u90a3\u4ec0\u4e48\u662f\u5f52\u7eb3\uff1f\u5c31\u662f \u53d1\u73b0 \u5f88\u591a\u4e8b\u7269 \u4e4b\u95f4\u7684 \u5171\u6027\u3002</p> <p>\u4e3e\u4f8b\u5b50\uff0c\u732b\u4f1a\u53eb\u3001\u72d7\u4f1a\u53eb\u3001\u9e2d\u4f1a\u53eb\u3001\u9e21\u4f1a\u53eb\uff0c\u90a3\u6211\u4eec \u603b\u7ed3\u51fa \u52a8\u7269\u90fd\u4f1a\u53eb\uff0c\u8fd9\u4e2a\u5c31\u662f\u4e00\u4e2a \u5f52\u7eb3\u6cd5\uff1b</p> <p>\u76f8\u5bf9\u7684 \u8fd8\u6709\u4e00\u4e2a\u65b9\u6cd5 \u662f \u6f14\u7ece\u6cd5\uff1b\u6f14\u7ece\u6cd5 \u6bd4\u65b9\u8bf4\uff0c\u4e0b\u96e8\u5929\u8981\u5e26\u4f1e\uff0c\u660e\u5929\u8981\u4e0b\u96e8\uff0c\u6240\u4ee5\u660e\u5929\u9700\u8981\u5e26\u4f1e\u3002\u8fd9\u4e2a\u5c31\u662f\u6f14\u7ece\u6cd5\u3002\u8fd9\u662f\u5f52\u7eb3\u548c\u6f14\u7ece\u7684\u533a\u522b\u3002</p> <p>\u8fd9\u91cc\u6240\u8bf4\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5c31\u662f\u5c06\u4eba\u7c7b\u6240\u603b\u7ed3\u7684\u7ecf\u9a8c\uff0c\u4ee3\u5165\u5230 \u6211\u4eec\u8bbe\u8ba1\u7684\u6a21\u578b\u7684 \u8fc7\u7a0b\u4e4b\u4e2d\u3002</p> <p>\u4e0a\u9762\u662f\u8bb2\u7684Transformer\u6a21\u578b\u7684\u7ed3\u6784 \u4ee5\u53ca \u4f18\u7f3a\u70b9\u3002\u56de\u7b54\u4e86 \u5982\u4f55\u8bc4\u4ef7Transformer\u7684\u95ee\u9898</p> <p>\u4ee5\u4e0a\uff1a</p> <ul> <li>\u5f52\u7eb3\u504f\u7f6e  &amp; \u6570\u636e\u91cf</li> <li>MHSA &amp; FFN</li> </ul>"},{"location":"learning/vit/#14-transformer","title":"1.4 Transformer\u7684\u4f7f\u7528\u7c7b\u578b","text":"<p>Transformer\u7684\u53d8\u4f53\u6709\uff0c\u53ea\u4f7f\u7528Transformer\u7684encoder\uff0c\u6bd4\u5982\u8bf4\u6211\u4eec\u5e38\u8bf4\u7684bert\uff0cbert\u4e3a\u4ec0\u4e48\u53eb \u53cc\u5411\u7684\u5462\uff1f\u5c31\u662f\u5728bert\u4e2d \u9884\u8bad\u7ec3 \u662f\u91c7\u7528\u7684\u4e24\u4e2aloss \u4e00\u4e2a\u662fMLM\uff0c\u8fd8\u6709\u4e00\u4e2a\u662fNSP\uff0c\u5c31\u662f\u5b83\u662f\u53bb\u9884\u6d4b\u7684\u88ab\u63a9\u7801\u7684\u4f4d\u7f6e\u4e0a\u7684\uff0c\u6ca1\u6709\u548cGPT\u4e00\u6837\uff0c\u7528\u7684\u81ea\u56de\u5f52\u7684\u65b9\u5f0f \u8fdb\u884c\u8bed\u8a00\u5efa\u6a21\uff0c\u662f\u53ea\u4f7f\u7528encoder\u7684\u65b9\u5f0f\u3002</p> <p>\u597d\u5904\u662f\u901f\u5ea6\u5f88\u5feb\uff0c\u4e0d\u9700\u8981\u505a \u81ea\u56de\u5f52\u7684 \u9012\u63a8\u3002\u53e6\u5916\u4e00\u79cd \u4f7f\u7528 \u573a\u666f\u662f decoder only\uff0c\u6bd4\u5982GPT\u7cfb\u5217\uff0c\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u7684 \u8bed\u8a00\u5efa\u6a21\uff0c\u5305\u62ec\u81ea\u56de\u5f52\u751f\u6210 \u4ee5\u53ca\u4e00\u4e9b \u6d41\u5f0f\u7684\u4efb\u52a1\uff0c\u4e00\u822c \u6211\u4eec\u53ea\u4f1a\u7528\u5230 Transformer decoder\u7684\u90e8\u5206\uff0c\u8fd9\u662f\u7b2c\u4e8c\u79cd\u573a\u666f\uff1b</p> <p>\u90a3\u7b2c\u4e09\u79cd\u573a\u666f\u5462\uff0c\u5c31\u662fTransformer\u539f\u59cb\u8bba\u6587\u7684\u573a\u666f\uff0c\u5c31\u662f\u50cf\u673a\u5668\u7ffb\u8bd1\u3001\u8bed\u8a00\u8bc6\u522b\u7b49\uff0c\u5c31\u662f\u4ece\u4e00\u4e2a\u5e8f\u5217\u7a7a\u95f4 \u5230 \u53e6\u5916\u4e00\u4e2a\u5e8f\u5217\u7a7a\u95f4\u7684\u8f6c\u6362\uff0c\u6211\u4eec\u5c31\u4f1a\u7528\u5230\u5b8c\u6574\u7684Transformer\u7684encoder\u548cdecoder\u8fd9\u6837\u4e00\u4e2a\u7ed3\u6784\uff1b\u8fd9\u4e09\u79cd\u4f7f\u7528\u573a\u666f \u90fd\u5f88\u5e38\u7528\u3002\u5e76\u4e14\u5462 \u5404\u81ea\u6709\u5404\u81ea\u7684\u7279\u70b9\u3002\u60f3\u5b66\u81ea\u5df1\u770b\u8bba\u6587\u3002</p> <p>\u4eca\u5929\u8bb2\u7684vision Transformer\u662fencoder only\u7684\u7ed3\u6784\uff0c\u518d\u6b21\u5f3a\u8c03ViT\u53ea\u7528\u5230\u4e86 encoder only\u7684\u90e8\u5206\uff0c\u4e8e\u662f\u6211\u4eec\u5c31\u4e0d\u7528\u8003\u8651\u81ea\u56de\u5f52\u3001\u4e0b\u4e09\u89d2\u7684\u63a9\u7801\u77e9\u9635\u7b49\u7b49\uff1b</p>"},{"location":"learning/vit/#2-vit","title":"2 vit \u6846\u67b6","text":"<p>\u5e76\u4e14vit\u53c8\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1\uff0c\u66f4\u7b80\u5355\u4e86\uff0c\u6700\u540e\u53ea\u9700\u8981\u9884\u6d4b\u4e00\u4e2a\u6982\u7387\uff0c\u5c31\u53ef\u4ee5\u4e86\u3002\u76f8\u6bd4\u4e8e\u751f\u6210\u4efb\u52a1 \u662f\u8981\u7b80\u5355\u5f88\u591a\u7684\u3002</p> <p>\u9996\u5148 vit\u7684\u6846\u67b6\uff1a</p> <p></p> <p>vit\u7684\u601d\u60f3 \u5c31\u662f\u60f3\u628a Transformer\u6a21\u578b\u5e94\u7528\u5230 \u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0c\u4f46\u662f\u76f4\u63a5\u5c06Transformer \u5e94\u7528\u5230\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0c\u9762\u4e34\u7684\u56f0\u96be\u5c31\u662f\uff1aTransformer\u5728nlp\u5f53\u4e2d \u662f\u4ee5\u5b57\u4e3a\u5355\u4f4d\uff0c\u5c31\u662f\u4e00\u53e5\u8bdd\u4e2d word\u7684\u6570\u91cf\u8fd8\u662f\u6bd4\u8f83\u5c11\u7684\uff1b\u4f46\u662f\u5982\u679c\u6211\u4eec \u76f4\u63a5\u5c06Transformer \u5e94\u7528\u5230\u4e00\u5f20\u56fe\u7247\u4e0a\u7684\u8bdd\uff0c\u90a3\u56fe\u7247\u7684\u57fa\u672c\u5355\u4f4d \u5c31\u662f\u4e00\u4e2a\u4e2a\u50cf\u7d20\u70b9\uff0c\u7136\u540e\u6bcf\u4e2a\u56fe\u50cf\u7684\u50cf\u7d20\u70b9 \u662f\u975e\u5e38\u975e\u5e38\u591a\u7684\uff0c\u5c11\u5219\u51e0\u767e\uff0c\u5927\u5219 \u51e0\u5343 \u51e0\u4e07 \u90fd\u6709\u3002\u6240\u4ee5\u6211\u4eec\u628aTransformer \u76f4\u63a5\u5e94\u7528\u5230\u56fe\u50cf\u70b9\u4e0a\u7684\u8bdd\uff0c\u7b2c\u4e00\u4e2a\u5c31\u662f\u8bf4 \u8ba1\u7b97\u91cf \u975e\u5e38\u5927\uff0c\u7b2c\u4e8c\u4e2a \u5c31\u662f\u5bf9\u56fe\u50cf\u800c\u8a00\uff0c\u5b83\u7684\u5355\u4e2a\u50cf\u7d20\u70b9\u4e0d\u50cf\u5728\u4e00\u4e2a\u53e5\u5b50\u4e2d  \u5355\u4e2a\u5b57\u6240\u5305\u542b\u7684\u4fe1\u606f\u91cf\u3002\u5728\u4e00\u53e5\u8bdd\u4e2d\uff0c\u5355\u4e2a\u5b57 \u6240\u5305\u542b\u7684\u4fe1\u606f\u91cf \u8fd8\u662f \u975e\u5e38\u4e30\u5bcc\u7684\u3002</p> <p>\u4f46\u662f\u5bf9\u4e00\u5f20\u56fe\u50cf\u5f53\u4e2d\u7684 \u67d0\u4e2a\u50cf\u7d20\u70b9 \u5e76\u4e0d\u5305\u542b \u4ec0\u4e48\u4fe1\u606f\u91cf\uff1b\u5bf9\u4e8e\u56fe\u50cf\u6765\u8bf4 \u4fe1\u606f\u91cf\u8fd8\u662f\u4e3b\u8981\u805a\u7126\u5728 \u4e00\u5c0f\u5757\u533a\u57df\u4e2d\uff1b\u5c31\u662f\u8bf4 \u5f88\u591a\u4e2a\u50cf\u7d20\u70b9 \u6240\u6784\u6210\u7684\u533a\u57df \u6784\u6210\u7684\u4fe1\u606f\u91cf \u624d\u4f1a\u6bd4\u8f83\u4e30\u5bcc\uff1b</p> <p>\u5982\u679c\u5355\u72ec\u770b\u4e00\u4e2a\u50cf\u7d20\u70b9\u7684\u8bdd \u53ef\u80fd\u6ca1\u6709\u4ec0\u4e48\u4fe1\u606f\u91cf\uff1b</p> <p>\u4ee5\u4e0a\u662f \u56fe\u50cf \u76f8\u6bd4\u4e8e \u6587\u672c\u53e5\u5b50\u7684\u533a\u522b\uff0c</p> <p>\u6240\u4ee5\u4e3a\u4e86\u5c06Transformer\u5e94\u7528\u5230 \u56fe\u50cf\u9886\u57df\u4e2d \u4ece\u8fd9\u4e24\u4e2a\u89d2\u5ea6\u51fa\u53d1 \u5c31\u4e0d\u80fd\u628a Transformer \u4ece\u50cf\u7d20\u70b9\u5c42\u9762 \u4e00\u4e2a\u4e2a\u4e2a\u53bb\u7b97 \u81ea\u6ce8\u610f\u529b\u3002\u90a3\u4e48\u4e00\u4e2a\u5f88\u7b80\u5355 \u5f88\u76f4\u63a5\u7684\u60f3\u6cd5 \u5c31\u662f\u628a \u5f88\u591a\u4e2a\u50cf\u7d20\u70b9 \u7ec4\u6210\u4e00\u4e2a\u5757\uff0c\u7136\u540e\u628a\u56fe\u50cf\u5206\u6210\u5f88\u591a\u4e2a\u5757\uff0c\u7136\u540e\u5462 \u628a\u4e00\u4e2a\u4e2a\u56fe\u50cf\u5757 \u53bb\u5f53\u505a\u4e00\u4e2atoken\uff0c\u7136\u540e\u9001\u5165\u5230Transformer\u4e2d\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f88\u76f4\u63a5\u7684\u60f3\u6cd5\u3002\u8fd9\u4e2a\u662f\u8f93\u5165\u7279\u5f81\u90e8\u5206\u3002</p>"},{"location":"learning/vit/#21-patch","title":"2.1 patch\u7684\u6784\u5efa","text":"<p>\u5bf9\u4e8e\u8fd9\u4e2a\u5757 \u6709\u4e24\u79cd\u89d2\u5ea6 \u7406\u89e3\u3002</p> <p></p> <p>\u7b2c\u4e00\u79cd\u89d2\u5ea6\uff0c\u901a\u8fc7DNN\u7684\u89d2\u5ea6\u7406\u89e3\uff0c\u4e5f\u5c31\u662f\u8bf4 \u9996\u5148\u628a \u56fe\u7247 \u5207\u5272\u6210 \u5f88\u591a\u4e2a\u5757\uff0c\u4e5f\u5c31\u662fimage to patch\u7684\u8fc7\u7a0b\u3002\u7136\u540e \u6211\u4eec\u518d\u5bf9 \u5f88\u591a\u4e2apatch \u7ecf\u8fc7\u4e00\u4e2a\u4eff\u5c04\u53d8\u6362\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u5411\u91cf\uff0c\u53eb\u505aembedding\uff0c\u4e5f\u5c31\u662f patch to embedding\uff0c\u8fd9\u662f\u4eceDNN\u7684\u89d2\u5ea6\u7406\u89e3</p> <p></p> <p>\u90a3\u53e6\u5916\u4e00\u4e2a\u89d2\u5ea6\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a \u6211\u4eec\u628a \u56fe\u7247 \u5f97\u5230 embedding\u7684\u8fc7\u7a0b \uff0c\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u5377\u79ef\u7f51\u7edc\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6211\u4eec\u4f1a\u7528\u4e00\u4e2a\u4e8c\u7ef4\u7684\u5377\u79ef \u5728\u56fe\u50cf\u4e0a \u5e94\u7528\u4e00\u4e2a\u5377\u79ef\uff0c\u5e76\u4e14\u8bf4\u5377\u79ef kernel size\u7b49\u4e8estride\u7684\uff0c\u7136\u540e\u6211\u4eec\u518d\u628a\u5f97\u5230\u7684\u5377\u79ef\u56fe\u62c9\u76f4\u4e00\u4e0b\uff0c\u5c31\u5f97\u5230\u6211\u4eec\u4e00\u4e2a\u4e2a\u7684token embedding</p> <p>\u8fd9\u662f\u6211\u4eec\u4ece \u4e24\u79cd\u89d2\u5ea6 \u53bb\u770b image to embedding\u7684\u8fc7\u7a0b</p>"},{"location":"learning/vit/#22-cls-token","title":"2.2  CLS token","text":"<p>\u63a5\u4e0b\u6765 \u4e3a\u4e86\u53bb\u505a\u5206\u7c7b\u4efb\u52a1\uff0cvit\u662f\u501f\u9274\u4e86bert\u4e2d\u7684\u4e00\u4e2a class token\u7684\u5360\u4f4d\u7b26\uff0c\u5173\u4e8e\u8fd9\u4e2aclass token\uff0c\u5927\u5bb6\u4e5f\u6709\u4e0d\u540c\u7684\u4e89\u8bae\uff0c\u6bd4\u5982\u4e3a\u4ec0\u4e48\u8981\u6709\u4e00\u4e2a class token\uff0c\u4ee5\u53caclass token \u65e2\u7136 \u5145\u5f53\u4e86 query\u7684\u4f5c\u7528\uff0c\u90a3\u4e3a\u4ec0\u4e48 \u5176\u4ed6 \u4f4d\u7f6e\u4e0a\u7684 \u91cf \u53c8\u53ef\u4ee5\u5bf9\u5b83\u53bb\u8ba1\u7b97\u4e00\u4e2a \u6ce8\u610f\u529b\u7684\u6743\u91cd\uff0c\u603b\u4e4b \u8fd9\u91cc\u6709\u5f88\u591a\u4e89\u8bae\u3002\u4f46\u662fbert\u8bba\u6587\u4e2d \u4e5f\u505a\u4e86\u5bf9\u6bd4\uff0c\u7528\u4e86class token\u7684\u6548\u679c \u662f\u6bd4 \u76f4\u63a5Pooling\u7684\u6548\u679c \u8981\u597d\u7684\u3002</p>"},{"location":"learning/vit/#23-position-embedding","title":"2.3 Position embedding","text":"<p>\u53e6\u5916 \u5728vit\u4e2d \u540c\u6837\u5f15\u5165\u4e86 position embedding\uff0c\u8fd9\u91cc\u5bf9\u6bd4\u4e86\u597d\u51e0\u79cd embedding\uff0c\u6700\u540e\u4f7f\u7528\u4e86 \u53ef\u8bad\u7ec3\u7684\u4e00\u7ef4embedding\uff0c\u6548\u679c\u4f1a\u6bd4\u8f83\u597d\u4e00\u70b9</p>"},{"location":"learning/vit/#24-transformer-encoder","title":"2.4 Transformer encoder","text":"<p>\u53e6\u5916\uff0cvit\u662f\u4e3b\u8981\u7528\u4e86Transformer encoder\u7684\u4e00\u4e2a\u6a21\u5757\uff0c \u5e76\u6ca1\u6709\u4f7f\u7528\u5230 decoder\uff0c\u662f\u6bd4\u8f83\u7b80\u5355\u7684\u3002</p>"},{"location":"learning/vit/#25-classification-head","title":"2.5 classification head","text":"<p>\u6700\u540e \u6211\u4eec\u901a\u8fc7class token\u8fd9\u4e2a\u4f4d\u7f6e\u4e0a\u7684  \u72b6\u6001\u91cf \u62ff\u51fa\u6765\uff0c\u7136\u540e\u5c31\u53ef\u4ee5\u53bb\u505a \u5206\u7c7b\u4efb\u52a1\u3002\u8fd9\u5c31\u662fvit\u7684\u4e00\u4e2a\u5168\u5c40\u7684\u7ed3\u6784\u3002\u63a5\u4e0b\u6765 \u6211\u4eec\u6765\u770b \u5177\u4f53\u8bba\u6587</p>"},{"location":"learning/vit/#3","title":"3 \u539f\u8bba\u6587","text":"<p>\u8bba\u6587\u7684\u6807\u9898\uff1a\u4e00\u56fe\u80dc16\u00d716\u7684\u5b57\uff0c\u6539\u7f16\u81ea \u4e00\u56fe\u80dc\u5343\u8a00\uff1b</p> <p>AN IMAGE IS WORTH 16\u00d716 WORDS</p> <p>\u6807\u9898\u7684\u610f\u601d\u5c31\u662f\u8bf4 \u5982\u679c\u628a \u4e00\u4e2a\u56fe\u50cf\u7684\u6bcf\u4e2a\u50cf\u7d20\u70b9\u770b\u6210 \u4e00\u4e2a\u5355\u8bcd\u7684\u8bdd \u5176\u5b9e\u6211\u4eec\u53ef\u4ee5\u628a16\u00d716 \u50cf\u7d20\u70b9 \u5f53\u6210\u4e00\u5f20\u56fe \u5c31\u591f\u4e86 \u3002\u6211\u4eec\u4e0d\u9700\u8981\u628a16\u00d716\u4e2a\u50cf\u7d20\u70b9 \u62ff\u51fa\u6765 \u5355\u72ec\u8fdb\u884c\u5efa\u6a21\uff1b\u800c\u662f\u53ef\u4ee5\u628a\u5b83\u4eec \u76f4\u63a5\u770b\u6210\u4e00\u4e2a\u6574\u4f53\u3002\u518d\u53d8\u6210\u4e00\u4e2aembedding \u8fdb\u884c\u5efa\u6a21\uff0c\u8fd9\u6837\u7684\u6548\u679c\u4f1a\u66f4\u597d\u3002</p> <p>TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</p> <p>\u4e0b\u9762\u4e00\u884c \u5c31\u662f\u8bf4 Transformer \u5e94\u7528\u5230 \u56fe\u50cf\u8bc6\u522b\uff0c\u6240\u4ee5\u5728\u672c\u6587\u4e2d \u4e3b\u8981\u662f\u628atransformer\u5e94\u7528\u5230 \u56fe\u50cf\u8bc6\u522b\uff0ccv\u9886\u57df \u603b\u5171 \u6709\u4e09\u5927\u4efb\u52a1\uff1a\u8bc6\u522b\u3001\u68c0\u6d4b\u3001\u5206\u5272\uff1b\u672c\u6587\u4e2d \u53ea\u8bb2\u5230\u4e86 \u8bc6\u522b\u3002\u540e\u9762\u8fd8\u6709\u8bba\u6587\u628atransformer\u7528\u5230cv\uff0c\u4e0d\u4ec5\u4f1a\u8bb2\u5230image recognition\u8fd8\u4f1a\u8bb2\u5230 \u68c0\u6d4b\u548c\u5206\u5272</p>"},{"location":"learning/vit/#31","title":"3.1 \u6458\u8981","text":"<p>\u9996\u5148 \u6458\u8981\u90e8\u5206\uff1a</p> <p></p> <p>\u5c3d\u7ba1transformer\u5df2\u7ecf\u6210\u4e3anlp\u4efb\u52a1\u7684\u6807\u914d\uff1b\u4f46\u662f\u5b83\u5728cv\u8fd9\u4e2a\u9886\u57df\u7684\u6f5c\u529b\u8fd8\u6ca1\u6709\u88ab\u6316\u6398</p> <p>\u90a3\u4e48\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\uff0c\u6ce8\u610f\u529b\u673a\u5236\u8981\u4e48\u5e94\u7528\u5230\u5377\u79ef\u7f51\u7edc\u4e2d\uff0c\u8981\u4e48\u76f4\u63a5\u66ff\u4ee3\u5377\u79ef\u7f51\u7edc\u7684\u67d0\u4e00\u90e8\u5206\uff1b</p> <p>\u5728\u672c\u6587\u4e2d\uff0c\u4f5c\u8005\u5c55\u793a\u4e86CNN\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u7b80\u5355\u7684transformer\u6a21\u578b\u76f4\u63a5\u5e94\u7528\u5230\u56fe\u50cf\u5757\u4e0a\uff0c\u5c31\u53ef\u4ee5\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a \u8868\u73b0\u5f97\u5f88\u597d\uff1b\u4f46\u662f\u8fd9\u4e2a\u5f88\u597d\u662f\u6709\u6210\u672c\u7684\uff1b\u5f53\u6211\u4eec\u9996\u5148\u5c06\u8fd9\u4e2avit\u6a21\u578b\u5728\u5927\u91cf\u7684\u6570\u636e\u4e0a \u505a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u518d\u628a\u5b83\u8fc1\u79fb\u5230 \u4e2d\u7b49\u5927\u5c0f \u6216\u8005 \u5c0f\u7684\u6570\u636e\u96c6\u4e0a\uff0cvit\u5c31\u53ef\u4ee5\u53d6\u5f97 \u76f8\u6bd4\u4e8e\u5377\u79ef\u7f51\u7edc \u4e00\u6837 \u751a\u81f3 \u66f4\u597d\u7684\u6548\u679c\u3002</p> <p>\u8ddf\u4e4b\u524d\u8bb2\u7684transformer\u6709\u5f02\u66f2\u540c\u5de5\u4e4b\u5999\uff0c\u56e0\u4e3atransformer\u7684\u5f52\u7eb3\u504f\u7f6e \u662f\u6bd4\u8f83\u5c11\u7684\u3002\u5c31\u662f\u8bf4\u5e76\u6ca1\u6709\u628a\u6211\u4eec\u4eba\u7c7b\u603b\u7ed3\u7684\u4e00\u4e9b\u7ecf\u9a8c \u6ce8\u5165\u5230transformer\u6a21\u578b\u4e4b\u4e2d\uff0c\u6240\u4ee5\u5355\u7eaf\u8ba9transformer\u4f9d\u8d56\u6570\u636e\u53bb\u5b66\u4e60\u5230\u8fd9\u4e9b\u7ecf\u9a8c \u662f\u4e00\u4e2a\u6bd4\u8f83\u6f2b\u957f\u7684\u8fc7\u7a0b\uff0c\u6240\u4ee5\u6211\u4eec\u5fc5\u987b\u9700\u8981\u5927\u91cf\u7684\u6570\u636e\u91cf \u5927\u91cf\u7684\u6570\u636e\u96c6 \u624d\u53ef\u4ee5\u8ba9transformer vit\u53d6\u5f97\u6bd4\u8f83\u597d\u7684\u6548\u679c\uff0c\u5b8c\u5168\u662f \u6570\u636e\u9a71\u52a8\u7684\uff1b</p>"},{"location":"learning/vit/#32","title":"3.2 \u6a21\u578b\u56fe","text":"<p>introduction\u76f4\u63a5\u8df3\u8fc7\uff0c\u76f4\u63a5\u770b\u6a21\u578b\u56fe\uff1a</p> <p></p> <p>vit\u7684\u7ed3\u6784\u5c31\u662f \u56fe1\u6240\u793a\uff1b\u9996\u5148\u770bdecription\uff1b</p> <p>\u9996\u5148\u628a\u56fe\u7247\u5206\u6210\u5f88\u591a\u4e2a\u56fa\u5b9a\u5927\u5c0f\u7684 \u5757\u3002\u7136\u540e\u5728\u7528\u7ebf\u6027\u7f51\u7edc \u5c06\u8fd9\u4e9b\u5757 \u5f62\u6210\u4e00\u4e2a\u5d4c\u5165\u8868\u5f81\uff0c\u7136\u540e\u518d\u5728\u8868\u5f81\u4e0a \u52a0\u5165\u4f4d\u7f6e\u7f16\u7801\uff1b\u7136\u540e\u518d\u628a\u8fd9\u4e00\u7cfb\u5217\u7684\u5411\u91cf \u9001\u5165\u5230\u6807\u51c6\u7684transformer encoder\u4e4b\u4e2d\uff0c\u6a21\u578b\u5c31\u7528\u8fd9\u4e48\u4e00\u53e5\u8bdd \u63cf\u8ff0\u5b8c\u4e86\u3002\u7136\u540e\u4e3a\u4e86\u53bb\u505a \u5206\u7c7b\u4efb\u52a1\uff0c\u5c31\u662f\u50cfbert\u4e2d\u4e00\u6837\uff0c\u589e\u52a0\u4e86\u4e00\u4e2a \u989d\u5916\u7684classification token\u4f4d\u7f6e\uff0c\u5c31\u662f\u5bf9\u5e8f\u5217 \u65b0\u589e\u4e86\u4e00\u4e2a\u4f4d\u7f6e\uff1b</p> <p>\u53ef\u4ee5\u7406\u89e3\u4e3a \u8fd9\u4e2a\u4f4d\u7f6e \u5c31\u662f\u8d77\u4e00\u4e2a query\u7684\u4f5c\u7528\u3002\u5c31\u662f\u5b83\u4f1a\u53bb\u6536\u96c6 \u4f7f\u5f97\u8fd9\u4e2a\u6a21\u578b \u80fd\u505a\u597d \u5206\u7c7b\u4efb\u52a1\u7684\u4fe1\u606f\u3002\u6700\u540e\u5462 \u6211\u4eec\u5c06\u6700\u540e\u4e00\u5c42\u7684 \u4f4d\u7f6e\u4fe1\u606f \u62ff\u51fa\u6765 \u505a\u4e00\u4e2a\u7ebf\u6027\u6620\u5c04\uff0c\u6620\u5c04\u5230 \u7c7b\u522b\u7684\u6982\u7387\u5206\u5e03\u4e0a\uff0c\u7136\u540e\u5c31\u53ef\u4ee5\u4e86\u3002\u8fd9\u4e2a\u6a21\u578b\u662f\u6bd4\u8f83\u7b80\u5355\u7684 \u5982\u679c\u975e\u5e38\u4e86\u89e3transformer\u6a21\u578b\u7684\u8bdd\u3002vit\u662f\u6ca1\u6709\u96be\u5ea6\u7684\uff08\u4ee3\u7801 \u6f14\u793a\u5b9e\u4f8b\uff09</p> <p>\u63a5\u4e0b\u6765 \u770b\u5de6\u56fe\uff0c\u53f3\u56fe\u770b\u8fc7 \u5f88\u591a\u904d\u4e86</p> <p></p> <ul> <li> <p>\u9996\u5148\u5c06\u4e00\u526f\u56fe\u7247 \u5206\u6210\u5f88\u591a\u4e2a \u5757\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f \u6bcf\u4e2a\u5757\u7684\u5927\u5c0f\u662f\u4e0d\u53d8\u7684\uff1b\u56fe\u50cf\u7684\u5927\u5c0f \u53ef\u80fd\u4f1a\u53d8\u5316 \u4f46\u662f \u6bcf\u4e2a\u5757\u7684\u5927\u5c0f \u662f\u4e0d\u4f1a \u53d1\u751f\u53d8\u5316\u7684</p> </li> <li> <p>\u5728\u540c\u4e00\u4e2a\u6a21\u578b\u4e2d \u5757\u7684\u5927\u5c0f \u4e0d\u4f1a\u53d1\u751f\u53d8\u5316\uff1b</p> </li> <li> <p>\u6362\u53e5\u8bdd\u8bf4\uff0c\u5982\u679c\u56fe\u7247\u5927\u4e00\u70b9\uff0c\u90a3\u53cd\u5e94\u5728 \u5e8f\u5217\u957f\u5ea6\u4e0a \u957f\u4e00\u70b9\uff1b</p> </li> <li>\u5c06\u4e00\u4e2a\u56fe\u7247\u5206\u6210\u5f88\u591a\u5757\uff0c\u50cf\u5377\u79ef\u4e2d \u5e73\u79fb\u7684\u987a\u5e8f\u4e00\u6837\uff0c\u5148\u5de6\u5230\u53f3\uff0c\u628a\u56fe\u7247\u62c9\u76f4\uff0c\u62c9\u6210\u4e00\u4e2a\u5e8f\u5217\u7684\u5f62\u72b6\uff1b</li> <li>\u7136\u540e\u518d\u628a \u6bcf\u4e2a\u5757\u7684\u50cf\u7d20\uff0c\u50cf\u7d20\u70b9\u7684\u503c \u5f52\u4e00\u5316\uff1b\u5c31\u662f\u8bf4\u4e4b\u524d\u5c31\u5df2\u7ecf \u505a\u597d \u5f52\u4e00\u5316\u4e86 \u5f52\u4e00\u5316\u52300-1\u4e4b\u95f4\u3002</li> <li>\u7136\u540e\u518d\u628a\u5757\u91cc\u7684 \u503c \u901a\u8fc7\u7ebf\u6027\u53d8\u6362 \u6620\u5c04\u5230 \u6a21\u578b\u7684\u7ef4\u5ea6\uff0c\u6216\u8005\u8bf4 \u6211\u4eec\u5f97\u5230\u4e86patch embedding\uff1b</li> <li>\u6709\u4e86patch embedding\u4ee5\u540e\uff0c\u4e3a\u4e86\u505a\u5206\u7c7b\u4efb\u52a1\uff0c\u9700\u8981\u5728\u5e8f\u5217\u7684\u5f00\u5934 \u589e\u52a0\u4e00\u4e2a \u53ef\u8bad\u7ec3\u7684embedding\uff0c\u8fd9\u4e2aembedding\u662f\u968f\u673a\u521d\u59cb\u5316\u7684 embedding\u3002\u90a3\u8fd9\u6837 \u5c31\u6784\u6210\u4e86 \u4e00\u4e2a\u957f\u5ea6+1\u7684\u5e8f\u5217\u3002</li> <li>\u7136\u540e\u6211\u4eec\u518d\u589e\u52a0position embedding\uff0c\u5c31\u662f\u4f4d\u7f6e\u7f16\u7801\uff0c\u90a3\u8fd9\u6837 \u52a0\u5b8c \u540e\u7684 \u5e8f\u5217\u8868\u5f81 \u5c31\u53ef\u4ee5\u76f4\u63a5 \u9001\u5165\u5230 transformer encoder\u4e2d\uff0c\u7136\u540e\u6211\u4eec\u5728encoder \u6700\u540e\u4e00\u5c42\u4e2d\uff0c\u53d6\u51fa \u65b0\u52a0\u7684 \u4e5f\u5c31\u662f\u591a\u4f59\u4f4d\u7f6e\u4e0a\u7684 \u8f93\u51fa\u72b6\u6001 \u7ecf\u8fc7\u4e00\u4e2aMLP\uff0c\u5f97\u5230\u7c7b\u522b\u7684\u6982\u7387\u5206\u5e03\uff0c\u5c31\u53ef\u4ee5\u7528\u4ea4\u53c9\u71b5 \u53bb\u7b97\u51fa \u5206\u7c7bloss\uff0c\u5c31\u5b8c\u6210\u4e86 \u4e00\u4e2avit\u6a21\u578b\u7684\u642d\u5efa\uff1b</li> </ul>"},{"location":"learning/vit/#4","title":"4 \u4ee3\u7801\u5b9e\u73b0","text":"<p>\u63a5\u4e0b\u6765 \u4ee3\u7801 \u5b9e\u73b0\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u7684\u91cd\u70b9\uff1a</p>"},{"location":"learning/vit/#41-patch","title":"4.1 \u5207\u5206 patch","text":"<ul> <li>Image2embedding</li> </ul> <p>\u4e5f\u5c31\u662f\u8bf4 \u5b9e\u73b0\u7684\u91cd\u70b9 \u5728transformer\u4e4b\u524d\u7684\u90e8\u5206\uff1b\u56e0\u4e3atransformer encoder\u7684\u4ee3\u7801 pytorch \u5df2\u7ecf\u5305\u88c5\u8d77\u6765\u4e86\u3002\u800c\u4e14 \u4e4b\u524d \u4e5f\u5df2\u7ecf\u8be6\u7ec6\u8bb2\u8fc7</p> <p>\u6240\u6709\u7684\u6240\u6709 \u90fd\u662f\u4e3a\u4e86 \u4ee3\u7801 \u55ef \u522b\u5fd8\u4e86 \u5f00\u59cb\u7684\u76ee\u7684</p> <p>\u7279\u70b9\uff1a\u5b9e\u73b0 \u529f\u80fd\u4e3a\u4e3b \uff0c\u4e0d\u662f\u8dd1\u6a21\u578b\uff1b\u56f4\u7ed5\u4f8b\u5b50 \u5b9e\u73b0 \u8fc7\u7a0b\uff1b</p> <p>\u9996\u5148 \u5bfc\u5165\u5e93\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n</code></pre> <p>,\u6765\u770b\u4e00\u4e0b\uff0c\u9996\u5148\u7b2c\u4e00\u90e8\u5206 \u8981\u505a\u4ec0\u4e48\uff0c\u6211\u4eec\u9700\u8981\u5c06 \u4e00\u5e45 \u56fe\u7247\u53d8\u6210embedding\uff0c\u7b2c\u4e00\u6b65\u6211\u4eec\u8981\u505a\u8fd9\u4e2a\u4e8b\u60c5\uff0c\u6211\u4eec\u53ef\u4ee5\u9996\u5148 \u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570</p> <pre><code>def image2emb():\n  pass\n</code></pre> <p></p> <p>\u4e4b\u524d\u5728 \u601d\u7ef4\u5bfc\u56fe\u91cc \u8bb2\u4e86 \u8fd9\u4e2a image2emb\u8fd9\u4e2a\u8fc7\u7a0b \u53ef\u4ee5\u4ece\u4e24\u4e2a\u89d2\u5ea6 \u53bb\u7406\u89e3\uff0c\u4e00\u4e2a\u89d2\u5ea6 \u662fDNN\u7684\u89d2\u5ea6\uff0c\u6211\u4eec\u628aimage\u624b\u52a8 \u5207\u6210 \u4e00\u4e2a\u4e2a\u5757\uff0c\u518d\u628a\u6bcf\u4e2a\u5757 \u53d8\u6210 embedding</p> <p>\u7b2c\u4e8c\u4e2a\u89d2\u5ea6\u5c31\u662f\u8bf4 \u76f4\u63a5\u4ece \u4e8c\u7ef4\u5377\u79ef\u7684 \u89d2\u5ea6\u53bb\u7406\u89e3\uff0c\u5c31\u662f \u76f4\u63a5\u5bf9\u56fe\u7247\u505a \u4e8c\u7ef4\u5377\u79ef \uff0c\u7136\u540e\u628a\u5377\u79ef\u540e\u7684\u7ed3\u679c \u62c9\u76f4\u4e00\u4e0b\uff0c\u6784\u6210embedding</p> <p>\u6240\u4ee5vit\u7ed3\u6784 \u7b2c\u4e00\u5c42 \u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u5377\u79ef\uff1b\u90a3 \u5199\u4e24\u4e2a\u51fd\u6570 \u5b9e\u73b0\u5b83</p> <p>\u7b2c\u4e00\u4e2a\u51fd\u6570 \u53ebnavie,\u5f88\u76f4\u63a5\u7684\u5b9e\u73b0\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u51fd\u6570\u53d6\u540d\u4e3aconv\uff0c\u6211\u4eec\u4ee5\u5377\u79ef\u7684\u89d2\u5ea6\uff0c\u6765\u5b9e\u73b0</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie():\n    pass\n\ndef image2emb_conv():\n    pass\n</code></pre>"},{"location":"learning/vit/#411-naivetorchunfold","title":"4.1.1 naive\u7248\u672c\uff1atorch.unfold()","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    patch_embedding = patch @ weight\n    return patch_embedding\ndef image2emb_conv():\n    pass\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim)\n\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)\nprint(patch_embedding_naive.shape)\n</code></pre> <p>\u6ce8\u91ca\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width = 1,3,8,8\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    # patchshape = torch.Size([1, 48, 4])   patch_size = 4\n    # 1\uff1abatchsize\n    # 48 = 3*4*4\uff08\u5377\u79ef\u8986\u76d6\u7684input region\uff09\n    # 4\uff1a1,3,8,8\u7684\u8f93\u5165\u56fe\u7247\u7528 1344\u7684\u5377\u79ef\u6838\u5377\u79ef\uff0c\u5f97\u52304\u4e2ainput region\n    # transpose(-1,-2) \u2192 1,4,48  \n    patch_embedding = patch @ weight\n    # 1,4,48 @ 48,8 = 4 \u00d7 8\n    return patch_embedding\ndef image2emb_conv():\n    pass\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim)\n\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)\nprint(patch_embedding_naive.shape)\n</code></pre> <p>\u4ee5\u4e0b\u662f\u4ee3\u7801\u7684\u600e\u4e48\u5199\u51fa\u6765\u7684\u8be6\u89e3\uff0c\u53ef\u8df3\uff1a</p> <p>\u9996\u5148 \u6211\u4eec\u6765\u5b9e\u73b0 navie\u7684\u7248\u672c</p> <p>\u90a3\u4e48\u65e2\u7136\u662f image2embedding\uff0c\u6211\u4eec\u9700\u8981\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570 \u5c31\u662fimage\u7684\u5f20\u91cf\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570 \u5c31\u662f \u5757\u7684\u5927\u5c0f\uff0c\u9996\u5148\u5c06image\u5206\u6210\u5f88\u591a\u5f88\u591a\u7684\u5757\uff0c\u6bcf\u4e2a\u5757 \u80af\u5b9a\u662f \u65b9\u5f62\u7684 \u8fb9\u957f\u662f\u591a\u5c11\uff0c\u90a3\u6211\u4eec \u4f20\u5165\u7684\u662f\u5c31\u662f <code>patch_size</code> \u4e5f\u5c31\u662f \u5757\u7684\u8fb9\u957f\uff0c\u90a3\u8fd8\u6709\u5c31\u662f \u6211\u4eec\u65e2\u7136\u8981\u5f97\u5230embedding\uff0c\u9996\u5148\u5f97\u5230\u5757\uff0c\u5757\u91cc\u9762\u7684\u6240\u6709\u50cf\u7d20\u70b9\uff0c\u6211\u4eec\u4f1a\u5bf9\u5b83\u505a\u4e00\u4e2a \u7ebf\u6027\u53d8\u6362\uff0c\u7ebf\u6027\u53d8\u6362\u7684\u8bdd \u9700\u8981\u4e00\u4e2aweight\uff0c\u8fd9\u4e2a \u53d8\u6362\u7684\u6743\u91cd\u77e9\u9635</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,wieght):\n    pass\n\ndef image2emb_conv():\n    pass\n</code></pre> <p>\u8fd9\u91cc\u7684image size\u4e00\u822c\u662f\u8ddf \u5377\u79ef\u4e2d\u7c7b\u4f3c\uff0c\u5b83\u7684\u683c\u5f0f \u5c31\u662f batch size\u00d7channel\u00d7height\u00d7width\uff0c\u8fd9\u4e2a\u662fimage\u7684shape\uff0c\u5728\u4e8c\u7ef4\u5377\u79ef\u4e2d \u4e5f\u662f\u8fd9\u6837\u683c\u5f0f</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    pass\ndef image2emb_navie():\n    pass\n</code></pre> <p>\u9996\u5148\u7b2c\u4e00\u6b65 \u6211\u4eec\u8981\u5bf9 \u56fe\u50cf\u5206\u5757\uff0c\u90a3\u8fd9\u4e2a\u56fe\u7247\u5206\u5757 \u5176\u5b9e\u5c31\u662f \u7c7b\u4f3c\u4e4b\u524d\u5377\u79ef\u8bb2\u8fc7\u7684</p> <p></p> <p>\u6211\u4eec\u6709\u4e24\u79cd\u89d2\u5ea6 \u7406\u89e3\u5377\u79ef\uff0c\u7b2c\u4e00\u79cd \u89d2\u5ea6 \u662f\u628a\u6bcf\u6b21 \u6ed1\u52a8\u76f8\u4e58\u7684\u533a\u57df \u62ce\u51fa\u6765\uff0c\u5176\u5b9e\u5c31\u662fimage2patch\uff0c\u53ea\u662f \u8fd9\u91cc\u7684stride \u521a\u597d \u7b49\u4e8e kernel size</p> <p>\u53e6\u5916\u4e00\u79cd\u89d2\u5ea6 \u5c31\u662f\u6211\u4eec\u5bf9kernel \u8fdb\u884c \u586b\u5145\uff0c\u628akernel \u586b\u5145\u6210 \u8ddf input feature map\u4e00\u6837\u5927\u5c0f\u7684\uff0c\u7136\u540e \u63a8\u51fa\u4e86 \u8f6c\u7f6e\u5377\u79ef</p> <p>\u6240\u4ee5\u8fd9\u91cc \u5c31\u662f \u7b2c\u4e00\u79cd\u89d2\u5ea6\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7image2patch\uff0c\u5c06\u6bcf\u4e00\u6b65 \u5377\u79ef\u7684\u533a\u57df \u62ff\u51fa\u6765 \u6784\u6210\u4e00\u4e2a\u4e2apatch\uff0c\u90a3\u8fd9\u91cc \u4e0d\u5199for \u5faa\u73af \u62ffpatch</p> <p>\u4e4b\u524d \u4e5f\u8bb2\u8fc7 \u5728pytorch \u4e2d\uff0c\u6709\u4e00\u4e2aapi\u53eb\u505a<code>pytorch nn functional unfold</code>\u8fd9\u4e2aapi</p> <p></p> <p>\u8fd9\u4e2aapi\u505a\u7684\u4e8b\u60c5\uff0c\u5c31\u662f\u62ff\u51fa\u5377\u79ef\u7684\u533a\u57df</p> <p></p> <p>\uff0c\u7b80\u5355\u6765\u8bb2 \u5c31\u662f \u62ff\u51fa \u5377\u79ef\u7684\u533a\u57df\uff0c\u4f60\u770b\u5b83\u9700\u8981\u7684\u53c2\u6570 \u4e5f\u975e\u5e38\u7684\u5377\u79ef</p> <p></p> <p>\u6709input\u3001kernel size\u3001dilation\uff0cpadding\u3001stride</p> <p>\u5176\u5b9e\u5c31\u662f\u8bf4 \u6839\u636einput \u5377\u79ef\u7684\u53c2\u6570 \u5c31\u80fd\u5c06 \u6bcf\u4e00\u6b21 \u6ed1\u52a8\u7684 \u533a\u57df\u7684\u8f93\u5165 \u5355\u72ec\u7684 \u62ff\u51fa\u6765</p> <p>\u8fd9\u91cc \u5c31\u662f image2patch\u7684\u8fc7\u7a0b</p> <p>\u8fd9\u91cc \u6211\u4eec\u76f4\u63a5\u53bb\u8c03\u7528</p> <p>\u90a3\u4e5f\u5c31\u662f\u8bf4 \u521a\u597d\u6709\u8fd9\u4e9b\u53c2\u6570\uff0c\u5c31\u53ef\u4ee5\u76f4\u63a5\u53bb\u8c03\u7528</p> <p>\u6211\u4eec\u5df2\u7ecf import \u7b80\u5199\u6210F\u4e86\uff0c\u6240\u4ee5\u5c31\u5199F.unfold,input\u5176\u5b9e\u5c31\u662f image\uff0c\u521a\u597d\u662f\u6ee1\u8db3\u8fd9\u4e2a\u683c\u5f0f\u7684\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570 \u5c31\u662fkernel size\u53c2\u6570\uff0ckernel size \u5176\u5b9e\u5c31\u662f patch size\uff0c\u7b2c\u4e09\u4e2a\u5462 \u5176\u5b9e\u5c31\u662fdilation\uff0cdilation\u8fd9\u91cc\u6211\u4eec\u4e0d\u9700\u8981\u8003\u8651\uff0c\u56e0\u4e3a \u6211\u4eec\u5e76\u6ca1\u6709 \u7a7a\u6d1e\uff0cpadding\u4e5f\u4e0d\u7528\u8003\u8651\uff0c\u4e5f\u6ca1\u6709\u505a\u586b\u5145\uff0c\u6700\u540e\u4e00\u4e2astride\u6211\u4eec\u9700\u8981\u8003\u8651\uff0c\u6211\u4eec\u7684stride\u5e76\u4e0d\u662f1\uff0c\u56e0\u4e3a \u6211\u4eec\u8fd9\u91cc\u7684\u56fe\u50cf\u5206\u5757 \u662f\u6ca1\u6709\u4ea4\u53e0\u7684\uff0c\u7ed3\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230 \uff0c\u6bcf\u4e2a\u5757\u4e0e\u5757\u4e4b\u95f4 \u662f\u6ca1\u6709\u4ea4\u53e0\u7684\uff0c\u8fd9\u5c31\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u5377\u79ef stride=kernel size\uff0c\u901a\u8fc7F.unfold\u51fd\u6570 \u5c31\u80fd\u628a \u56fe\u50cf\u5206\u5757\uff0c\u7ed3\u679c\u5b9a\u4e49\u4e3apatch</p> <pre><code>    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size)\n</code></pre> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size)\n    pass\ndef image2emb_navie():\n    pass\n</code></pre> <p>\u8fd9\u4e2a \u5c31\u662f \u8c03\u7528pytorch F.unfold\u8fd9\u4e2a\u51fd\u6570 \u5c06\u56fe\u7247 \u8fdb\u884c\u5206\u5757\uff0c\u539f\u7406\u5c31\u662f \u5c06\u56fe\u7247\u770b\u6210 kernel size=stride\u7684\u5377\u79ef\u5c31\u597d\u4e86\uff0c\u7136\u540e\u5c31\u53ef\u4ee5\u628a\u6bcf\u4e00\u6b21 \u6bcf\u4e00\u6b65 \u8f93\u5165\u7684\u533a\u57df \u5355\u72ec\u62ff\u51fa\u6765\uff0c\u653e\u5230patch\u4e2d</p> <p>\u5f53\u7136 \u8fd9\u4e2apatch\u662f\u4ec0\u4e48\u5f62\u72b6\u5462\uff1f\u6211\u4eec\u53ef\u4ee5\u5148\u6765 \u6d4b\u8bd5\u4e00\u4e0b\uff0c\u9700\u8981\u5148 \u6d4b\u8bd5\u4e00\u4e0b \u8fd9\u4e2a\u51fd\u6570 \u6765\u770b\u4e00\u4e0b patch\u7684\u5f62\u72b6\u3002</p> <p>\u4e3a\u4e86 \u6d4b\u8bd5 \u6211\u4eec\u9700\u8981 \u5148 \u5b9a\u4e49\u4e00\u4e9b\u5e38\u91cf\uff0c\u6bd4\u5982\u8bf4 \u6211\u4eec\u9700\u8981\u5b9a\u4e49 batch size\u3001input channel\u3001\u56fe\u7247\u7684\u9ad8\u5ea6 \u4ee5\u53ca \u56fe\u7247\u7684\u5bbd\u5ea6</p> <p>\u5047\u8bbe \u6211\u4eec\u8bbe\u7f6ebatch size=1\uff0cchannel=3\uff0c\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u5199\u4e2a8\u548c8</p> <p>\u540c\u65f6\u6211\u4eec\u8fd8\u9700\u8981 \u5b9a\u4e49 patch size\uff0c\u8fd9\u91cc \u6211\u4eec\u53ef\u4ee5\u5b9a\u4e494\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6211\u4eec\u662f4\u00d74\u4e3a\u4e00\u4e2a patch</p> <p>\u90a3\u8fd8\u6709 \u6211\u4eec\u9700\u8981\u5f97\u5230 \u4e00\u4e2a embedding\uff0c\u6240\u4ee5\u6211\u4eec\u8fd8\u9700\u8981\u6709\u4e00\u4e2a patch embedding dim\uff0c\u5176\u5b9e\u5c31\u662ftransformer\u4e2d\u7684 model dim\uff0c\u6211\u4eec\u53ef\u4ee5 \u5b9a\u4e49\u4e3a8</p> <pre><code># test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n</code></pre> <p>\u5728\u5b9a\u4e49\u597d\u4e86\u8fd9\u4e9b\u91cf\u4ee5\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u6765\u6d4b\u8bd5\u8fd9\u4e9b\u51fd\u6570\u4e86</p> <p>\u9996\u5148 \u6211\u4eec\u9700\u8981\u751f\u6210 \u56fe\u7247</p> <pre><code>image = torch.randn(bs,ic,image_h,image_w)\n</code></pre> <p>\u8fd8\u9700\u8981 weight\uff0cweight\u600e\u4e48\u6837\u5b9a\u4e49\u5462\uff1f</p> <p>weight \u5176\u5b9e\u5c31\u662f patch2embedding\u8fc7\u7a0b\u7684\u4e58\u6cd5\u77e9\u9635\uff0c\u4e5f\u5c31\u662f\u8bf4 \u6211\u4eec\u5c06patch\u7684\u5927\u5c0f \u6620\u5c04\u6210model dim\u8fd9\u4e2a\u5927\u5c0f\uff0c\u6240\u4ee5weight \u662f\u4e00\u4e2a \u4e8c\u7ef4\u7684\u5f20\u91cf</p> <p>\u5f20\u91cf\u7684\u7b2c\u4e00\u4e2a\u5f62\u72b6\uff0c\u5148\u6682\u65f6\u8bbe\u4e3aNone\uff0c\u7b2c\u4e8c\u4e2a\u5f62\u72b6 \u5176\u5b9e \u5c31\u662f model dim\uff0c\u662f\u4e00\u4e2a \u8fd9\u6837\u7684 \u4e58\u6cd5\u77e9\u9635 </p> <pre><code>weight = torch.randn(None,model_dim)\n</code></pre> <p>\u90a3\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5462\uff1f\u7b2c\u4e00\u4e2a\u7ef4\u5ea6 \u5176\u5b9e\u5c31\u662f patch\u7684\u5927\u5c0f\uff0c\u6309\u7167\u8bba\u6587\u7684\u610f\u601d patch\u7684\u5927\u5c0f \u662f\u4ec0\u4e48\u5462\uff1fpatch\u7684\u5927\u5c0f \u5176\u5b9e\u5c31\u662fpatch\u7684\u8fb9\u957f\uff0c\u8fb9\u957f\u7684\u5e73\u65b9\u521a\u597d\u662f \u9762\u79ef\uff0c\u7136\u540e\u518d\u4e58\u4ee5 \u901a\u9053\u6570\u76ee\uff0c\u4e5f\u5c31\u662f\u8bf4 \u5982\u679c\u56fe\u7247 \u6709 \u4e09\u4e2a\u901a\u9053\u7684\u8bdd\uff0c\u6bcf\u4e2apatch \u5176\u5b9e\u662f \u5305\u542b \u4e09\u4e2a \u901a\u9053\u7684\uff0c\u6240\u4ee5\u8fd9\u91cc \u6211\u4eec\u9700\u8981\u7b97\u51fa\u4e00\u4e2a\u91cf</p> <p>patch_depth \u4e5f\u5c31\u662f patch\u7684\u6df1\u5ea6\uff0c\u5b83\u5e94\u8be5\u5c31\u662f patch size\u518d\u4e58\u4ee5 patch size\u518d\u4e58\u4ee5 ic \u8f93\u5165\u7684\u901a\u9053\u6570\u76ee\uff0c\u90a3\u8fd9\u91cc \u6211\u4eec\u5c31\u53ef\u4ee5\u628a weight\u7684\u77e9\u9635 \u7ed9\u5199\u51fa\u6765\uff0c\u4e5f\u5c31\u662f patch_depth</p> <pre><code>patch_depth = patch_size * patch_size * ic\nweight = torch.randn(patch_depth,model_dim)\n</code></pre> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size)\n    pass\ndef image2emb_navie():\n    pass\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim)\n</code></pre> <p>\u8fd9\u5c31\u662f weight \u77e9\u9635\uff0c\u6709\u4e86weight \u6709\u4e86image\uff0cpatch_size \u6211\u4eec\u5c31\u53ef\u4ee5 \u8c03\u7528\u8fd9\u4e2a\u51fd\u6570image2emb_navie</p> <p>\u7136\u540e \u6211\u4eec\u9996\u5148\u6253\u5370\u4e00\u4e0b \u91cc\u9762\u7684patch\u7684\u5f62\u72b6</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size)\n    print(patch.shape)\ndef image2emb_conv():\n    pass\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim)\n\nimage2emb_navie(image,patch_size,weight)\n</code></pre> <p>\u8f93\u51fa\uff1atorch.Size([1, 48, 4])</p> <p>\u7ed3\u679c\u89e3\u8bfb\uff1apatch\u7684\u5f62\u72b6\u662f1\u00d748\u00d74 \u77e5\u9053\u4e3a\u4ec0\u4e48 \u662f \u8fd93\u4e2a\u6570\u5b57\u5417</p> <p>\u9996\u5148 4\u5f88\u597d\u7406\u89e3 \u56e0\u4e3a \u56fe\u50cf\u662f 8\u00d78\u7684 \u8fd9\u6837\u4e00\u4e2a \u9762\u79ef\uff0c\u7136\u540e patch size\u662f4\u00d74\uff0c\u90a3\u4e00\u4e2a8\u00d78\u7684\u56fe\u7247\uff0c\u4ee54\u00d74\u4e3a\u4e00\u4e2a\u5757\u7684\u8bdd\uff0c\u521a\u597d\u662f 4\u5757\uff1b\u5c31\u6784\u6210\u4e864\u5757\uff1b</p> <p>\u8fd9\u91cc\u76844 \u5176\u5b9e\u5c31\u662f \u5757\u7684\u6570\u76ee\uff0c\u5c31\u662f \u56fe\u7247 \u5206\u5757\u4ee5\u540e \u5757\u7684\u6570\u76ee</p> <p>\u7136\u540e 48\u600e\u4e48\u6765\u7684\u5462\uff1f48\u5176\u5b9e\u5c31\u662f  patch size\u00d7patch size \u00d7input channel \u5c31\u662f 4\u00d74\u00d73=48,\u628a\u6bcf\u4e2a\u5377\u79ef\u90fd\u62c9\u76f4\u4e86\uff0c\u4e3a\u4e86\u66f4\u4fbf\u4e8e\u7406\u89e3 \u6211\u4eec\u53ef\u4ee5\u589e\u52a0\u4e00\u4e2atranspose\uff0c\u5c31\u662f\u628a\u6700\u540e\u4e00\u7ef4\u548c\u5012\u6570\u7b2c\u4e8c\u7ef4 \u4ea4\u6362\u4e00\u4e0b\uff0c\u518d\u8fd0\u884c</p> <p></p> <p>1\u662fbatch size\uff1b4\u662fpatch\u7684\u6570\u76ee\uff1b48\u662f\u6bcf\u4e2apatch\u6240\u5305\u542b\u7684\u50cf\u7d20\u70b9\u7684\u6570\u76ee</p> <p>\u5f97\u5230patch\u4ee5\u540e\uff0c\u6253\u5370weight\u5f62\u72b6</p> <p></p> <p>weight\u5f62\u72b6 \u521a\u597d\u662f48\u00d78\uff0c\u6240\u4ee5\u6211\u4eec\u628apatch\u8ddfweight \u8fdb\u884c\u4e00\u4e2a\u77e9\u9635\u76f8\u4e58\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230patch embedding\uff0c\u7528@\u7b26\u53f7\uff0c\u7136\u540e\u8fd4\u56depatch embedding\uff0c\u7136\u540e\u628a\u5f97\u5230patch embedding naive\uff0c\u7136\u540e\u6253\u5370patch embedding naive\u7684\u5f62\u72b6</p> <p></p> <p>\u8fd9\u6837 \u5c31\u5b8c\u6210\u4e86 \u7b2c\u4e00\u4e2a\u51fd\u6570\u7684\u6d4b\u8bd5 \u628a\u8fd9\u4e2a3\u00d78\u00d78\u7684\u56fe\u7247\uff0c\u53d8\u6210\u4e86 embedding\u7684\u5f62\u5f0f\uff0cembedding\u7684\u5927\u5c0f\u662f4\u00d78\uff1b\u4e5f\u5c31\u662f\u4e00\u5f20\u56fe\u7247\u88ab\u5206\u6210\u4e864\u5757 \u5e76\u4e14\u6bcf\u4e00\u5757 \u53d8\u6210\u4e86\u957f\u5ea6\u4e3a8\u7684\u5411\u91cf\uff0c\u6765\u8868\u793a\u8fd9\u4e2a\u5757</p> <p>\u4ee5\u4e0a\u662f\u6240\u6709naive\u7684\u5b9e\u73b0\u6b65\u9aa4</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    patch_embedding = patch @ weight\n    return patch_embedding\ndef image2emb_conv():\n    pass\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim)\n\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)\nprint(patch_embedding_naive.shape)\n</code></pre>"},{"location":"learning/vit/#412-convflatten-output","title":"4.1.2 conv\u7248\u672c\uff1aflatten output","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# step1 convert image to embedding vector sequence\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    patch_embedding = patch @ weight\n    return patch_embedding\ndef image2emb_conv(image,kernel,stride):\n    conv_output = F.conv2d(image,kernel,stride=stride) # bs*oc*oh*ow\n    bs,oc,oh,ow = conv_output.shape\n    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n\n    return patch_embedding\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim) # model_dim\u662f\u8f93\u51fa\u901a\u9053\u6570\u76ee\uff0cpatch depth\u662f\u5377\u79ef\u6838\u7684\u9762\u79ef\u4e58\u4ee5\u8f93\u5165\u901a\u9053\u6570\n\n# \u5206\u5757\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)\nkernel = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size))\n\n# \u4e8c\u7ef4\u5377\u79ef\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_conv = image2emb_conv(image,kernel,patch_size)\n\nprint(patch_embedding_naive)\nprint(patch_embedding_conv)\n</code></pre> <p>\u6ce8\u91ca\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# step1 convert image to embedding vector sequence\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width = 1,3,8,8\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    # patchshape = torch.Size([1, 48, 4])   patch_size = 4\n    # 1\uff1abatchsize\n    # 48 = 3*4*4\uff08\u5377\u79ef\u8986\u76d6\u7684input region\uff09\n    # 4\uff1a1,3,8,8\u7684\u8f93\u5165\u56fe\u7247\u7528 1344\u7684\u5377\u79ef\u6838\u5377\u79ef\uff0c\u5f97\u52304\u4e2ainput region\n    # transpose(-1,-2) \u2192 1,4,48  \n    patch_embedding = patch @ weight\n    # 1,4,48 @ 48,8 = 1\u00d7 4 \u00d7 8\n    return patch_embedding\n\ndef image2emb_conv(image,kernel,stride):\n    # image = bs,ic,image_h,image_w = 1,3,8,8 \n    # kernel = 8 \u00d7 3 \u00d7 4 \u00d7 4\n    # stride = patch_size = 4 \n    conv_output = F.conv2d(image,kernel,stride=stride) # bs*oc*oh*ow\n    # (h-k+2p+s)/s = (8-4+4)/4  = 2\n    # conv_output = 8 \u00d7 1 \u00d7 2 \u00d7 2\n    bs,oc,oh,ow = conv_output.shape\n    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n    # conv_output = 1 \u00d7 8 \u00d7 2 \u00d7 2\n    # reshape \uff1a 1 \u00d7 8 \u00d7 4\n    # transpose(-1,-2)  1 \u00d7 4 \u00d7 8\n    #\uff08\u8f93\u5165\u56fe\u7247 \u5212\u5206\u6210 4\u4e2apatch\uff0c\u6bcf\u4e2apatch\u7531\u539f\u6765\u7684 48\u4e2a\u50cf\u7d20\u8868\u793a\uff0c\u964d\u7ef4\u62108\u7ef4\u8868\u793a\uff09\n    return patch_embedding\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim) # model_dim\u662f\u8f93\u51fa\u901a\u9053\u6570\u76ee\uff0cpatch depth\u662f\u5377\u79ef\u6838\u7684\u9762\u79ef\u4e58\u4ee5\u8f93\u5165\u901a\u9053\u6570\n\n# \u5206\u5757\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)\n\n# conv\u7248\u672c\uff1a\nkernel = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size))\n# weight = 48 \u00d7 8\n# transpose(0,1) : 8 \u00d7 48\n# reshape :8 \u00d7 3 \u00d7 4 \u00d7 4\n\n# \u4e8c\u7ef4\u5377\u79ef\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_conv = image2emb_conv(image,kernel,patch_size)\n# image = bs,ic,image_h,image_w = 1,3,8,8 \n# kernel = 8 \u00d7 3 \u00d7 4 \u00d7 4\n# patch_size = 4\n\nprint(patch_embedding_naive)\nprint(patch_embedding_conv)\n</code></pre> <p>\u4ee3\u7801\u8be6\u89e3\uff1a</p> <p>\u63a5\u4e0b\u6765 \u7528\u5377\u79ef\u5b9e\u73b0 conv\u7684\u7248\u672c</p> <p>\u65e2\u7136\u662f\u5377\u79ef \u5c31\u9700\u8981\u5bf9\u4f20\u5165\u7684\u53c2\u6570\u6539\u4e00\u4e0b\uff0c\u7b2c\u4e00\u4e2a\u53c2\u6570 \u8fd8\u662f image\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662fkernel\uff0c\u7b2c\u4e09\u4e2a\u53c2\u6570 \u9700\u8981 stride\uff0c\u8fd9\u6837\u6211\u4eec\u5b9a\u4e49\u597d\u4e86 \u5377\u79ef\u7684\u4e09\u8981\u7d20 </p> <p>\u9996\u5148\u5b9a\u4e49F.conv2d\u7b2c\u4e00\u4e2a\u53c2\u6570 image\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570 kernel\uff0cstride\u6b65\u957f\u8bbe\u7f6e\u4e3astride\uff0c\u8fd9\u6837\u5f97\u5230\u4e86conv_output,\u8fd9\u6837\u505a\u4e86\u4e00\u4e2a\u5377\u79ef\uff0c\u7b49\u4e0b \u4f1a\u8bb2\u89e3 \u8fd9\u4e2a\u5f97\u5230\u7684\u548c\u662f\u4ec0\u4e48</p> <pre><code>def image2emb_conv(image,kernel,stride):\n    conv_output = F.conv2d(image,kernel,stride=stride)\n    pass\n</code></pre> <p>\u8fd9\u6837\u64cd\u4f5c\u7684\u8bdd  \u5377\u79ef\u8f93\u51fa\u7684\u5927\u5c0f\u662f\u4ec0\u4e48\u5462\uff1f</p> <p>\u5377\u79ef\u8f93\u51fa\u7684\u5927\u5c0f\u662f batch_size\u00d7output channel\u00d7output height\u00d7output width  </p> <p>\u6700\u7ec8 \u6211\u4eec\u8981\u5f97\u5230 patch embedding\uff0c\u5176\u5b9e\u6211\u4eec\u5377\u79ef\u540e\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\uff0c\u6211\u4eec\u4f1a\u628a\u5b83\u62c9\u6210\u4e00\u4e2a\u5e8f\u5217\uff0c\u770b\u6846\u67b6\u56fe\uff1a</p> <p></p> <p>\u5c31\u662f\u8bf4 \u5c06 output feature map\u62c9\u76f4\uff0c\u62c9\u76f4\u7684\u5c31\u662foutput height\u00d7output width\u7684\u90e8\u5206\uff0c\u4e5f\u5c31\u662f\u8bf4 \u53ef\u4ee5\u5bf9conv output\u8fdb\u884c\u4e00\u4e2areshape\uff0creshape\u6210bs\u00d7oc\u00d7\uff08oh\u00d7ow\uff09\u518dtranspose\u4e00\u4e0b\uff0c\u628a\u5e8f\u5217\u957f\u5ea6\u8fd9\u4e00\u7ef4 \u653e\u5230\u4e2d\u95f4\uff0c\u8fd9\u6837\u5f97\u5230 patch embedding\uff0c\u6700\u540e \u8fd4\u56de patch embedding</p> <p>\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u9996\u5148\u5c06 image2embedding\u7684\u8fc7\u7a0b \u9996\u5148\u770b\u51fa\u4e8c\u7ef4\u5377\u79ef\uff0c\u5377\u79ef\u7684\u7ed3\u679c\u662f\u4e00\u4e2abatch size\u00d7\u901a\u9053\u6570\u518d\u2716\ufe0f\u9ad8\u5ea6\u2716\ufe0f\u5bbd\u5ea6\uff0c\u56e0\u4e3a\u6211\u4eec\u8fd9\u91cc\u6a21\u4effnlp\u4e2d \u628a\u56fe\u7247\u53d8\u6210\u5e8f\u5217\uff0c\u4e8e\u662f\u6211\u4eec\u53ef\u4ee5\u628a\u7279\u5f81\u56fe \u9ad8\u5ea6\u548c\u5bbd\u5ea6\u6d53\u7f29\u6210\u4e00\u8d77\uff0c\u5c31\u662f\u62c9\u76f4\u7684\u610f\u601d\uff0c\u7136\u540e\u518d\u628a\u901a\u9053\u6570\u548c\u5d4c\u5165\u4f4d\u7f6e \u4ea4\u6362\u4e00\u4e0b\u7ef4\u5ea6\uff0c\u901a\u9053\u6570 \u5c31\u662f patch size\uff0coh\u00d7ow\u5c31\u662fsequence\u7684\u957f\u5ea6</p> <pre><code>def image2emb_conv(image,kernel,stride):\n    conv_output = F.conv2d(image,kernel,stride=stride) # bs*oc*oh*ow\n    bs,oc,oh,ow = conv_output.shape\n    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n\n    return patch_embedding\n</code></pre> <p>\u63a5\u4e0b\u6765\u6700\u5173\u952e\u7684\u5c31\u662f \u5b9a\u4e49\u597dkernel\uff0c\u90a3kernel\u600e\u4e48\u5b9a\u4e49\u5462\uff1fkernel\u5c31\u662fweight\uff0c\u53ea\u4e0d\u8fc7\u8981\u628a\u5f62\u72b6\u53d8\u4e00\u53d8\u3002</p> <p></p> <p>\u9996\u5148 \u4e0a\u9762\u8fd9\u4e2aweight\u7684\u5f62\u72b6\u662f patch depth\u00d7model dim\uff0c\u5176\u5b9e\u53ef\u4ee5\u600e\u4e48\u7406\u89e3\u5462\uff1fmodel dim\u5c31\u53d8\u6210\u4e86oc\uff0c\u6240\u4ee5model dim\u5c31\u662f\u8f93\u51fa\u901a\u9053\u6570\u76ee </p> <p>patch depth\u5c31\u662f\u5377\u79ef\u6838\u7684\u9762\u79ef\u00d7\u8f93\u5165\u901a\u9053\u6570</p> <p>kernel\u7684\u5f62\u72b6\u6309\u7167\u4e8c\u7ef4\u5377\u79ef\u7684\u5f62\u5f0f\uff0coc\u00d7ic\u00d7kh\u00d7kw \u8f93\u51fa\u901a\u9053\u6570\u3001\u8f93\u5165\u901a\u9053\u6570\u3001kernel\u5377\u79ef\u6838\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\uff0c\u6240\u4ee5\u628akernel  reshape\u6210oc\u00d7ic\u00d7kh\u00d7kw \u8fd9\u79cd\u5f62\u72b6\uff1b</p> <p>\u9996\u5148\u5c06kernel\u8f6c\u7f6e\u4e00\u4e0b\uff0c\u901a\u9053\u6570\u653e\u5230\u6700\u524d<code>kernel.transpose(0,1)</code></p> <p>\u7136\u540e\u518dreshape\u4e00\u4e0b\uff0c\u90a3reshape\u6210\u4ec0\u4e48\u5f62\u72b6\u5462\uff1f\u6309\u7167<code>oc\u00d7ic\u00d7kh\u00d7kw</code>\u7684\u6570\u636e\u683c\u5f0f\uff0c</p> <p></p> <p>oc\u4e0d\u77e5\u9053\u5927\u5c0f\uff0c\u7528-1\u8868\u793a\uff0cic\u524d\u9762\u5b9a\u4e49\u4e86\uff0c\u63a5\u4e0b\u6765kh\u00d7kw\uff0ckh\u3001kw\u5c31\u662fpatch size\uff0c\u8fd9\u91cc\u6709\u4e2a\u7b14\u8bef</p> <p></p> <p>kernel=weight.transpose</p> <p>weight\u7684\u5f62\u72b6\uff1a<code>patch_depth \u00d7 model_dim</code>\u3001<code>patch depth=patch size\u00d7patch size\u00d7ic</code>\u3001<code>model dim=oc</code></p> <p>\u6240\u4ee5\u6211\u4eec\u7684\u505a\u6cd5 \u9996\u5148\u4ea4\u63620\u30011\u7ef4\u5ea6 \u628a\u8f93\u51fa\u901a\u9053\u6570\u653e\u5230\u524d\u9762\uff0c\u7136\u540e\u505areshape\u64cd\u4f5c\uff0c\u7136\u540e\u628akernel\u4ee3\u5165\u51fd\u6570\u5f53\u4e2d\uff0c\u505aimage2emb\uff0c\u9996\u5148\u4f20\u5165\u7684\u662fimage\uff0c\u7136\u540e\u662fkernel\uff0c\u7136\u540e\u8fd9\u91cc\u7684stride\u5c31\u662fpatch size\uff0c\u7ed3\u679c\u4f20\u7ed9<code>patch embedding conv</code></p> <p></p> <p>\u5176\u4e2d\uff0cpatch embedding naive\u662f\u5206\u5757\u65b9\u6cd5\u5f97\u5230patch embedding</p> <p>patch embedding conv\u662f\u4e8c\u7ef4\u5377\u79ef\u7684\u65b9\u6cd5\u5f97\u5230patch embedding</p>"},{"location":"learning/vit/#413","title":"4.1.3 \u9a8c\u8bc1\u7ed3\u679c\u4e00\u6837","text":"<p>\u6253\u5370\u67e5\u770b\u7ed3\u679c \u7ed3\u679c\u57fa\u672c\u4e0a\u662f\u4e00\u6837\u7684</p> <p></p>"},{"location":"learning/vit/#414-patch","title":"4.1.4 patch \u6784\u5efa\u7684\u5168\u90e8\u4ee3\u7801","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# step1 convert image to embedding vector sequence\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    patch_embedding = patch @ weight\n    return patch_embedding\ndef image2emb_conv(image,kernel,stride):\n    conv_output = F.conv2d(image,kernel,stride=stride) # bs*oc*oh*ow\n    bs,oc,oh,ow = conv_output.shape\n    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n\n    return patch_embedding\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim) # model_dim\u662f\u8f93\u51fa\u901a\u9053\u6570\u76ee\uff0cpatch depth\u662f\u5377\u79ef\u6838\u7684\u9762\u79ef\u4e58\u4ee5\u8f93\u5165\u901a\u9053\u6570\n\n# \u5206\u5757\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)\nkernel = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size))\n\n# \u4e8c\u7ef4\u5377\u79ef\u65b9\u6cd5\u5f97\u5230embedding\npatch_embedding_conv = image2emb_conv(image,kernel,patch_size)\n\n\nprint(patch_embedding_naive)\nprint(patch_embedding_conv)\n</code></pre> <p>\u603b\u7ed3\uff1a\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5176\u5b9e\u662f\u4e00\u79cd\u65b9\u6cd5\uff0c\u5982\u679c\u6309\u7167\u539f\u6587\u7684\u610f\u601d\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u539f\u6587\u9996\u5148\u662f\u5206\u6210\u5757\uff0c\u7528\u4e00\u4e2a\u77e9\u9635\u8fdb\u884c\u76f8\u4e58\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u8fd9\u4e2a\u8fc7\u7a0b\u770b\u6210\u4e8c\u7ef4\u5377\u79ef\u7684\u8fc7\u7a0b\uff1b\u5c31\u662f\u8bf4\u628a\u77e9\u9635\u8f6c\u7f6e\u6210kernel\u7684\u5f62\u72b6\uff0c\u7136\u540e\u4ee5kernel size\u7b49\u4e8estride\u7684\u5377\u79ef \u6765\u5bf9\u4e8c\u7ef4\u56fe\u5f62\u8fdb\u884c\u5377\u79ef\uff0c\u5377\u79ef\u8fc7\u540e\uff0c\u628a\u5377\u79ef\u8f93\u51fa\u7684\u7279\u5f81\u56fe\uff0c\u901a\u9053\u770b\u6210embedding\u7684\u5927\u5c0f\uff0c\u5377\u79ef\u7279\u5f81\u56fe\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u4e8c\u7ef4\u5f62\u5f0f\u62c9\u76f4\uff0c\u62c9\u6210\u4e00\u7ef4\u5e8f\u5217\u957f\u5ea6\uff0c\u5370\u8bc1\u4e86\u6211\u4eec\u7684\u89d2\u5ea6\uff0c\u4eceCNN\u7684\u89d2\u5ea6\u7406\u89e3\uff0c\u505a\u4e00\u4e2a\u4e8c\u7ef4\u5377\u79ef\uff0c\u518d\u628a\u8f93\u51fa\u7684\u7279\u5f81\u56fe\u62c9\u76f4\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u7ef4\u7684embedding\u5e8f\u5217</p> <p></p> <p>\u4ee5\u4e0a\u662f\u7b2c\u4e00\u6b65 \u7531\u56fe\u7247\u5f97\u5230embedding\uff1aconvert image to embedding vector sequence</p>"},{"location":"learning/vit/#42-cls-token","title":"4.2 CLS token","text":"<pre><code># step2 prepend CLS token embedding\n# patch_embedding_conv = 1 \u00d7 4 \u00d7 8\n# cls_token_embedding = 1 \u00d7 1 \u00d7 8\n\ncls_token_embedding = torch.randn(bs,1,model_dim,requires_grad=True)\n# token_embedding\n# \u7b2c\u4e00\u4e2a\u4f4d\u7f6e \u662f cls token\uff0ccls token\u7684\u5d4c\u5165\u7ef4\u5ea6\u662f 8\n# \u6240\u4ee5 dim = 1\ntoken_embedding = torch.cat([cls_token_embedding,patch_embedding_conv],dim=1)\n</code></pre> <p>\u7b2c\u4e8c\u6b65\u52a0\u4e0a\u4e2a cls  token\uff1bclassification token\uff0c\u8fd9\u662f\u6a21\u4effbert\u6a21\u578b\u4e2d\uff0c</p> <p></p> <p>\u9700\u8981\u5728\u5e8f\u5217\u5f00\u59cb \u52a0\u5165\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684 embedding\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a query\uff0cup\u4e3b\u4e5f\u6709\u7591\u95ee\uff0c\u65e2\u7136\u662fquery \u4e3a\u4ec0\u4e48\u8981\u52a0\u53ef\u5b66\u4e60\u7684embedding\u4e3a\u4ec0\u4e48\u4e5f\u53ef\u4ee5\u52a0position embedding\uff0c\u4e3a\u4ec0\u4e48\u5176\u4ed6\u4f4d\u7f6e\u4e5f\u53ef\u4ee5\u5bf9\u5b83\u8ba1\u7b97\u6ce8\u610f\u529b\u673a\u5236  \uff1b\u603b\u4e4b\u8fd9\u91cc\u9762\u8fd8\u6709\u5f88\u591a\u96be\u4ee5\u89e3\u51b3\u7684\u95ee\u9898</p> <p>\u6839\u636e\u539f\u6587\u7684\u610f\u601d\uff0cCLS\u662f\u968f\u673a\u521d\u59cb\u5316\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u5f20\u91cf\uff1a<code>cls_token_embeddding torch.randn()</code>\u5f62\u72b6\u662f batch size\uff0c\u957f\u5ea6\u663e\u7136\u662f1\uff0c\u5927\u5c0f\u662fmodel dim\uff0c\u9700\u8981\u589e\u52a0\u4e00\u4e2a\u53c2\u6570<code>requires_grad=True</code>\u56e0\u4e3a\u662f\u53ef\u8bad\u7ec3\u7684</p> <p></p> <p>\u4ee5\u4e0a\u589e\u52a0\u4e86cls_token_embedding</p> <p></p> <p>\u63a5\u4e0b\u6765\u5c06 naive\u6216\u8005conv\u7ed9\u62fc\u8d77\u6765\uff0c\u8c03\u7528torch.cat\u51fd\u6570\u8fdb\u884c\u62fc\u63a5\uff0c\u628acls embedding\u653e\u5728\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\uff0cpatch embedding\u653e\u5728\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e\u4e0a\uff0c\u63a5\u4e0b\u6765\u8003\u8651\u5728\u54ea\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u62fc\u63a5 \uff0c\u8fd9\u4e2adim\u4f200\u8fd8\u662f1\u8fd8\u662f2\uff0c\u56e0\u4e3a\u6709\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u56e0\u4e3a\u6211\u4eec\u5728\u4f4d\u7f6e\u4e0a\u62fc\u63a5\uff0c\u4e0d\u662fbatch size\u4e0d\u662f\u901a\u9053\uff0c\u800c\u662f\u5728\u4f4d\u7f6e\u4e0a\u62fc\u63a5\uff0c\u6240\u4ee5dimension\u4f20\u51651\uff0c\u4e5f\u5c31\u662f\u4e2d\u95f4\u7684\u7ef4\u5ea6\uff0c\u8fd9\u6837\u6211\u4eec\u5f97\u5230\u4e86token embedding</p> <p>token embedding=patch embedding+cls token\uff0c\u4e5f\u5c31\u662f\u6211\u4eec\u589e\u52a0\u7684\u5206\u7c7b\u5b57\u7b26</p> <p></p>"},{"location":"learning/vit/#43-position-embedding","title":"4.3 Position embedding","text":"<pre><code># step3 add position embedding\npositon_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=True)\nseq_len = token_embedding.shape[1]\npositon_embedding = torch.tile(positon_embedding_table[:seq_len],[token_embedding.shape[0],1,1])\ntoken_embedding += positon_embedding\n</code></pre> <p>\u6ce8\u91ca\uff1a</p> <pre><code># step3 add position embedding\n# max_num_token = 16\n# model_dim = 8\n# positon_embedding_table = 16,8\npositon_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=True)\n\n# token_embedding = 1,5,8 (bs,5\u4e2a\u4f4d\u7f6e(1\u4e2acls token\u30014\u4e2a\u5355\u8bcd),model_dim = 8)\n# seq_len = 5\nseq_len = token_embedding.shape[1]\n\npositon_embedding = torch.tile(positon_embedding_table[:seq_len],[token_embedding.shape[0],1,1])\n# positon_embedding_table[:seq_len] = positon_embedding_table[:5] \u53d6\u524d5\u4e2a8\u7ef4\n# [:5] \u8868\u793a \u5bf9 \u7b2c\u4e00\u7ef4 \u7d22\u5f15\n# positon_embedding_table[:seq_len] = 5,8\n# [token_embedding.shape[0],1,1] = [1,1,1]\n# positon_embedding = 1,5,8\ntoken_embedding += positon_embedding\n# token_embedding = 1,5,8\n</code></pre> <p>\u63a5\u4e0b\u6765 \u6211\u4eec\u8fd8\u9700\u8981\u589e\u52a0position embedding</p> <p>\u6587\u7ae0\u4e2d \u4f5c\u8005\u5bf9\u6bd4\u4e86\u5f88\u591a\u79cdposition emebedding\uff0c\u6548\u679c\u90fd\u5dee\u4e0d\u591a\uff0c\u6700\u7ec8\u91c7\u7528\u7684\u662f\u4e00\u4e2a \u968f\u673a\u7684 \u53ef\u5b66\u4e60\u7684 emebedding</p> <p>\u9996\u5148 \u5b9a\u4e49\u4e00\u4e2a embedding table\uff0c\u5173\u4e8e embedding table\uff0c\u4e4b\u524d\u8bb2\u8fc7\u5f88\u591a\u6b21\uff0c\u9996\u5148table\u7684\u5f62\u72b6\uff0c\u662f\u5355\u8bcd\u7684\u6570\u76ee\u00d7\u5d4c\u5165\u7684\u7ef4\u5ea6\uff1b\u4f4d\u7f6e\u7684\u6570\u76ee\u00d7\u6a21\u578b\u7ef4\u5ea6\uff0c\u6240\u4ee5\u6211\u4eec\u8fd8\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u91cf max_num_token,\u5c31\u662ftoken\u7684\u6700\u5927\u6570\u76ee max_num_token=16,\u540c\u6837\u8bbe\u7f6e requires grad=True \u662f\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684</p> <p></p> <p>\u5728\u8bbe\u7f6e\u597d\u4e86 position embedding table\u4ee5\u540e\uff0c\u53ef\u4ee5\u5c06table\u53d6\u51fa\u6765\uff0ctable\u53d6\u51fa\u6765\uff0c\u5c31\u53ef\u4ee5\u4f9d\u8d56\u4e8e\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u6240\u4ee5\u9996\u5148 \u6211\u4eec\u5f97\u5230sequence length,\u5e94\u8be5\u5c31\u662f\u5f97\u5230token embedding</p> <pre><code>seq_len = token_embedding.shape[1]\n</code></pre> <p>\u5176\u5b9e\u5728\u56fe\u50cf\u4e2d\uff0c\u540c\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u56fe\u50cf\u5927\u5c0f\u4e00\u822c\u662f\u56fa\u5b9a\u7684\uff0c\u5c31\u662f\u8bf4\u4e00\u822c\u662f\u76f8\u540c\u7684\uff0c\u5c31\u8bf4\u5728\u56fe\u50cf\u4e2d\uff0c\u6240\u4ee5\u5bf9\u4e8emask\u6bd4nlp\u4e2d \u5c31\u4f1a\u5c11\u4e00\u70b9\uff1b\u4f46\u4eca\u5929\u8fd8\u662f\u4e0d\u8bb2mask\u4e86\uff0c\u4eca\u5929\u5ffd\u7565mask\u3002</p> <p>\u9996\u5148\u5f97\u5230sequence length\uff0c\u5c31\u53ef\u4ee5\u6839\u636esequence length\uff0c\u4eceposition embedding table\u4e2d\u53d6\u51fa\u4f4d\u7f6e\u7f16\u7801\uff0c\u53d6\u51fa\u524dsequence length\u4e2a\uff0c\u53d6\u51fa\u6765\u4ee5\u540e \u8fd8\u662f\u4e00\u4e2a\u4e8c\u7ef4\u7684\u5f20\u91cf\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u4e00\u4e2a\u590d\u5236\uff0c\u590d\u5236\u6210\u4e00\u4e2a\u4e09\u7ef4\u7684\uff0c\u4e3b\u8981\u8fd8\u662f\u590d\u5236\u6210 batch size\u8fd9\u4e2a\u6570\u76ee, \u5c31\u662ftoken_embedding.shape[0],\u590d\u5236\u8fd9\u4e48\u591a\u4efd\uff0c\u56e0\u4e3a\u8fd9\u91cc\u662fbatch\u8fd9\u4e2a\u683c\u5f0f\uff0c\u90a3\u540e\u9762\u8fd9\u4e2a\u4f4d\u7f6e \u6216\u8005\u8bf4\u901a\u9053 \u90fd\u4e0d\u7528\u590d\u5236\uff0c\u8fd9\u6837\u5f97\u5230 position embedding\uff0c</p> <p></p> <p>\u6700\u540eposition embedding \u518d\u548c token embedding\u76f8\u52a0\uff1b</p> <p></p> <p>\u8fd9\u6837\u5b9e\u73b0\u4e86position embedding\u52a0\u5165\u5230token embedding\u4e4b\u4e2d\uff0c\u5b8c\u6210\u4e86\u7b2c\u4e09\u6b65</p>"},{"location":"learning/vit/#44-transformer-encoder","title":"4.4  Transformer Encoder","text":"<pre><code># step4 Pass embedding to Transformer Encoder\n# d_model = model_dim = 8\nencoder_layer = nn.TransformerEncoderLayer(d_model=model_dim,nhead=8)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\n# token_embedding = 1,5,8(\u53ef\u4ee5\u7406\u89e3\u4e3a 5\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd \u5d4c\u5165 8\u4e2a\u7ef4\u5ea6)\nencoder_output = transformer_encoder(token_embedding)\n</code></pre> <p>\u63a5\u4e0b\u6765 \u7b2c\u56db\u6b65\uff0c\u6309\u7167\u8bba\u6587\u4e2d\u7684\u56fe\uff0c\u7b2c\u56db\u6b65\u6bd4\u8f83\u7b80\u5355\u4e86\uff0c\u76f4\u63a5\u5c06embedding\u9001\u5230transformer encoder\uff0c\u8fd9\u90e8\u5206\u6bd4\u8f83\u7b80\u5355</p> <p></p> <p>\u6211\u4eec\u9700\u8981\u6765\u770b\u4e00\u4e0b pytorch\u7684transformer encoder\u7684api</p> <p></p> <p>\uff0c\u800c\u4e0d\u53bb\u5199 \u4e00\u4e2a\u5177\u4f53\u7684\u4ee3\u7801</p> <p></p> <p>\u8fd9\u91ccpytorch\u5df2\u7ecf\u628atransformer \u5b8c\u6574\u7684\u5b9e\u73b0\u4e86\u5206\u4e3aencoder\u3001decoder\u3001encoderLayer\u3001decoderLayer\u4e4b\u7c7b\u7684\uff1b\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4f8b\u5b50</p> <p></p> <p>\u9996\u5148 \u5b9e\u4f8b\u5316\u4e00\u4e2a encoderLayer,encoderLayer\u5b9e\u9645\u4e0a\u5c31\u662fMHA+FFN\u6784\u6210\u7684\uff1b\u7136\u540e\u518d\u628alayer\u8fd9\u4e2a\u5bf9\u8c61\uff0c\u9001\u5230 encoder\u4e2d\uff0c\u5e76\u4e14\u5b9a\u4e49\u597d num_layers,\u5f97\u5230encoder\u5bf9\u8c61\uff1b</p> <p>\u7b2c\u56db\u6b65 \u628a embedding\u9001\u5165\u5230 transformer\u4e2d\uff0c</p> <p>\u590d\u5236\u4f8b\u5b50\uff0c\u628ad_model=512\u6539\u6210 d_model=model_dim</p> <pre><code>encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim,n_head=8)\n</code></pre> <p>\u628a encoder layer\u9001\u5165\u5230 encoder\u4e2d\uff0c\u5b9e\u4f8b\u5316\u4e00\u4e2a nn.TransformerEncoder,\u90fd\u4eff\u7167\u4f8b\u5b50\uff0c\u7b2c\u4e00\u4e2a\u53c2\u6570\u662fencoder_layer,\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f num_layers,\u8fd9\u6837\u5c31\u5f97\u5230\u4e86 transformer_encoder</p> <pre><code>transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\n</code></pre> <p>transformer_encoder\u76f4\u63a5\u4ee5 token_embedding\u4f5c\u4e3a\u8f93\u5165\u5c31\u597d\u4e86,\u5ffd\u7565 mask,\u5f97\u5230 encoder output</p> <pre><code>encoder_output = transformer_encoder(token_embedding)\n</code></pre>"},{"location":"learning/vit/#45-classification-head","title":"4.5 classification head","text":"<pre><code># step5 do classification\ncls_token_output = encoder_output[:,0,:]\nlinear_layer = nn.Linear(model_dim,num_classes)\nlogits = linear_layer(cls_token_output)\nloss_fn = nn.CrossEntropyLoss()\nloss = loss_fn(logits,label)\nprint(loss)\n</code></pre> <p>\u7b2c\u4e94\u6b65\uff0c</p> <p></p> <p>\u53d6\u51fa class token\u8fd9\u4e2a\u4f4d\u7f6e\u4e0a\u7684 \u7279\u5f81\u8f93\u51fa\uff0c\u7136\u540e\u628a\u5b83\u6620\u5c04\u5230\u7c7b\u522b\u4e0a\u9762\uff0c\u5f97\u5230\u6982\u7387\u5206\u5e03\uff0c\u8ddf\u6807\u7b7e \u8ba1\u7b97 \u4ea4\u53c9\u71b5\uff0c\u8fdb\u884c\u6a21\u578b\u7684 \u8bad\u7ec3</p> <pre><code># step5 do classification\n</code></pre> <p>\u9996\u5148 \u5c06\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u7684 \u72b6\u6001 \u53d6\u51fa\u6765\uff0c\u5b9a\u4e49 cls_token_output,\u56e0\u4e3a encoder output\u662f \u4e09\u7ef4\u7684\uff0c\u90a3\u4e48 \u7b2c\u4e00\u7ef4\u662fbatch size\u4e0d\u7528\u7ba1\uff1b\u7b2c\u4e8c\u7ef4\u662f\u4f4d\u7f6e \u5199\u4e2a0\uff0c\u8868\u793a\u7b2c\u4e00\u4e2a\uff1b\u7b2c\u4e09\u7ef4\u662f \u901a\u9053\u6570\u76ee \u4e5f\u4e0d\u7528\u7ba1</p> <pre><code>cls_token_output = encoder_output[:,0,:]\n</code></pre> <p>\u8fd9\u6837\u5f97\u5230\u4e86 \u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u4e0a \u7684 encoder \u7684\u8f93\u51fa\uff1b\u6211\u4eec\u5c06\u5176 \u6620\u5c04\u5230\u7c7b\u522b\u4e0a\uff1b\u6240\u4ee5 \u6211\u4eec\u9700\u8981 \u518d\u5b9a\u4e49\u4e00\u4e2a \u5e38\u91cf \u53ebnum_classes\uff0c\u8fd9\u662f\u7c7b\u522b\u6570\u76ee</p> <pre><code>num_classes = 10\n</code></pre> <p>\u518d\u5b9e\u4f8b\u5316 \u4e00\u4e2ann.Linear()\u5c42\uff0c\u4e5f\u5c31\u662fpytorch\u4e2d\u7684nn.Linear()\u5c42\uff0c\u8f93\u5165\u7684\u901a\u9053\u6570 \u662f model_dim,\u56e0\u4e3a transformer\u7684\u8f93\u51fa\u5c31\u662f model_dim  \u5c31\u662f\u5927\u5c0f\uff0c\u8f93\u51fa\u7684\u7279\u5f81\u6570 \u5c31\u662f num_classes;\u8fd9\u662f\u4e3a\u4e86 \u5206\u7c7b\u4e4b\u524d \u5bf9 encoder output \u505a\u4e00\u4e2a \u6620\u5c04\uff0c\u5f97\u5230 linear_layer</p> <pre><code>linear_layer = nn.Linear(model_dim,num_classes)\n</code></pre> <p>\u7136\u540e\u8fd9\u4e2a linear_layer\u8fdb\u884c \u8c03\u7528\u4e00\u4e0b\uff0c\u4ee5class token output \u4f5c\u4e3a\u8f93\u5165\uff0c\u8fd9\u6837\u5f97\u5230 logits\uff0c</p> <pre><code>logits = linear_layer(cls_token_output)\n</code></pre> <p>logits\u662f\u6ca1\u6709\u8fc7softmax\u7684\uff0c\u63a5\u4e0b\u6765 \u56e0\u4e3a \u5728 loss function\u4e2d\u662f\u4f1a\u8c03\u7528softmax\u7684</p> <p>\u73b0\u5728 \u6211\u4eec\u5b9e\u4f8b\u5316\u4e00\u4e2a loss function\uff0cnn.CrossEntropyLoss()\u8fd9\u4e2a\u51fd\u6570\u7528\u5f97\u6bd4\u8f83\u591a \u4e0d\u7528\u67e5\u4e86</p> <pre><code> loss_fn = nn.CrossEntropyLoss()\n</code></pre> <p>\u5f53\u7136 \u8fd9\u4e2a loss_fn\uff0c\u662f\u4ee5logits\u548cLabel\u4f5c\u4e3a \u8f93\u5165</p> <pre><code>loss_fn(logits,label)\n</code></pre> <p>\u5f97\u5230loss</p> <pre><code>loss = loss_fn(logits,label)\n</code></pre> <p>\u63a5\u4e0b\u6765 \u5b9a\u4e49  Label \u8fd9\u4e2a\u6807\u7b7e\uff0cLabel \u662f\u4e00\u4e2a\u6574\u578b\u7684\u6807\u7b7e\uff0ctorch.randint()\u751f\u62100\u2014\u201410\u4ee5\u5185\u7684\u5f20\u91cf\uff0c\u5927\u5c0f\u662f batch size\u8fd9\u4e2a\u7ef4\u5ea6\uff0c\u8fd9\u6837 \u6211\u4eec\u751f\u6210\u4e86Label</p> <pre><code>label = torch.randint(10,(bs,))\n</code></pre> <p>\u8fd9\u6837 \u6211\u4eec \u751f\u6210\u4e86 label\uff0c\u6211\u4eec\u628a label\uff0c\u4f20\u5165\u5230loss function\u4e2d\uff0c\u8ba1\u7b97 loss\uff0c\u6700\u540e\u6253\u5370loss\uff0c\u63a5\u4e0b\u6765 \u6d4b\u8bd5</p>"},{"location":"learning/vit/#_1","title":"\u603b\u7ed3","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef image2emb_navie(image,patch_size,weight):\n    # image shape:bs  \u00d7 channel \u00d7 height \u00d7 width\n    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n    patch_embedding = patch @ weight\n    return patch_embedding\ndef image2emb_conv(image,kernel,stride):\n    conv_output = F.conv2d(image,kernel,stride=stride) # bs*oc*oh*ow\n    bs,oc,oh,ow = conv_output.shape\n    patch_embedding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n\n    return patch_embedding\n\n# test code for image2emb\nbs,ic,image_h,image_w = 1,3,8,8\npatch_size = 4\nmodel_dim = 8\nmax_num_token = 16\nnum_classes = 10\nlabel = torch.randint(10,(bs,))\n\npatch_depth = patch_size * patch_size * ic\nimage = torch.randn(bs,ic,image_h,image_w)\nweight = torch.randn(patch_depth,model_dim) # model_dim\u662f\u8f93\u51fa\u901a\u9053\u6570\u76ee\uff0cpatch depth\u662f\u5377\u79ef\u6838\u7684\u9762\u79ef\u4e58\u4ee5\u8f93\u5165\u901a\u9053\u6570\n\npatch_embedding_naive = image2emb_navie(image,patch_size,weight)  # \u5206\u5757\u65b9\u6cd5\u5f97\u5230embedding\nkernel = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size))   # oc*ic*kh*kw\n\npatch_embedding_conv = image2emb_conv(image,kernel,patch_size) # \u4e8c\u7ef4\u5377\u79ef\u65b9\u6cd5\u5f97\u5230embedding\n\n# print(patch_embedding_naive)\n# print(patch_embedding_conv)\n\n# step2 prepend CLS token embedding\ncls_token_embedding = torch.randn(bs,1,model_dim,requires_grad=True)\ntoken_embedding = torch.cat([cls_token_embedding,patch_embedding_conv],dim=1)\n\n# step3 add position embedding\npositon_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=True)\nseq_len = token_embedding.shape[1]\npositon_embedding = torch.tile(positon_embedding_table[:seq_len],[token_embedding.shape[0],1,1])\ntoken_embedding += positon_embedding\n\n# step4 Pass embedding to Transformer Encoder\nencoder_layer = nn.TransformerEncoderLayer(d_model=model_dim,nhead=8)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\nencoder_output = transformer_encoder(token_embedding)\n\n# step5 do classification\ncls_token_output = encoder_output[:,0,:]\nlinear_layer = nn.Linear(model_dim,num_classes)\nlogits = linear_layer(cls_token_output)\nloss_fn = nn.CrossEntropyLoss()\nloss = loss_fn(logits,label)\nprint(loss)\n</code></pre> <p>\u4ee5\u4e0a\u5b9e\u73b0\u4e86 \u6574\u4e2a vit\uff0c\u4ece\u8f93\u5165 \u5230 loss\uff0c\u5168\u90e8\u5b9e\u73b0\u4e86\uff1b\u5176\u4e2dimage2embedding\u7684\u8fc7\u7a0b\u7528\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\u4e86</p> <p>\u8fd9\u4e24\u79cd\u65b9\u5f0f\uff0c\u662f\u5728 \u5f00\u59cb\u7684\u65f6\u5019 \u5c55\u5f00\uff0c\u8fd8\u662f\u5728 \u5377\u79ef\u8fc7\u540e \u518d\u5c55\u5f00\uff0c\u4e24\u4e2a\u4e0d\u540c\u7684\u89d2\u5ea6\uff1b</p> <p>\u7136\u540e\u9700\u8981\u5728\u5e8f\u5217\u4e4b\u524d \u52a0\u5165cls token\uff1b</p> <p>\u7b2c\u4e09\u6b65 \u5bf9token embedding \u52a0\u5165 position embedding\uff1b\u6309\u7167\u8bba\u6587\u7684\u610f\u601d\u5c31\u662f\u589e\u52a0\u4e00\u4e2a \u53ef\u8bad\u7ec3\u7684embedding</p> <p>\u7b2c\u56db\u6b65 \u5c06 embedding\u4f20\u5165\u5230encoder\u4e2d</p> <p>\u7b2c\u4e94\u6b65 \u5c31\u662fclass token \u90a3\u4e2a\u4f4d\u7f6e\u4e0a\u7684output\uff0c\u505a\u4e00\u4e2a\u53d8\u6362\uff0c\u5f97\u5230 \u8981\u5206\u7c7b\u7684\u6982\u7387\u5206\u5e03\uff0c\u6700\u540e\u901a\u8fc7 \u4ea4\u53c9\u71b5\uff0c\u6765\u7b97\u51fa \u5206\u7c7b loss\uff1b</p> <p>\u603b\u4f53\u4e0a\u5c31\u662f \u8fd9\u6837\u7684\u8fc7\u7a0b\uff1b\u6240\u4ee5vit\u6a21\u578b \u4e0d\u7ba1\u662f \u6a21\u578b\u4e0a \u8fd8\u662f \u4ee3\u7801\u5b9e\u73b0 \u4e0a \u90fd\u6bd4\u8f83\u7b80\u5355\uff1b\u60f3\u6cd5\u4e5f\u5f88\u7b80\u5355 \u4f46\u662f\uff0c\u8bad\u7ec3\u6210\u672c\u5f88\u9ad8\uff0c\u9700\u8981\u5f88\u591a \u56fe\u7247\u6570\u636e \u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u624d\u80fd\u53d6\u5f97 \u8ddfCNN\u4e00\u6837\u7684\u6548\u679c\uff1b\u4e4b\u540e\u4e5f\u6709\u5f88\u591a\u5de5\u4f5c \u5c06 transformer\u5e94\u7528\u5230 \u68c0\u6d4b\u3001\u5206\u5272\u9886\u57df \uff1b\u73b0\u5728 \u53ea\u662f \u8bb2\u4e86 \u8bc6\u522b\u7684\u9886\u57df\uff1bvit\u4f5c\u8005\u4e5f\u63d0\u5230\u4e86 \u76ee\u524d\u53ea\u662f\u7528\u5230\u4e86 \u8bc6\u522b\u9886\u57df\uff0c\u540e\u9762\u6bd4\u8f83\u706b\u7684 swintransformer\uff0c\u4e00\u65b9\u9762\u964d\u4f4evit\u6a21\u578b\u7684\u8ba1\u7b97\u91cf\uff0c\u53e6\u4e00\u65b9\u9762 vit\u66f4\u52a0\u6a21\u4eff\u4e86CNN\u7684\u7ed3\u6784\uff0c\u6765\u53bb \u4e0d\u65ad\u5bf9 patch \u8fd9\u4e2a\u7ef4\u5ea6 \u8fdb\u884c\u964d\u7ef4\uff0c\u7136\u540e\u4e5f\u4f1a\u5bf9patch \u7684 weight \u8fdb\u884c\u53d8\u52a8\uff0cswintransformer\u4e0d\u4ec5\u5728 \u56fe\u50cf\u8bc6\u522b\u4e0a\uff0c\u5728\u68c0\u6d4b\u4e0a \u5728 \u5206\u5272\u4e0a \u90fd\u53d6\u5f97\u4e86 \u6bd4\u8f83\u597d\u7684\u6548\u679c\u3002</p>"},{"location":"learning/LittleLesson/","title":"Index","text":"<p>\u591a\u6a21\u6001\u6a21\u578b\u4e32\u8bb2</p>"},{"location":"learning/LittleLesson/1_clip/","title":"CLIP","text":""},{"location":"learning/LittleLesson/1_clip/#_1","title":"\u52a8\u673a","text":"<p>\u4e4b\u524d\u7684\u56fe\u7247\u5206\u7c7b\u7f51\u7edc\u90fd\u53ea\u80fd\u5bf9\u56fa\u5b9a\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\uff0c\u4e0d\u8bba\u662f ImageNet \u7684 1000 \u7c7b\uff0cCIFAR10 \u7684 10 \u7c7b\uff0c\u8fd8\u662fCIFAR100 \u7684 100 \u7c7b\uff0c\u5e26\u6765\u7684\u95ee\u9898\u6709\u4e24\u4e2a\uff1a</p> <p>\uff081\uff09\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u80fd\u5bf9\u56fa\u5b9a\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\uff0c\u4e0d\u80fd\u8fc1\u79fb\u5230\u5176\u4ed6\u7c7b\u522b</p> <p></p> <p>\uff082\uff09\u56fa\u5b9a\u597d\u7684\u7c7b\u522b\u9700\u8981\u6839\u636e\u7c7b\u522b\u6765\u6807\u6ce8\u6570\u636e\uff0c\u8d39\u65f6\u8d39\u529b</p> <p>\u7531\u6b64\u5f15\u53d1\u601d\u8003\uff1a\u6709\u6ca1\u6709\u89c6\u89c9\u6a21\u578b\u4e0d\u7528\u5bf9\u6570\u636e\u8fdb\u884c\u6807\u6ce8\u5c31\u53ef\u4ee5\u8bad\u7ec3\uff0c\u5e76\u4e14\u53ef\u4ee5\u5bf9\u4efb\u610f\u7c7b\u522b\u8fdb\u884c\u9884\u6d4b\uff0c\u4e0d\u7528\u4e8b\u5148\u56fa\u5b9a\u5206\u7c7b\u7684\u7c7b\u522b\uff1f</p>"},{"location":"learning/LittleLesson/1_clip/#_2","title":"\u542f\u53d1","text":"<p>\u5728\u4f5c\u8005\u63d0\u51fa CLIP \u6a21\u578b\u7684\u540c\u65f6\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u56e0\u4e3a Transformer \u67b6\u6784\u7684\u63d0\u51fa\u53d1\u751f\u4e86\u7ffb\u5929\u8986\u5730\u7684\u53d8\u5316</p> <p></p> <p>Encoder\u7684\u90e8\u5206\u6210\u5c31\u4e86 Bert\uff1bdecoder \u90e8\u5206\u6210\u5c31\u4e86 GPT</p> <p></p> <p>\u9996\u5148\u770b Bert \u6a21\u578b\uff0c\u9884\u6d4b\u4efb\u52a1\u6709\u4e24\u4e2a\uff0c\u4f46\u90fd\u662f\u81ea\u76d1\u7763\u7684\uff0c\u4e0d\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u6570\u636e</p> <p>\uff081\uff09\u7b2c 1 \u4e2a\u4efb\u52a1\u662f\u8bad\u7ec3\u5e26\u63a9\u7801\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5c31\u662f\u628a\u4e00\u53e5\u8bdd\u91cc\u9762\u7684\u90e8\u5206\u8bcd\u906e\u4f4f\uff0c\u8ba9\u6a21\u578b\u6839\u636e\u4e0a\u4e0b\u6587\u9884\u6d4b\u88ab\u906e\u4f4f\u7684\u8bcd</p> <p>\uff082\uff09\u7b2c 2 \u4e2a\u4efb\u52a1\u662f\u7ed9\u51fa\u4e24\u53e5\u8bdd\uff0c\u8ba9\u6a21\u578b\u5224\u65ad\u4e24\u53e5\u8bdd\u662f\u5426\u5728\u539f\u6587\u4e2d\u662f\u8fde\u7eed\u7684</p> <p>\u8fd9\u4e24\u4e2a\u4ee3\u7406\u4efb\u52a1\uff0c\u4e00\u4e2a\u8bad\u7ec3\u4e86\u6a21\u578b\u5bf9\u8bcd\u8bed\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e00\u4e2a\u8bad\u7ec3\u4e86\u6a21\u578b\u5bf9\u6574\u4e2a\u53e5\u5b50\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e8e\u662f\u5f97\u5230\u4e86\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u9884\u8bad\u7ec3\u6a21\u578b</p> <p>\uff08Bert \u6a21\u578b\uff09</p> <p>Bert \u89e3\u51b3\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u6807\u6ce8\u95ee\u9898\uff0c\u4f46\u662f\u6ca1\u6709\u89e3\u51b3\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4f9d\u7136\u9700\u8981\u5fae\u8c03\u7684\u95ee\u9898\uff0c\u4e0d\u8bba\u662f\u5355\u8bcd\u7ea7\u522b\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u8fd8\u662f\u53e5\u5b50\u7ea7\u522b\u7684\u60c5\u7eea\u8bc6\u522b\uff0c\u90fd\u8fd8\u662f\u9700\u8981\u52a0\u4e00\u5c42\u7ebf\u6027\u5206\u7c7b\u5934\u8fdb\u884c\u5fae\u8c03</p> <p>\uff08GPT \u6a21\u578b\uff09</p> <p>\u4ee5 GPT3 \u4e3a\u4f8b\uff0c\u5b83\u7684\u8bad\u7ec3\u6570\u636e\u4e5f\u4e0d\u9700\u8981\u4eba\u5de5\u6807\u6ce8\uff0c\u4efb\u52a1\u5c31\u662f\u6587\u5b57\u63a5\u9f99\uff0c\u6839\u636e\u524d\u6587\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5b57</p> <p>\u540c\u65f6\u4e5f\u89e3\u51b3\u4e86\u6a21\u578b\u8fc1\u79fb\u7684\u95ee\u9898\uff0cGPT3 \u901a\u8fc7 prompt \u53ef\u4ee5\u5b8c\u6210\u5404\u79cd\u4efb\u52a1\u5305\u62ec\u7ffb\u8bd1\u3001\u60c5\u7eea\u8bc6\u522b\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7b49</p> <p>\u53ea\u8981\u628a\u4efb\u52a1\u901a\u8fc7 prompt \u544a\u8bc9 GPT3\uff0c\u5b83\u5c31\u80fd\u5b8c\u6210\u5bf9\u5e94\u7684\u4efb\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60</p> <p>GPT3 \u662f OpenAI \u7684\u5de5\u4f5c\uff0cCLIP \u4e5f\u662f OpenAI \u63d0\u51fa\u7684\uff0c\u6240\u4ee5\u81ea\u7136\u5c31\u60f3\u5230\u662f\u5426\u80fd\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u6570\u636e\u9700\u8981\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528 Prompt \u6765\u5b9e\u73b0\u4e0b\u6e38\u4efb\u52a1\u9700\u8981\u8fc1\u79fb\u5b66\u4e60\u7684\u95ee\u9898</p>"},{"location":"learning/LittleLesson/1_clip/#_3","title":"\u5173\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60","text":"<p>\u81ea\u76d1\u7763\u5b66\u4e60 &amp; \u6768\u7acb\u6606\u86cb\u7cd5\u56fe</p> <p>\u89e3\u8bfb \u6768\u7acb\u6606\u86cb\u7cd5\u56fe</p> <p>\u5982\u679c\u628a\u6a21\u578b\u5b66\u5230\u7684\u4e1c\u897f\u6bd4\u4f5c 1 \u4e2a\u86cb\u7cd5\u7684\u8bdd\uff1a</p> <ul> <li>\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5b66\u5230\u7684\u4e1c\u897f\u5c31\u662f\u86cb\u7cd5\u4e0a\u7684\u6a31\u6843\uff0c\u975e\u5e38\u5c11</li> <li>\u76d1\u7763\u5b66\u4e60\u80fd\u591f\u5b66\u5230\u7684\u4e1c\u897f\u5c31\u662f\u86cb\u7cd5\u5916\u5c42\u7684\u7cd6\u971c</li> <li>\u800c\u81ea\u76d1\u7763\u5b66\u4e60\u80fd\u591f\u5b66\u5230\u7684\u4e1c\u897f\u624d\u662f\u8fd9\u4e2a\u86cb\u7cd5\u7684\u672c\u8d28</li> </ul> <p>\u600e\u4e48\u7406\u89e3 \u6768\u7acb\u6606\u86cb\u7cd5\u56fe\uff1f</p> <ul> <li>\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u578b\u76d1\u7763\u4fe1\u53f7\u53ea\u6709\u5355\u4e00\u7684\u5956\u52b1\u5236\uff0c\u4fe1\u606f\u975e\u5e38\u5c11</li> <li>\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u76d1\u7763\u4fe1\u53f7\u662f\u4eba\u9020\u7684\u7c7b\u522b\u4fe1\u606f</li> <li>\u800c\u81ea\u76d1\u7763\u5b66\u4e60\u76d1\u7763\u4fe1\u53f7\u5c31\u662f\u771f\u5b9e\u7684\u8f93\u5165</li> </ul> <p>\u6bd4\u5982 GPT3 \u6a21\u578b\u7528\u7684\u5927\u91cf\u771f\u5b9e\u6587\u672c\u8bad\u7ec3\uff0c\u9884\u6d4b\u7684\u5c31\u662f\u771f\u5b9e\u7684\u6587\u672c\uff0c\u4e5f\u5c31\u4f7f\u5f97 GPT3 \u53d6\u5f97\u4e86\u5f88\u597d\u7684\u6548\u679c</p>"},{"location":"learning/LittleLesson/1_clip/#_4","title":"\u6570\u636e","text":"<p>OpenAI \u4f5c\u4e3a\u5927\u516c\u53f8\uff0c\u5728\u8bad\u7ec3 CLIP \u65f6\uff0c\u6536\u96c6\u4e864 \u4ebf\u4e2a\u6587\u672c\u56fe\u7247\u5bf9\uff0c\u5176\u4e2d\u5149\u6587\u672c\u7684\u91cf\u5c31\u548c\u8bad\u7ec3 GPT2 \u7684\u6587\u672c\u76f8\u5f53</p> <p>\u5982\u56fe\u662f\u6587\u672c\u5bf9\u7684\u4f8b\u5b50</p> <p>\u5982\u679c\u4f20\u7edf\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u7b2c\u4e00\u5f20\u56fe\u7247\u53ef\u80fd\u88ab\u6253\u4e0a\u8349\u5730\u7684\u6807\u7b7e\uff0c\u76d1\u7763\u4fe1\u53f7\u5c31\u53ea\u6709\"\u8349\u5730\"\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u56fe\u7247\u5c31\u88ab\u6253\u4e0a\"\u623f\u95f4\"\u7684\u6807\u7b7e</p> <p>\u63a5\u4e0b\u6765\uff0c\u770b\u5b9e\u9645\u7684\u6587\u672c\u662f\u5982\u4f55\u63cf\u8ff0\u7b2c\u4e00\u5f20\u56fe\u7684\"\u6e7f\u6f09\u6f09\u7684\u94f6\u674f\u6d12\u6ee1\u8349\u576a......\"\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7684\u6587\u5b57\"\u5c0f\u732b\uff0c\u8737\u7f29\u5728\u5e8a\u4e0a......\"</p> <p>\u53ef\u4ee5\u660e\u663e\u770b\u5230\"\u8349\u5730\"\u3001\"\u623f\u95f4\"\u8fd9\u6837\u7684\u6807\u7b7e\u76f8\u6bd4\u5b9e\u9645\u6536\u96c6\u5230\u7684\u6587\u672c\u542b\u6709\u66f4\u591a\u7684\u4fe1\u606f\u91cf\uff0c\u5bf9\u56fe\u7247\u7684\u63cf\u8ff0\u4e5f\u66f4\u52a0\u7ec6\u8282\uff0c\u4e5f\u5c31\u662f\u8bf4\u6709\u66f4\u591a\u7684\u76d1\u7763\u4fe1\u53f7</p> <p>\u5982\u679c\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u76d1\u7763\u4fe1\u53f7\uff0c\u4e00\u5b9a\u53ef\u4ee5\u8bad\u7ec3\u51fa\u66f4\u597d\u7684\u6a21\u578b</p>"},{"location":"learning/LittleLesson/1_clip/#_5","title":"\u8bba\u6587\u9898\u76ee","text":"<p>\u770b CLIP \u539f\u8bba\u6587\u7684\u9898\u76ee\uff1a\u5229\u7528\u81ea\u7136\u8bed\u8a00\u7684\u76d1\u7763\u4fe1\u53f7\u5b66\u4e60\u4e00\u4e2a\u53ef\u8fc1\u79fb\u7684\u89c6\u89c9\u6a21\u578b</p> <p>\u8fd9\u91cc\u7684\u53ef\u8fc1\u79fb\u6307\u7684\u662f\u5728\u4e0b\u6e38\u4efb\u52a1\u65f6\u4e0d\u9700\u8981\u518d\u8bad\u7ec3\u6a21\u578b\uff0c\u54ea\u6015\u662f\u5206\u7c7b\u5934\u4e5f\u4e0d\u518d\u9700\u8981\u3002</p> <p>\u601d\u8003\uff1a\u90a3\u5982\u4f55\u5229\u7528\u81ea\u7136\u8bed\u8a00\u7684\u76d1\u7763\u4fe1\u53f7\u5462\uff1f</p>"},{"location":"learning/LittleLesson/1_clip/#_6","title":"\u8bad\u7ec3\u4efb\u52a1","text":"<p>\u521a\u5f00\u59cbOpenAI \u60f3\u5230\u7684\u662f\u901a\u8fc7\u56fe\u7247\u6765\u9884\u6d4b\u6587\u672c\uff0c\u53ef\u662f\u8bad\u7ec3\u65f6\u53d1\u73b0\u8bad\u7ec3\u901f\u5ea6\u592a\u6162\u4e86\uff0c\u4e3a\u4ec0\u4e48\u5462\uff1f</p> <p>\u56e0\u4e3a\u6839\u636e\u4e00\u5f20\u56fe\u7247\u751f\u6210\u6587\u672c\u672c\u6765\u5c31\u662f\u4e00\u4e2a\u5f88\u96be\u7684\u4efb\u52a1\uff0c\u5bf9\u4e8e\u540c\u4e00\u5f20\u56fe\u7247\u53ef\u4ee5\u6709\u65e0\u6570\u79cd\u6b63\u786e\u7684\u7b54\u6848</p> <p>\u6bd4\u5982\u5de6\u56fe\u63cf\u8ff0\u53ef\u4ee5\u662f</p> <p>\uff081\uff09\u6e7f\u6f09\u6f09\u7684\u94f6\u674f\u6d12\u6ee1\u8349\u576a\uff0c\u5ba3\u544a\u7740\u79cb\u5929\u7684\u7ed3\u675f</p> <p>\uff082\uff09\u8def\u8fb9\u7684\u7eff\u8272\u8349\u5730\u4e0a\u6709\u5f88\u591a\u9ec4\u8272\u53f6\u5b50</p> <p>\uff083\uff09\u6df1\u79cb\u7684\u8def\u8fb9\u6ee1\u662f\u843d\u53f6</p> <p>\u6240\u4ee5\u4f5c\u8005\u5c31\u6539\u4e3a\u4e86\u53f3\u56fe\u4e2d\u7684 <code>\u56fe\u7247\u548c\u6587\u672c\u7684\u914d\u5bf9\u4efb\u52a1</code></p> <p>\u53ef\u4ee5\u660e\u663e\u770b\u5230\u56fe\u6587\u914d\u5bf9\u7684\u4efb\u52a1\u76f8\u5bf9\u7b80\u5355\uff0c\u6a21\u578b\u4e5f\u66f4\u5bb9\u6613\u8bad\u7ec3\uff0cCLIP \u505a\u56fe\u7247\u548c\u6587\u672c\u914d\u5bf9\u4e5f\u975e\u5e38\u7b80\u5355\u76f4\u63a5</p>"},{"location":"learning/LittleLesson/1_clip/#_7","title":"\u6a21\u578b\u8bad\u7ec3","text":"<p>\u6587\u672c\u901a\u8fc7 <code>Text Encoder</code> </p> <p>\u56fe\u7247\u901a\u8fc7 <code>Image Encoder</code></p> <p>\u7136\u540e\u5206\u522b\u5f97\u5230\u6587\u672c\u7684\u5411\u91cf\u8868\u793a\u548c\u56fe\u7247\u7684\u5411\u91cf\u8868\u793a\uff0c\u518d\u5206\u522b\u901a\u8fc7\u4e00\u4e2a\u7ebf\u6027\u6295\u5c04\u5c42\uff0c\u6295\u5c04\u5230\u4e00\u4e2a\u5171\u540c\u7684\u591a\u6a21\u6001\u5411\u91cf\u7a7a\u95f4\u4e2d\uff0c\u5728\u8fd9\u4e2a\u5411\u91cf\u7a7a\u95f4\u4e2d\uff0c\u5c3d\u91cf\u62c9\u8fd1\u914d\u5bf9\u6587\u672c\u548c\u56fe\u7247\u7684\u5411\u91cf\uff0c\u800c\u8ba9\u4e0d\u914d\u5bf9\u7684\u6587\u672c\u548c\u56fe\u7247\u5411\u91cf\u8ddd\u79bb\u5c3d\u53ef\u80fd\u7684\u8fdc</p> <p>\u8fd9\u91cc\u7684\u6587\u672c Encoder \u548c\u56fe\u7247 Encoder \u7528\u4ec0\u4e48\u6a21\u578b\u90fd\u53ef\u4ee5\uff0c\u53ea\u8981\u80fd\u628a\u56fe\u7247\u548c\u6587\u672c\u53d8\u6210\u5411\u91cf\u5373\u53ef</p> <p></p> <p>\u4f5c\u8005\u5b9e\u9a8c\u65f6\uff0c\u5bf9\u4e8e\u56fe\u50cf\u7f16\u7801\u5668\u5c1d\u8bd5\u4e86 ResNet50\u3001ResNet101\u3001\u4ee5\u53ca\u5bf9 ResNet50 \u7684\u6df1\u5ea6\u5bbd\u5ea6\u8fdb\u884c\u6269\u5c55\u7684 ResNet50\u00d74\u3001ResNet50\u00d716\u3001ResNet50\u00d764</p> <p>\u00d74 \u7684\u610f\u601d\u662f ResNet50\u00d74 \u7684\u8ba1\u7b97\u4ee3\u4ef7\u662f ResNet50 \u7684 4 \u500d</p> <p>\u540c\u65f6\u4f5c\u8005\u4e5f\u5c1d\u8bd5\u4e86 ViT\u6a21\u578b\uff0cViT-B/32\u3001ViT-B/16\u3001ViT-L/14</p> <p>B\uff1aBase\u3001L\uff1aLarge\u3001<code>32 16 14</code>\uff1a\u56fe\u7247 patch \u7684\u5927\u5c0f</p> <p>patch \u8d8a\u5c0f\uff0c\u56fe\u7247\u5e8f\u5217\u8d8a\u957f\uff0c\u8ba1\u7b97\u4ee3\u4ef7\u8d8a\u5927\uff0c\u5176\u4e2d ResNet \u91cc\u6700\u5927\u7684\u6a21\u578b\u662f ResNet50\u00d764\uff0c\u8bad\u7ec3\u4e86 1(w)0656 \u4e2a V100 \u5929\uff0c\u800c ViT-L/14 \u53ea\u7528\u4e86 3072 \u4e2a V100 \u5929</p> <p>\u800c\u4e14  ViT-L/14  \u662f\u6548\u679c\u6700\u597d\u7684\uff0c\u8fd9\u4e5f\u8bc1\u660e\u4e86\u5728\u8bad\u7ec3\u4ebf\u7ea7\u522b\u7684\u56fe\u7247\u65f6\uff0cViT \u6a21\u578b\u66f4\u8282\u7701\u7b97\u529b\uff0c\u6548\u679c\u4e5f\u66f4\u597d</p> <p>\u6700\u540e\u4f5c\u8005\u8fd8\u5728  ViT-L/14   \u8fd9\u4e2a\u6a21\u578b\u4e0a\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b \\(336*336\\)\u7684\u56fe\u7247\u4e0a\u989d\u5916\u8bad\u7ec3\u4e86\u4e00\u4e2a epoch\uff0c\u6765\u63d0\u5347\u6027\u80fd</p> <p>\u53e6\u5916\u4f5c\u8005\u53d1\u73b0\uff0c\u5982\u679c\u60f3\u901a\u8fc7\u6269\u5927\u6a21\u578b\u6765\u63d0\u5347\u7cbe\u5ea6\u6700\u597d\u662f\u5728\u6a21\u578b\u7684\u6df1\u5ea6\u3001\u5bbd\u5ea6\u3001\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u4e0a\u7b49\u540c\u65f6\u589e\u52a0\uff0c\u8fd9\u6837\u6536\u76ca\u6700\u5927\uff0c\u800c\u4e0d\u662f\u628a\u6240\u6709\u7684\u8ba1\u7b97\u91cf\u90fd\u589e\u52a0\u5728\u5176\u4e2d\u4e00\u9879\u4e0a</p> <p>\u5bf9\u4e8e\u6587\u672c\u7f16\u7801\u5668\uff0c\u4f5c\u8005\u91c7\u7528\u4e86\u7c7b\u4f3c GPT \u7684\u7ed3\u6784\uff0c\u6bcf\u4e2a token \u90fd\u53ea\u80fd\u770b\u5230\u81ea\u5df1\u524d\u9762\u7684 token</p> <p>\u8f93\u5165\u6587\u672c\u524d\u8fb9\u4f1a\u52a0\u4e00\u4e2a SOS token\uff0c\u8868\u793a start of sentence\uff0c\u6587\u672c\u540e\u8fb9\u4f1a\u589e\u52a0\u4e00\u4e2a EOS token\uff0c\u8868\u793a end of sentence</p> <p>\u7136\u540e\u62ff\u6700\u540e\u4e00\u5c42 EOS \u4f4d\u7f6e token \u7684\u8f93\u51fa\u6765\u4f5c\u4e3a\u6574\u4e2a\u53e5\u5b50\u7684 embedding\uff0c\u56e0\u4e3a\u53ea\u6709EOS token \u53ef\u4ee5\u770b\u5230\u6574\u4e2a\u5e8f\u5217\u7684\u5176\u4ed6 token \u6765\u603b\u7ed3\u6574\u4e2a\u53e5\u5b50\u7684\u610f\u601d</p> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136\u8fd9\u91cc\u6a21\u578b\u7ed3\u6784\u7c7b\u4f3c\u4e8e GPT\uff0c\u4f46\u662f\u8fd9\u91cc\u662f\u4f5c\u4e3a\u7f16\u7801\u5668\u4f7f\u7528\u7684\uff0c\u53ea\u63d0\u53d6\u6700\u540e\u4e00\u4e2a EOS token \u7684\u8f93\u51fa\u4f5c\u4e3a\u6574\u4e2a\u53e5\u5b50\u7684\u7f16\u7801\uff0c\u6587\u672c\u7f16\u7801\u5668\u53ea\u6709 6300 \u4e07\u7684\u53c2\u6570</p> <p></p> <p>\u4f5c\u8005\u53d1\u73b0\u6587\u672c\u7f16\u7801\u5668\u7684\u5927\u5c0f\u5bf9\u4e8e CLIP \u6a21\u578b\u7684\u6027\u80fd\u5f71\u54cd\u4e0d\u5927\uff0c\u6240\u4ee5\u5e76\u6ca1\u6709\u5c1d\u8bd5\u4e0d\u540c\u5927\u5c0f\u7684\u6587\u672c\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u7684 batchsize \u4e3a3(w)2(k)768\uff0c\u4e00\u4e2a\u975e\u5e38\u5927\u7684 batchsize\uff0c\u800c\u4e14\u4ece\u5de6\u8fb9\u56fe\u4e0a\u4e5f\u53ef\u4ee5\u770b\u5230 \u968f\u7740\u89c6\u89c9\u7f16\u7801\u5668\u6a21\u578b\u8ba1\u7b97\u91cf\u7684\u589e\u5927\uff0cCLIP \u6a21\u578b\u7684\u9519\u8bef\u7387\u662f\u7a33\u6b65\u51cf\u5c0f\u7684</p> <p>\u800c\u4e14\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u6709 4 \u4ebf\u4e2a\u56fe\u6587\u5bf9\uff0c\u6570\u636e\u91cf\u8db3\u591f\u5927\uff0c\u6240\u4ee5\u56fe\u7247\u7f16\u7801\u5668\u548c\u6587\u672c\u7f16\u7801\u5668\u90fd\u6ca1\u6709\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u76f4\u63a5\u4ece\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u8bad\u7ec3\uff0c\u56fe\u7247\u4e5f\u6ca1\u6709\u505a\u8fc7\u591a\u7684\u56fe\u50cf\u589e\u5f3a\uff0c\u552f\u4e00\u505a\u7684\u5c31\u662f\u968f\u673a\u88c1\u526a\uff0c\u6574\u4e2a CLIP \u5b9e\u73b0\u975e\u5e38\u7b80\u5355</p>"},{"location":"learning/LittleLesson/1_clip/#_8","title":"\u6a21\u578b\u8bad\u7ec3\u4f2a\u4ee3\u7801","text":"<p>i \u4ee3\u8868 batch \u7684 n \u4e2a\u56fe\u7247\u6570\u636e</p> <p>t \u4ee3\u8868 1 \u4e2a batch \u7684 n \u4e2a\u6587\u672c\u6570\u636e</p> <p></p> <ul> <li>\u56fe\u7247\u6570\u636e\u7ecf\u8fc7\u56fe\u7247\u7f16\u7801\u5668\uff0c\u6587\u672c\u6570\u636e\u7ecf\u8fc7\u6587\u672c\u7f16\u7801\u5668\uff0c\u5206\u522b\u5f97\u5230\u539f\u59cb\u7684\u56fe\u50cf\u5411\u91cf\u8868\u793a\u548c\u6587\u672c\u5411\u91cf\u8868\u793a </li> </ul> <p></p> <ul> <li>\u63a5\u7740\u901a\u8fc7\u5404\u81ea\u7684\u7ebf\u6027\u6620\u5c04\u5c42\u6620\u5c04\u5230\u5171\u540c\u7684\u591a\u6a21\u6001\u5411\u91cf\u7a7a\u95f4 </li> </ul> <p>\u7ebf\u6027\u6620\u5c04\u5c42\u662f\u901a\u8fc7\u4e24\u4e2a\u70b9\u79ef\u5b9e\u73b0\u7684</p> <ul> <li>\u7136\u540e\u8fdb\u884c L2 \u5f52\u4e00\u5316\uff0c\u65b9\u4fbf\u540e\u9762\u76f4\u63a5\u901a\u8fc7\u70b9\u79ef\uff0c\u6765\u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6</li> <li>\u4e0b\u9762 \u901a\u8fc7\u4e24\u4e2a\u70b9\u79ef\u8ba1\u7b97\uff0c\u5f97\u5230\u4efb\u610f\u4e24\u4e2a\u56fe\u7247\u548c\u6587\u672c\u7684\u76f8\u4f3c\u5ea6</li> </ul> <p></p> <ul> <li>\u540e\u9762 \\(*np.\\exp(t)\\) \u7684\u90e8\u5206\uff0c\u662f\u6e29\u5ea6\u7cfb\u6570\uff0c\u4e00\u822c\u8bad\u7ec3\u5bf9\u6bd4\u6a21\u578b\u65f6\uff0c\u90fd\u4f1a\u6709\u4e00\u4e2a \\(&gt;0\\) \u7684\uff0c\u53ef\u4ee5\u8c03\u8282\u7684\u8d85\u53c2\u6570\uff0c\u6e29\u5ea6\u7cfb\u6570 t \uff0c\u4f46\u662f CLIP \u4e2d\u76f4\u63a5\u628a\u6e29\u5ea6\u7cfb\u6570\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u4ee5\u5b66\u4e60\u7684\u53c2\u6570</li> </ul> <p>\u4e3a\u4e86\u9632\u6b62\u6e29\u5ea6\u7cfb\u6570\u4e3a\u8d1f\u6570\uff0c\u6240\u4ee5\u5bf9 t \u8fdb\u884c \u6307\u6570\u8fd0\u7b97</p> <p>\u6e29\u5ea6\u7cfb\u6570 \u4f5c\u4e3a \u53ef\u8c03\u8282\u7684\u8d85\u53c2\u6570\u65f6\uff0c\u4e00\u822c\u90fd\u662f\u7528 logits \u7684\u503c\u9664\u4ee5\u6e29\u5ea6\u7cfb\u6570</p> <p>\u6e29\u5ea6\u7cfb\u6570\u7684\u4f5c\u7528\u5c31\u662f\u8c03\u8282\u5206\u5e03\u7684\u9661\u5ced\u7a0b\u5ea6</p> <p>\u6e29\u5ea6\u8d8a\u9ad8\uff0clogits \u901a\u8fc7 softmax \u540e\uff0c\u5dee\u522b\u4f1a\u88ab\u5e73\u6ed1</p> <p>\u6e29\u5ea6\u8d8a\u4f4e\uff0clogits \u901a\u8fc7 softmax \u540e\uff0c\u5dee\u522b\u4f1a\u66f4\u52a0\u9661\u5ced</p> <p>\u4f46\u662f\u56e0\u4e3a\u8fd9\u91cc t \u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u7528\u4e58\u6cd5\u548c\u9664\u6cd5\u90fd\u662f\u4e00\u6837\u7684\uff0c\u8ba9\u6a21\u578b\u81ea\u5df1\u53bb\u5b66\u4e60</p> <p></p> <ul> <li>\u4e0b\u4e00\u6b65\uff0c\u751f\u6210 label\uff0c\u53ef\u4ee5\u770b\u53f3\u8fb9\u7684\u56fe\uff0c\u548c batch \u7684\u7b2c\u4e00\u4e2a\u56fe\u7247\u5bf9\u5e94\u7684\u662f batch \u91cc\u7684\u7b2c\u4e00\u4e2a\u6587\u672c</li> <li>\u7b2c 2 \u4e2a\u56fe\u7247\u5bf9\u5e94\u7684\u662f\u7b2c 2 \u4e2a\u6587\u672c\uff0c\u4e5f\u5c31\u662f label \u90fd\u662f\u5728\u5bf9\u89d2\u7ebf\u4e0a\u7684\u5143\u7d20\uff0c\u6240\u4ee5\u8fd9\u91cc\u7528\u4e86 <code>np.arrang</code>\uff0c\u6765\u751f\u6210 label</li> <li>\u7136\u540e\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u56fe\u7247\u4e0e\u6240\u6709\u6587\u672c\u4e4b\u95f4\u76f8\u4f3c\u5ea6\u7684\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u662f\u5728\u884c\u7ef4\u5ea6</li> <li>\u63a5\u7740\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u6587\u672c\u4e0e\u6240\u6709\u56fe\u7247\u4e4b\u95f4\u76f8\u4f3c\u5ea6\u7684\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u662f\u5728\u5217\u7ef4\u5ea6\u4e0a</li> <li>\u6700\u540e\uff0c\u603b\u7684\u635f\u5931\u7b49\u4e8e\u4e24\u4e2a\u635f\u5931\u7684\u5e73\u5747\u503c</li> </ul>"},{"location":"learning/LittleLesson/1_clip/#_9","title":"\u6a21\u578b\u63a8\u7406","text":"<p>\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5982\u4f55\u8fdb\u884c\u63a8\u7406\uff1f</p> <p>\u4ee5 ImageNet \u91cc\u7684 1k \u4e2a\u7c7b\u522b\u4e3a\u4f8b\uff0c\u53ef\u4ee5\u5148\u751f\u6210 1k \u53e5\u8bdd\uff0c\u5206\u522b\u662f\uff1a</p> <ul> <li>\uff08\u770b\u56fe\uff09A photo of a plane</li> <li>A photo of a car</li> <li>A photo of a dog</li> <li>.......</li> <li>A photo of a bird</li> </ul> <p>\u5982\u6b64\uff0c\u5f97\u5230 1k \u53e5\u8bdd\uff0c\u5e76\u5bf9\u8fd9 1k \u53e5\u8bdd\uff0c\u901a\u8fc7\u6587\u672c Encoder \u8fdb\u884c\u7f16\u7801\uff0c\u7136\u540e\u8fdb\u884c\u7ebf\u6027\u6620\u5c04\u5c42\uff0c\u5f97\u5230 1k \u4e2a\u4ee3\u8868\u4e0d\u540c\u7c7b\u522b\u7684\u6587\u672c\u5411\u91cf</p> <p>\u51c6\u5907\u597d\u8fd9 1k \u4e2a\u5411\u91cf\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u5bf9\u56fe\u7247\u8fdb\u884c\u9884\u6d4b</p> <p>\u5047\u8bbe\u7ed9\u5b9a\u4e00\u5f20\u56fe\u7247\uff0c\u901a\u8fc7\u56fe\u50cf\u7f16\u7801\u5668 \u548c \u7ebf\u6027\u6620\u5c04\u5c42\u540e\uff0c\u5f97\u5230\u56fe\u7247\u7684\u5411\u91cf\u8868\u793a\uff0c\u548c\u4e4b\u524d\u5f97\u5230\u7684 1k \u4e2a\u7c7b\u522b\u7684\u6587\u672c\u5411\u91cf\u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u54ea\u4e2a\u76f8\u4f3c\u5ea6\u6700\u5927\uff0c\u5c31\u5bf9\u5e94\u54ea\u4e2a\u7c7b\u522b</p>"},{"location":"learning/LittleLesson/1_clip/#prompt-enginerring-ensembling","title":"Prompt Enginerring &amp; Ensembling","text":"<p>\u4e3a\u4ec0\u4e48\u8981\u628a\u7c7b\u522b\u540d\u5d4c\u5165\u5230\u53e5\u5b50\u4e2d\u518d\u505a\u6587\u672c \u7f16\u7801\u5462\uff1f</p> <p>\u8bba\u6587\u7684\u4f5c\u8005\u4e5f\u505a\u4e86\u5b9e\u9a8c\uff0c\u548c\u76f4\u63a5\u7528\u7c7b\u522b\u540d\u505a\u7f16\u7801\u5411\u91cf\u76f8\u6bd4\uff0c\u628a\u7c7b\u522b\u540d\u5d4c\u5165\u5230\u4e00\u4e2a\u7c7b\u4f3c\u4e8e\"\u8fd9\u662f\u4ec0\u4e48\u4ec0\u4e48\u4ec0\u4e48\u7684\u7167\u7247\"\u8fd9\u6837\u7684\u7b80\u5355\u53e5\u5b50\u4e2d\uff0c\u5728 ImageNet \u4e0a\u7684\u7cbe\u5ea6\u53ef\u4ee5\u63d0\u5347 1.3%</p> <p>\u5982\u679c\u662f\u9488\u5bf9\u7279\u5b9a\u7684\u6570\u636e\u96c6\u6bd4\u5982\u5bf9\u4e8e\u5ba0\u7269\u5206\u7c7b\u7684\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u5728\u7f16\u7801\u65f6\u5728\u53e5\u5b50\u4e2d\u660e\u786e\u7684\u8bf4\u660e\"\u8fd9\u662f\u4e00\u4e2a\u4ec0\u4e48\u4ec0\u4e48\u4ec0\u4e48\u5ba0\u7269\u7684\u7167\u7247\"\uff0c\u628a\"\u5ba0\u7269\"\u8fd9\u4e2a\u4fe1\u606f</p>"},{"location":"learning/LittleLesson/3_ViT/","title":"ViT","text":"<p>\u4e00\u53e5\u8bdd\uff1a\u7eaf Transformer \u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898</p> <p>\u5e76\u4e14\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6570\u636e\u96c6\u4e0b\u4e0e CNN \u8fdb\u884c\u5bf9\u6bd4\uff0c\u63a2\u8ba8 Transformer \u7684\u4f18\u52bf</p>"},{"location":"learning/LittleLesson/3_ViT/#vit_1","title":"ViT \u7684\u7ed3\u8bba","text":"<ul> <li>ViT \u8bc1\u660e\u4e86 Transformer \u6a21\u578b\u7684\u901a\u7528\u6027</li> </ul>"},{"location":"learning/LittleLesson/3_ViT/#transformer","title":"Transformer \u67b6\u6784\u7684\u8bc4\u4ef7\uff1a","text":"<p>\u8bad\u7ec3\u6548\u7387\u9ad8\uff0c\u53ef\u4ee5\u901a\u8fc7\u6ce8\u610f\u529b\u63d0\u53d6\u590d\u6742\u8bed\u4e49\uff0c\u53ef\u4ee5\u652f\u6301\u591a\u79cd\u6a21\u6001\uff0c\u5e76\u4e14\u7ed3\u6784\u7b80\u5355\uff0c\u53ef\u4ee5\u81ea\u7531\u6269\u5c55\u6a21\u578b\u5927\u5c0f\uff0c\u5373\u4f7f\u4f7f\u7528\u5343\u4ebf\u7ea7\u522b\u7684\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4f9d\u7136\u6ca1\u6709\u51fa\u73b0\u6027\u80fd\u9971\u548c\uff0cViT \u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0cTransformer \u67b6\u6784\u7684\u7edf\u4e00\uff0c\u4e5f\u8ba9\u57fa\u4e8e Transformer \u67b6\u6784\u7684\u5de5\u7a0b\u4f18\u5316\uff0c\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u591a\u6a21\u6001\u9886\u57df</p> <p>\u60f3\u6cd5\u662f\uff1a</p> <p>\u5bf9 Transformer \u6a21\u578b\u4e0d\u505a\u4efb\u4f55\u7684\u4fee\u6539 \u6765\u5b8c\u6210\u5bf9\u56fe\u50cf\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5982\u679c\u6a21\u578b\u4e0d\u80fd\u6539\u53d8\uff0c\u90a3\u5c31\u6539\u53d8\u56fe\u7247\u6570\u636e\uff0c\u8ba9\u56fe\u7247\u6570\u636e\u53d8\u5f97\u50cf\u6587\u672c\uff0c\u6240\u4ee5 ViT \u8bba\u6587\u7684\u9898\u76ee\uff1a\u4e00\u5f20\u56fe\u50cf=16\u00d716 \u7684\u8bcd</p>"},{"location":"learning/LittleLesson/3_ViT/#vit_2","title":"ViT \u662f\u5982\u4f55\u5c06\u56fe\u7247\u8f6c\u6362\u6210\u6587\u672c\u7684\u5462\uff1f","text":"<p>\u5c06\u56fe\u7247\u5212\u5206\u6210\u56fa\u5b9a\u5927\u5c0f\u7684 patch\uff0c\u8bba\u6587\u4e2d\u8f93\u5165\u56fe\u7247 224\u00d7224\uff0c\u5982\u679c patch \u5927\u5c0f\u4e3a 14\u00d714\uff0c\u5219\u53ef\u4ee5\u5206\u4e3a 16\u00d716 \u7684\u5e8f\u5217\uff0c\u4e0d\u6309\u7167\u5355\u4e2a\u50cf\u7d20\u5212\u5206\u7684\u539f\u56e0\u662f\u4f1a\u5bfc\u81f4\u5e8f\u5217\u957f\u5ea6\u8fc7\u957f\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u592a\u9ad8</p> <p>\uff081\uff09\u4e00\u4e2a\u50cf\u7d20\u53ea\u6709 RGB 3\u4e2a\u503c\uff0c\u8bed\u4e49\u4fe1\u606f\u592a\u5c11\uff0c\u7528\u4e00\u4e2a\u957f\u5ea6\u51e0\u767e\u4e0a\u5343\u7684\u5411\u91cf\u53ea\u505a\u4e00\u4e2a\u50cf\u7d20\u7684 embedding\uff0c\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u6240\u4ee5\u9009\u62e9\u4e00\u4e2a patch \u4f5c\u4e3a\u4e00\u4e2a\u8bed\u4e49\u5355\u5143\uff0c\u5bf9\u5e94\u6587\u672c\u4e2d\u7684\u4e00\u4e2a token</p> <p>\uff082\uff09\u76f8\u90bb\u50cf\u7d20\u8bed\u4e49\u76f8\u4f3c</p>"},{"location":"learning/LittleLesson/3_ViT/#patch-embedding","title":"\u56fe\u50cf patch \u5982\u4f55\u6587\u672c\u4e2d\u5bf9\u5e94\u7684 embedding \u5411\u91cf\u5462\uff1f","text":"<p>\u5c06<code>patch</code>\u7684<code>\u957f\u00d7\u5bbd\u00d7\u901a\u9053\u6570</code>\u7684\u591a\u7ef4\u77e9\u9635\u8868\u793a\u5c55\u5e73\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5171\u4eab\u7684\u7ebf\u6027\u5c42\u6295\u5c04\u5230 Transformer \u6a21\u578b\u91cc\u7684\u7279\u5f81\u7ef4\u5ea6\uff0c\u6bd4\u5982 1024\uff0c\u8fd9\u6837\u5c31\u5b8c\u6210\u4e86\u628a\u4e00\u4e2a\u56fe\u7247\u8f6c\u6362\u6210\u4e00\u4e2a\u5411\u91cf\u5e8f\u5217\u7684\u8f6c\u6362\uff0c\u56fe\u50cf\u5207\u7247\u76f8\u5f53\u4e8e\u6587\u672c\u91cc\u9762\u7684\u5206\u8bcd\uff0c\u7ebf\u6027\u6295\u5c04\u5c42\u76f8\u5f53\u4e8e embedding \u5c42\uff0c\u63a5\u4e0b\u6765\u9700\u8981\u8003\u8651\u4f4d\u7f6e\u7f16\u7801\u3002</p>"},{"location":"learning/LittleLesson/3_ViT/#vit_3","title":"ViT \u4e2d\u7684\u4f4d\u7f6e\u7f16\u7801","text":"<p>ViT \u4e2d\u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u52a0\u4e0a 1 \u4e2a\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u6bd4\u5982\u56fe\u4e2d\u6709 9 \u4e2a\u4e0d\u540c\u7684\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u56e0\u4e3a ViT \u505a\u7684\u4efb\u52a1\u662f\u8981\u5bf9\u56fe\u7247\u8fdb\u884c\u5206\u7c7b\uff0c\u53c2\u8003\u81ea\u7136\u8bed\u8a00\u5904\u7406\u91cc\u7684 Bert \u6a21\u578b\u7684\u505a\u6cd5\uff0c\u5728\u6700\u524d\u9762\u52a0\u4e0a\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u7528\u6765\u5206\u7c7b\u7684 token\uff0c\u5e76\u4e14\u6709\u81ea\u5df1\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u56e0\u4e3a\u540e\u7eed\u91c7\u7528\u7684Transformer \u7684 Encoder \u67b6\u6784\uff0c\u6bcf\u4e2a token \u65e0\u8bba\u662f\u5728\u5e8f\u5217\u91cc\u7684\u4ec0\u4e48\u4f4d\u7f6e\u90fd\u53ef\u4ee5\u770b\u5230\u6240\u6709\u7684\u5176\u4ed6 token\uff0c\u6240\u4ee5\u5373\u4f7f\u628a\u8fd9\u4e2a token \u56fa\u5b9a\u5230\u7b2c 1 \u4e2a\u4f4d\u7f6e\u4e0a\uff0c\u4e5f\u53ef\u4ee5\u6c47\u96c6\u6240\u6709\u56fe\u50cf patch \u7684\u4fe1\u606f\uff0c\u7f51\u7edc\u7ed3\u6784\u91c7\u7528\u7684\u4e5f\u662f Transformer \u4e2d\u7684 Encoder</p> <p></p>"},{"location":"learning/LittleLesson/3_ViT/#vit-transformer-encoder","title":"ViT \u91c7\u7528\u7684\u662f Transformer Encoder \u67b6\u6784","text":"<p>\u4f20\u7edf\u7684\u5927\u6a21\u578b\u91c7\u7528\u7684\u662f decoder\uff0c\u56e0\u4e3a\u5927\u6a21\u578b\u4e00\u822c\u662f\u505a\u751f\u6210\u4efb\u52a1\u7684</p> <p>\u800c\u56fe\u7247\u5206\u7c7b\u662f\u505a\u4fe1\u606f\u63d0\u53d6\u7684\uff0c\u6240\u4ee5\u7528\u7c7b\u4f3c Bert \u7684\u67b6\u6784\uff0c\u91c7\u7528 Encoder \u7684\u6a21\u5757\uff0c\u6700\u540e\u901a\u8fc7\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u5206\u7c7b token \u7684\u4fe1\u606f\uff0c\u52a0\u4e0a\u4e00\u4e2a\u7b80\u5355\u7684 MLP\u5934\u8fdb\u884c\u56fe\u7247\u7684\u5206\u7c7b</p> <p></p> <p>\u7c7b\u4f3c\u4e8e Bert\uff0cViT \u4e5f\u8bad\u7ec3\u4e86\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b\uff0c\u5206\u4e3a Base\u3001Large\u3001Huge\uff0c\u5206\u522b\u6709\u4e0d\u540c\u7684\u5c42\u6570\u3001hidden size\u3001MLP size\u548c\u6ce8\u610f\u529b\u5934</p> <p></p> <p>\u5bf9\u4e8e ViT \u6a21\u578b\u7684\u8868\u793a\uff0c\u4e00\u822c\u4f1a\u7528\u7c7b\u4f3c\u4e8e ViT-L /16\u6765\u8fdb\u884c\u8868\u793a\uff0c\u8868\u793a\u8fd9\u662f\u4e00\u4e2a ViT Large \u6a21\u578b\uff0c\u5176\u4e2d patch \u7684\u5927\u5c0f\u4e3a 16\u00d716</p> <p>patch\u8d8a\u5c0f\uff0c\u4e00\u5f20\u56fe\u7247\u5206\u51fa\u6765\u7684 patch \u5c31\u8d8a\u591a\uff0c\u8f93\u5165\u5230 Transformer \u4e2d\u7684\u5e8f\u5217\u5c31\u8d8a\u957f\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u5c31\u8d8a\u9ad8\uff0c\u6a21\u578b\u6548\u679c\u4e5f\u8d8a\u597d</p>"},{"location":"learning/LittleLesson/3_ViT/#_1","title":"\u5b9e\u9a8c\u6548\u679c","text":"<p>\u4f5c\u8005\u9009\u62e9\u4e86\u5f53\u65f6\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u597d\u7684\u5206\u7c7b\u6a21\u578b\uff1aResNet \u548c EfficientNet \u8fdb\u884c\u6bd4\u8f83\uff0c\u53ef\u4ee5\u770b\u5230\u5728 JFT \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0cViT-H/14 \u6a21\u578b\u51e0\u4e4e\u90fd\u53d6\u5f97\u4e86\u6700\u597d\u7684\u6210\u7ee9</p> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\u6700\u540e\u4e00\u884c\uff0c\u5728 <code>GPU V3 \u7684\u6838\u6570\u00d7\u8bad\u7ec3\u5929\u6570</code> \u8868\u793a\u7684\u8ba1\u7b97\u4ee3\u4ef7\u4e0a\uff0cViT \u6a21\u578b\u5177\u6709\u975e\u5e38\u5927\u7684\u4f18\u52bf\uff0c\u6240\u4ee5\u8bf4\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8bad\u7ec3 ViT \u6a21\u578b\u66f4\u6709\u4f18\u52bf</p>"},{"location":"learning/LittleLesson/3_ViT/#_2","title":"\u5b9e\u73b0\u7ec6\u8282","text":""},{"location":"learning/LittleLesson/3_ViT/#embedding","title":"\u5b9e\u73b0\u7ec6\u8282\uff1a\u56fe\u50cf\u8f6c\u6362\u6210 embedding \u7684\u4e24\u79cd\u65b9\u5f0f\uff08\u770b\u56fe\uff09","text":"<p>\u7ebf\u6027\uff1a</p> <p>input \\(224*224*3\\) </p> <p>patch \\(16 * 16 * 3 = 768\\)</p> <p><code>#num</code> \\(14*14=\\)196\u4e2a patch</p> <p>\\(196*768 \u2192 196 * 1024\\)</p> <p>\u5377\u79ef\uff1a</p> <p>input \\(224*224*3\\) </p> <p>kernel $16 * 16 * 3 $</p> <p><code>#kernel</code> = 1024</p> <p>output \\(1024*14*14\\)</p> <p>flatten feature map \\(1024*14*14 \u2192 1024 * 196\\) </p> <p></p> <ul> <li>\u5377\u79ef\u64cd\u4f5c\u76f8\u6bd4\u7ebf\u6027\u6620\u5c04\u7701\u53bb\u4e86 patch \u5207\u5206\u7684\u64cd\u4f5c</li> <li>\u5377\u79ef\u8f93\u51fa\uff1a<code>1024\u00d7\u8f93\u51fa\u7279\u5f81\u56fe\u5927\u5c0f</code>  \uff08<code>\u56fe \u5377 \u56fe\u2192 \u56fe \u2192 flatten</code>\uff09</li> </ul>"},{"location":"learning/LittleLesson/3_ViT/#_3","title":"\u5b9e\u73b0\u7ec6\u8282\uff1a\u4f4d\u7f6e\u7f16\u7801\u7684\u9009\u62e9","text":"<p>\u4f5c\u8005\u539f\u6587\u5173\u4e8e\u4f4d\u7f6e\u7f16\u7801\u6240\u505a\u7684\u5b9e\u9a8c\uff1a</p> <p>\uff081\uff09\u4e0d\u52a0\u4f4d\u7f6e\u7f16\u7801</p> <p>\uff082\uff091 \u7ef4\u4f4d\u7f6e\u7f16\u7801</p> <p>\uff083\uff092 \u7ef4\u4f4d\u7f6e\u7f16\u7801</p> <p>\uff084\uff09\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801</p> <p>\u6bd4\u5982 \u56fe\u4e2d\u5c06\u56fe\u7247\u5207\u5206\u4e3a 9 \u4e2a patch</p> <p>\u5bf9\u4e8e 1 \u7ef4\u4f4d\u7f6e\u7f16\u7801\u800c\u8a00\uff0c\u5c31\u662f\u751f\u6210 9 \u4e2a\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u6bcf\u4e2a\u4f4d\u7f6e\u7f16\u7801\u7684\u957f\u5ea6\u90fd\u548c\u7279\u5f81\u7ef4\u5ea6\u4e00\u6837\uff0c\u4e3a 1024</p> <p>\u5bf9\u4e8e 2 \u7ef4\u4f4d\u7f6e\u7f16\u7801\uff0c\u751f\u6210 3 \u4e2a\u8868\u793a\u884c\u7684\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c3 \u4e2a\u8868\u793a\u5217\u7684\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c \u957f\u5ea6\u90fd\u4e3a\u7279\u5f81\u957f\u5ea6\u7684\u4e00\u534a 512 \uff0c\u901a\u8fc7\u884c\u5217\u7279\u5f81\u7684\u62fc\u63a5\u6765\u6784\u6210 1 \u4e2a\u4e8c\u7ef4\u4f4d\u7f6e\u7f16\u7801\uff0c\u6bd4\u5982\u5bf9\u4e8e\u7b2c 2 \u884c\u7b2c 1 \u5217\u7684 patch\uff0c\u5b83\u7684\u4f4d\u7f6e\u7f16\u7801\u5c31\u662f\u53d6\u7b2c 2\u4e2a\u884c\u4f4d\u7f6e\u7f16\u7801\u62fc\u63a5\u4e0a\u7b2c 1 \u5217\u4f4d\u7f6e\u7f16\u7801\u6784\u6210\u7684</p> <p>\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4e0d\u52a0\u4f4d\u7f6e\u7f16\u7801\u6548\u679c\u6700\u5dee\uff0c\u800c\u5176\u4f59\u7684\u4f4d\u7f6e\u7f16\u7801\u7531\u8868\u53ef\u77e5\uff0c\u6548\u679c\u662f\u5dee\u4e0d\u591a\u7684</p> <p>\u601d\u8003\uff1a\u4e3a\u4ec0\u4e48\u4e0d\u52a0\u4f4d\u7f6e\u7f16\u7801\u6548\u679c\u4e5f\u8fd8\u53ef\u4ee5\uff080.61382\uff09\uff0c\u5e76\u6ca1\u6709\u5dee\u592a\u591a\uff1f</p> <p>\u8fd9\u662f\u56e0\u4e3a\u56fe\u7247\u88ab\u5207\u6210\u4e86 patch\uff0cpatch \u5185\u90e8\u662f\u542b\u6709\u4f4d\u7f6e\u4fe1\u606f\u7684\uff0c\u5c31\u50cf\u4e0a\u9762\u7684\u56fe\u7247\uff0c\u5b83\u7684 patch\u5373\u4f7f\u88ab\u6253\u4e71\u4e86\u4f4d\u7f6e\u4e5f\u53ef\u4ee5\u770b\u51fa\u6765\u662f1 \u4e2a\u5efa\u7b51\u7684\u56fe\u7247</p>"},{"location":"learning/LittleLesson/3_ViT/#_4","title":"\u4f4d\u7f6e\u7f16\u7801\u7684\u8fdb\u4e00\u6b65\u7814\u7a76","text":"<p>\u5728\u539f\u6587\u4e2d\uff0c\u4f5c\u8005\u5bf9\u4f4d\u7f6e\u7f16\u7801\u8fdb\u884c\u4e86\u8fdb\u4e00\u6b65\u7684\u7814\u7a76</p> <p>\u901a\u8fc7 1 \u7ef4\u4f4d\u7f6e\u7f16\u7801\u4e5f\u662f\u80fd\u591f\u5b66\u4e60\u5230 2 \u7ef4\u4fe1\u606f\u7684\uff0c\u6bd4\u5982\u8fd9\u91cc\u8fd9\u4e2a\u4f4d\u7f6e\u7f16\u7801\u7684\u76f8\u5173\u6027\u7684\u56fe\uff0c\u53ef\u4ee5\u770b\u5230\u548c\u6bcf\u4e2a patch\u4f4d\u7f6e\u7f16\u7801\u6700\u76f8\u5173\u7684\u8fd8\u662f\u81ea\u5df1\u9644\u8fd1\u7684\u4ee5\u53ca\u81ea\u5df1\u6240\u5728\u884c\u5217\u7684 patch</p> <p>\u53e6\u5916\u4f5c\u8005\u7814\u7a76\u53d1\u73b0\u968f\u7740 Encoder \u5c42\u7684\u589e\u52a0\uff0c\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u5173\u6ce8\u7684\u5e73\u5747\u50cf\u7d20\u8ddd\u79bb\u53ef\u4ee5\u770b\u5230\u4e0d\u540c\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u7f51\u7edc\u6d45\u5c42\u6709\u7684\u5934\u5173\u6ce8\u8fd1\u8ddd\u79bb\u7684\u50cf\u7d20\uff0c\u4f46\u662f\u4e5f\u6709\u5f88\u591a\u5934\u5df2\u7ecf\u5173\u6ce8\u5230\u4e86\u8fdc\u8ddd\u79bb\u7684\u50cf\u7d20\uff0c\u968f\u7740\u6a21\u578b\u5c42\u6570\u7684\u52a0\u6df1\uff0c\u6a21\u578b\u8d8a\u6765\u8d8a\u5173\u6ce8\u8fdc\u8ddd\u79bb\u7684\u5168\u5c40\u4fe1\u606f\u4e86</p>"},{"location":"learning/LittleLesson/3_ViT/#_5","title":"\u5bf9\u4e8e\u6a21\u578b\u7ed3\u6784\u7684\u5c1d\u8bd5","text":"<p>\u5173\u4e8e\u6a21\u578b\u7ed3\u6784\u7684\u5c1d\u8bd5\uff0c\u4f5c\u8005\u9009\u62e9\u4e86\u4e09\u79cd\u7ed3\u6784\u8fdb\u884c\u5bf9\u6bd4</p> <p>\u4e00\u79cd\u662f\u539f\u59cb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u6bd4\u5982 ResNet</p> <p>\u4e00\u79cd\u662f\u53ea\u7528 Transformer \u7684 ViT</p> <p>\u6700\u540e\u4e00\u79cd \u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c Transformer \u7684\u6df7\u5408\u6a21\u578b</p>"},{"location":"learning/LittleLesson/3_ViT/#_6","title":"\u4ecb\u7ecd\u6df7\u5408\u6a21\u578b\u7684\u6a21\u578b\u67b6\u6784","text":"<p>\u9996\u5148\u7531\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u63d0\u53d6\u7279\u5f81\uff0c\u6700\u540e\u5728\u63d0\u53d6\u7684\u7279\u5f81\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u5c31\u662f\u4e00\u4e2a\u56fe\u50cf\u7684 patch\uff0c\u7136\u540e\u518d\u505a\u7ebf\u6027\u6620\u5c04\u8fdb\u5165 Transformer Encoder</p> <p>\u56fe\u4e2d\uff0c\u5706\u5f62\u8868\u793a ViT\uff0c\u65b9\u5f62\u8868\u793a ResNet\uff0c\u52a0\u53f7\u8868\u793a\u6df7\u5408\u6a21\u578b</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u5728\u76f8\u540c\u7684\u9884\u8bad\u7ec3\u4ee3\u4ef7\u4e0b\uff0c\u521a\u5f00\u59cb\u6df7\u5408\u6a21\u578b\u6709\u4f18\u52bf\uff0c\u4f46\u662f\u968f\u7740\u8ba1\u7b97\u4ee3\u4ef7\u7684\u589e\u5927\uff0c\u4e5f\u5c31\u662f\u6a21\u578b\u7684\u589e\u5927\uff0c\u6700\u7ec8 ViT \u6a21\u578b\u7684\u6548\u679c\u8fd8\u662f\u7565\u597d\u4e8e\u6df7\u5408\u6a21\u578b\uff0c\u6240\u4ee5\u8bc1\u660e\u4e86 Transformer \u67b6\u6784\u5728\u89c6\u89c9\u9886\u57df\u5b8c\u5168\u53ef\u4ee5\u53d6\u4ee3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc</p>"},{"location":"learning/LittleLesson/3_ViT/#_7","title":"\u5bf9\u4e8e\u56fe\u7247\u5206\u7c7b\u901a\u5e38\u4e5f\u6709\u4e24\u79cd\u505a\u6cd5","text":"<p>\u4e00\u79cd\u662f \u901a\u8fc7\u5728\u5e8f\u5217\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u589e\u52a0\u4e00\u4e2a\u5206\u7c7btoken \u6765\u63d0\u53d6\u56fe\u50cf\u7684\u5168\u5c40\u4fe1\u606f</p> <p>\u4e00\u79cd \u4e0d\u989d\u5916\u589e\u52a0 token\uff0c\u5c31\u7528\u6240\u6709\u56fe\u50cf patch \u6700\u540e\u4e00\u5c42\u8f93\u51fa\u7684\u5168\u5c40\u5e73\u5747\u6c60\u5316\u6765\u505a\u5168\u5c40\u4fe1\u606f</p> <p>\u4f5c\u8005\u505a\u4e86\u6bd4\u8f83\uff0c\u4e24\u4e2a\u6548\u679c\u662f\u7c7b\u4f3c\u7684</p>"},{"location":"learning/LittleLesson/3_ViT/#_8","title":"\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u6a21\u578b\u7684\u5f71\u54cd","text":"<p>\u56fe\u7247\u4e2d\u65b9\u5757\u8868\u793a\u7684\u662f\u4e0d\u540c\u5927\u5c0f\u7684 ResNet \u5728\u4e0d\u540c\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5706\u5f62\u8868\u793a\u7684\u662f\u4e0d\u540c\u5927\u5c0f\u7684 ViT \u5728\u4e0d\u540c\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53ef\u4ee5\u770b\u5230\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a ResNet \u8868\u73b0\u597d\u4e8e ViT</p> <p>\u800c\u968f\u7740\u6570\u636e\u96c6\u89c4\u6a21\u7684\u589e\u5927\uff0cViT \u7684\u6548\u679c \u662f\u597d\u4e8e ResNet \u7684\uff0c\u6240\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u56fe\u53ef\u4ee5\u77e5\u9053\uff0c\u5728\u767e\u4e07\u7ea7\u522b\u7684\u6570\u636e\u96c6\u4e0a ResNet \u597d\u4e8e ViT\uff0c\u5343\u4e07\u7ea7\u522b\u7684\u6570\u636e\u4e0a ResNet \u548c ViT \u5dee\u4e0d\u591a\uff0c\u4ebf\u7ea7\u522b\u7684\u6570\u636e\u4e0a\uff0cResNet \u4e0d\u5982 ViT</p>"},{"location":"learning/LittleLesson/3_ViT/#vit-resnet","title":"\u2b50\ufe0f\u4e3a\u4ec0\u4e48\u5728\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u7684\u8bad\u7ec3 ViT\u4e0d\u5982 ResNet\uff1f","text":"<p>\u8fd9\u662f\u7531\u5f52\u7eb3\u504f\u7f6e\u5f15\u8d77\u7684</p> <p>\u4ec0\u4e48\u662f\u5f52\u7eb3\u504f\u7f6e\uff1f</p> <p></p> <p>\u5f52\u7eb3\u504f\u7f6e\u5c31\u662f\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u4eba\u4e3a\u5f15\u5165\u7684\u5148\u9a8c\u77e5\u8bc6\u7ed9\u6a21\u578b\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u662f\u4eba\u7ed9\u7684\uff0c\u4e0d\u662f\u6a21\u578b\u4ece\u6570\u636e\u4e2d\u5b66\u6765\u7684\uff0c\u6bd4\u5982\u5377\u79ef\u64cd\u4f5c\u4e2d\uff0c\u6bcf\u4e00\u5c42\u90fd\u6709\u4e24\u4e2a\u5f52\u7eb3\u504f\u7f6e\uff1a\uff081\uff09\u5c40\u90e8\u6027\uff082\uff09\u5e73\u79fb\u4e0d\u53d8\u6027</p> <p>\u5377\u79ef\u6838\u4e3a\u4ec0\u4e48\u53ea\u4f5c\u7528\u5728\u4e00\u5f20\u56fe\u7247\u7684\u5c40\u90e8\u5462\uff1f</p> <p>\u56e0\u4e3a\u56fe\u7247\u76f8\u5173\u4fe1\u606f\u90fd\u96c6\u4e2d\u5728\u5c40\u90e8</p> <p>\u5377\u79ef\u6838\u4e3a\u4ec0\u4e48\u5728\u56fe\u7247\u4e0a\u8fdb\u884c\u5e73\u79fb\uff1f</p> <p>\u56e0\u4e3a\u7269\u4f53\u4e0d\u8bba\u662f\u5728\u56fe\u7247\u4e0a\u7684\u4ec0\u4e48\u4f4d\u7f6e\uff0c\u7269\u4f53\u7684\u7279\u5f81\u662f\u4e0d\u53d8\u7684</p> <p>\u4ee5\u4e0a\u4e24\u70b9\u5148\u9a8c\u77e5\u8bc6\u90fd\u7ed9\u4e86\u6a21\u578b\uff0c\u6240\u4ee5\u6a21\u578b\u5b66\u4e60\u8d77\u6765\u76f8\u5bf9\u7b80\u5355</p> <p>\u8fd9\u4e2a\u5f52\u7eb3\u504f\u7f6e\uff0c\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6bcf\u4e00\u5c42\u90fd\u8d77\u4f5c\u7528\uff0c\u4f46\u662f ViT \u91c7\u7528\u7684 Transformer\u67b6\u6784\uff0c\u5f15\u5165\u7684\u5f52\u7eb3\u504f\u7f6e\u6bd4\u8f83\u5c11\uff0c\u5c31\u662f\u5728\u5207\u5206 patch \u65f6\u5f15\u5165\u4e86\u5c40\u90e8\u6027\uff0c\u56e0\u4e3a\u628a\u539f\u59cb\u56fe\u7247\u5212\u5206\u6210\u4e86 patch\uff0c\u800c\u4e0d\u662f\u968f\u673a\u53d6\u4e00\u4e9b\u50cf\u7d20\uff0c\u8fd8\u6709\u5c31\u662f\u5bf9\u6240\u6709\u7684 patch\uff0c\u90fd\u7528\u540c\u6837\u7684\u7ebf\u6027\u5c42\u8fdb\u884c embedding\uff0c\u8fd9\u91cc\u76f8\u5f53\u4e8e\u5f15\u5165\u4e86\u5e73\u79fb\u4e0d\u53d8\u6027</p> <p>\u4f46\u662f ViT \u53ea\u662f\u5728\u5207\u5206 patch \u548c\u5bf9 patch \u8fdb\u884c\u7f16\u7801\u65f6\u5f15\u5165\u4e86\u5f52\u7eb3\u504f\u7f6e\uff0c\u540e\u9762\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u662f\u5b8c\u5168\u6ca1\u6709\u5f15\u5165\u5f52\u7eb3\u504f\u7f6e\u7684\uff0c\u6240\u4ee5\u5bfc\u81f4\u4e86 ViT \u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b66\u4e60\u6bd4 ResNet \u8981\u6162\u4e00\u4e9b</p>"},{"location":"learning/LittleLesson/3_ViT/#_9","title":"\u81ea\u76d1\u7763\u5b66\u4e60","text":"<p>\u6700\u540e\uff0cViT \u7684\u4f5c\u8005\u8fd8\u5c1d\u8bd5\u4e86\u8ba9\u56fe\u7247\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u56e0\u4e3a\u6709\u6807\u8bb0\u7684\u6570\u636e\u603b\u662f\u5c11\u6570\u7684\uff0c\u60f3\u8ba9\u6a21\u578b\u53d6\u5f97\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u53d6\u5f97\u7a81\u7834\u6027\u7684\u8fdb\u5c55\uff0c\u4e00\u5b9a\u8981\u662f\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5c31\u50cf Bert \u7684\u6210\u529f\u548c GPT \u7684\u6210\u529f</p> <p>\u4f5c\u8005\u5728\u5c1d\u8bd5\u81ea\u76d1\u7763\u5b66\u4e60\u65f6\uff0c\u501f\u9274\u4e86 Bert \u7684\u505a\u6cd5\uff0c\u5177\u4f53\u64cd\u4f5c\uff1a</p> <p>\u5c06 50%\u7684\u56fe\u50cf patch \u8fdb\u884c\u6807\u8bb0\uff0c\u5728\u8fd9\u4e9b\u6807\u8bb0\u7684 patch \u4e2d 80%\u5c06 embedding \u66ff\u6362\u6210\u53ef\u5b66\u4e60\u7684 mask \u6807\u7b7e\uff0c10%\u7684 embedding \u66ff\u6362\u4e3a\u5176\u4ed6\u7684 patch embedding\uff0c10%\u7684 embedding \u4fdd\u6301\u4e0d\u53d8\uff0c\u6700\u7ec8\u8ba9\u5229\u7528\u6807\u8bb0\u7684\u8fd9\u4e9b patch \u7684\u8f93\u51fa\u9884\u6d4b\u539f\u59cb\u56fe\u7247\u7684\u50cf\u7d20\u503c</p> <p>\u4e3a\u4e86\u7b80\u5316\u95ee\u9898\uff0c\u5c06\u539f\u6765 RGB \\(255*255*255\\) \u4e00\u5171 1658 \u4e07\u591a\u79cd\u989c\u8272\u7b80\u5316\u5230 RGB \u5206\u522b\u5bf9\u5e94 8 \u4e2a\u503c  \\(8*8*8\\) \u4e00\u5171 256 \u79cd\u989c\u8272</p> <p>\u6700\u7ec8\u7684\u6548\u679c\u975e\u5e38\u4e0d\u9519</p> <p>ViT \u6253\u5f00\u4e86 Transformer \u67b6\u6784\u5904\u7406\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u5927\u95e8\uff0c\u8ba9\u591a\u6a21\u6001\u901a\u7528\u4eba\u5de5\u667a\u80fd\u6210\u4e3a\u53ef\u80fd</p>"},{"location":"literature/","title":"Index","text":"<p>\u7ecf\u5178\u6587\u732e\u5f15\u7528\u683c\u5f0f</p> <p>ViT</p> <pre><code>@article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}\n</code></pre> <p>Attention is all you need</p> <pre><code>\n</code></pre>"},{"location":"literature/ObejectCounting/","title":"\u7d22\u5f15\u9875","text":""},{"location":"literature/ObejectCounting/#todo","title":"TODO","text":"<ul> <li> <p> \u6574\u7406\u8bfb\u8fc7\u7684\u6587\u732e</p> </li> <li> <p> DINO \u68c0\u6d4b&amp;\u8ba1\u6570</p> </li> </ul> <p></p>"},{"location":"literature/ObejectCounting/#_2","title":"\u4e92\u8054\u7f51\u8d44\u6e90","text":"<ul> <li>CCF\u671f\u520a\u5206\u533a\u67e5\u8be2</li> </ul> <ul> <li>Object Counting on FSC147 in paper with code</li> </ul> <ul> <li>\u90d1\u4e4b\u6770 \u76ee\u6807\u8ba1\u6570(Object Counting) </li> </ul> <p>Tips</p> <p>\u9605\u8bfb\u903b\u8f91\uff1a</p> <ol> <li>\u6458\u8981\u3001\u5f15\u8a00-\u8d21\u732e\u3001\u7ed3\u8bba\uff08\u6709\u4e9b\u4f1a\u7ed9\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff09  </li> <li>introdution\u76f8\u5f53\u4e8e \u7814\u7a76\u80cc\u666f\u53ca\u610f\u4e49 \u7ed9\u51fa motivation  </li> <li>related work \u4f1a\u7ed9\u51fa\u7814\u7a76\u73b0\u72b6  </li> <li>method \u90e8\u5206 \u5173\u6ce8\u5c0f\u6807\u9898  </li> <li>experiment \u90fd\u505a\u4e86\u4ec0\u4e48\u5b9e\u9a8c \uff1a\u6cdb\u5316\uff08\u6570\u636e\u96c6\u3001\u4efb\u52a1\uff09\u3001\u5bf9\u6bd4\uff08\u65b9\u6cd5\uff09\u3001\u6d88\u878d \uff08\u6a21\u5757\uff09</li> </ol> <p>\u4e00\u70b9\u6709\u8da3\u7684\u53d1\u73b0\uff1a</p> <p>GeCo\u4f5c\u8005\uff1a27 Sep 2024 \u00b7 Jer Pelhan, Alan Luke\u017ei\u010d, Vitjan Zavrtanik, Matej Kristan</p> <p>LOCA\u4f5c\u8005\uff1a ICCV 2023  \u00b7 Nikola Djukic, Alan Lukezic, Vitjan Zavrtanik, Matej Kristan </p> <p>DAVE\u4f5c\u8005\uff1a25 Apr 2024 \u00b7 Jer Pelhan, Alan Luke\u017ei\u010d, Vitjan Zavrtanik, Matej Kristan \uff08\u4ed3\u5e93\u7684\u5171\u540c\u4f5c\u8005\u4e4b\u4e00\uff1aCounTR\uff09\uff08Jer\uff1aDAVE &amp; GeCo\uff09</p> <p>CounTR\u4f5c\u8005\uff1a29 Aug 2022 \u00b7 Chang Liu, Yujie Zhong, Andrew Zisserman, Weidi Xie  SHJT</p> <p>semAug counTR\u4f5c\u8005\uff1a26 Oct 2023 \u00b7 Perla Doubinsky, Nicolas Audebert, Michel Crucianu, Herv\u00e9 Le Borgne \u00b7</p> <p>SemAug-SAFECount\uff1a26 Oct 2023 \u00b7 Perla Doubinsky, Nicolas Audebert, Michel Crucianu, Herv\u00e9 Le Borgne \u00b7</p> <p></p> <p>22 Jan 2022 \u00b7 Zhiyuan You, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le \uff08Tsinghua\u3001SHJT\uff09</p>"},{"location":"literature/ObejectCounting/#1","title":"1","text":"<ul> <li>multi-scale feature fusion module</li> <li>Transformer \u2192ViT\u2192SwinTransformer</li> <li>GAN\u2192diffusion\uff08text to image\uff09</li> <li>multimodle\uff1aclip\uff08text and image\uff09\uff1buser interaction</li> <li>SPDCN \u4fee\u6539\u4e86 \u635f\u5931\u51fd\u6570 \uff1b\u6839\u636e\u793a\u4f8b\u5c3a\u5bf8\u7684\u4e0d\u540c \u8c03\u6574\u635f\u5931\u51fd\u6570\u7684\u5f62\u5f0f</li> </ul>"},{"location":"literature/ObejectCounting/#2","title":"2","text":"<ul> <li>\u8ba1\u6570\u65b9\u6cd5  rank8 CounTR</li> <li>\u7c7b\u65e0\u5173\u8ba1\u6570 rank8 CounTR</li> <li>\u6570\u636e\u751f\u6210 rank7 SemAug CounTR</li> </ul>"},{"location":"literature/ObejectCounting/#_3","title":"\u6587\u732e\u7efc\u8ff0","text":"<ol> <li> <p>FamNet</p> </li> <li> <p>SAFECount</p> </li> <li> <p>GMN</p> </li> <li> <p>BMNet</p> </li> <li> <p>CounTR</p> </li> <li> <p>CountGD</p> </li> </ol>"},{"location":"literature/ObejectCounting/rank1%20CountGD/","title":"rank1 CountGD","text":""},{"location":"literature/ObejectCounting/rank1%20CountGD/#_1","title":"\u591a\u6a21\u6001\u7279\u5f81\u5f00\u653e\u4e16\u754c\u76ee\u6807\u8ba1\u6570","text":"<p>2024\u5e747\u67085\u65e5 \u53d1\u8868</p> <p>5 Jul 2024 \u00b7 Niki Amini-Naieni, Tengda Han, Andrew Zisserman \u00b7 </p> <p></p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#countgd-multi-modal-open-world-counting","title":"CountGD: Multi-Modal Open-World Counting","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u4e3a\u4e86\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4ec0\u4e48\u6837\u7684\u89e3\u51b3\u65b9\u6cd5\u3002</p> <p>\u6587\u672c\u63cf\u8ff0\u548c\u89c6\u89c9\u4fe1\u53f7\uff0c\u4e00\u8d77\u8fdb\u884c\u8ba1\u6570\uff0c\u6587\u672c\u63cf\u8ff0\u6bd4\u5982\u4f1a\u8fc7\u6ee4\u989c\u8272\uff0c\u4f4d\u7f6e\u7b49\uff1b</p> <p>\u4e00\u53e5\u8bdd\u603b\u7ed3\u672c\u6587\uff1aHere, we describe COUNTGD, a single-stage model for open-world object counting that accepts either visual exemplars or text or both together as prompts to specify the object to count.</p> <p>\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86COUNTGD\uff0c\u4e00\u4e2a\u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570\u7684\u5355\u9636\u6bb5\u6a21\u578b\uff0c\u5b83\u63a5\u53d7\u89c6\u89c9\u6837\u672c\u6216\u6587\u672c\u6216\u4e24\u8005\u5171\u540c\u4f5c\u4e3a\u63d0\u793a\u6765\u6307\u5b9a\u8981\u8ba1\u6570\u7684\u7269\u4f53\u3002</p> <p>Note</p> <p>COUNTGD\u7684\u51e0\u4e2a\u5173\u952e\u8bcd\uff0c\u5f00\u653e\u4e16\u754c\u7684\u7269\u4f53\u8ba1\u6570\uff0c\u5355\u9636\u6bb5\u6a21\u578b\uff0c\u89c6\u89c9\u4fe1\u53f7\u548c\u6587\u672c\u4fe1\u53f7</p> <p>\u6587\u672c\u7279\u5f81\u54ea\u91cc\u6765\u5462\uff1f</p> <ul> <li>\u5bf9\u4e8eFSC147\u6570\u636e\u96c6\uff0cFor text descriptions, we use the singular forms of the class names in <code>FSC-147-D [1]</code> with any prefixes such as \u201cthe\" removed. For example, we change \u201cthe donuts in the donut tray\" in FSC-147-D to \u201cdonut\" by removing the prefix \u201cthe,\" extracting the class name \u201cdonuts,\" and then singularizing it to \u201cdonut.\"  \u6258\u76d8\u4e2d\u7684\u751c\u751c\u5708 \\(\\rightarrow\\) \u751c\u751c\u5708</li> <li>CARPK\uff1aWe use the class name \u201ccar\" as the text description.</li> </ul>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#_2","title":"\u6458\u8981","text":"<p>The goal of this paper is to improve the generality and accuracy of open-vocabulary object counting in images. </p> <p>\u4e3a\u4e86\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\uff0c\u8ba1\u6570\u73b0\u5b9e\u8bcd\u8868\u4e2d\u7684\u4e00\u5207\u7269\u4f53</p> <p>To improve the generality, we repurpose an open vocabulary detection foundation model (GroundingDINO) for the counting task, and also extend its capabilities by introducing modules to enable specifying the target object to count by visual exemplars. </p> <p>\u4e3a\u4e86\u63d0\u9ad8\u6cdb\u5316\u6027\uff0c\u91cd\u65b0\u8bbe\u8ba1\u57fa\u4e8e\u5f00\u653e\u8bcd\u8868\u7684\u68c0\u6d4b\u6a21\u578b\uff1aGroundingDINO</p> <p>\u589e\u52a0\u6a21\u5757\uff0c\u901a\u8fc7\u6837\u4f8b\u6846\u6307\u5b9a\u8ba1\u6570\u76ee\u6807</p> <p>In turn, these new capabilities \u2013 being able to specify the target object by multi-modalites (text and exemplars) \u2013 lead to an improvement in counting accuracy. </p> <p>\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u6307\u5b9a\u8ba1\u6570\u76ee\u6807\uff0c\u63d0\u9ad8\u8ba1\u6570\u7684\u51c6\u786e\u6027</p> <p>we introduce the first open-world counting model, COUNTGD, where the prompt can be specified by a text description or visual exemplars or both; </p> <p>\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\uff1aCOUNTGD\uff0c\u53ef\u4ee5\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff1a\u6587\u672c\u3001\u89c6\u89c9\u6837\u4f8b\u3001\u6216\u8005\u4e24\u4e2a\u90fd</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#-","title":"\u5f15\u8a00-\u8d21\u732e","text":"<p>In summary, we make the following three contributions: </p> <p>First, we introduce COUNTGD, the first openworld object counting model that accepts either text or visual exemplars or both simultaneously, in a single-stage architecture; </p> <p>\u5355\u4e00\u7ed3\u6784\uff0c\u540c\u65f6\u63a5\u6536\u6587\u672c\u4fe1\u53f7\u548c\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u8ba1\u6570</p> <p>Second, we evaluate the model on multiple standard counting benchmarks, including FSC-147 [39], CARPK [18] and CountBench [36], and show that COUNTGD significantly improves on the state-of-the-art performance by specifying the target object using both exemplars and text. It also meets or improves on the state-of-the-art for text-only approaches when trained and evaluated using text-only; </p> <p>\u672c\u6587\u6240\u7528\u6570\u636e\u96c6\uff1aFSC-147 [39], CARPK [18] and CountBench [36]</p> <p>Third, we investigate how the text can be used to refine the visual information provided by the exemplar, for example by filtering on color or relative position in the image, to specify a sub-set of the objects to count.</p> <p>\u6587\u672c\u662f\u5982\u4f55\u7ec6\u5316\u7531\u6837\u4f8b\u63d0\u4f9b\u7684\u89c6\u89c9\u4fe1\u606f\u7684\uff0c\u6bd4\u5982\uff1a\u901a\u8fc7\u5bf9\u56fe\u50cf\u4e2d\u7684\u989c\u8272\u6216\u76f8\u5bf9\u4f4d\u7f6e\u8fdb\u884c\u8fc7\u6ee4\uff0c\u6765\u6307\u5b9a\u8981\u8ba1\u6570\u7684\u5bf9\u8c61\u7684\u5b50\u96c6</p> <p>In addition we make two minor improvements to the inference stage: one that addresses the problem of double counting due to self-similarity, and the other to handle the problem of a very high count.</p> <p>\u63a8\u7406\u9636\u6bb5\u7684\u4e24\u4e2a\u6539\u8fdb\uff1a</p> <ul> <li>\u7531\u4e8e\u81ea\u76f8\u4f3c\u6027\u7684\u91cd\u590d\u8ba1\u6570\u95ee\u9898</li> <li>\u6781\u5ea6\u5bc6\u96c6\u573a\u666f\u7684\u8ba1\u6570\u95ee\u9898</li> </ul> <p>\u76ee\u6807\u8ba1\u6570\u7684\u4e24\u5927\u95ee\u9898\uff1a</p> <p>\u7269\u4f53\u5806\u53e0\u5bfc\u81f4\u7684\u91cd\u590d\u8ba1\u6570</p> <p>\u5bc6\u96c6\u573a\u666f\u7684\u8ba1\u6570\u95ee\u9898 </p> \u96be\u9053\u4e0d\u662f\u540c\u4e00\u4e2a\u95ee\u9898\uff1f"},{"location":"literature/ObejectCounting/rank1%20CountGD/#conclusion-future-work","title":"Conclusion &amp; Future Work","text":"<p>We have extended the generality of open-world counting by introducing a model that can accept visual exemplars or text descriptions or both as prompts to specify the target object to count. </p> <p>\u7b2c\u4e00\u70b9\u6211\u4eec\u8fdb\u884c\u4e86\u5f00\u653e\u4e16\u754c\u7684\u8bed\u4e49\u7269\u4f53\u8ba1\u6570\u95ee\u9898\uff0c\u63a5\u6536\u6587\u672c\u63cf\u8ff0\u548c\u793a\u4f8b\u6846\u4fe1\u606f\u6216\u8005\u5171\u540c</p> <p>me\uff1a\u5f00\u653e\u4e16\u754c\u7684\u8bed\u4e49\u7269\u4f53\u8ba1\u6570\u3001\u6587\u672c\u63cf\u8ff0\u548c\u89c6\u89c9\u793a\u4f8b\u6846</p> <p>\u6587\u672c\u63cf\u8ff0\u662f\u5bf9\u793a\u4f8b\u6846\u7269\u4f53\u7684\u9009\u62e9\u5f15\u5165\u989d\u5916\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u77ed\u8bed\u9650\u5236</p> <p>The complementarity of these prompts in turn leads to improved counting performance. </p> <p>\u672a\u6765\u7684\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\uff1a</p> <p>There are three research directions that naturally follow on from this work: </p> <p>(i) the performance could probably be further improved by training on larger scale datasets, for example using synthetic data as demonstrated recently for counting [24]; </p> <p>\u7b2c\u4e00\u4e2a\u7814\u7a76\u65b9\u5411\uff1a\u6269\u5c55\u8f93\u5165\u6570\u636e\u7684\u4e30\u5bcc\u6027\uff0c\u6bd4\u5982\u5408\u6210\u6570\u636e\uff0c\u545c\u545c\u545c\u545c\uff0c\u8ddf\u6211\u60f3\u7684\u4e00\u6837 </p> <p>(ii) a larger training set would enable a thorough investigation of freezing more of the GroundingDINO model when adding our new visual exemplar modules; and finally, </p> <p>\u66f4\u5927\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5bf9GroundingDINO\u6a21\u578b\u8fdb\u884c\u66f4\u5f7b\u5e95\u7684\u5b9e\u9a8c</p> <p>(iii) the model does not currently predict the errors of its counting. We discuss this point in the Limitations in the Appendix.</p> <p>Note</p> <p>countGD\uff1b\u5f00\u653e\u4e16\u754c\u7684\u8bed\u4e49\u7269\u4f53\u8ba1\u6570\u95ee\u9898\uff1b\u6cdb\u5316\u6027\u51c6\u786e\u6027\uff1btext and exemplar</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#related-work","title":"Related Work","text":"<p>Prior work on object counting has developed along three axes: \u76ee\u6807\u8ba1\u6570\u7684\u4e09\u4e2a\u7814\u7a76\u7ef4\u5ea6</p> <p>(1) the density map versus detection axis, \u57fa\u4e8e\u56de\u5f52\u7684 &amp; \u57fa\u4e8e\u68c0\u6d4b\u7684</p> <p>(2) the class-specific versus open-world (also referred to as \u201cclass-agnostic\") axis, and \u7279\u5b9a\u7c7b\u522b\u8ba1\u6570 &amp; \u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570\uff08\u7c7b\u522b\u4e0d\u654f\u611f\u8ba1\u6570\uff09</p> <p>(3) the visual exemplar versus text axis.  \u57fa\u4e8e\u89c6\u89c9\u4fe1\u53f7\u7684\u8ba1\u6570 \u548c \u57fa\u4e8e\u6587\u672c\u7684\u8ba1\u6570</p> <p>The pattern is that detection, open-world, and text-based methods tend to offer more capabilities and be more general than their analogues along each axis. </p> <p>\u57fa\u4e8e\u68c0\u6d4b\u3001\u5f00\u653e\u4e16\u754c\u3001\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u6cdb\u5316\u6027\u66f4\u597d</p> <p>On the other hand, density map, class-specific, and visual exemplar-based methods tend to be more accurate at the counting tasks they apply to. </p> <p>\u57fa\u4e8e\u56de\u5f52\u5bc6\u5ea6\u56fe\u3001\u7279\u5b9a\u7c7b\u522b\u3001\u89c6\u89c9\u6837\u4f8b\u6846\u7684\u51c6\u786e\u6027\u66f4\u597d</p> <p>COUNTGD integrates the third axis \u2013 the visual exemplar versus text axis \u2013 to achieve more general and accurate counting overall. Below, we discuss where prior work falls along each axis and where COUNTGD stands.</p> <p>COUNTGD\u6574\u5408\u4e86\u7b2c\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u65e2\u7528\u6587\u672c\uff0c\u53c8\u7528\u793a\u4f8b\u6846</p> <p>Note</p> <p>\u6211\u4eec\u8fd9\u4e2acounGD\u6574\u5408\u4e86\u7b2c\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u65e2\u6709\u89c6\u89c9\u4fe1\u53f7\u53c8\u6709\u6587\u672c\u4fe1\u53f7\uff0c\u518d\u6b21\u5f3a\u8c03\uff0c\u6587\u672c\u4fe1\u53f7\u6cdb\u5316\u6027\u597d\u3001\u89c6\u89c9\u4fe1\u53f7\u51c6\u786e\u6027\u597d\uff0c\u56e0\u6b64\u65e2\u6709\u89c6\u89c9\u4fe1\u53f7\u53c8\u6709\u6587\u672c\u4fe1\u53f7\u7684\u6cdb\u5316\u6027\u548c\u51c6\u786e\u6027\u90fd\u5f88\u597d\u3002\u63a5\u4e0b\u6765\u8ba8\u8bba\u5148\u524d\u7684\u5de5\u4f5c\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u53d1\u5c55\u4ee5\u53caCOUNTGD</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#density-map-versus-detection-based-object-counting-axis-1","title":"Density Map versus Detection-based Object Counting (Axis 1).","text":"<p>\u7ef4\u5ea6\u4e00\uff1a\u57fa\u4e8e\u5bc6\u5ea6\u548c\u57fa\u4e8e\u68c0\u6d4b</p> <p>Density Map versus Detection-based Object Counting (Axis 1). </p> <p>In the past, counting techniques that regress and sum density maps [2, 3, 6, 25, 26, 33, 42], instead of detecting and enumerating bounding boxes [5, 8, 18, 35], have proven more accurate in cluttered and dense scenes. \u5728\u5148\u524d\u7684\u5de5\u4f5c\u4e2d\uff0c\u5df2\u7ecf\u8bc1\u660e\u4e86\uff0c\u57fa\u4e8e\u5bc6\u5ea6\u7684\u7269\u4f53\u8ba1\u6570\u65b9\u6cd5\u5728\u5bc6\u96c6\u573a\u666f\u4e0b\u7684\u8ba1\u6570\u9002\u7528\u6027</p> <p>For example, density map-based approaches like CounTX [1], LOCA [10], and CounTR [29] achieve lower counting errors than detection-based approaches such as Mask-RCNN [16] and RetinaNet [27] on standard counting benchmarks. </p> <p>\u4e3e\u4f8b\u8bf4\u660e\uff0c\u57fa\u4e8e\u5bc6\u5ea6\u6bd4\u57fa\u4e8e\u68c0\u6d4b\u7684\u53d1\u5c55\u4f18\u52bf\u3002</p> <p>Concurrent to our work, DAVE [37], integrates density map regression with object detection to construct a more accurate and explainable two-stage counting system. Like DAVE, COUNTGD outputs explicit object locations.</p> <p>\u4e0e\u6211\u4eec\u7684\u5de5\u4f5c\u76f8\u4e00\u81f4\uff0cDAVE [ 37 ]\u5c06\u5bc6\u5ea6\u56fe\u56de\u5f52\u4e0e\u76ee\u6807\u68c0\u6d4b\u7ed3\u5408\u8d77\u6765\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u66f4\u7cbe\u786e\u548c\u53ef\u89e3\u91ca\u7684\u4e24\u9636\u6bb5\u8ba1\u6570\u7cfb\u7edf\u3002\u4e0eDAVE\u4e00\u6837\uff0cCOUNTGD\u8f93\u51fa\u660e\u786e\u7684\u5bf9\u8c61\u4f4d\u7f6e\u3002</p> <p>Note</p> <p>DAVE\u5bc6\u5ea6\u56fe\u56de\u5f52\u548c\u7269\u4f53\u68c0\u6d4b\uff0c\u8f93\u51fa\u76ee\u6807\u7684\u4f4d\u7f6e \u4e24\u9636\u6bb5\uff0c\u51c6\u786e\u6027&amp;\u6cdb\u5316\u6027</p> <p>However, COUNTGD is a single-stage approach that achieves better counting accuracy than DAVE and other density map-based techniques.</p> <p>\u7136\u800c\uff0cCOUNTGD\u662f\u4e00\u79cd\u5355\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u5176\u8ba1\u6570\u7cbe\u5ea6\u4f18\u4e8eDAVE\u548c\u5176\u4ed6\u57fa\u4e8e\u5bc6\u5ea6\u56fe\u7684\u6280\u672f\u3002</p> <p>Note</p> <p>\u55ef\uff0cDAVE\u4e24\u9636\u6bb5\uff0c\u6211\u4e00\u9636\u6bb5\uff0c\u800c\u4e14\u662f text&amp;exemplar</p> <p>Therefore,while density map-based approaches tend to be more accurate than detectors in highly populated scenes, recent detection-based techniques, including COUNTGD, are beginning to achieve better accuracy than density map-based alternatives.</p> <p>\u867d\u7136\u57fa\u4e8e\u5bc6\u5ea6\u56fe\u7684\u65b9\u6cd5\u5728\u4eba\u53e3\u7a20\u5bc6\u7684\u573a\u666f\u4e2d\u5f80\u5f80\u6bd4\u68c0\u6d4b\u5668\u66f4\u51c6\u786e\uff0c\u4f46\u6700\u8fd1\u7684\u57fa\u4e8e\u68c0\u6d4b\u7684\u6280\u672f\uff0c\u5305\u62ecCOUNTGD\uff0c\u5df2\u7ecf\u5f00\u59cb\u53d6\u5f97\u6bd4\u57fa\u4e8e\u5bc6\u5ea6\u56fe\u7684\u65b9\u6cd5\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002</p> <p>Note</p> <p>\u867d\u7136\u57fa\u4e8e\u5bc6\u5ea6\u7684\u5f88\u597d\uff0c\u4f46\u6700\u8fd1\u57fa\u4e8e\u68c0\u6d4b\u7684\u53d1\u5c55\u4e0d\u7518\u793a\u5f31 COUNTGD \u5c31\u662f\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570</p> \u4e3a\u4ec0\u4e48\u53ebCountGD\uff1f"},{"location":"literature/ObejectCounting/rank1%20CountGD/#_3","title":"\u7ef4\u5ea6\u4e8c\uff1a\u7279\u5b9a\u7c7b\u522b \u5bf9\u6bd4 \u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570","text":"<p>Class-specific versus Open-world Object Counting (Axis 2). </p> <p>Object counting methods first developed as class-specific techniques [3, 4, 34, 42], solving the counting problem for only one category of object, but recent methods have generalized these approaches to open-world settings, where counting arbitrary objects is possible. \u5c31\u662f\u8bf4\uff0c\u6700\u5f00\u59cb\u53d1\u5c55\u7684\u5bf9\u7279\u5b9a\u7c7b\u522b\u7684\u8ba1\u6570\uff0c\u540e\u6765\u6f14\u53d8\u6210\u5bf9\u4efb\u610f\u7269\u4f53\u7684\u8ba1\u6570\u95ee\u9898</p> <p>Class-specific methods have been developed to count cars [22], humans [4], and cells [13]. In contrast, open-world methods can count instances from all three categories [32]. </p> <p>\u4e3e\u4f8b\u5177\u4f53\u8bf4\u660e</p> <p>Because class-specific techniques are more specialized than open-world approaches, they tend to be more accurate at counting instances from the class they were designed for. </p> <p>\u5c31\u662f\u8bf4\uff0c\u9488\u5bf9\u7279\u5b9a\u7c7b\u522b\u7684\u7269\u4f53\u8ba1\u6570\u51c6\u786e\u6027\u786e\u5b9e\u5f88\u597d</p> <p>Recent advancements in Vision-Language Foundation Models (VLMs) such as CLIP [38] and GroundingDINO [30] trained on web-scale image-text pairs produce semantically rich visual and textual features. </p> <p>\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b( Vision-Language Foundation Models\uff0cVLMs )\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5982CLIP [ 38 ]\u548cGroundingDINO [ 30 ]\uff0c\u5728\u7f51\u7edc\u89c4\u6a21\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ea7\u751f\u4e86\u8bed\u4e49\u4e30\u5bcc\u7684\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u3002</p> <p>Note</p> <p>emm\u5728\u8fd9\u4e48\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6700\u540e\u5c31\u662f\u5f97\u5230\u4e30\u5bcc\u7684\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u7279\u5f81</p> <p>These features generalize to a wide range of open-world downstream tasks. Building on top of these pre-trained VLMs, recent open-world methods [1, 7, 10, 21, 29, 40, 45] have begun to surpass class-specific approaches in counting accuracy. COUNTGD, like these recent approaches, is an open-world object counter that achieves competitive performance in comparison to class-specific alternatives.</p> <p>\u8fd9\u4e9b\u7279\u5f81\u6cdb\u5316\u5230\u4e86\u5e7f\u6cdb\u7684\u5f00\u653e\u4e16\u754c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u3002\u5728\u8fd9\u4e9b\u9884\u8bad\u7ec3\u7684VLMs\u7684\u57fa\u7840\u4e0a\uff0c\u6700\u8fd1\u7684\u5f00\u653e\u4e16\u754c\u65b9\u6cd5[ 1\u30017\u300110\u300121\u300129\u300140\u300145]\u5df2\u7ecf\u5f00\u59cb\u8d85\u8d8a\u7279\u5b9a\u7c7b\u522b\u7684\u65b9\u6cd5\u5728\u8ba1\u6570\u7cbe\u5ea6\u4e0a\u3002\u4e0e\u8fd9\u4e9b\u6700\u8fd1\u7684\u65b9\u6cd5\u4e00\u6837\uff0cCOUNTGD\u662f\u4e00\u4e2a\u5f00\u653e\u4e16\u754c\u7684\u5bf9\u8c61\u8ba1\u6570\u5668\uff0c\u4e0e\u7279\u5b9a\u7c7b\u7684\u66ff\u4ee3\u54c1\u76f8\u6bd4\uff0c\u5b83\u5177\u6709\u7ade\u4e89\u6027\u7684\u6027\u80fd\u3002</p> <p>Note</p> <p>\u83b7\u5f97\u8bed\u4e49\u66f4\u52a0\u4e30\u5bcc\u7684\u7279\u5f81\uff0c\u53ef\u4ee5\u66f4\u597d\u7684\u6cdb\u5316\u5230\u4e0b\u6e38\u4efb\u52a1\u3002   </p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#_4","title":"\u89d2\u5ea6\u4e09\uff1a\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u7279\u5f81","text":"<p>Counting with Visual Exemplars versus Counting with Text (Axis 3).</p> <p>Most open-world object counters approach the problem by using visual exemplars to select the objects in the input image [10, 14, 28, 29, 32, 35, 39, 40, 44, 45], but very recent work [1, 7, 19, 21, 43] has attempted to replace the visual exemplars with text, enabling new capabilities at the cost of reduced accuracy. The stateof-the-art text-based approaches, such as GroundingREC [7], CounTX [1], CLIP-Count [19], and VLCounter [21] are built on top of vision-language foundation models pretrained on large quantities of data to relate images to textual inputs and map them to a joint embedding space. </p> <p>\u5927\u591a\u6570\u5f00\u653e\u4e16\u754c\u5bf9\u8c61\u8ba1\u6570\u5668\u901a\u8fc7\u4f7f\u7528\u89c6\u89c9\u6837\u672c\u6765\u9009\u62e9\u8f93\u5165\u56fe\u50cf[ 10\u300114\u300128\u300129\u300132\u300135\u300139\u300140\u300144\u300145]\u4e2d\u7684\u5bf9\u8c61\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u6700\u8fd1\u7684\u5de5\u4f5c[ 1\u30017\u300119\u300121\u300143]\u5c1d\u8bd5\u7528\u6587\u672c\u4ee3\u66ff\u89c6\u89c9\u6837\u672c\uff0c\u4ee5\u964d\u4f4e\u51c6\u786e\u6027\u4e3a\u4ee3\u4ef7\u6765\u5b9e\u73b0\u65b0\u7684\u529f\u80fd\u3002\u76ee\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u5982GroundingREC [ 7 ]\uff0cCounTX [ 1 ]\uff0cCLIP-Count [ 19 ]\u548cVLCounter [ 21 ]\uff0c\u90fd\u662f\u5efa\u7acb\u5728\u57fa\u4e8e\u5927\u91cf\u6570\u636e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u4e4b\u4e0a\uff0c\u5c06\u56fe\u50cf\u4e0e\u6587\u672c\u8f93\u5165\u76f8\u5173\u8054\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u4e00\u4e2a\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u3002</p> <p>Note</p> <p>\u4e4b\u524d\u7684\u8ba1\u6570\u65b9\u6cd5\u90fd\u662f\u901a\u8fc7\u793a\u4f8b\u6846\u9009\u62e9\u8f93\u5165\u56fe\u50cf\u7684\u76ee\u6807\uff1b\u6700\u8fd1\u7684\u5de5\u4f5c\u5f00\u59cb\u4f7f\u7528\u6587\u672c\uff0c\u8fd8\u8bb0\u5f97\u5427\uff0c\u6587\u672c\u4fe1\u53f7\u6cdb\u5316\u6027\u597d\uff0c\u89c6\u89c9\u4fe1\u53f7\u51c6\u786e\u6027\u597d\uff0c\u56e0\u6b64\u5f53\u4f7f\u7528\u6587\u672c\u4fe1\u53f7\u65f6\uff0c\u662f\u727a\u7272\u4e86\u51c6\u786e\u6027\u3002\u7136\u540e\u5c31\u8bf4\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u9884\u6d4b\u65b9\u6cd5</p> <p>This allows these foundation models to understand general concepts learned during extensive pretraining and provides a mechanism for users to specify extrinsic object properties through text. However, text-based approaches perform significantly worse than state-of-the-art visual exemplar-based approaches such as LOCA [10], CounTR [29], and few-shot DAVE [37]. For example, while both GroundingREC and COUNTGD use the pretrained GroundingDINO [30] vision-language foundation model, unlike GroundingREC, COUNTGD allows the user to input both visual exemplars and text instead of just text. This enables COUNTGD to achieve superior counting accuracy in comparison to GroundingREC.</p> <p>\u8fd9\u4f7f\u5f97\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u80fd\u591f\u7406\u89e3\u5728\u5e7f\u6cdb\u7684\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u5230\u7684\u4e00\u822c\u6982\u5ff5\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u6307\u5b9a\u5916\u90e8\u5bf9\u8c61\u5c5e\u6027\u7684\u673a\u5236\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u7684\u6027\u80fd\u660e\u663e\u5dee\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u89c6\u89c9\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u5982LOCA [ 10 ]\uff0cCounTR [ 29 ]\u548c\u5c0f\u6837\u672cDAVE [ 37 ]\u3002\u4f8b\u5982\uff0cGroundingREC\u548cCOUNTGD\u90fd\u4f7f\u7528\u4e86\u9884\u8bad\u7ec3\u7684GroundingDINO [ 30 ]\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u4e0eGroundingREC\u4e0d\u540c\u7684\u662f\uff0cCOUNTGD\u5141\u8bb8\u7528\u6237\u540c\u65f6\u8f93\u5165\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6587\u672c\u3002\u8fd9\u4f7f\u5f97COUNTGD\u76f8\u6bd4GroundingREC\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u6570\u7cbe\u5ea6\u3002</p> <p>Note</p> <p>\u4f60\u77e5\u9053\u7684\u5427\uff0c\u5728\u89c6\u89c9\u6587\u672c\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u5b66\u5230\u4e00\u822c\u6982\u5ff5\uff0c\u6240\u4ee5\u6cdb\u5316\u6027\u66f4\u597d\u3002\u4f46\u8fd8\u662f\u90a3\u53e5\u8bdd\uff0c\u51c6\u786e\u6027\u4e0d\u591f\u3002\u8fd9\u91cc\u8fd8\u8bf4\u4e86\u4e0eCountGD\u5de5\u4f5c\u5f88\u76f8\u4f3c\u7684\u6a21\u578bGroundingREC\uff0c\u4f46\u662f\u8f93\u5165\u4fe1\u53f7\u4e0d\u4e00\u6837\uff0c\u76f8\u4f3c\u5728\u4e8e\u90fd\u662f\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684GroundingDINO\u3002\u4e0d\u76f8\u4f3c\u5728\u4e8e\u8f93\u5165\u4fe1\u53f7\u4e0d\u4e00\u6837\u7684</p> <p>Notably, DAVE [37] is a visual exemplar-based approach that also enables textual prompts, but differs from COUNTGD in three important ways:COUNTGD \u4e0eDAVE\u7684\u4e09\u4e2a\u663e\u8457\u4e0d\u540c</p> <p>(1) it does not address the case when both text and visual exemplars are available while COUNTGD does,\u89c6\u89c9\u4fe1\u53f7\u548c\u6587\u672c\u4fe1\u53f7\u90fd\u6765</p> <p>(2) its comparison between text features and image features is not learned as it is by COUNTGD with attention, and     COUNTGD\u6709\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u6587\u672c\u7279\u5f81\u548c\u56fe\u50cf\u7279\u5f81</p> <p>(3) it is a two-stage approach, while COUNTGD solves the problem in a single stage, without relying on another visual exemplar-based counting model. DAVE\u4e24\u9636\u6bb5\u68c0\u6d4b\u65b9\u6cd5</p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cDAVE [ 37 ]\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u6837\u4f8b\u7684\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u6587\u672c\u63d0\u793a\uff0c\u4f46\u4e0eCOUNTGD\u67093\u4e2a\u91cd\u8981\u7684\u533a\u522b\uff1a( 1 )\u5b83\u6ca1\u6709\u89e3\u51b3COUNTGD\u540c\u65f6\u63d0\u4f9b\u6587\u672c\u548c\u89c6\u89c9\u6837\u4f8b\u7684\u60c5\u51b5\uff1b( 2 )\u5b83\u6ca1\u6709\u50cfCOUNTGD\u90a3\u6837\u5728\u6709\u6ce8\u610f\u529b\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u6587\u672c\u7279\u5f81\u548c\u56fe\u50cf\u7279\u5f81\u4e4b\u95f4\u7684\u6bd4\u8f83\uff1b( 3 )\u5b83\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u800cCOUNTGD\u5728\u4e00\u4e2a\u9636\u6bb5\u4e2d\u89e3\u51b3\u95ee\u9898\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u53e6\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u6837\u4f8b\u7684\u8ba1\u6570\u6a21\u578b\u3002</p> <p>Note</p> <p>\u8fd9\u7bc7\u8bba\u6587\u7684\u6539\u8fdb\u8bba\u6587\u662fDAVE\uff0c\u76ee\u6807\u90fd\u662f\u4e00\u6837\u7684\uff0cmotivation\uff1a\u63d0\u9ad8\u51c6\u786e\u7387 &amp; \u53ec\u56de\u7387</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#relation-of-counting-to-other-areas","title":"Relation of Counting to other areas. \u4e0e\u5176\u5b83\u9886\u57df\u5de5\u4f5c\u7684\u5173\u7cfb","text":"<p>\u8ddf\u5f00\u5c71\u4e4b\u4f5c\u7684\u76f8\u5173\u5de5\u4f5c\u53d9\u8ff0\u6709\u70b9\u50cf</p> <p>Our work is related to few-shot image classification [41] and image detection [12, 20] methods.   \u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u548c\u68c0\u6d4b</p> <p>These works require a few query images of novel objects, and then compare the test image with these image examples to determine its semantic content (for image classification), or to spatially localize instances (for object detection). </p> <p>\u6211\u4eec\u7684\u5de5\u4f5c\u4e0e\u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7b[ 41 ]\u548c\u56fe\u50cf\u68c0\u6d4b[ 12\u300120]\u65b9\u6cd5\u76f8\u5173\u3002\u8fd9\u4e9b\u5de5\u4f5c\u9700\u8981\u4e00\u4e9b\u65b0\u9896\u5bf9\u8c61\u7684\u67e5\u8be2\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u6d4b\u8bd5\u56fe\u50cf\u4e0e\u8fd9\u4e9b\u56fe\u50cf\u793a\u4f8b\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u786e\u5b9a\u5176\u8bed\u4e49\u5185\u5bb9(\u5bf9\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b)\uff0c\u6216\u8005\u5bf9\u5b9e\u4f8b(\u9488\u5bf9\u76ee\u6807\u68c0\u6d4b)\u8fdb\u884c\u7a7a\u95f4\u5b9a\u4f4d\u3002</p> <p>Like these methods, COUNTGD enables us to specify the object to count with visual exemplars (i.e., \u201cquery images\") but also allows for textual inputs, and then compares the test image with the multi-modal specifications to get the final count. Furthermore, we focus on the counting problem, a challenging task for object detectors.</p> <p>\u4e0e\u8fd9\u4e9b\u65b9\u6cd5\u4e00\u6837\uff0cCOUNTGD\u5141\u8bb8\u6211\u4eec\u7528\u53ef\u89c6\u5316\u793a\u4f8b(\u5373\"\u67e5\u8be2\u56fe\u50cf\")\u6307\u5b9a\u8981\u8ba1\u6570\u7684\u5bf9\u8c61\uff0c\u4f46\u4e5f\u5141\u8bb8\u6587\u672c\u8f93\u5165\uff0c\u7136\u540e\u5c06\u6d4b\u8bd5\u56fe\u50cf\u4e0e\u591a\u6a21\u6001\u89c4\u8303\u8fdb\u884c\u6bd4\u8f83\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u8ba1\u6570\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5173\u6ce8\u8ba1\u6570\u95ee\u9898\uff0c\u8fd9\u662f\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002</p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684\u65b9\u6cd5\u3001\u5b8c\u6210\u8ba1\u6570\u4efb\u52a1\uff0c\u5176\u5b9e\u73b0\u5728\u7528\u57fa\u4e8e\u68c0\u6d4b\u7684\u7b97\u6cd5\u6765\u8fdb\u884c\u8ba1\u6570\u4efb\u52a1\u662f\u6bd4\u8f83\u5c11\u7684 </p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#3-counting-with-visual-exemplars-text","title":"3 Counting with Visual Exemplars &amp; Text","text":"<p>Here, we describe COUNTGD, a single-stage model for open-world object counting that accepts either visual exemplars or text or both together as prompts to specify the object to count.</p> <ul> <li>a single-stage model</li> <li>\u63a5\u6536\u7684\u8f93\u5165\uff1avisual exemplars or text or both together</li> </ul>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#31-overview","title":"3.1 Overview","text":"<p>\u7b26\u53f7\u8bf4\u660e</p> <p></p> <p>\u63a5\u6536\u6307\u5b9a\u7269\u4f53\u7684\u4fe1\u53f7\uff1a</p> <ul> <li>\u89c6\u89c9\u4fe1\u53f7\uff1a\\(B=\\{b_1,......,b_N\\}\\)</li> <li>\u6587\u672c\u4fe1\u53f7\uff1a\\(t\\)</li> <li>both\uff1a\\(\\{B,t\\}\\)</li> </ul> <p>\u67e5\u8be2\u56fe\u7247\uff1a\\(X\\in \\mathbb{R}^{H\u00d7W\u00d73}\\)</p> <p>\\(\\hat{y}=f(X,B,t)\\)</p> <p>\u8ba1\u6570\u6a21\u578b\u8bb0\u4e3a\uff1af\uff0c\u8f93\u51fa\u8ba1\u6570\u6570\u91cf \\(\\hat{y}\\)</p> <p></p> <p></p> <p>\u56fe2\uff1a\u6a21\u578b\u7ed3\u6784\u56fe </p> <p>GD \u6307\u7684\u662f  GroundingDINO\u7684\u9996\u5b57\u6bcd\u7f29\u5199</p> <p>COUNTGD\uff1a\u662f\u57fa\u4e8eGroundingDINO\u7684</p> <p>GroundingDINO\uff1a\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u5f00\u653e\u8bcd\u8868\u5efa\u7acb\u7684\u76ee\u6807\u68c0\u6d4b\u5668</p> <p>\u4e0eGroundingDINO\u4e0d\u540c\u7684\u662f\uff1a</p> <ul> <li>GroundingDINO\u53ea\u63a5\u6536\u6587\u672c\u6307\u5b9a\u67e5\u8be2\u5bf9\u8c61\u3001COUNTGD\u4e5f\u53ef\u4ee5\u63a5\u53d7\u89c6\u89c9\u4fe1\u53f7</li> </ul> <p>\u9996\u5148 \u4ecb\u7ecdCOUNTGD\uff0c\u7136\u540e\u4ecb\u7ecdCOUNTGD\u4e0eGroundingDINO\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662fGroundingDINO\u54ea\u91cc\u662f\u88ab\u51bb\u7ed3\u7684\uff0c\u54ea\u91cc\u662f\u88ab\u8bad\u7ec3\u7684\uff0c\u4ee5\u53ca\u54ea\u91cc\u662f\u88ab\u6dfb\u52a0\u5230GroundingDINO\u7684</p> <p></p> <p>\u7ec4\u4ef6\u8bf4\u660e\uff1a</p> <ul> <li>\u63a8\u7406\u9636\u6bb5\uff1aAt inference the object to be counted can be specified by visual exemplars or text prompts or both.\u6307\u5b9a\u8ba1\u6570\u7269\u4f53\u7684\u65b9\u6cd5</li> <li>\u8f93\u5165\u56fe\u50cf\u7684\u5904\u7406\uff1a\\(f_{\\theta_{SwinT}}\\) \u63d0\u53d6\u4e0d\u540c\u5c3a\u5ea6\u7684\u4fe1\u606f</li> <li>\\(RoIAlign\\)\u83b7\u5f97 visual exemplar token</li> <li>text token\u7684\u83b7\u5f97\uff1a\\(f_{\\theta_{TT}}\\)</li> <li>\u7279\u5f81\u589e\u5f3a\u6a21\u5757 \\(f_{\\phi}\\) \uff1a\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u53d6 \u89c6\u89c9\u4fe1\u53f7\u548c\u6587\u672c\u4fe1\u53f7\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u878d\u5408\u89c6\u89c9\u4fe1\u53f7\u548c\u6587\u672c\u4fe1\u53f7\uff0c\u4ea7\u751f\u7279\u5f81 \\(z_{v,t}\\) \u3001\u56fe\u50cf\u7279\u5f81 \\(z_I\\)</li> <li>\u4ea7\u751f\u8de8\u6a21\u6001\u7279\u5f81\uff1a \u4f59\u5f26\u76f8\u4f3c\u5ea6\u6700\u9ad8\u7684 \\(k\\) \u4e2a\u56fe\u50cf\u7279\u5f81 \\(z_I\\) \u548c \u89c6\u89c9\u4fe1\u53f7\u548c\u6587\u672c\u4fe1\u53f7\u7684 \u878d\u5408\u7279\u5f81 \\(z_{v,t}\\) \u4f20\u5165 \u8de8\u6a21\u6001\u89e3\u7801\u5668 \\(f_{\\psi}\\)</li> <li>\u8ba1\u7b97 \u8de8\u6a21\u6001\u89e3\u7801\u5668 $ f_{\\psi}$ \u7684\u8f93\u51fa \u548c \\(z_{v,t}\\)  \u5f97\u5230 \\(\\hat{Y}\\)</li> <li>\u6700\u7ec8\u7684\u68c0\u6d4b\u8f93\u51fa\uff1a \\(z_{v,t} \uff1e \\sigma\\) \u83b7\u5f97\u6700\u5927\u76f8\u4f3c\u5ea6\u7684\u8f93\u51fa</li> <li>\u6574\u4e2a\u7684\u6a21\u578b\u6846\u67b6 \u90fd\u662f\u5728 GroundingDINO \u6846\u67b6\u7684\u57fa\u7840\u4e0a\u4e0a\u6539\u8fdb\u7684\uff0c\u53e6\u5916\u6dfb\u52a0\u7684\u90e8\u5206 \u7528\u84dd\u8272\u8868\u793a</li> </ul> <p></p> <p></p> <p>\u56fe3\uff1a\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u6a21\u5757</p> <p>\uff08a\uff09\u5bf9\u4e8e\u8f93\u5165\u56fe\u50cf\uff0c\u6807\u51c6\u7684SwinTransformer\u6a21\u578b\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\u7684 \u591a\u7a7a\u95f4\u7279\u5f81</p> <p>\uff08b\uff09\u5bf9\u4e8e\u6709\u6837\u4f8b\u6846\u7684\u8f93\u5165\u7279\u5f81\u56fe\uff0c\u5c06\u591a\u4e2a\u89c6\u89c9\u7279\u5f81\u6837\u4f8b\u6846\u4e0a\u91c7\u6837\u5230\u8f93\u5165\u56fe\u50cf\u7684\u5927\u5c0f\uff0c\u62fc\u63a5\u8fd9\u4e9b\u7279\u5f81\u56fe\uff0c\u901a\u8fc71\u00d71\u5377\u79ef\uff0c\u6295\u5f71\u5230256\u4e2a\u901a\u9053\uff1b\u6700\u540e\u5e94\u7528 \u5e26\u6709\u8fb9\u754c\u6846\u5750\u6807\u7684  RoIAlign \u83b7\u5f97 \u6837\u4f8b\u6846\u7684\u89c6\u89c9\u7279\u5f81</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#32-countgd-architecture-components","title":"3.2 COUNTGD Architecture Components","text":"<p>\u56fe\u50cf\u7f16\u7801\u5668 \\(f_{\\theta_{SwinT}}\\)</p> <ul> <li>\u5904\u7406\u4e24\u4e2a\u8f93\u5165\u4fe1\u53f7\uff1a\u8f93\u5165\u56fe\u50cf \\(X\\) \u548c \u89c6\u89c9\u6837\u4f8b\u6846 \\(B\\)</li> <li>\u4f7f\u7528\u7684SwinTransformer\u7684\u7248\u672c Swin-B</li> <li>\uff08\u8f93\u5165\u56fe\u50cfX\u7684\u5efa\u6a21\uff09\u5982\u56fe3(a) \u6240\u793a\uff0c\u5bf9\u4e8e\u8f93\u5165\u56fe\u50cf \\(X\\) \u63d0\u53d63\u4e2a\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u7684 \u7279\u5f81 <ul> <li>\u4e09\u4e0d\u540c\u5927\u5c0f\u7684\u7edf\u5efa\u7279\u5f81\u56fe \u901a\u8fc71\u00d71\u7684\u5377\u79ef\u6838 \u6295\u5f71\u5230256\u7ef4\uff0c\u4ea7\u751f\u56fe\u50cftoken</li> <li>\u4e0d\u540c\u5c3a\u5ea6\u7684\u56fe\u50cf\u5757\u5bf9\u5e94\u957f\u5ea6\u4e3a256\u7684\u7279\u5f81\u5411\u91cf\uff0c\u4f5c\u4e3a\u7279\u5f81\u589e\u5f3a\u6a21\u5757 \\(f_{\\phi}\\)\u7684\u8f93\u5165</li> </ul> </li> <li>\uff08\u6837\u4f8b\u6846 B\u7684\u5efa\u6a21\uff09\u5982\u56fe3(b)\uff0c\u5bf9\u4e8e\u89c6\u89c9\u6837\u4f8b\u6846 B\uff0c\u590d\u7528 \u8f93\u5165\u56fe\u50cf \\(X\\) \u7684 \u7a7a\u95f4\u7279\u5f81\u56fe \\(f_{\\theta_{SwinT}}(X)\\)</li> <li>\u4f7f\u7528 ROI pooling\uff08\u5bf9\u9f50\u7684\u611f\u5174\u8da3\u533a\u57df\u6c60\u5316?\uff09\uff0c\u4e0e\u89c6\u89c9\u793a\u4f8bB\u6307\u5b9a\u7684\u50cf\u7d20\u5750\u6807</li> <li>\u4ea7\u751f\u7684\u89c6\u89c9\u7279\u5f81 \u548c \u56fe\u50cf\u548c\u6587\u672ctoken \u4e00\u6837\u7684\u5927\u5c0f\uff1a256\u7ef4\u5ea6</li> </ul> <p></p> <p></p> <p>\u6587\u672c\u7f16\u7801\u5668</p> <ul> <li> <p>\u5bf9\u4e8e\u6587\u672c\u7f16\u7801\u5668 \\(f_{\\theta_{TT}}\\)\uff0c\u4f7f\u7528\u57fa\u4e8ebert\u7684\u6587\u672cTransformer</p> </li> <li> <p>\u9884\u8bad\u7ec3\u7684\u6570\u636e\u96c6\uff1a\u68c0\u6d4b\u548c\u77ed\u8bed\u5b9a\u4f4d\u6570\u636e\uff0c\u6709\u56fe\u50cf\u7f16\u7801\u5668 \\(f_{\\theta_{SwinT}}\\)</p> </li> <li> <p>\u6587\u672c\u7f16\u7801\u5668\u5c06\u8f93\u5165\u5bf9\u8c61\u63cf\u8ff0 \\(t\\) \u6295\u5f71\u5230\u6700\u591a256\u4e2a token</p> </li> <li> <p>\u7f16\u7801\u540e\u7684\u6587\u672c \u7279\u5f81\u5411\u91cf\u662f256\u7ef4\u7684\uff08256\u4e2atoken\uff09</p> </li> <li> <p>\u56fe\u50cf\u7f16\u7801\u5668 \\(f_{{\\theta}_{SwinT}}\\) \u4ece\u8f93\u5165\u56fe\u50cf\u4e2d \u63d0\u53d6 \\(n\\) \u4e2a \u56fe\u50cf\u5757 \u7279\u5f81</p> </li> </ul> <ul> <li>\u5f53\u6709 \\(p\\) \u4e2a \u89c6\u89c9\u793a\u4f8b \u53ef\u7528\u65f6\uff0c\u89c6\u89c9\u7f16\u7801\u5668\u4ea7\u751f p \u4e2a\u89c6\u89c9\u793a\u4f8b\u7279\u5f81</li> <li>\u5f53bert\u5206\u8bcd\u5668\u5728\u6587\u672ct\u4e2d\u6709q\u4e2atoken\uff0c\u6587\u672c\u7f16\u7801\u5668\u4ea7\u751fq \u4e2a\u6807\u8bb0</li> </ul> <ul> <li>\u6700\u540e\uff0c\u83b7\u5f97 \u7279\u5f81\u589e\u5f3a\u6a21\u5757\u7684 \u8f93\u5165 \\(f_{\\phi}\\) \u6709 \u2460 n\u4e2a\u56fe\u50cftoken   \u2461p\u4e2a\u89c6\u89c9\u6837\u4f8b\u6846token   \u2462q\u4e2a\u6587\u672ctoken</li> </ul> <p>\u4f7f\u7528\u6ce8\u610f\u529b\u6a21\u5757\u878d\u5408\u8fd9\u4e09\u4e2a\u6e90\u7684\u4fe1\u606f</p> <p>Note</p> <p>\u6587\u672c\u7279\u5f81\uff1a clip   bert</p> <p></p> <p>\u7279\u5f81\u589e\u5f3a\u6a21\u5757 \\(f_{\\phi}\\)</p> <ul> <li>\u75316\u4e2a\u5757\u7ec4\u6210</li> <li>\u81ea\u6ce8\u610f\u529b\u673a\u5236 \u878d\u5408 \u89c6\u89c9\u6837\u4f8b\u6846\u7279\u5f81\u548c \u6587\u672ctoken</li> <li>\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757 \u878d\u5408 \u5408\u5e76\u540e\u7684\u7279\u5f81\u548c \u56fe\u50cf\u5757\u6807\u8bb0</li> <li>\u6bcf\u4e2a\u6a21\u5757\u5305\u62ec \u89c6\u89c9\u793a\u4f8b \u548c \u6587\u672c\u6807\u8bb0 \u8fde\u63a5\u540e\u7684\u81ea\u6ce8\u610f\u529b\u3001\u56fe\u50cf\u5757 \u6807\u8bb0\u4e4b\u95f4\u7684\u53ef\u53d8\u5f62\u81ea\u6ce8\u610f\u529b\uff0c\u4ee5\u53ca\u878d\u5408\u540e\u7684 \u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u6807\u8bb0 \u4e0e\u56fe\u50cf\u5757\u6807\u8bb0\u4e4b\u95f4 \u7684\u56fe\u50cf\u5230\u6587\u672c\u7684\u4ea4\u53c9\u6ce8\u610f\u529b \u548c \u6587\u672c\u5230\u56fe\u50cf\u7684\u4ea4\u53c9\u6ce8\u610f\u529b</li> <li>\u8fd9\u4e9b\u6a21\u5757\u4f7f\u5f97COUNTGD\u80fd\u591f\u5b66\u4e60\u5c06\u8f93\u5165\u56fe\u50cf\u3001\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u67e5\u8be2\u7684\u4fe1\u606f\u7efc\u5408\u8d77\u6765</li> <li>\u7279\u5f81\u589e\u5f3a\u6a21\u5757 \\(f_{\\phi}\\) \u8f93\u51fa\u4e24\u7ec4\u7279\u5f81\uff0c\u5206\u522b\u8868\u793a\u4e3a  \\(z_{v,t}\\)  \u548c \\(z_I\\)</li> </ul> <p></p> <ul> <li>\u878d\u5408\u56fe\u50cf\u5757token\u3001\u6587\u672ctoken\u3001\u6837\u4f8b\u6846token</li> </ul> <p></p> <p>\u8bed\u8a00\u548c\u89c6\u89c9\u6837\u4f8b\u6846\u5f15\u5bfc\u7684\u67e5\u8be2\u9009\u62e9</p> <ul> <li>k\u4e2a\u56fe\u50cf\u5757 token \\(z_I\\) \u548c\u6587\u672c \u6807\u8bb0 \\(z_{v,t}\\) \u5177\u6709\u6700\u9ad8\u7684\u76f8\u4f3c\u5ea6\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u8868\u793a \\(Select(z_I,z_Iz_v^T,k)\\) </li> <li>\u5176\u4e2d \\(z_Iz_{v,t}^T \\in \\mathbb{R}^{n \u00d7(p+q)}\\)  \u8868\u793a n\u4e2a\u56fe\u50cf\u5757 token \u548c p+q\u4e2a\u89c6\u89c9\u6837\u4f8b\u6846\u548c\u6587\u672c token</li> <li>\u6b63\u5982 GroundingDINO\uff0ck\u8bbe\u7f6e\u4e3a900</li> <li>\u5177\u6709\u66f4\u9ad8\u76f8\u4f3c\u5ea6\u7684900\u4e2a\u56fe\u50cftoken\u4f5c\u4e3a\u8de8\u6a21\u6001\u67e5\u8be2 \u8f93\u5165\u5230 \u8de8\u6a21\u6001\u89e3\u7801\u5668 \\(f_{\\phi}\\)</li> </ul> <p></p> <p>\u8de8\u6a21\u6001\u89e3\u7801\u5668 \\(f_{\\psi}\\)</p> <ul> <li>\u8de8\u6a21\u6001\u89e3\u7801\u5668 \\(f_{\\psi}\\) \u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u589e\u5f3a\u8de8\u6a21\u6001\u67e5\u8be2</li> <li>\u56fe\u50cf\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u56fe\u50cf\u5757\u7279\u5f81 \\(z_I\\) \u878d\u5408\u5230 \u8de8\u6a21\u6001\u67e5\u8be2\u4e2d</li> <li>\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u7279\u5f81 \\(z_{v,t}\\) \u878d\u5408\u5230\u8de8\u6a21\u6001\u67e5\u8be2\u4e2d</li> <li>\u8de8\u6a21\u6001\u89e3\u7801\u5668\u75316\u4e2a\u8fd9\u6837\u7684\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u7ec4\u6210</li> <li>\u8de8\u6a21\u6001\u67e5\u8be2\u4e0e\u878d\u5408\u540e\u7684\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u6807\u8bb0 \\(z_{v,t}\\) \u8fdb\u884c\u70b9\u79ef\uff0c\u5e76\u901a\u8fc7\u9010\u5143\u7d20\u7684Sigmoid\u51fd\u6570\u5904\u7406\uff0c\u4ee5\u83b7\u5f97\u6700\u7ec8\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u5982\u4e0b\u6240\u793a\uff1a</li> </ul> <p></p> <ul> <li>\\(z_{v,t}\\)\u662f\u878d\u5408\u7684\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u7279\u5f81</li> <li>\\(z_I\\) \u662f\u67e5\u8be2\uff08\u5373\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u7684\u6700\u5927\u6570\u91cf\uff09</li> <li>\\(\\hat{Y}\\) \u6700\u7ec8\u76f8\u4f3c\u5ea6\u5206\u6570</li> <li>\u5206\u6570\u6839\u636e\u7f6e\u4fe1\u5ea6\u9608\u503c \\(\\sigma\\) \u8fdb\u884c\u9608\u503c\u5904\u7406\uff0c\u5e76\u5728\u63a8\u7406\u65f6 \u7528\u4e8e\u4f30\u8ba1\u6700\u7ec8\u7684\u5bf9\u8c61\u8ba1\u6570 \\(\\hat{y}\\)</li> </ul> <p></p> <p>\u8bbe\u8ba1\u9009\u62e9\u4e0eGroundingDINO\u7684\u5173\u7cfb</p> <p>\u6211\u4eec\u9009\u62e9GroundingDINO[30]\u800c\u4e0d\u662f\u5176\u4ed6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u662f\u56e0\u4e3a\u5176\u5728\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u4e0a\u7684\u9884\u8bad\u7ec3\uff0c\u4f7f\u5176\u4e0e\u5176\u4ed6VLMs\uff08\u5982CLIP[17]\uff09\u76f8\u6bd4\u5177\u6709\u66f4\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u3002</p> <p>\u4e3a\u4e86\u6269\u5c55GroundingDINO\u4ee5\u63a5\u53d7\u89c6\u89c9\u793a\u4f8b\uff0c\u6211\u4eec\u5c06\u5176\u89c6\u4e3a\u6587\u672c\u6807\u8bb0\u3002\u56e0\u4e3a\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u90fd\u6307\u5b9a\u4e86\u5bf9\u8c61\uff0c\u6211\u4eec\u8ba4\u4e3a\u89c6\u89c9\u793a\u4f8b\u53ef\u4ee5\u88abGroundingDINO\u50cf\u6587\u672c\u6807\u8bb0\u4e00\u6837\u5bf9\u5f85\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002\u5728\u5c06\u89c6\u89c9\u793a\u4f8b\u89c6\u4e3a\u77ed\u8bed\u4e2d\u7684\u9644\u52a0\u6587\u672c\u6807\u8bb0\u65f6\uff0c\u6211\u4eec\u5728\u5bf9\u5e94\u4e8e\u89c6\u89c9\u793a\u4f8b\u7684\u77ed\u8bed\u548c\u89c6\u89c9\u793a\u4f8b\u4e4b\u95f4\u6dfb\u52a0\u81ea\u6ce8\u610f\u529b\uff0c\u800c\u4e0d\u662f\u5c06\u5b83\u4eec\u5206\u5f00\u3002\u8fd9\u4f7f\u5f97COUNTGD\u80fd\u591f\u5b66\u4e60\u878d\u5408\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u6807\u8bb0\uff0c\u4ee5\u5f62\u6210\u5bf9\u8981\u8ba1\u6570\u5bf9\u8c61\u7684\u66f4\u5177\u4fe1\u606f\u6027\u7684\u89c4\u8303\u3002\u540c\u6837\uff0cGroundingDINO\u7684\u7279\u5f81\u589e\u5f3a\u5668\u548c\u8de8\u6a21\u6001\u89e3\u7801\u5668\u4e2d\u56fe\u50cf\u548c\u6587\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5728COUNTGD\u4e2d\u53d8\u4e3a\u56fe\u50cf\u4e0e\u878d\u5408\u540e\u7684\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u3002\u5728GroundingDINO\u4e2d\u7684\u8bed\u8a00\u5f15\u5bfc\u67e5\u8be2\u9009\u62e9\u5728COUNTGD\u4e2d\u53d8\u4e3a\u8bed\u8a00\u548c\u89c6\u89c9\u793a\u4f8b\u5f15\u5bfc\u7684\u67e5\u8be2\u9009\u62e9\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cCOUNTGD\u81ea\u7136\u5730\u6269\u5c55\u4e86GroundingDINO\uff0c\u4f7f\u5176\u80fd\u591f\u8f93\u5165\u6587\u672c\u548c\u89c6\u89c9\u793a\u4f8b\u6765\u63cf\u8ff0\u5bf9\u8c61\u3002</p> <p>\u5728GroundingDINO\u4e2d\uff0c\u56fe\u50cf\u7f16\u7801\u5668 \\(f_{\\theta_{\\text{SwinT}}}\\) \u4e0e\u6587\u672c\u7f16\u7801\u5668 \\(f_{\\theta_{\\text{TT}}}\\) \u4e00\u8d77\u5728\u4e30\u5bcc\u7684\u68c0\u6d4b\u548c\u77ed\u8bed\u5b9a\u4f4d\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\u4e3a\u5176\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u533a\u57df\u548c\u6587\u672c\u611f\u77e5\u7279\u5f81\u3002\u7531\u4e8e\u6211\u4eec\u5e0c\u671b\u5efa\u7acb\u5728\u8fd9\u4e2a\u9884\u8bad\u7ec3\u7684\u8054\u5408\u89c6\u89c9-\u8bed\u8a00\u5d4c\u5165\u4e4b\u4e0a\uff0c\u6211\u4eec\u4fdd\u6301\u56fe\u50cf\u7f16\u7801\u5668  \\(f_{\\theta_{\\text{SwinT}}}\\) \u548c\u6587\u672c\u7f16\u7801\u5668 \\(f_{\\theta_{\\text{TT}}}\\) \u4e0d\u53d8\u3002</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/","title":"rank10 SPDCN","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>\u5f15\u7528\uff1a2022\u5e74\u7684\u6587\u732e</p> <pre><code>@inproceedings{Lin_2022_BMVC,\nauthor    = {Wei Lin and Kunlin Yang and Xinzhu Ma and Junyu Gao and Lingbo Liu and Shinan Liu and Jun Hou and Shuai Yi and Antoni Chan},\ntitle     = {Scale-Prior Deformable Convolution for Exemplar-Guided Class-Agnostic Counting},\nbooktitle = {33rd British Machine Vision Conference 2022, {BMVC} 2022, London, UK, November 21-24, 2022},\npublisher = {{BMVA} Press},\nyear      = {2022},\nurl       = {https://bmvc2022.mpi-inf.mpg.de/0313.pdf}\n}\n</code></pre> <p>\u6807\u9898\uff1aScale-Prior Deformable Convolution for Exemplar-Guided Class-Agnostic Counting  \u57fa\u4e8e\u793a\u4f8b\u7684\uff0c\u5c3a\u5ea6\u5148\u9a8c\u53ef\u53d8\u5377\u79ef\uff0c\u7c7b\u65e0\u5173\u8ba1\u6570\uff08\u7a81\u7136\u60f3\u8d77\u6765\uff0c\u5404\u79cd\u5377\u79ef\uff0c\u7a7a\u6d1e\u5377\u79ef\u3001\u8f6c\u7f6e\u5377\u79ef\u3001\u5206\u7ec4\u5377\u79ef\uff09</p> <p>\u4f5c\u8005\uff1aConference 2022  \u00b7 Wei Lin, Kunlin Yang, Xinzhu Ma, Junyu Gao, Lingbo Liu, Shinan Liu, Jun Hou, Shuai Yi, Antoni B. Chan    \u9999\u6e2f\u5927\u5b66</p> <p></p> <p>\u671f\u520a\uff1aBMVC2022\uff1bCCF-C\u7c7b</p> <p>\u672c\u6587 \u4e3a\u4e86\u89e3\u51b3.......\u63d0\u51fa\u4e86...........</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_1","title":"\u6458\u8981","text":"<p>CAC\u4efb\u52a1\u8fdb\u5c55\u4e0d\u9519(\u8fd9\u662f22\u5e74\u7684\u6587\u7ae0\uff0c\u73b0\u5728\u505a\u7684\u90fd\u662fCAC\u7684)</p> <p>Class-agnostic counting has recently emerged as a more practical counting task, which aims to predict the number and distribution of any exemplar objects, instead of counting specific categories like pedestrians or cars. </p> <p>\u73b0\u6709\u7684\u65b9\u6cd5</p> <ul> <li>\u8bbe\u8ba1\u5408\u9002\u7684\u76f8\u4f3c\u6027\u5339\u914d\u89c4\u5219 \u5728\u793a\u4f8b\u548c\u67e5\u8be2\u56fe\u50cf\u4e4b\u95f4</li> <li>\u5ffd\u7565\u4e86\u63d0\u53d6\u7279\u5f81\u7684\u9c81\u68d2\u6027</li> </ul> <p>However, recent methods are developed by designing suitable similarity matching rules between exemplars and query images, but ignoring the robustness of extracted features. </p> <p>\u4e3a\u4e86 \u63d0\u53d6\u7279\u5f81\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa  \u5c3a\u5ea6\u5148\u9a8c\u53ef\u53d8\u5377\u79ef\uff0c\u6574\u5408\u793a\u4f8b\u4fe1\u606f</p> <p>To address this issue, we propose a scale-prior deformable convolution by integrating exemplars\u2019 information, e.g., scale, into the counting network backbone.\uff08\u6548\u679c\uff09As a result, the proposed counting network can extract semantic features of objects similar to the given exemplars and effectively filter irrelevant backgrounds. \u63d0\u51fa\u7684\u8ba1\u6570\u7f51\u7edc\u53ef\u4ee5\u63d0\u53d6 \u7ed9\u5b9a\u793a\u4f8b\u76f8\u4f3c\u76ee\u6807\u7684 \u8bed\u4e49\u7279\u5f81 \u5e76\u4e14 \u8fc7\u6ee4\u6389 \u4e0d\u76f8\u5173\u7684\u80cc\u666f\u4fe1\u606f</p> <p>\u4f20\u7edf\u7684L2\u635f\u5931\u548c\u6cdb\u5316\u635f\u5931 \u5bf9\u4e8eCAC \u8ba1\u6570\u95ee\u9898 \u4e0d\u5408\u9002\uff1b\u56e0\u4e3a\u5bf9\u4e8e\u4e0d\u540c\u76ee\u6807\u7684\u5c3a\u5ea6\u53d8\u5316\u662f\u6bd4\u8f83\u5927\u7684</p> <p>Besides, we find that traditional L2 and generalized loss are not suitable for class-agnostic counting due to the variety of object scales in different samples. </p> <p>\u4e3a\u4e86\u89e3\u51b3 \u4f20\u7edf\u7684L2\u635f\u5931\u5bf9\u4e8e\u793a\u4f8b\u5c3a\u5ea6\u591a\u53d8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898</p> <p>Here we propose a scale-sensitive generalized loss to tackle this problem. </p> <p>\u6211\u4eec\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570 \u80fd\u505a\u4ec0\u4e48\uff1f</p> <ul> <li>\u6839\u636e\u7ed9\u5b9a\u793a\u4f8b\u7684\u5927\u5c0f \u8c03\u6574\u635f\u5931\u51fd\u6570\u7684\u5f62\u5f0f\u2192\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u7684\u5dee\u5f02\u66f4\u52a0\u660e\u663e</li> </ul> <p>It can adjust the cost function formulation according to the given exemplars, making the difference between prediction and ground truth more prominent. </p> <p>\u7ed3\u679c</p> <p>Extensive experiments show that our model obtains remarkable improvement and achieves state-of-the-art performance on a public class-agnostic counting benchmark. the source code is available at https://github.com/Elin24/SPDCN-CAC.</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_2","title":"\u603b\u7ed3\u6458\u8981","text":"<ol> <li>\u4e3a\u4e86 \u63d0\u53d6\u7279\u5f81\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa  \u5c3a\u5ea6\u5148\u9a8c\u53ef\u53d8\u5377\u79ef\uff0c\u6574\u5408\u793a\u4f8b\u4fe1\u606f</li> <li>\u4e3a\u4e86\u89e3\u51b3 \u4f20\u7edf\u7684L2\u635f\u5931\u5bf9\u4e8e\u793a\u4f8b\u5c3a\u5ea6\u591a\u53d8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898</li> </ol>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_3","title":"\u5f15\u5165\u2014\u2014\u8d21\u732e","text":"<p>To summarize, the key contributions of this paper are:</p> <ul> <li>To address class-agnostic counting, we propose a scale-prior deformable network to better extract exemplar-related features, followed by a segmentation-then-counting stage to count objects. </li> </ul> <p>\u4e3a\u4e86\u89e3\u51b3CAC\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5c3a\u5ea6\u4f18\u5148\u53ef\u53d8\u5377\u79ef \u66f4\u597d\u7684\u63d0\u53d6\u4e0e\u6837\u4f8b\u6846\u6709\u5173\u7684\u7279\u5f81\uff1b\u8ddf\u7740\u4e00\u4e2a\u5148\u5206\u5272\u540e\u8ba1\u6570\u7684\u9636\u6bb5\u6765\u6570\u76ee\u6807</p> <ul> <li>We propose a scale-sensitive generalized loss to make the model training adaptive to objects of different sizes, boosting the performance and generalization of trained models. </li> </ul> <p>\u63d0\u51fa\u4e86 \u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931\uff0c\u4f7f\u5f97\u6a21\u578b\u66f4\u597d\u7684\u9002\u7528 \u4e0d\u540c\u5c3a\u5bf8\u7684\u7269\u4f53\uff1b\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u6027\u80fd</p> <ul> <li>\uff08\u7ed3\u679c\uff09Extensive experiments and visualizations demonstrate these two designs work well, and outstanding performance is obtained when our model is tested on benchmarks.\uff08\u6709\u5b9e\u9a8c\u3001\u6709\u53ef\u89c6\u5316\uff09</li> </ul>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_4","title":"\u603b\u7ed3","text":"<ul> <li>\u5c3a\u5ea6\u4f18\u5148\u53ef\u53d8\u5377\u79ef \u63d0\u53d6\u6837\u4f8b\u6846\u7279\u5f81\uff1b</li> <li>\u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931</li> </ul>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_5","title":"\u7ed3\u8bba","text":"<p>\u7b2c\u4e00\u4e2a\u63d0\u51fa</p> <p>In this paper, we explore exemplar-guided class-agnostic counting. </p> <p>\u8fd9\u7bc7\u6587\u7ae0 \u96c0\u6c0f\u4e3b\u8981\u7814\u7a76\u7684\u6837\u4f8b\u6846\u6307\u5bfc\u7684CAC\u8ba1\u6570\u95ee\u9898</p> <p>\u5c3a\u5ea6\u4f18\u5148\u7684\u53ef\u53d8\u5377\u79ef\u662f\u4e3a\u4e86 \u6574\u5408\u6837\u4f8b\u6846\u4fe1\u606f\uff0c\u4f7f\u5f97\u63d0\u53d6\u5230\u7684\u7279\u5f81\u66f4\u5177\u6709\u7a33\u5065\u6027</p> <p>To take advantage of scale information provided by exemplars, scale-prior deformable convolution is proposed to adjust the receptive fields according to the given exemplars. </p> <p>\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528\u5c3a\u5ea6\u4fe1\u606f\uff0c\u5c3a\u5ea6\u4f18\u5148\u7684\u53ef\u53d8\u5377\u79ef \u88ab\u63d0\u51fa \u66f4\u597d\u7684\u9002\u5e94 \u7ed9\u5b9a\u793a\u4f8b\u7684\u611f\u53d7\u91ce</p> <p>\uff08\u7ed3\u679c\uff09Experimental results show that this operation decreases counting errors dramatically and gives a more accurate density distribution. </p> <p>\u7b2c\u4e8c\u4e2a\u63d0\u51fa</p> <p>We also propose scale-sensitive generalized loss to adapt the cost function according to exemplars, so that different training samples with different object scales have their own distance function for optimal transport.</p> <p>\u540c\u6837\u63d0\u51fa \u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931 \u9002\u5e94\u635f\u5931\u51fd\u6570\uff0c\u56e0\u4e3a\u793a\u4f8b\u662f\u5c3a\u5ea6\u53d8\u5316\u7684</p> <p>\u4e0d\u540c\u7684\u8bad\u7ec3\u6837\u672c\u6709\u4e0d\u540c\u7684\u5c3a\u5ea6\u53d8\u5316</p> <p>\uff08\u7ed3\u679c\uff09 This new loss further helps our model perform better than previous models on the class-agnostic counting benchmark.</p> <p>Note</p> <p>\u5199\u4f5c\u903b\u8f91\uff1a   (1)\u63d0\u51fa\u65b9\u6cd5   (2)\u8bf4\u660e\u4f60\u7ed3\u679c  </p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_6","title":"\u5f15\u5165","text":"<p>Introduction</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#p1","title":"P1 \u4ece\u7279\u5b9a\u7c7b\u522b\u8bf4\u8d77","text":"<p>Info</p> <p>P1 </p> <p>\uff081\uff09\u4ece\u7279\u5b9a\u7c7b\u522b\u7684\u76ee\u6807\u8ba1\u6570\u5f00\u59cb\u8bf4\uff0c\u5e94\u8be5\u662f\u6587\u7ae0\u6bd4\u8f83\u65e9\uff0c\u6240\u4ee5\u5bf9\u4e8e\u4e3a\u4ec0\u4e48\u8981\u6c42CAC\u8ba1\u6570\u7684\u80cc\u666f\u548c\u52a8\u673a\u8bf4\u7684\u5f88\u5177\u4f53</p> <p>\uff082\uff09\u5f15\u5165\u90e8\u5206 \u90fd\u662f\u4ecesepcific\u8bf4\u8d77</p> <p>In recent years, remarkable progress has been achieved in counting tasks. However, most methods only work in a category-specific manner, like counting crowd [34] or vehicles [20], and thus they fail to meet the requirements of some real-world applications.  \u7279\u5b9a\u7c7b\u522b\u7684\u8ba1\u6570\uff1a\u4eba\u7fa4\u8ba1\u6570&amp;\u8f66\u8f86\u8ba1\u6570\uff1b\u4e0d\u80fd\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u7684\u9700\u8981\uff1b</p> <p>CAC\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f</p> <p>\uff081\uff09For example, there exist demands for counting goods in various categories in supermarkets or warehouses [7]; \u8d85\u5e02\u6216\u8005\u4ed3\u5e93\u4e2d \u5546\u54c1\u8ba1\u6570</p> <p>\uff082\uff09 in agriculture, predicting the crop yield of different fruits/vegetables is required [14, 37]; \u519c\u4e1a\u4e2d\uff0c\u4e0d\u540c\u679c\u852c\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b</p> <p>\uff083\uff09and some may want to know the number of different trees [3, 21]. \u4e0d\u540c\u6811\u6570\u91cf\u9884\u6d4b</p> <p>\u4f46\u662f\uff0c\u5f15\u51fa\u4e0b\u4e00\u6bb5   However, with traditional counting methods, a separate counting model is needed for each object class, which limits its practical applications.</p> <p>Info</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#p2-cac","title":"P2  CAC\u95ee\u9898\u7684\u8bf4\u660e\uff0c\u4ece\u5b9a\u4e49\u3001\u635f\u5931\u5230\u6a21\u578b\u6982\u8ff0","text":"<pre><code>\u8fd9\u91cc\u7684\u6a21\u578b\u6982\u8ff0\u8ddf\u4ee5\u524d\u7684\u4e0d\u5927\u4e00\u6837\uff0c\u662f\u8bf4\u6a21\u578b\u65b9\u6cd5\u4ee5\u540e \u5c31\u4f1a\u6307\u51fa\u95ee\u9898\n</code></pre> <p>CAC\u95ee\u9898\u7684\u5b9a\u4e49</p> <p>To tackle the above problem, this paper considers class-agnostic counting, in which counting models predict the number and distribution of objects indicated by a few object exemplars in a set of query images. </p> <p>CAC\u7684\u635f\u5931\u662f\u5982\u4f55\u5b9a\u4e49\u7684</p> <p>During training, both images and exemplars are input to the counting model, and then the loss is calculated between the predicted density maps and human-annotated dot maps [23]. </p> <p>\u73b0\u5728\u7684CAC\u633a\u597d\u7684\uff0c\u4f46\u8fd8\u6709\u6539\u8fdb\u7684\u7a7a\u95f4</p> <p>Although existing class-agnostic counting methods have achieved good performance, there is still much room for improvement.</p> <p>CAC\u6587\u732e\u6982\u8ff0</p> <p>\u7b2c\u4e00\u4e2aCAC\u5f15\u7528\uff1aGMN</p> <p>For example, GMN [16] resizes the given exemplars to a fixed size and then calculates the distance between the exemplar\u2019s feature and local regions of the query image to localize the object of interest. One problem in this process is that exemplar features will lose the scale information provided by the exemplar\u2019s size.  \u95ee\u9898\u5728\u4e8e\uff1a\u635f\u5931\u4e86\u793a\u4f8b\u7684\u5c3a\u5bf8\u4fe1\u606f</p> <p>\u7b2c\u4e8c\u4e2aCAC\u5f15\u7528\uff1aBMNet [26] </p> <p>Although BMNet [26] adds a scale embedding to its network to tackle this problem, its function is not intuitive. BMNet\u89e3\u51b3\u901a\u8fc7 \u5c3a\u5ea6\u5d4c\u5165 \u89e3\u51b3GMN\u7684\u95ee\u9898\uff1b\u4f46\u662f\u5e76\u4e0d\u76f4\u89c2</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#p3","title":"P3 \u5f15\u51fa\u5c3a\u5ea6\u4f18\u5148\u53ef\u53d8\u5377\u79ef","text":"<p>\u4e0a\u9762\u63d0\u51fa\u95ee\u9898\uff0c\u63a5\u4e0b\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u5229\u7528\u5c3a\u5ea6\u4fe1\u606f\uff0c\u63d0\u51fa\u5c3a\u5ea6\u4f18\u5148\u53ef\u53d8\u5377\u79ef\uff0c\u63d0\u53d6\u7279\u5b9a\u5c3a\u5bf8\u7684\u76ee\u6807\u7279\u5f81</p> <p>To take advantage of scale information, we design a Scale-Prior Deformable Convolution Network (SPDCN) to extract features of objects with specific size. </p> <p>\u5177\u4f53\u600e\u4e48\u5b9e\u73b0\u7684</p> <p>SPDCN embeds the scale information into the deformable convolution, so that its receptive field is adjusted automatically and extracts features corresponding to the scale of the given exemplars. </p> <p>SPDCN\u628a\u5c3a\u5ea6\u4fe1\u606f\u5d4c\u5165\u5230\u53ef\u53d8\u5377\u79ef\u4e2d\uff0c\u8fd9\u6837\u611f\u53d7\u91ce\u5c31\u80fd\u81ea\u9002\u5e94\u7684\u8c03\u6574\uff1b\u5e76\u4e14\u63d0\u53d6\u7ed9\u5b9a\u793a\u4f8b\u5c3a\u5ea6\u7279\u5f81</p> <p>This design significantly boosts the counting performance because objects in the same category typically have similar scale in an image, whereas different object categories may have vastly different scales. </p> <p>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u663e\u8457\u63d0\u9ad8\u8ba1\u6570\u6027\u80fd\uff0c\u56e0\u4e3a\u76f8\u540c\u7c7b\u522b\u7684\u76ee\u6807\u5728\u540c\u4e00\u5f20\u56fe\u7247\u4e0a\u7684\u5c3a\u5bf8\u662f\u76f8\u540c\u7684\uff1b\u7136\u800c\u4e0d\u540c\u7c7b\u522b\u7684\u76ee\u6807\u7269\u4f53\u5c3a\u5bf8\u53d8\u5316\u662f\u5f88\u5927\u7684</p> <p>With the extracted features, SPDCN then computes the similarity between exemplars and query images to segment out regions containing the counted objects. After that, the generated similarity map and features are sent to a decoder to estimate the density map.</p> <p>\u7136\u540e\uff0c\u5229\u7528\u63d0\u53d6\u7684\u7279\u5f81\uff0cSPDCN\u8ba1\u7b97\u793a\u4f8b\u56fe\u50cf\u548c\u67e5\u8be2\u56fe\u50cf\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c\u4ee5\u5206\u5272\u51fa\u5305\u542b\u88ab\u7edf\u8ba1\u5bf9\u8c61\u7684\u533a\u57df\u3002\u4e4b\u540e\uff0c\u5c06\u751f\u6210\u7684\u76f8\u4f3c\u5ea6\u56fe\u548c\u7279\u5f81\u9001\u5165\u89e3\u7801\u5668\u6765\u4f30\u8ba1\u5bc6\u5ea6\u56fe\u3002 </p> <p>SPDCN \u4e3b\u8981\u5904\u7406\u7684\u5c31\u662f\u6837\u4f8b\u6846\u5c3a\u5bf8\u53d8\u5316\u6bd4\u8f83\u5927\u7684\u95ee\u9898\uff0c\u8fd9\u91cc\u7528\u7684\u8bcd\uff1a\u5206\u5272</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#p4-scale-sensitive-generalized-loss","title":"P4  \u6307\u51fa \u6211\u4eec\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570 scale-sensitive generalized loss","text":"<p>We apply the generalized loss [31] to train SPDCN. However, we find that the vanilla generalized loss is unsuitable for class-agnostic counting because its cost function assumes all objects (people) are the same size, whereas in class-agnostic counting, different object categories have different scales. To tackle this problem, we propose a scale-sensitive generalized loss, in which the cost function is adjusted adaptively based on the object scale. Experiments show that the performance is further improved with our adaptive loss function.</p> <p>P5 \u8d21\u732e \u60f3\u770b\u70b9\u5de6\u4fa7\u76ee\u5f55\u8df3\u8f6c</p> <p>Info</p> <p>\u5f15\u5165\u90e8\u5206\u7684\u5199\u4f5c\u903b\u8f91</p> <ol> <li>\u7279\u5b9a\u7c7b\u522b\u7684\u76ee\u6807\u8ba1\u6570   </li> <li>CAC\u8ba1\u6570   </li> <li>\u5f15\u51fa\u53ef\u53d8\u5377\u79ef    </li> <li>\u5f15\u51fa \u5c3a\u5ea6\u654f\u611f\u7684\u635f\u5931    </li> <li>\u8d21\u732e   </li> </ol>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_7","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>Info</p> <p>\u5206\u6210\u7684\u4e09\u90e8\u5206\uff1a</p> <p>Class-Agnostic Counting. \u7c7b\u522b\u4e0d\u654f\u611f\u8ba1\u6570</p> <p>Deformable Convolution. \u53ef\u53d8\u5377\u79ef</p> <p>Generalized Loss. \u6cdb\u5316\u635f\u5931</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#sepcific-cac","title":"\u7b2c\u4e00\u6bb5\uff1a\u4ecesepcific \u2192 CAC","text":"<p>Class-Agnostic Counting. </p> <p>\u7279\u5b9a\u7c7b\u522b\u8ba1\u6570</p> <p>Previous counting tasks mainly aim at counting objects in a specific category. </p> <p>The most popular task is crowd counting [17, 19, 29, 31, 32, 34, 35]. Vehicle [11, 20], cell [8, 9] and animal [24] counting also attract researchers\u2019 attention, and are applied in various aspects like vehicular management [33], medical research [4], wildlife conservation [1], and so on.</p> <p>However, only a few methods have considered class-agnostic object counting, and relevant datasets are rare. </p> <p>CAC\u8ba1\u6570</p> <p>FamNet</p> <p>FamNet [23] defines class-agnostic counting as predicting the number of given objects represented by only a few exemplars in the same image and constructs the first dataset called FSC-147 [23]. Its baseline model is designed based on self-similarities matching [25]. One problem is that the scale of exemplars is modeled by the kernel size, which is normally too large to compute, so FamNet freezes the parameters in the extractor to overcome this problem. </p> <p>GMN</p> <p>Another similar work is the generic matching network (GMN) [16], which encodes the semantic feature of exemplars to an embedding, and then uses a matching network to model the relation between the exemplar embedding and the image\u2019s feature maps. However, GMN does not consider the scale problem because the size of the embedding vector is fixed. </p> <p>BMNet</p> <p>BMNet [26] considers the scale problem and adds scale embedding into its model, but it is not intuitive. </p> <p>Our SPDCN</p> <p>Compared with these previous works, our proposed SPDCN embeds scale information into the deformable convolution so that the extracted feature can match the exemplar more accurately, yielding improved performance.</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_8","title":"\u7b2c\u4e8c\u6bb5 \u53ef\u53d8\u5377\u79ef\u7684\u76f8\u5173\u5de5\u4f5c","text":"<p>Deformable Convolution. </p> <p>\u56e0\u4e3a\u672c\u6587\u7528\u5230\u4e86\u53ef\u53d8\u5377\u79ef\uff0c\u6240\u4ee5\u4ecb\u7ecd\u4e86\u53ef\u53d8\u5377\u79ef\u7684\u76f8\u5173\u5de5\u4f5c</p> <p>Deformable convolution [2, 38] was proposed for modeling geometric transformations dynamically, and has been applied to video super-resolution [30], font generation [36] and other computer vision tasks. \u53ef\u53d8\u5377\u79ef\u7528\u6765\u5e72\u5565\u7684\uff1a\u52a8\u6001\u5efa\u6a21\u51e0\u4f55\u53d8\u6362\uff0c\u5df2\u7ecf\u5e94\u7528\u4e8e\u89c6\u9891\u8d85\u5206\u8fa8\u7387[ 30 ]\u3001\u5b57\u4f53\u751f\u6210[ 36 ]\u7b49\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u6ca1\u4e86\u89e3\u8fc7 \u53ef\u53d8\u5377\u79ef</p> <p>Compared to the previous works, we introduce scale-prior deformable convolution to class-agnostic counting, where the receptive fields of the counting network are adjusted according to the given exemplars.\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u6211\u4eec\u5c06\u5c3a\u5ea6\u5148\u9a8c\u53ef\u53d8\u5f62\u5377\u79ef\u5f15\u5165\u5230\u7c7b\u522b\u65e0\u5173\u8ba1\u6570\u4e2d\uff0c\u5176\u4e2d\u8ba1\u6570\u7f51\u7edc\u7684\u611f\u53d7\u91ce\u6839\u636e\u7ed9\u5b9a\u7684\u6837\u672c\u8fdb\u884c\u8c03\u6574\u3002</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_9","title":"\u7b2c\u4e09\u6bb5 \u5173\u4e8e\u635f\u5931\u7684\u989d\u76f8\u5173\u5de5\u4f5c","text":"<p>Generalized Loss. </p> <p>\uff08\u4e4b\u524d\u4eba\u7684\u5de5\u4f5c\uff09</p> <p>The generalized loss [31] is designed based on the unbalanced optimal transport (UOT) problem. [31] prove that both L2 loss and Bayesian loss [18] are special cases of the generalized loss. </p> <p>\uff08\u4f5c\u8005\u7684\u5de5\u4f5c\uff09</p> <p>In contrast to [31], which uses a fixed cost function assuming all objects are similar sizes, we propose a scale-sensitive generalized loss for class-agnostic counting, where different object categories have different sizes. Experimental results show that class-agnostic counting models perform better with the scale-sensitive generalized loss, compared to the original version.</p> <ul> <li>\u6211\u4eec\u6ca1\u6709\u91c7\u7528\u56fa\u5b9a\u4e0d\u53d8\u7684\u635f\u5931\u51fd\u6570\u3001\u4e3a\u76f8\u4f3c\u5c3a\u5bf8\u7684\u6240\u6709\u76ee\u6807</li> <li>\u6211\u4eec\u63d0\u51fa\u5c3a\u5ea6\u654f\u611f\u7684 \u6cdb\u5316\u635f\u5931\uff0c\u4e0d\u540c\u7c7b\u7684\u76ee\u6807 \u6709 \u4e0d\u540c\u7684\u5c3a\u5bf8\uff0c\u6240\u4ee5\u635f\u5931\u51fd\u6570\u4e5f\u4e0d\u4e00\u6837</li> <li>\u7ed3\u679c</li> </ul>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/","title":"rank11 GCA SUN","text":"<p>2024\u5e74\u7684\u65b0\u6587\u7ae0\uff0c\u53ef\u4ee5\u770b\u770b</p> <p>\u6e90\u7801\u672a\u516c\u5f00\uff0c\u7b80\u5355\u770b\u770b</p> <p></p> <p>arxiv\u65e5\u671f\uff1a2024\u5e749\u670818\u65e5</p> <p>\u4e00\u773c\u6807\u9898\uff1abuff\u53e0\u6ee1\u4e86\uff0cSwinTransformer &amp; Unet</p> <p>\u4f5c\u8005\uff1a</p> <p>18 Sep 2024 \u00b7 Yuzhe Wu, Yipeng Xu, Tianyu Xu, Jialu Zhang, Jianfeng Ren, Xudong Jiang </p> <p>\u5b81\u6ce2\u8bfa\u4e01\u6c49\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u9662</p> <p>\u5b81\u6ce2\u8bfa\u4e01\u6c49\u5927\u5b66\u5353\u8d8a\u7814\u7a76\u521b\u65b0\u4e2d\u5fc3</p> <p>\u65b0\u52a0\u5761\u5357\u6d0b\u7406\u5de5\u5927\u5b66\u7535\u6c14\u4e0e\u7535\u5b50\u5de5\u7a0b\u5b66\u9662</p> <ul> <li> \u6b63\u5f0f\u53d1\u8868\u65e5\u671f\uff1a</li> </ul> <p>GCA \u8ba9\u6211\u60f3\u8d77\u6765 \u7ea7\u8054\u6ce8\u610f\u529b</p> <ul> <li> \u4e3a\u4ec0\u4e482024\u5e74\u7684\u6587\u7ae0\uff0c\u6392\u540d\u8fd8\u8fd9\u4e48\u4f4e\uff1f</li> <li> \u671f\u520a\uff1f</li> </ul> <p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u672a\u516c\u5f00</p> <p>\u6807\u9898\uff1aGCA-SUN: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting </p> <p>\u95e8\u63a7\u4e0a\u4e0b\u6587\u611f\u77e5\u3001Swin-UNet\u67b6\u6784\u3001Exemplar-Free Counting\uff08\u5c31\u662f0-shot\u95ee\u9898\uff09</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_1","title":"\u6458\u8981","text":"<p>\uff08\u672c\u6587\u4e3b\u9898\uff1aExemplar-Free Counting\uff09Exemplar-Free Counting aims to count objects of interest without intensive annotations of objects or exemplars. </p> <p>\u672c\u6587\u7684\u7b2c\u4e00\u4e2a\u63d0\u51fa\uff1a\u4e3a\u4e86\u5b9e\u73b0 Exemplar-Free Counting\uff0c\u63d0\u51fa Gated Context-Aware Swin-UNet (GCA-SUN) </p> <p>To achieve this, we propose Gated Context-Aware Swin-UNet (GCA-SUN) to directly map an input image to the density map of countable objects. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u95e8\u63a7\u4e0a\u4e0b\u6587\u611f\u77e5Swin - UNet ( GCA-SUN )\uff0c\u5c06\u8f93\u5165\u56fe\u50cf\u76f4\u63a5\u6620\u5c04\u4e3a\u53ef\u6570\u7269\u4f53\u7684\u5bc6\u5ea6\u56fe</p> <p>Swin-UNet \u7684\u529f\u80fd \u4e00\u53e5\u8bdd\u8bf4\u660e\uff1a\u76f4\u63a5\u5c06\u8f93\u5165\u56fe\u50cf\u6620\u5c04\u5230\u5bc6\u5ea6\u56fe</p> <p>\u5c55\u5f00\u8bf4\u8bf4</p> <p>Specifically, a Gated Context-Aware Modulation module is designed in the encoder to suppress irrelevant objects or background through a gate mechanism and exploit the attentive support of objects of interest through a self-similarity matrix.</p> <p>\u5728\u7f16\u7801\u5668\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u95e8\u63a7\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5236\u6a21\u5757\uff0c\u901a\u8fc7\u95e8\u673a\u5236\u6765\u6291\u5236\u65e0\u5173\u5bf9\u8c61\u6216\u80cc\u666f\uff0c\u5e76\u901a\u8fc7\u81ea\u76f8\u4f3c\u77e9\u9635\u6765\u5229\u7528\u611f\u5174\u8da3\u5bf9\u8c61\u7684\u6ce8\u610f\u529b\u652f\u6301\u3002</p> <p>The gate strategy is also incorporated into the bottleneck network and the decoder to highlight the features most relevant to objects of interest.\u95e8\u7b56\u7565\u4e5f\u88ab\u7eb3\u5165\u5230\u74f6\u9888\u7f51\u7edc\u548c\u89e3\u7801\u5668\u4e2d\uff0c\u4ee5\u7a81\u51fa\u4e0e\u611f\u5174\u8da3\u5bf9\u8c61\u6700\u76f8\u5173\u7684\u7279\u5f81\u3002</p> <p>By explicitly exploiting the attentive support among countable objects and eliminating irrelevant features through the gate mechanisms, the proposed GCA-SUN focuses on and counts objects of interest without relying on predefined categories or exemplars.</p> <p>\u901a\u8fc7\u663e\u5f0f\u5730\u5229\u7528\u53ef\u6570\u5bf9\u8c61\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u652f\u6301\u548c\u901a\u8fc7\u95e8\u673a\u5236\u6d88\u9664\u65e0\u5173\u7279\u5f81\uff0cGCA - SUN\u5728\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u7c7b\u522b\u6216\u793a\u4f8b\u7684\u60c5\u51b5\u4e0b\u5173\u6ce8\u548c\u8ba1\u6570\u611f\u5174\u8da3\u7684\u5bf9\u8c61\u3002</p> <p>\u7ed3\u679c\uff09</p> <p>Experimental results on the FSC-147 and CARPK datasets demonstrate that GCA-SUN outperforms state-of-the-art methods. </p> <p>\u6240\u7528\u6570\u636e\u96c6\uff1aFSC147\u3001CARPK</p> <p>Index Terms\u2014Object counting, Exemplar-free counting, Gate mechanism, Self-similarity matrix</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_2","title":"\u5f15\u5165\u2014\u8d21\u732e","text":"<p>Our contributions can be summarized as follows. </p> <ol> <li>The proposed GCA-SUN achieves exemplar-free counting through a UNet-like architecture that utilizes Swin transformer blocks for feature encoding and decoding, avoiding the sample bias of exemplar-based approaches [11].   \u6240\u63d0\u51fa\u7684GCA - SUN\u901a\u8fc7\u7c7bUNet\u7ed3\u6784\u5b9e\u73b0\u4e86\u65e0\u6837\u672c\u8ba1\u6570\uff0c\u8be5\u7ed3\u6784\u5229\u7528Swin\u53d8\u6362\u5757\u8fdb\u884c\u7279\u5f81\u7f16\u7801\u548c\u89e3\u7801\uff0c\u907f\u514d\u4e86\u6837\u672c\u504f\u5dee\uff08EFC\u8ba1\u6570\uff09</li> <li>The proposed GCAM exploits attentive support of repetitive objects through the self similarity matrix, to focus on countable objects. \u63d0\u51fa\u7684GCAM\u901a\u8fc7\u81ea\u76f8\u4f3c\u77e9\u9635\u5229\u7528\u5bf9\u91cd\u590d\u5bf9\u8c61\u7684\u7ec6\u5fc3\u652f\u6301\uff0c\u805a\u7126\u4e8e\u53ef\u6570\u5bf9\u8c61\u3002</li> <li>The gate mechanism is integrated into various modules, e.g., GCAM, GEFS and GAFU, which suppresses the features of irrelevant objects or background while highlighting the most relevant features to countable objects.  \u5c06\u95e8\u673a\u5236\u96c6\u6210\u5230GCAM\u3001GEFS\u548cGAFU\u7b49\u6a21\u5757\u4e2d\uff0c\u5728\u7a81\u51fa\u4e0e\u53ef\u6570\u5bf9\u8c61\u6700\u76f8\u5173\u7684\u7279\u5f81\u7684\u540c\u65f6\uff0c\u6291\u5236\u65e0\u5173\u5bf9\u8c61\u6216\u80cc\u666f\u7684\u7279\u5f81\u3002</li> <li> <p>The proposed GCA-SUN is evaluated on the FSC-147 and CARPK datasets. It outperforms state-of-the-art methods for exemplar-free counting.  \u5728FSC - 147\u548cCARPK\u6570\u636e\u96c6\u4e0a\u5bf9\u63d0\u51fa\u7684GCA - SUN\u8fdb\u884c\u8bc4\u4f30\u3002\u5728\u65e0\u6837\u672c\u8ba1\u6570\u65b9\u9762\uff0c\u5b83\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002</p> </li> <li> <p>\u672c\u6587\u63d0\u51fa\u7684\u7f51\u7edc\u7ed3\u6784\uff1a GCA-SUN\u3001\u4f7f\u7528\u4e86SwinTransformer</p> </li> <li>\u6a21\u5757\uff1a GCAM</li> <li>\u95e8\u673a\u5236\uff1aGCAM, GEFS and GAFU</li> <li>\u6570\u636e\u96c6\uff1a FSC-147 and CARPK datasets</li> </ol>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_3","title":"\u7ed3\u8bba","text":"<p>\u4e00\u3001GCA-SUN</p> <p>The proposed GCA-SUN effectively tackles the problems of exemplar-free counting by using a Swin-UNet architecture to directly map the input image to the density map of countable objects. </p> <p>GCA - SUN\u901a\u8fc7\u4f7f\u7528Swin - UNet\u67b6\u6784\u5c06\u8f93\u5165\u56fe\u50cf\u76f4\u63a5\u6620\u5c04\u5230\u53ef\u6570\u7269\u4f53\u7684\u5bc6\u5ea6\u56fe\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u65e0\u6837\u672c\u8ba1\u6570\u95ee\u9898\u3002</p> <p>\u4e8c\u3001GCAM </p> <p>The proposed GCAM exploits the attention information among the tokens of repetitive objects through the self-similarity matrix, and suppresses the features of irrelevant objects through a gate mechanism.</p> <p>\u6240\u63d0\u51fa\u7684GCAM\u901a\u8fc7\u81ea\u76f8\u4f3c\u77e9\u9635\u6316\u6398\u91cd\u590d\u5bf9\u8c61\u6807\u8bb0\u95f4\u7684\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u95e8\u673a\u5236\u6291\u5236\u65e0\u5173\u5bf9\u8c61\u7684\u7279\u5f81\u3002</p> <p>\u4e09\u3001The gate mechanism &amp;  GEFS module  &amp; GAFU module</p> <p>The gate mechanism is also incorporated into the GEFS module and the GAFU module, which highlight the features most relevant to countable objects while suppressing irrelevant ones. </p> <p>\u95e8\u673a\u5236\u4e5f\u88ab\u7eb3\u5165\u5230GEFS\u6a21\u5757\u548cGAFU\u6a21\u5757\u4e2d\uff0c\u7a81\u51fa\u4e0e\u53ef\u6570\u5bf9\u8c61\u6700\u76f8\u5173\u7684\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u4e0d\u76f8\u5173\u7684\u7279\u5f81\u3002</p> <p>\u56db\u3001\u7ed3\u679c</p> <p>Our experiments on the FSC-147 and CARPK datasets demonstrate that GCASUN outperforms state-of-the-art methods, achieving superior performance in both intra-domain and cross-domain scenarios.</p> <p>\u5728FSC - 147\u548cCARPK\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGCASUN\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5728\u57df\u5185\u548c\u8de8\u57df\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_4","title":"\u5f15\u5165","text":""},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#exemplar-free","title":"\u7b2c\u4e00\u6bb5 \u76ee\u6807\u8ba1\u6570\u5206\u6210\u4e09\u7c7b\uff0c\u7279\u5b9a\u7c7b\u522b\u8ba1\u6570\u3001\u7c7b\u65e0\u5173\u8ba1\u6570\u3001exemplar-free\u8ba1\u6570","text":"<p>Object counting determines the number of instances of a specific object class in an image [1], e.g., vehicles [2], crowd [3], and cells [4]. It can be broadly categorized as:</p> <p>1) Class-Specific Counting (CSC), counting specific categories like fruits [5] and animals [6];  2) Class-Agnostic Counting (CAC), counting objects based on visual exemplars [1], [7], [8] or text prompts [9], [10];  3) Exemplar-Free Counting (EFC), counting objects without exemplars, presenting a significant challenge in discerning countable objects and determining their repetitions [8], [11], [12].</p> <p>Note</p> <p>CSC\u8ba1\u6570\u3001CAC\u8ba1\u6570\uff1b\u7279\u5b9a\u7c7b\u522b\u8ba1\u6570\u3001\u7c7b\u522b\u4e0d\u654f\u611f\u8ba1\u6570</p> <p>FSC\u8ba1\u6570\u3001ZSC\u8ba1\u6570\uff1b\u5c0f\u6837\u672c\u8ba1\u6570\u30010\u6837\u672c\u8ba1\u6570</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#exemplar-free-counting-efc","title":"\u7b2c\u4e8c\u6bb5   Exemplar-Free Counting (EFC)\u7684\u7814\u7a76\u73b0\u72b6","text":"<p>Exemplar-Free Counting shows promise for automated systems such as wildlife monitoring [13], healthcare [14], and anomaly detection [15]. </p> <p>Hobley and Prisacariu directly regressed the image-level features learned by attention modules into a density map [12]. </p> <p>CounTR [8] and LOCA [16] are originally designed for CAC tasks, but can be adapted to EFC tasks by using trainable components to simulate exemplars. </p> <p>RepRPN-Counter i==dentifies exemplars from region proposals by majority voting [11], and ==DAVE selects valuable objects using a strategy similar to majority voting based on [17].</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#gca-sunencoder-bottleneckdecoder","title":"\u7b2c\u4e09\u6bb5 GCA-SUN=encoder + bottleneck+decoder","text":"<p>\u73b0\u6709EFC\u8ba1\u6570\u5b58\u5728\u4e0d\u8db3 RepRPN-Counter </p> <p>Despite the advancements, existing models [8], [16], [17] often explicitly require exemplars to count similar objects.EFC methods such as RepRPN-Counter do not require exemplars but generate them through region proposal [11]. Either explicit or implicit exemplars may induce sample bias as exemplars can\u2019t cover the sample distribution.  \u7531\u4e8e\u6837\u4f8b\u65e0\u6cd5\u8986\u76d6\u6837\u672c\u5206\u5e03\uff0c\u4e0d\u8bba\u662f\u5916\u663e\u6837\u4f8b\u8fd8\u662f\u5185\u9690\u6837\u4f8b\u90fd\u53ef\u80fd\u5bfc\u81f4\u6837\u672c\u504f\u5dee\u3002</p> <p>\u95e8\u63a7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684 Gated Context-Aware Swin-UNet (GCA-SUN)\uff1b\u76f4\u63a5\u5c06\u8f93\u5165\u56fe\u7247\u6620\u5c04\u6210\u5bc6\u5ea6\u56fe\uff0c\u4e0d\u9700\u8981\u4efb\u4f55\u793a\u4f8b</p> <p>To address the challenge, we propose Gated Context-Aware Swin-UNet (GCA-SUN), which directly maps an input image to the density map of countable objects, without any exemplars. </p> <p>encoder\u5305\u542b\u4e24\u90e8\u5206\uff1a</p> <ul> <li>SwinTransformer \u63d0\u53d6\u7279\u5f81</li> <li>\u95e8\u63a7\u611f\u77e5\u6a21\u5757 </li> </ul> <p>Specifically, the encoder consists of a set of Swin Transformers to extract features, and Gated Context-Aware Modulation (GCAM) blocks to exploit the attentive supports of countable objects. </p> <p>bottleneck network  \u95e8\u63a7\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u589e\u5f3a encoder\u7279\u5f81 Gated Enhanced Feature Selector (GEFS)</p> <p>The bottleneck network includes a Gated Enhanced Feature Selector (GEFS) to emphasize the encoded features that are relevant to countable objects. </p> <p>decoder\uff1a</p> <ul> <li>SwinTransformer \u751f\u6210\u5bc6\u5ea6\u56fe\uff1a\u7ed3\u5408Gated Adaptive Fusion Units (GAFUs) \u95e8\u63a7\u9002\u5e94\u878d\u5408\u5355\u5143\uff0c\u6309\u7167\u4e0e\u76ee\u6807\u7684\u76f8\u5173\u5ea6\u8fdb\u884c\u52a0\u6743</li> <li>\u6700\u540e \u56de\u5f52\u5934 \u88ab\u4f7f\u7528\uff0c\u4ece\u52a0\u6743\u7279\u5f81\u4e2d\u4ea7\u751f\u5bc6\u5ea6\u56fe</li> </ul> <p>The decoder includes a set of Swin transformers for generating the density map, with the help of Gated Adaptive Fusion Units (GAFUs) to selectively weigh features based on their relevance to countable objects. Finally, a regression head is utilized to derive the density map from the aggregated features.</p> <p>\u603b\u7ed3\u8fd9\u6bb5</p> <ol> <li>Gated Context-Aware Swin-UNet (GCA-SUN)</li> <li>encoder= Swin Transformers +  Gated Context-Aware Modulation (GCAM) blocks </li> <li>bottleneck = Gated Enhanced Feature Selector (GEFS)</li> <li>decoder = Swin transformers + Gated Adaptive Fusion Units (GAFUs)</li> <li>regression head</li> </ol> <p>\u7528\u4e86\u5f88\u591a\u95e8\u63a7\u673a\u5236</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#gcam","title":"\u7b2c\u56db\u6bb5 GCAM","text":"<p>One key challenge in EFC is to effectively differentiate countable objects from other objects.  </p> <p>EFC\u7684\u4e00\u4e2a\u6311\u6218\u662f \u5982\u4f55\u6709\u6548\u7684\u4ece\u5176\u4ed6\u76ee\u6807\u4e2d \u533a\u5206\u51fa \u8ba1\u6570\u7269\u4f53\uff1b</p> <p>EFC\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u6709\u6548\u5730\u533a\u5206\u53ef\u6570\u5bf9\u8c61\u4e0e\u5176\u4ed6\u5bf9\u8c61</p> <p>The GCAM blocks tackle the challenge by first evaluating feature qualities by computing the feature score for each token, and then prioritizing those with informative content.</p> <p>GCAM\u6a21\u5757\u9996\u5148\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2atoken\u7684\u7279\u5f81\u5f97\u5206\u6765\u8bc4\u4f30\u7279\u5f81\u8d28\u91cf\uff0c\u7136\u540e\u4f18\u5148\u8003\u8651\u90a3\u4e9b\u5177\u6709\u4fe1\u606f\u542b\u91cf\u7684\u7279\u5f81\u3002</p> <p>GCAM\u6a21\u5757\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898</p> <p>GACM\u6a21\u5757\u7684\u529f\u80fd</p> <p>\u95ee\u9898\uff1aeffectively differentiate countable objects from other objects  Solution\uff1aGCAM</p> <p>In addition, GCAM computes pairwise similarities between tokens through a self-similarity matrix, exploiting the support of repeating objects in the same scene.</p> <p>\u6b64\u5916\uff0cGCAM\u901a\u8fc7\u81ea\u76f8\u4f3c\u77e9\u9635\u8ba1\u7b97token\u4e4b\u95f4\u7684\u6210\u5bf9\u76f8\u4f3c\u5ea6\uff0c\u5229\u7528\u540c\u4e00\u573a\u666f\u4e2d\u91cd\u590d\u5bf9\u8c61\u7684\u652f\u6301\u5ea6\u3002</p> <p>Lastly, a gate mechanism is incorporated to highlight the most relevant features while suppressing irrelevant ones.</p> <p>\u6700\u540e\uff0c\u5f15\u5165\u95e8\u673a\u5236\uff0c\u7a81\u51fa\u6700\u76f8\u5173\u7684\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u4e0d\u76f8\u5173\u7684\u7279\u5f81\u3002\u2018</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_5","title":"\u7b2c\u4e94\u6bb5","text":"<p>Another challenge is that foreground objects often share similar low-level features with background content. </p> <p>\u53e6\u4e00\u4e2a\u6311\u6218\u662f\uff0c\u524d\u666f\u5bf9\u8c61\u5f80\u5f80\u4e0e\u80cc\u666f\u5185\u5bb9\u5171\u4eab\u76f8\u4f3c\u7684\u4f4e\u7ea7\u7279\u5f81\u3002</p> <p>The skip connections directly fuse low-level features in the encoder with high-level semantics in the decoder, potentially impeding counting performance as the background information could disturb the foreground objects. </p> <p>\u8df3\u8dc3\u8fde\u63a5\u76f4\u63a5\u5c06\u7f16\u7801\u5668\u4e2d\u7684\u4f4e\u7ea7\u7279\u5f81\u4e0e\u89e3\u7801\u5668\u4e2d\u7684\u9ad8\u7ea7\u8bed\u4e49\u8fdb\u884c\u878d\u5408\uff0c\u7531\u4e8e\u80cc\u666f\u4fe1\u606f\u4f1a\u5bf9\u524d\u666f\u7269\u4f53\u4ea7\u751f\u5e72\u6270\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cd\u8ba1\u6570\u6027\u80fd\u3002</p> <p>To tackle this issue, gate mechanisms are incorporated into both GEFS and GAFU to suppress irrelevant low-level features while preserving as much information on objects of interest as possible. </p> <p>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728GEFS\u548cGAFU\u4e2d\u90fd\u878d\u5165\u4e86\u95e8\u673a\u5236\uff0c\u4ee5\u6291\u5236\u4e0d\u76f8\u5173\u7684\u4f4e\u7ea7\u7279\u5f81\uff0c\u540c\u65f6\u5c3d\u53ef\u80fd\u591a\u5730\u4fdd\u7559\u611f\u5174\u8da3\u5bf9\u8c61\u7684\u4fe1\u606f\u3002</p> <p>The former selectively enhances the compressed features at the bottleneck, and the latter filters the features in the decoder.</p> <p>\u524d\u8005\u9009\u62e9\u6027\u5730\u589e\u5f3a\u74f6\u9888\u5904\u7684\u538b\u7f29\u7279\u5f81\uff0c\u540e\u8005\u5728\u89e3\u7801\u5668\u4e2d\u8fc7\u6ee4\u7279\u5f81\u3002</p> <p>\u7b2c\u516d\u6bb5\uff1a\u8d21\u732e</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_6","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>\u6ca1\u6709\u76f8\u5173\u5de5\u4f5c\uff0c\u672c\u6587\u7684\u76ee\u5f55\u7ed3\u6784\uff1a</p> <p>\u6807\u9898\uff1aGCA-SUN: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting</p> <p>Abstract</p> <p>I. INTRODUCTION</p> <p>II. PROPOSED METHOD</p> <p>A. Overview of Proposed Method</p> <p>B. Swin-T Encoder with GCAM</p> <p>C. Bottleneck with GEFS</p> <p>D. Swin-T Decoder with GAFU</p> <p>III. EXPERIMENTAL RESULTS</p> <p>A. Experimental Settings</p> <p>B. Comparison with State-of-the-Art Methods</p> <p>C. Cross-Domain Evaluation on CARPK Dataset</p> <p>D. Visualization of GCAM </p> <p>E. Ablation Study</p> <p>IV. CONCLUSION</p> <p>24\u00b711\u00b719\uff1a\u7b2c\u4e00\u6b21\u8bfb\uff0c\u56de\u987e</p> <p>CSC\u8ba1\u6570\uff0c\u51e0\u4e2a\u8ba1\u6570\u7684\u7b80\u5199 \u672c\u6587\u95e8\u63a7\u673a\u5236\u591a \u5176\u5b9e\uff0c\u6ca1\u7528U-Net\uff1fSwinTransformer\u6bd4\u8f83\u7c7b\u4f3cU-Net</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/","title":"rank12 SAFECount","text":"<p>today\uff1a241119 TUE</p> <p></p> <p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>axriv\u65e5\u671f\uff1a2022\u5e749\u670811\u65e5</p> <p>\u4f5c\u8005\uff1a\u6e05\u534e\u5927\u5b66\u3001\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66</p> <p>\u4f1a\u8bae\uff1a</p> <p>WACV </p> <p>WACV\uff08IEEE Winter Conference on Applications of Computer Vision\uff09 IEEE\u51ac\u5b63\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4f1a\u8bae  \u4f1a\u8bae\uff08\u91cd\u8981\u4f1a\u8bae\uff09</p> <p>22 Jan 2022 \u00b7 Zhiyuan You, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le </p> <p>\u4e09\u5e74\u524d\u7684\u6587\u7ae0</p> <p></p> <p>Title\uff1aFew-shot Object Counting with Similarity-Aware Feature Enhancement</p> <p>\u57fa\u4e8e\u76f8\u4f3c\u6027\u611f\u77e5\u7279\u5f81\u589e\u5f3a\u7684\u5c11\u6837\u672c\u8ba1\u6570</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#_1","title":"\u6458\u8981","text":"<p>\uff08\u95ee\u9898\u5b9a\u4e49\uff09This work studies the problem of few-shot object counting, which counts the number of exemplar objects (i.e., described by one or several support images) occurring in the query image. </p> <p>\uff08\u6307\u51fa\u95ee\u9898\uff09The major challenge lies in that the target objects can be densely packed in the query image, making it hard to recognize every single one.</p> <p>\uff08\u89e3\u51b3\u95ee\u9898\uff09 To tackle the obstacle, we propose a novel learning block, equipped with a similarity comparison module and a feature enhancement module. </p> <p>\u63d0\u51fa\u76f8\u4f3c\u6027\u6bd4\u8f83\u6a21\u5757\u548c\u7279\u5f81\u589e\u5f3a\u6a21\u5757 \u89e3\u51b3 \u76ee\u6807\u5806\u53e0\u7684\u73b0\u8c61</p> <p>\uff08\u5177\u4f53\u6765\u8bf4\uff09Concretely, given a support image and a query image, we first derive a score map by comparing their projected features at every spatial position. </p> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u7ed9\u5b9a\u4e00\u5e45\u652f\u6301\u56fe\u50cf\u548c\u4e00\u5e45\u67e5\u8be2\u56fe\u50cf\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u6bd4\u8f83\u5b83\u4eec\u5728\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u4e0a\u7684\u6295\u5f71\u7279\u5f81\u5f97\u5230\u4e00\u4e2a\u5f97\u5206\u56fe\u3002</p> <p>The score maps regarding all support images are collected together and normalized across both the exemplar dimension and the spatial dimensions, producing a reliable similarity map. </p> <p>\u6240\u6709\u652f\u6301\u56fe\u50cf\u7684\u5f97\u5206\u56fe\u88ab\u6536\u96c6\u5728\u4e00\u8d77\uff0c\u5e76\u5728\u6837\u672c\u7ef4\u5ea6\u548c\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4ece\u800c\u4ea7\u751f\u53ef\u9760\u7684\u76f8\u4f3c\u5ea6\u56fe\u3002</p> <p>We then enhance the query feature with the support features by employing the developed point-wise similarities as the weighting coefficients. </p> <p>\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u70b9\u76f8\u4f3c\u6027\u4f5c\u4e3a\u52a0\u6743\u7cfb\u6570\uff0c\u7528\u652f\u6301\u7279\u5f81\u6765\u589e\u5f3a\u67e5\u8be2\u7279\u5f81\u3002</p> <p>Such a design encourages the model to inspect the query image by focusing more on the regions akin to the support images, leading to much clearer boundaries between different objects. </p> <p>\u8fd9\u6837\u7684\u8bbe\u8ba1\u9f13\u52b1\u6a21\u578b\u5bf9\u67e5\u8be2\u56fe\u50cf\u8fdb\u884c\u68c0\u67e5\uff0c\u66f4\u591a\u5730\u5173\u6ce8\u4e0e\u652f\u6301\u56fe\u50cf\u76f8\u4f3c\u7684\u533a\u57df\uff0c\u4f7f\u5f97\u4e0d\u540c\u5bf9\u8c61\u4e4b\u95f4\u7684\u8fb9\u754c\u66f4\u52a0\u6e05\u6670\u3002</p> <p>\uff08\u7ed3\u679c\uff09Extensive experiments on various benchmarks and training setups suggest that we surpass the state-of-the-art methods by a sufficiently large margin. For instance, on a recent large-scale FSC-147 dataset, we surpass the state-of-the-art method by improving the mean absolute error from 22.08 to 14.32 (35%\u2191). Code has been released here.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#intro-contribution","title":"intro-contribution","text":"<p>In this work, we propose a S imilarity-A ware F eature  E  nhancement block for object Counting (SAFECount). </p> <p>\u76f8\u4f3c\u6027\u611f\u77e5\u7279\u5f81\u589e\u5f3a\u6a21\u5757</p> <p>As discussed above, feature is more informative while similarity better captures the support-query relationship. Our novel block adequately integrates both of the advantages by exploiting similarity as a guidance to enhance the features for regression. Intuitively, the enhanced feature not only carries the rich semantics extracted from the image, but also gets aware of which regions within the query image are similar to the exemplar object. Specifically, we come up with a similarity comparison module (SCM) and a feature enhancement module (FEM), as illustrated in Fig. 2c. On one hand, different from the naive feature comparison in Fig. 2b, our SCM learns a feature projection, then performs a comparison on the projected features to derive a score map. This design helps select from features the information that is most appropriate for object counting. After the comparison, we derive a reliable similarity map by collecting the score maps with respect to all support images (i.e., few-shot) and normalizing them along both the exemplar dimension and the spatial dimensions. On the other hand, the FEM takes the point-wise similarities as the weighting coefficients, and fuses the support features into the query feature. Such a fusion is able to make the enhanced query feature focus more on the regions akin to the exemplar object defined by support images, facilitating more precise counting.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#conclusion","title":"Conclusion","text":""},{"location":"literature/ObejectCounting/rank12%20SAFECount/#intro","title":"intro","text":""},{"location":"literature/ObejectCounting/rank12%20SAFECount/#p1-specific-object-class-csc","title":"P1 specific object class  CSC\u8ba1\u6570","text":"<p>Object counting [3, 4], which aims at investigating how many times a certain object occurs in the query image, has received growing attention due to its practical usage [8, 13, 19, 51]. Most existing studies assume that the object to count at the test stage is covered by the training data [1, 10, 11, 19, 31, 50, 51]. As a result, each learned model can only handle a specific object class, greatly limiting its application.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#p2-fsc","title":"P2  FSC\u8ba1\u6570 \u5b9a\u4e49","text":"<p>To alleviate the generalization problem, few-shot object counting (FSC) is recently introduced [24]. Instead of predefining a common object that is shared by all training images, FSC allows users to customize the object of their own interests with a few support images, as shown in Fig. 1. In this way, we can use a single model to unify the counting of various objects, and even adapt the model to novel classes (i.e., unseen in the training phase) without any retraining.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#p3-fsc","title":"P3  FSC\u65b9\u6cd5\u6982\u8ff0","text":"<p>A popular solution to FSC is to first represent both the exemplar object (i.e. the support image) and the query image with expressive features, and then pinpoint the candidates via analyzing the feature correlation [20, 24, 46].  FSC\u7684\u4e00\u4e2a\u6d41\u884c\u7684\u89e3\u51b3\u65b9\u6848\u662f\u9996\u5148\u5c06\u793a\u4f8b\u5bf9\u8c61(\u5373\u652f\u6491\u56fe\u50cf)\u548c\u67e5\u8be2\u56fe\u50cf\u90fd\u8868\u793a\u4e3a\u5177\u6709\u8868\u8fbe\u6027\u7684\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u5206\u6790\u7279\u5f81\u76f8\u5173\u6027[ 20\u300124\u300146 ]\u6765\u786e\u5b9a\u5019\u9009\u5bf9\u8c61\u3002</p> <p>Active attempts roughly fall into two folds. One is feature based [20], as shown in Fig. 2a, where the pooled support feature is concatenated onto the query feature, followed by a regress head to recognize whether the two features are close enough. </p> <p>\u5c1d\u8bd5\u5927\u81f4\u5206\u4e3a\u4e24\u79cd\u3002\u4e00\u79cd\u662f\u57fa\u4e8e\u7279\u5f81\u7684\u65b9\u6cd5[ 20 ]\uff0c\u5982\u56fe2a\u6240\u793a\uff0c\u5176\u4e2d\u6c60\u5316\u7684\u652f\u6301\u7279\u5f81\u88ab\u8fde\u63a5\u5230\u67e5\u8be2\u7279\u5f81\u4e0a\uff0c\u7136\u540e\u662f\u4e00\u4e2a\u56de\u5f52\u5934\u6765\u8bc6\u522b\u4e24\u4e2a\u7279\u5f81\u662f\u5426\u8db3\u591f\u63a5\u8fd1\u3002</p> <p>\u7a7a\u95f4\u4fe1\u606f\u7531\u4e8e\u6c60\u5316\u6ca1\u6709\u4e86</p> <p>However, the spatial information of the support image is omitted by pooling, leaving the feature comparison unreliable. </p> <p></p> <p>The other is similarity-based [24,46], as shown in Fig. 2b, where a similarity map is developed from raw features as the regression object. Nevertheless, the similarity is far less informative than feature, making it hard to identify clear boundaries between objects (see Fig. 5). Accordingly, the counting performance heavily deteriorates when the target objects are densely packed in the query image, like the shoal of fish in Fig. 1.</p> <p>\u53e6\u4e00\u79cd\u662f\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684[ 24\u300146 ]\uff0c\u5982\u56fe2b\u6240\u793a\uff0c\u5176\u4e2d\u76f8\u4f3c\u6027\u56fe\u662f\u7531\u539f\u59cb\u7279\u5f81\u4f5c\u4e3a\u56de\u5f52\u5bf9\u8c61\u5f00\u53d1\u7684\u3002\u7136\u800c\uff0c\u76f8\u4f3c\u5ea6\u7684\u4fe1\u606f\u91cf\u8fdc\u4e0d\u5982\u7279\u5f81\uff0c\u8fd9\u4f7f\u5f97(\u89c1\u56fe5)\u7684\u5bf9\u8c61\u4e4b\u95f4\u5f88\u96be\u8bc6\u522b\u51fa\u6e05\u6670\u7684\u8fb9\u754c\u3002\u56e0\u6b64\uff0c\u5f53\u76ee\u6807\u7269\u4f53\u5728\u67e5\u8be2\u56fe\u50cf\u4e2d\u5bc6\u96c6\u6392\u5217\u65f6\uff0c\u8ba1\u6570\u6027\u80fd\u4e25\u91cd\u6076\u5316\uff0c\u5982\u56fe1\u4e2d\u7684\u9c7c\u7fa4\u3002</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#p4","title":"P4\u8d21\u732e","text":"<p>P5\uff1aFSC147\u6570\u636e\u96c6 &amp; CARPK\u6570\u636e\u96c6</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#related-work","title":"related work","text":"<p>\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206</p> <ul> <li>\u7279\u5b9a\u7269\u4f53\u8ba1\u6570</li> <li>FSC \u5c0f\u6837\u672c\u8ba1\u6570</li> <li>\u5c0f\u6837\u672c\u5b66\u4e60</li> </ul>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#csc","title":"\u7b2c\u4e00\u90e8\u5206\uff1aCSC \u8ba1\u6570","text":"<p>CAC\u8ba1\u6570 &amp; CSC\u8ba1\u6570 \u8ba1\u6570\u7269\u4f53</p> <p>FSC\u8ba1\u6570 &amp; ZSC\u8ba1\u6570  \u8ba1\u6570\u65b9\u6cd5 density-based\uff1bdetection-based</p> <p>\u63cf\u8ff0\u73b0\u72b6 Class-specific object counting counts objects of a specific class, such as people [19, 31, 50, 51], animals [1], cars [10], among which crowd counting has been widely explored. For this purpose, traditional methods [14, 33, 38] count the number of people occurring in an image through person detection. </p> <p>\u51fa\u73b0\u95ee\u9898 However, object detection is not particularly designed for the counting task and hence shows unsatisfying performance when the crowd is thick. </p> <p>\u89e3\u51b3\u95ee\u9898 To address this issue, recent work [37] employs a deep model to predict the density map from the crowd image, where the sum over the density map gives the counting result [15]. Based on this thought, many attempts have been made to handle more complicated cases [2, 18, 23, 27\u201329, 42, 44, 47, 48, 51]. Some recent studies [31, 36] propose effective loss functions that help predict the position of each person precisely. </p> <p>\u51fa\u73b0\u95ee\u9898 However, all of these methods can only count objects regarding a particular class (e.g., person), making them hard to generalize. </p> <p>\u89e3\u51b3\u95ee\u9898 There are also some approaches targeting counting objects of multiple classes [13, 21, 32, 43]. In particular, Stahl et al. [32] propose to divide the query image into regions and regress the counting results with the inclusion-exclusion principle. Laradji et al. [13] formulate counting as a segmentation problem for better localization. Michel et al. [21] detect target objects and regress multi-class density maps simultaneously. Xu et al. [43] mitigate the mutual interference across various classes by proposing categoryattention module. </p> <p>\u8fd8\u662f\u4e0d\u80fd\u6570\u6ca1\u5728\u8bad\u7ec3\u96c6\u89c1\u8fc7\u7684 Nevertheless, they still can not handle the object classes beyond the training data.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#fsc","title":"\u7b2c\u4e8c\u90e8\u5206 FSC\u8ba1\u6570","text":"<p>Few-shot object counting (FSC) has recently been proposed [20, 24, 46] and presents a much stronger generalization ability. Instead of pre-knowing the type of object to count, FSC allows users to describe the exemplar object of their own interests with one or several support images. This setting makes the model highly flexible in that it does not require the test object to be covered by the training samples. </p> <p>In other words, a well-learned model could easily make inferences on novel classes (i.e., unseen in the training phase) as long as the support images are provided. To help the model dynamically get adapted to an arbitrary class, a great choice is to compare the object and the query image in feature space [20, 24, 46]. </p> <p>GMN [20] pools the support feature, and concatenates the pooling result onto the query feature, then learns a regression head for pointwise feature comparison. However, the comparison built on concatenation is not as reliable as the similarity [46]. </p> <p>Instead, CFOCNet [46] first performs feature comparison with dot production, and then regresses the density map from the similarity map derived before. </p> <p>FamNet [24] further improves the reliability of the similarity map through multi-scale augmentation and test-time adaptation. But similarities are far less informative than features, hence regressing from the similarity map fails to identify clear boundaries between the densely packed objects. In this work, we propose a similarity-aware feature enhancement block, which integrates the advantages of both features and similarities.</p>"},{"location":"literature/ObejectCounting/rank16%20Counting_DETR/","title":"rank16 Counting DETR","text":""},{"location":"literature/ObejectCounting/rank17%20RCC/","title":"rank17 RCC","text":""},{"location":"literature/ObejectCounting/rank18%20Omnicount/","title":"rank18 Omnicount","text":""},{"location":"literature/ObejectCounting/rank19%20FamNet/","title":"rank19 FamNet","text":""},{"location":"literature/ObejectCounting/rank2%20GeCo/","title":"rank2 GeCo","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>reading in 241117</p> <p>arxiv\u65e5\u671f\uff1a2024\u5e749\u670827\u65e5</p> <p>\u6807\u9898\uff1aA Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation</p> <p>\u4e0e\u5206\u5272\u6709\u4ec0\u4e48\u5173\u7cfb\uff1f</p>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#abstract","title":"Abstract","text":"<p>Low-shot object counters estimate the number of objects in an image using few or no annotated exemplars. \u95ee\u9898\u5b9a\u4e49</p> <p>Objects are localized by matching them to prototypes, which are constructed by unsupervised image-wide object appearance aggregation. </p> <p>Due to potentially diverse object appearances, the existing approaches often lead to over-generalization and false positive detections. </p> <p>**\u63d0\u51fa\u95ee\u9898\uff1a **   \u7531\u4e8e\u6f5c\u5728\u7684\u591a\u6837\u5316\u7684\u76ee\u6807\u5916\u89c2\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5f80\u5f80\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u6cdb\u5316\u548c\u8bef\u68c0\u3002</p> <p>Furthermore, the best-performing methods train object localization by a surrogate loss, that predicts a unit Gaussian at each object center. </p> <p>\u6b64\u5916\uff0c\u6027\u80fd\u6700\u597d\u7684\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u66ff\u4ee3\u635f\u5931\u6765\u8bad\u7ec3\u76ee\u6807\u5b9a\u4f4d\uff0c\u5373\u5728\u6bcf\u4e2a\u76ee\u6807\u4e2d\u5fc3\u9884\u6d4b\u4e00\u4e2a\u5355\u4f4d\u9ad8\u65af\u3002</p> <p>This loss is sensitive to annotation error, hyperparameters and does not directly optimize the detection task, leading to suboptimal counts. </p> <p>\u8fd9\u79cd\u635f\u5931\u5bf9\u6807\u6ce8\u9519\u8bef\u3001\u8d85\u53c2\u6570\u654f\u611f\uff0c\u5e76\u4e14\u6ca1\u6709\u76f4\u63a5\u4f18\u5316\u68c0\u6d4b\u4efb\u52a1\uff0c\u5bfc\u81f4\u6b21\u4f18\u8ba1\u6570\u3002</p> <p>We introduce GeCo, a novel low-shot counter that achieves accurate object detection, segmentation, and count estimation in a unified architecture. </p> <p>Note</p> <p>GeCo:\u5c0f\u6837\u672c\u8ba1\u6570\u5668\uff0c\u540c\u65f6\u662f\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u3001\u5206\u5272\u548c\u8ba1\u6570</p> <p>GeCo robustly generalizes the prototypes across objects appearances through a novel dense object query formulation. </p> <p>GeCo\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u7a20\u5bc6\u5bf9\u8c61\u67e5\u8be2\u516c\u5f0f\uff0c\u9c81\u68d2\u5730\u6982\u62ec\u4e86\u8de8\u8d8a\u5bf9\u8c61\u5916\u89c2\u7684\u539f\u578b\u3002</p> <p>In addition, a novel counting loss is proposed, that directly optimizes the detection task and avoids the issues of the standard surrogate loss. </p> <p>\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u6570\u635f\u5931\uff0c\u76f4\u63a5\u4f18\u5316\u4e86\u68c0\u6d4b\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u6807\u51c6\u66ff\u4ee3\u635f\u5931\u7684\u95ee\u9898\u3002</p> <p>\uff08\u8bf4\u7ed3\u679c\uff09GeCo surpasses the leading few-shot detection-based counters by \u223c25% in the total count MAE, achieves superior detection accuracy and sets a new solid state-of-the-art result across all low-shot counting setups. The code will be available on GitHub.</p> <p>GeCo\u5728\u603b\u8ba1\u6570MAE\u4e0a\u8d85\u8fc7\u4e86\u9886\u5148\u7684\u57fa\u4e8e\u5c0f\u6837\u672c\u68c0\u6d4b\u7684\u8ba1\u6570\u566825 %\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u6240\u6709\u7684\u5c0f\u6837\u672c\u8ba1\u6570\u8bbe\u7f6e\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u65b0\u7684\u56fa\u6001\u7ed3\u679c\u3002</p>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#introduction-contribution","title":"Introduction contribution","text":"<p>\uff08\u7b2c\u4e94\u6bb5\uff09 \u8d21\u732e</p> <p>We address the aforementioned challenges by proposing a new single-stage low-shot counter GeCo, which is implemented as an add-on network for SAM [12] backbone. </p> <ul> <li> <p>\u9488\u5bf9\u4e0a\u9762\u7684\u95ee\u9898\uff0c\u63d0\u51faGeCo </p> </li> <li> <p>\u4f18\u70b9\uff1a\u5355\u9636\u6bb5</p> </li> <li> <p>SAM\u662f\u5565\uff1f</p> </li> </ul> <p>A single architecture is thus trained for both few-shot and zero-shot setup, it enables counting by detection and provides segmentation masks for each of the detected objects. </p> <ul> <li>\u4f18\u70b9\uff1a\u5355\u9636\u6bb5\u68c0\u6d4b\u8ba1\u6570\u65b9\u6cd5\uff0c few-shot &amp; zero-shot \u90fd\u662f\u5355\u9636\u6bb5\u8ba1\u6570</li> <li>\u4e3a\u6bcf\u4e2a\u88ab\u68c0\u6d4b\u7684\u5bf9\u8c61\u63d0\u4f9b\u5206\u5272\u63a9\u7801</li> <li>\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5</li> </ul> <p>Our first contribution  is a dense object query formulation, which applies a non-parametric model for image-wide prototype generalization (hence GeCo) in the encoder, and decodes the queries into highly dense predictions.</p> <p>\u6211\u4eec\u7684\u7b2c\u4e00\u4e2a\u8d21\u732e\u662f\u4e00\u4e2a\u7a20\u5bc6\u5bf9\u8c61\u67e5\u8be2\u516c\u5f0f\uff0c\u5b83\u5728\u7f16\u7801\u5668\u4e2d\u5e94\u7528\u4e86\u4e00\u4e2a\u975e\u53c2\u6570\u6a21\u578b\u7528\u4e8e\u56fe\u50cf\u8303\u56f4\u7684\u539f\u578b\u6cdb\u5316(\u56e0\u6b64\u662fGeCo )\uff0c\u5e76\u5c06\u67e5\u8be2\u89e3\u7801\u4e3a\u9ad8\u5ea6\u7a20\u5bc6\u7684\u9884\u6d4b\u3002</p> <p>The formulation simultaneously enables reliable detection in densely-populated regions (Figure 1, column 3&amp;4) and prevents prototype over-generalization, leading to an improved detection precision at a high recall. </p> <p>\u8be5\u516c\u5f0f\u540c\u65f6\u5b9e\u73b0\u4e86\u5728\u4eba\u53e3\u5bc6\u96c6\u533a\u57df(\u56fe1\u7b2c3\u30014\u5217)\u7684\u53ef\u9760\u68c0\u6d4b\uff0c\u5e76\u9632\u6b62\u4e86\u539f\u578b\u7684\u8fc7\u5ea6\u6cdb\u5316\uff0c\u4ece\u800c\u5728\u9ad8\u53ec\u56de\u7387\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002</p> <p>Our second contribution  is a new loss function for dense detection training that avoids the ad-hoc surrogate loss with unit Gaussians, it directly optimizes the detection task, and leads to improved detection not biased towards blob-like regions (Figure 1, column 1&amp;2).</p> <p>\u6211\u4eec\u7684\u7b2c\u4e8c\u4e2a\u8d21\u732e\u662f\u4e00\u4e2a\u65b0\u7684\u7528\u4e8e\u5bc6\u96c6\u68c0\u6d4b\u8bad\u7ec3\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b83\u907f\u514d\u4e86\u4f7f\u7528\u5355\u4f4d\u9ad8\u65af\u7684ad - hoc\u4ee3\u7406\u635f\u5931\uff0c\u5b83\u76f4\u63a5\u4f18\u5316\u4e86\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u5bfc\u81f4\u6539\u8fdb\u7684\u68c0\u6d4b\u4e0d\u504f\u5411\u4e8e\u56e2\u5757\u72b6\u533a\u57df(\u56fe1\u7b2c1\u30012\u5217)\u3002</p> <p>\u8bf4\u660e\u4e24\u4e2a\u8d21\u732e</p> <ul> <li>\uff1f</li> <li>\u63d0\u51fa\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570</li> </ul> <p>\uff08\u7b2c\u516d\u6bb5\uff09  \u7ed3\u679c GeCo outperforms all detection-based counters on challenging benchmarks by 24% MAE and the density-based long-standing winner [4] by 27% MAE, while delivering superior detection accuracy. </p> <p>\u4f18\u4e8e\u6240\u6709 \u57fa\u4e8e\u68c0\u6d4b \u548c \u57fa\u4e8e\u5bc6\u5ea6\u7684 \u8ba1\u6570\u65b9\u6cd5</p> <p>The method shows substantial robustness to the number of exemplars. In one-shot scenario, GeCo outperforms the best detection method in 5% AP50, </p> <p>\u57281-shot\u573a\u666f\u7684\u68c0\u6d4b\u6027\u80fd</p> <p>45% MAE and by 14% in a zero-shot scenario. </p> <p>0-shot\u7684\u8ba1\u6570\u6027\u80fd</p> <p>GeCo is the first detection-based counter that outperforms density based counters in all measures by using the number of detections as the estimator, and thus sets a milestone in low-shot detection-based counting.</p>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#5-conclusion","title":"5 Conclusion","text":"<p>\u7b2c\u4e00\u6bb5 </p> <p>We proposed GeCo, a novel single-stage low-shot counter that integrates accurate detection, segmentation, and count prediction within a unified architecture, and covers all low-shot scenarios with a single trained model. </p> <p>GeCo</p> <ul> <li>\u5355\u9636\u6bb5\u8ba1\u6570\u65b9\u6cd5\u3001\u51c6\u786e\u7684\u68c0\u6d4b\u3001\u5206\u5272\u6027\u80fd</li> </ul> <p>GeCo features remarkables dense object query formulation, and prototype generalization across the image, rather than just into a few prototypes.</p> <p>\uff1fGe Co\u7684\u663e\u8457\u7279\u70b9\u662f\u5bf9\u8c61\u67e5\u8be2\u63cf\u8ff0\u5bc6\u96c6\uff0c\u539f\u578b\u6cdb\u5316\u904d\u5e03\u6574\u4e2a\u56fe\u50cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u51e0\u4e2a\u539f\u578b\u3002</p> <p>It employs a novel loss function specifically designed for detection tasks, avoiding the biases of traditional Gaussian-based losses. </p> <p>\uff1f\u5b83\u91c7\u7528\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a\u68c0\u6d4b\u4efb\u52a1\u8bbe\u8ba1\u7684\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u9ad8\u65af\u7684\u635f\u5931\u7684\u504f\u5dee\u3002</p> <p>The loss optimizes detection accuracy directly, leading to more precise detection and counting. </p> <p>\u8be5\u635f\u5931\u76f4\u63a5\u4f18\u5316\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5bfc\u81f4\u66f4\u7cbe\u786e\u7684\u68c0\u6d4b\u548c\u8ba1\u6570\u3002</p> <p>\u6307\u51fa\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411</p> <p>The main limitation of the presented method is that it cannot process arbitrarily large images, due to memory constraints, since it, as all current methods, operates globally. In future work, we will explore local counting and incremental image-wide count aggregation.</p> <p>\u672c\u6587\u65b9\u6cd5\u7684\u4e3b\u8981\u5c40\u9650\u6027\u5728\u4e8e\uff0c\u7531\u4e8e\u5185\u5b58\u9650\u5236\uff0c\u65e0\u6cd5\u5904\u7406\u4efb\u610f\u5927\u7684\u56fe\u50cf\uff0c\u56e0\u4e3a\u5b83\u4e0e\u73b0\u6709\u7684\u6240\u6709\u65b9\u6cd5\u4e00\u6837\uff0c\u662f\u5168\u5c40\u64cd\u4f5c\u7684\u3002\u5728\u672a\u6765\u7684\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u7d22\u5c40\u90e8\u8ba1\u6570\u548c\u589e\u91cf\u56fe\u50cf\u8303\u56f4\u5185\u7684\u8ba1\u6570\u805a\u5408\u3002</p> <p>\uff08\u7b2c\u4e8c\u6bb5\uff09  \uff08\u8bf4\u7ed3\u679c\u4e86\uff0c\u6ca1\u5565\u53ef\u770b\u7684\uff09 Extensive analysis showcases that GeCo surpasses the best detection-based counters by approximately 25% in total count MAE, achieving state-of-the-art performance in a few-shot counting setup and demonstrates superior detection capabilities. GeCo showcases remarkable robustness to the number of provided exemplars, and sets a new state-of-the-art in one-shot as well as zero-shot counting.</p> <p>\u5927\u91cf\u7684\u5206\u6790\u8868\u660e\uff0cGeCo\u5728\u603b\u8ba1\u6570MAE\u4e0a\u8d85\u8fc7\u4e86\u6700\u597d\u7684\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u5668\u7ea625 %\uff0c\u5728\u5c11\u91cf\u7684\u8ba1\u6570\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u793a\u51fa\u5353\u8d8a\u7684\u68c0\u6d4b\u80fd\u529b\u3002GeCo\u5bf9\u6240\u63d0\u4f9b\u7684\u6837\u672c\u6570\u91cf\u8868\u73b0\u51fa\u663e\u8457\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5355\u6837\u672c\u548c\u96f6\u6837\u672c\u8ba1\u6570\u4e2d\u8bbe\u7f6e\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002</p>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#1-introduction","title":"1 Introduction","text":"<p>\uff08\u7b2c\u4e00\u6bb5\uff09  Low-shot object counting considers estimating the number of objects of previously unobserved category in the image, given only a few annotated exemplars (few-shot) or without any supervision (zero-shot) [21]. The current state-of-the-art methods are predominantly based on density estimation [4; 14; 31; 25; 21; 30; 7; 30]. These methods predict a density map over the image and estimate the total count by summing the density.</p> <p>\u6700\u8fd1\u7684\u7814\u7a76\u90fd\u662f\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570</p> <p>\uff08\u7b2c\u4e8c\u6bb5\uff09   While being remarkably robust for global count estimation, density outputs lack explainability such as object location and size, which is crucial for many practical applications [32; 29]. This recently gave rise to detection-based low-shot counters [20; 19; 33], which predict the object bounding boxes and estimate the total count as the number of detections\uff08\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5\u662f\u5982\u4f55\u5177\u4f53\u5b9e\u73b0\u7684\uff1a\u9884\u6d4b\u8fb9\u754c\u6846\uff0c\u6570\u76d2\u5b50\u6570\uff09. Nevertheless, detection-based counting falls behind the density-based methods in total count estimation, leaving a performance gap.</p> <p>\uff08\u70b9\u660e\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff09\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u51c6\u786e\u6027\u6bd4\u8f83\u9ad8\uff0c\u4f46\u662f\u4e0d\u80fd\u7ed9\u51fa\u76ee\u6807\u7684\u5b9a\u4f4d\u548c\u5c3a\u5bf8</p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5\u80fd\u8be6\u7ec6\u7ed9\u51fa\u76ee\u6807\u7684\u4fe1\u606f\uff0c\u4f46\u662f\u8ba1\u6570\u51c6\u786e\u6027\u4e0d\u9ad8</p> <p>\uff08\u7b2c\u4e09\u6bb5\uff09   In detection-based counters, a dominant approach to identify locations of the objects in the image involves construction of object prototypes from few (e.g., three) annotated exemplar bounding boxes and correlating them with image features [20; 33; 19].</p> <p>\u5728\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u5668\u4e2d\uff0c\u8bc6\u522b\u56fe\u50cf\u4e2d\u7269\u4f53\u4f4d\u7f6e\u7684\u4e3b\u8981\u65b9\u6cd5\u662f\u901a\u8fc7\u5c11\u91cf\u7684(\u4f8b\u5982,\u4e09\u4e2a)\u5e26\u6ce8\u91ca\u7684\u6837\u672c\u8fb9\u754c\u6846\u6784\u5efa\u7269\u4f53\u539f\u578b\uff0c\u5e76\u5c06\u5176\u4e0e\u56fe\u50cf\u7279\u5f81[ 20 ; 33 ; 3 . 19 ]\u76f8\u5173\u8054\u3002</p> <p>The exemplar construction process is trained to account for potentially large diversity of object appearances in the image, often leading to overgeneralization, which achieves a high recall, but is also prone to false positive detection. </p> <p>\u6837\u4f8b\u6784\u5efa\u8fc7\u7a0b\u88ab\u8bad\u7ec3\u7528\u4e8e\u89e3\u91ca\u56fe\u50cf\u4e2d\u6f5c\u5728\u7684\u5de8\u5927\u7684\u7269\u4f53\u5916\u89c2\u591a\u6837\u6027\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u6cdb\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u53ec\u56de\u7387\uff0c\u4f46\u4e5f\u5bb9\u6613\u4ea7\u751f\u8bef\u68c0\u3002</p> <p>Note</p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff1a\u53ec\u56de\u7387\u592a\u9ad8\u4e86</p> <p>Post-hoc detection verification methods have been considered [20; 33] to address the issue, but their multi-stage formulation prevents exploiting the benefits of end-to-end training.</p> <p>\u4e8b\u540e\u68c0\u6d4b\u9a8c\u8bc1\u65b9\u6cd5\u4e00\u76f4\u88ab\u8ba4\u4e3a\u662f[ 20 ; 33]\u6765\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u7684\u591a\u9636\u6bb5\u5236\u5b9a\u963b\u6b62\u4e86\u5229\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u597d\u5904\u3002</p> <p>me\uff1a\u5148\u68c0\u6d4b\u518d\u9a8c\u8bc1\u633a\u597d\u7684\uff0c\u4f46\u662f \u4e0d\u662f\u5355\u4e00\u9636\u6bb5\u7684\uff0c\u4ed6\u8bf4\u7684\u662fDAVE\uff0c\u4e00\u4e2a\u4f5c\u8005\u7684\u5de5\u4f5c</p> <p></p> <p>\uff08\u7b2c\u56db\u6bb5\uff09    Currently, the best detection counters [20; 33] predict object locations based on the local maxima in the correlation map. </p> <p>\u76ee\u524d\uff0c\u6700\u597d\u7684\u68c0\u6d4b\u8ba1\u6570\u5668[ 20 ; 33]\u662f\u6839\u636e\u76f8\u5173\u56fe\u4e2d\u7684\u5c40\u90e8\u6781\u5927\u503c\u6765\u9884\u6d4b\u76ee\u6807\u4f4d\u7f6e\u3002</p> <p>During training, the map prediction is supervised by a unit Gaussian placed on each object center. </p> <p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u9884\u6d4b\u7531\u6bcf\u4e2a\u76ee\u6807\u4e2d\u5fc3\u7684\u5355\u4f4d\u9ad8\u65af\u5206\u5e03\u76d1\u7763</p> <p>However, the resulting surrogate loss is susceptible to the center annotation noise, requires nontrivial heuristic choice of the Gaussian kernel size and in practice leads to detection preference of compact blob-like structures (see Figure 1, column 1&amp;2). </p> <p>\u7136\u800c\uff0c\u7531\u6b64\u4ea7\u751f\u7684\u66ff\u4ee3\u635f\u5931\u5bb9\u6613\u53d7\u5230\u4e2d\u5fc3\u6807\u6ce8\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u9700\u8981\u975e\u5e73\u51e1\u7684\u542f\u53d1\u5f0f\u9009\u62e9\u9ad8\u65af\u6838\u5927\u5c0f\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u4e2d\u5bfc\u81f4\u7d27\u51d1\u7684\u5757\u72b6\u7ed3\u6784(\u89c1\u56fe1 ,\u52171\u548c2)\u7684\u68c0\u6d4b\u504f\u597d\u3002</p> <p>Recently, DETR [1] inspired counter was proposed to avoid this issue [19], however, it fails in densely populated regions even though it applies a very large number of detection queries in a regular grid (see Figure 1, column 3&amp;4).</p> <p>\u6700\u8fd1\uff0cDETR [ 1 ]\u542f\u53d1\u7684\u8ba1\u6570\u5668\u88ab\u63d0\u51fa\u6765\u907f\u514d\u8fd9\u4e2a\u95ee\u9898[ 19 ]\uff0c\u7136\u800c\uff0c\u5c3d\u7ba1\u5b83\u5728\u89c4\u5219\u7f51\u683c(\u89c1\u56fe1 ,\u7b2c3\u30014\u5217)\u4e2d\u5e94\u7528\u4e86\u5927\u91cf\u7684\u68c0\u6d4b\u67e5\u8be2\uff0c\u4f46\u5b83\u5728\u4eba\u53e3\u5bc6\u96c6\u7684\u5730\u533a\u5931\u8d25\u4e86\u3002</p> <p></p> <p>Figure 1: DAVE [20] predicts object centers (red dots) biased towards blob-like structures, leading to incorrect partial detections of ants (bottom left), while GeCo(ours) addresses this with the new loss (top left). \u56fe1\uff1aDAVE [ 20 ]\u9884\u6d4b\u7684\u76ee\u6807\u4e2d\u5fc3(\u7ea2\u8272\u5706\u70b9)\u504f\u5411\u4e8eblob - like\u7ed3\u6784\uff0c\u5bfc\u81f4\u9519\u8bef\u7684\u90e8\u5206\u8682\u8681\u68c0\u6d4b(\u5de6\u4e0b)\uff0c\u800cGeCo (\u6211\u4eec\u7684)\u7528\u65b0\u7684\u635f\u5931(\u5de6\u4e0a)\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</p> <p>CDETR [19] fails in densely populated regions (bottom right), while GeCo addresses this with the new dense query formulation by prototype generalization (top right). Exploiting the SAM backbone, GeCo delivers segmentations as well. Exemplars are denoted in blue.CDETR [ 19 ]\u5728\u4eba\u53e3\u7a20\u5bc6\u533a\u57df(\u53f3\u4e0b\u89d2)\u5931\u6548\uff0c\u800cGe Co\u901a\u8fc7\u539f\u578b\u6cdb\u5316(\u53f3\u4e0a\u89d2)\u7684\u65b0\u7684\u7a20\u5bc6\u67e5\u8be2\u5f62\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u5229\u7528SAM\u9aa8\u5e72\u7f51\uff0cGeCo\u4e5f\u63d0\u4f9b\u4e86\u5206\u6bb5\u3002\u56fe\u4f8b\u7528\u84dd\u8272\u8868\u793a\u3002</p> <p>\u8fd9\u4e00\u6bb5\u3001\u8fd9\u4e2a\u56fe\uff0c\u6211\u90fd\u770b\u4e0d\u61c2\uff1bGeCo\u6709\u4e00\u4e2a\u65b0\u635f\u5931\uff1bGeCo\uff0c\u5bf9\u6bd4\u4e86DAVE\uff08\u4ed6\u81ea\u5df1\u7684\u5de5\u4f5c\uff09\u3001CDETR\uff08\uff1f\uff09</p> <p>Summary</p> <p>\u95ee\u9898\u7684\u5f15\u5165\uff08\u80cc\u666f&amp;\u7814\u7a76\u610f\u4e49\uff09\uff1a</p> <ol> <li>Low-shot object counting</li> <li>\u57fa\u4e8e\u68c0\u6d4b &amp; \u57fa\u4e8e\u56de\u5f52\uff08\u8fd1\u6765\u4e3b\u6d41\uff0c\u4f46\u8ba1\u6570\u5c31\u4ec5\u4ec5\u662f\u8ba1\u6570\uff0c\u65e0\u6cd5\u7ed9\u51fa\u5173\u4e8e\u76ee\u6807\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\uff09</li> <li>\u57fa\u4e8e\u68c0\u6d4b</li> <li>\u57fa\u4e8e\u68c0\u6d4b</li> <li>\u672c\u6587\u8d21\u732e</li> <li>\u7ed3\u679c</li> </ol>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#2-related-works","title":"2 Related works","text":"<p>\u7b2c\u4e00\u6bb5 </p> <p>Traditional counting methods focus on predefined categories like vehicles[3], cells [5], people[15], and polyps, [32] requiring extensive annotated training data and lacking generalization to other categories, necessitating retraining or conceptual changes. Low-shot counting methods address this limitation by estimating counts for arbitrary categories with minimal or no annotations, enabling test-time adaptation.\u4f20\u7edf\u7684\u8ba1\u6570\u65b9\u6cd5\u96c6\u4e2d\u4e8e\u9884\u5b9a\u4e49\u7684\u7c7b\u522b\uff0c\u5982\u8f66\u8f86[ 3 ]\uff0c\u7ec6\u80de[ 5 ]\uff0c\u4eba[ 15 ]\u548c\u606f\u8089[ 32 ]\uff0c\u9700\u8981\u5927\u91cf\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u6982\u5ff5\u66f4\u6539\u3002\u4f4e\u6837\u672c\u8ba1\u6570\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u6700\u5c11\u6216\u6ca1\u6709\u6ce8\u91ca\u6765\u4f30\u8ba1\u4efb\u610f\u7c7b\u522b\u7684\u8ba1\u6570\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u95f4\u7684\u81ea\u9002\u5e94\u3002</p> <p>me\uff1a\u6307\u51fa\u4ece \u7279\u5b9a\u7269\u4f53 \u53d1\u5c55\u5230 \u901a\u7528\u7269\u4f53\u8ba1\u6570</p> <p>\u7b2c\u4e8c\u6bb5 </p> <p>With the proposal of the   FSC147   dataset [23] low-shot counting methods emerged, which predict global counts by summing over a predicted density maps. The first method [23] proposed an adaptation of a tracking backbone for density map regression. </p> <p>few-shot \u95ee\u9898\u7684\u7b2c\u4e00\u7bc7\u5de5\u4f5c\uff1aFSC147 \uff1b\u4e14\u662f\u57fa\u4e8e \u56de\u5f52\u7684</p> <p>BMNet+ [25] tackled learning representation and similarity metric, while SAFECount [31] introduced a new feature enhancement module, improving appearance generalization. CounTR [14] utilized a vision transformer for image feature extraction and a convolutional network for encoding the exemplar features. LOCA [4] argued that exemplar shape information should be considered along with the appearance, and proposed an iterative object prototype extraction module. This led to a simplified counter architecture that remains a top-performer among density-based counters.</p> <p>BMNet + [ 25 ]\u89e3\u51b3\u4e86\u5b66\u4e60\u8868\u793a\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u800cSAFECount [ 31 ]\u5f15\u5165\u4e86\u65b0\u7684\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u5916\u89c2\u6cdb\u5316\u6027\u3002Coun TR [ 14 ]\u4f7f\u7528\u89c6\u89c9\u8f6c\u6362\u5668\u8fdb\u884c\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\uff0c\u4f7f\u7528\u5377\u79ef\u7f51\u7edc\u5bf9\u6837\u672c\u7279\u5f81\u8fdb\u884c\u7f16\u7801\u3002LOCA [ 4 ]\u8ba4\u4e3a\u6837\u4f8b\u7684\u5f62\u72b6\u4fe1\u606f\u5e94\u8be5\u4e0e\u5916\u89c2\u4e00\u8d77\u8003\u8651\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fed\u4ee3\u7684\u5bf9\u8c61\u539f\u578b\u63d0\u53d6\u6a21\u5757\u3002\u8fd9\u5bfc\u81f4\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u8ba1\u6570\u5668\u67b6\u6784\uff0c\u5b83\u4ecd\u7136\u662f\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u5668\u4e2d\u7684\u4f7c\u4f7c\u8005\u3002</p> <p>Note</p> <p>5\u7bc7\u6587\u7ae0  1. \uff082021\u5e74\uff09FSC147\uff1a\u7b2c\u4e00\u7bc7  few-shot\u95ee\u9898\uff0c\u63d0\u51fa\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5 2. BMNet+ [25] 3. SAFECount [31] 4. CounTR [14]   ViT\u56fe\u7247\u7279\u5f81\u63d0\u53d6\uff1bCNN\u6837\u4f8b\u6846\u7279\u5f81 5. LOCA [4] \u5916\u89c2\u7279\u5f81\u3001\u5f62\u72b6\u7279\u5f81\u3001\u539f\u578b\u8fed\u4ee3\u6a21\u5757\uff1b\u57fa\u4e8e\u5bc6\u5ea6\uff08\u56de\u5f52\uff09\u7684\u8ba1\u6570\u65b9\u6cd5</p> <p>\u7b2c\u4e09\u6bb5</p> <p>To improve explainability of the estimated counts and estimate object locations as well, detectionbased methods emerged. </p> <p>\u4e3a\u4e86\u63d0\u9ad8\u4f30\u8ba1\u8ba1\u6570\u548c\u4f30\u8ba1\u76ee\u6807\u4f4d\u7f6e\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u57fa\u4e8e\u68c0\u6d4b\u7684\u65b9\u6cd5\u5e94\u8fd0\u800c\u751f\u3002</p> <p>\uff08\u5148\u8bf4 \u6700\u65e9\u7684\uff09   The first few-shot detection-based counter [19] was an extended transformer-based object detector [2] with the ability to detect objects specified by the exemplars. </p> <p>\u6700\u65e9\u7684\u57fa\u4e8e\u5c0f\u6837\u672c\u68c0\u6d4b\u7684\u8ba1\u6570\u5668[ 19 ]\u662f\u4e00\u79cd\u6269\u5c55\u7684\u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u76ee\u6807\u68c0\u6d4b\u5668[ 2 ]\uff0c\u5177\u6709\u68c0\u6d4b\u6837\u672c\u6307\u5b9a\u76ee\u6807\u7684\u80fd\u529b\u3002</p> <p>\uff08\u73b0\u5728\u6700\u597d\u7684\uff09   Current state-of-the-art DAVE [20] proposed a two-stage detect-and-verify paradigm for low-shot counting and detection, where in the first stage it generates object proposals with a high recall, but low precision, which is improved by a subsequent verification step. </p> <p>\u76ee\u524d\u6700\u5148\u8fdb\u7684DAVE [ 20 ]\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u68c0\u6d4b-\u9a8c\u8bc1\u8303\u5f0f\u7528\u4e8e\u4f4e\u955c\u5934\u8ba1\u6570\u548c\u68c0\u6d4b\uff0c\u5176\u4e2d\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7684\u7269\u4f53\u63d0\u8bae\u5177\u6709\u8f83\u9ad8\u7684\u53ec\u56de\u7387\uff0c\u4f46\u7cbe\u5ea6\u8f83\u4f4e\uff0c\u5e76\u901a\u8fc7\u540e\u7eed\u7684\u9a8c\u8bc1\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\u3002</p> <p>PSECO [33] proposed a three-stage approach called point-segment-and-count, which employs more involved proposal generation with better detection accuracy and also applies a verification step to improve precision. </p> <p>PSECO [ 33 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u70b9-\u6bb5-\u8ba1\u6570\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e86\u66f4\u591a\u53c2\u4e0e\u7684\u5efa\u8bae\u751f\u6210\uff0c\u5177\u6709\u66f4\u597d\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u4e14\u8fd8\u5e94\u7528\u4e86\u9a8c\u8bc1\u6b65\u9aa4\u6765\u63d0\u9ad8\u7cbe\u5ea6\u3002</p> <p>Both DAVE and PSECO are multi-stage methods that train a network for the surrogate task of predicting density maps for object centers, from which the bounding boxes are predicted. </p> <p>Although detection-based counters offer additional applicability, they fall behind the best density-based counters in global count estimation.</p> <p>DAVE\u548cPSECO\u90fd\u662f\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u76ee\u6807\u4e2d\u5fc3\u7684\u5bc6\u5ea6\u56fe\u7684\u66ff\u4ee3\u4efb\u52a1\uff0c\u5e76\u4ece\u4e2d\u9884\u6d4b\u8fb9\u754c\u6846\u3002\u5c3d\u7ba1\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u5668\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u9002\u7528\u6027\uff0c\u4f46\u5b83\u4eec\u843d\u540e\u4e8e\u5168\u5c40\u8ba1\u6570\u4f30\u8ba1\u4e2d\u6700\u597d\u7684\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u5668\u3002</p> <p>summary</p> <p>DAVE\u548cPSECO \u591a\u9636\u6bb5\u3001\u6bd4\u4e0d\u8fc7\u73b0\u5728\u6700\u597d\u7684\uff0c\u73b0\u5728\u6700\u597d\u7684\u662f \u57fa\u4e8e\u5bc6\u5ea6\u56de\u5f52\u56fe\u7684CountGD \u00b7 \u6587\u672c&amp;\u56fe\u50cf</p> <p>Note</p> <p>\u884c\u6587\u903b\u8f91</p> <p>\u7b2c\u4e00\u6bb5\uff1a\u4ece\u7279\u5b9a \\(\\rightarrow\\)  \u901a\u7528 \u76ee\u6807\u8ba1\u6570</p> <p>\u7b2c\u4e8c\u6bb5\uff1a\u901a\u7528\u76ee\u6807\u8ba1\u6570\uff1a\u57fa\u4e8e\u5bc6\u5ea6\u56de\u5f52\u56fe\u8ba1\u6570\u65b9\u6cd5\u7684\u53d1\u5c55</p> <p>\u7b2c\u4e09\u6bb5\uff1a\u901a\u7528\u76ee\u6807\u8ba1\u6570\uff1a\u57fa\u4e8e\u68c0\u6d4b\u8ba1\u6570\u65b9\u6cd5\u53d1\u5c55\uff0c\u53ef\u89e3\u91ca\u6027\u6bd4\u8f83\u597d\uff0c\u8ba1\u6570\u4e0d\u53ea\u662f\u4e3a\u4e86\u8ba1\u6570\uff0c\u4e5f\u8981\u6709\u76ee\u6807\u7684\u7279\u5b9a\u4fe1\u606f\uff1aLocation&amp;size</p> <p>241117</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/","title":"rank3 DAVE","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p></p> <p>arxiv\u65e5\u671f\uff1a2024\u5e744\u670825\u65e5</p> <p>\u8bba\u6587\u6b63\u5f0f\u53d1\u8868\u9875\u9762</p> <p>today\uff1a241117</p> <p>\u6807\u9898\uff1aDAVE \u2013 A Detect-and-Verify Paradigm for Low-Shot Counting </p> <ul> <li>\u4e24\u9636\u6bb5\u8ba1\u6570\u65b9\u6cd5\uff1a\u5148\u68c0\u6d4b\u518d\u9a8c\u8bc1</li> <li>\u68c0\u6d4b\uff1a\u9ad8\u53ec\u56de\uff0c\u590d\u7528\u4e86LOCA\u7684\u67b6\u6784</li> <li>\u9a8c\u8bc1\uff1a\u8c31\u805a\u7c7b\u9a8c\u8bc1</li> </ul> <p>\u671f\u520a\uff1aCVPR2024</p> <p>\u5f15\u7528\uff1a</p> <pre><code>@inproceedings{pelhan2024dave,\n  title={DAVE-A Detect-and-Verify Paradigm for Low-Shot Counting},\n  author={Pelhan, Jer and Zavrtanik, Vitjan and Kristan, Matej and others},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={23293--23302},\n  year={2024}\n}\n</code></pre> <p>LOCA  &amp; DAVE</p> <p></p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#abstract","title":"Abstract","text":"<p>Low-shot counters estimate the number of objects corresponding to a selected category, based on only few or no exemplars annotated in the image. The current state-ofthe-art estimates the total counts as the sum over the object location density map, but does not provide individual object locations and sizes\uff08\u63d0\u51fa\u95ee\u9898\uff09, which are crucial for many applications. This is addressed by detection-based counters, which, however fall behind in the total count accuracy \uff08\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\uff0c\u4f46\u662f\u51c6\u786e\u6027\u4e0d\u9ad8\uff09. Furthermore, both approaches tend to overestimate the counts in the presence of other object classes due to many false positives. \uff08\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5171\u6027\u95ee\u9898\uff1a\u5047\u9633\u6027\u8fc7\u9ad8\uff09</p> <p>\uff08\u672c\u6587\uff09We propose DAVE, a low-shot counter based on a detect-and-verify paradigm, that avoids the aforementioned issues by first generating a high-recall detection set and then verifying the detections to identify and remove the outliers. This jointly increases the recall and precision, leading to accurate counts. </p> <ul> <li>first generating a high-recall detection set and then </li> <li>verifying the detections to identify and remove the outliers. </li> </ul> <p>\uff08\u7ed3\u679c\uff09DAVE outperforms the top densitybased counters by \u223c20% in the total count MAE, it outperforms the most recent detection-based counter by \u223c20% in detection quality and sets a new state-of-the-art in zero-shot as well as text-prompt-based counting. The code and models are available on GitHub.</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#introduction-contribution","title":"Introduction-contribution","text":"<ul> <li> <p>We address the aforementioned issues by proposing a low-shot counter DAVE, which combines the benefits of density-based and detection-based formulations, and introduces a novel detect-and-verify paradigm. </p> </li> <li> <p>DAVE tackles the specificity-generalization issues of the existing counters by applying a two-stage pipeline (Figure 1). </p> </li> <li> <p>In the first, detection stage , DAVE leverages density-based estimation to obtain a high-recall set of candidate detections, which however may contain false positives. </p> </li> <li> <p>This is addressed by the second,  verification stage , where outliers are identified and rejected by analyzing the candidate appearances, thus increasing the detection precision. Regions corresponding to the outliers are then removed from the location density map estimated in the first stage, thus improving the densitybased total count estimates as well. </p> </li> <li> <p>In addition, we extend DAVE to text-prompt-based and to a zero-shot scenario, which makes DAVE the first zero-shot as well as textprompt detection-capable counter.</p> </li> </ul> <p>text prompt &amp; zero-shot</p> <p>The primary contribution of the paper is the detect-andverify paradigm for low-shot counting that simultaneously achieves high recall and precision. </p> <p>The proposed architecture is the first to extend to all low-shot counting scenarios. DAVE uniquely merges the benefits of both density and detection-based counting and is the first zero-shot-capable counter with detection output. </p> <p>\uff08\u7ed3\u679c\uff09</p> <ol> <li>DAVE outperforms all stateof-the-art density-based counters on the challenging benchmark [26], including the longstanding winner [6], achieving a relative 20% MAE and 43% RMSE total-count error reductions. </li> <li>It also outperforms all state-of-the-art detectionbased counters on the recent benchmark FSCD147 [22] by \u223c20% in detection metrics, as well as in the total count estimation by 38% MAE. </li> <li>Furthermore, it sets a new state-ofthe-art in text-prompt-based counting. </li> <li>The zero-shot DAVE variant outperforms all zero-shot density-based counters and delivers detection accuracy on-par with the most recent few-shot counters. </li> <li>DAVE thus simultaneously outperforms both density-based and detection-based counters in a range of counting setups.</li> </ol>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#5-conclusion","title":"5. Conclusion","text":"<p>\u7b2c\u4e00\u6bb5</p> <ol> <li>We presented a novel low-shot object counting and detection method DAVE, that narrows the performance gap between density-based and detection-based counters.</li> </ol> <p>DAVE\u662f\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5</p> <p>2.DAVE spans the entire low-shot spectrum, also covering text-prompt setups, and is the first method capable of zero-shot detection-based counting. </p> <p>DAVE \u57fa\u4e8e\u6587\u672c &amp; 0-shot</p> <p>3.This is achieved by the novel detect-and-verify paradigm, which increases the recall as well as precision of the detections. \u68c0\u6d4b&amp;\u9a8c\u8bc1\uff0c\u51c6\u786e\u7387 \u53ec\u56de\u7387\u90fd\u5f88\u9ad8</p> <p>\u7b2c\u4e8c\u6bb5</p> <p>Extensive analysis demonstrates that DAVE sets a new state-of-the-art in total count estimation, as well as in detection accuracy on several benchmarks with comparable complexity to related methods, running 110ms/image.</p> <p>In particular, DAVE outperforms the long-standing top low-shot counter [6], as well as the recent detection-based counter [22]. </p> <p>In a zero-shot setup, DAVE outperforms all density-based counters and delivers detections on par with the most recent few-shot counter that requires at least few annotations. </p> <p>DAVE also sets a new state-of-the-art in prompt-based counting.</p> <p>In our future work, we plan to explore interactive counting with the human in the loop and improve detection in extremely dense regions.\u672a\u6765\u7684\u5de5\u4f5c\uff1a\u4eba\u7684\u4ea4\u4e92\u3001\u5bc6\u96c6\u573a\u666f\u7684\u68c0\u6d4b</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#1-introduction","title":"1. Introduction","text":"<p>P1 \u4eceLow-shot counting\u5f00\u59cb\u8bf4</p> <p>Low-shot counting considers estimating the number of target objects in an image, based only on a few annotated exemplars (few-shot) or even without providing the exemplars (zero-shot). Owing to the emergence of focused benchmarks [22, 26], there has been a surge in low-shot counting research recently. The current state-of-the-art low-shot counters are all density-based [6, 26, 28, 38]. This means that they estimate the total count by summing over an estimated object presence density map. Only recently, fewshot detection-based methods emerged [22] that estimate the counts as the number of detected objects.</p> <p>P2 \u6bd4\u8f83 Density-based &amp; detection-based \uff1b\u6307\u51fa\u76ee\u524d\u57fa\u4e8e\u5bc6\u5ea6\uff08Density-based \uff09\u7684\u8ba1\u6570\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002\u5e76\u8bf4\u660e\uff1aexplainability is crucial \u672c\u6587\u503e\u5411\u4e8e\u57fa\u4e8e\u68c0\u6d4b\u7684\u65b9\u6cd5</p> <p>Density-based methods substantially outperform the detection-based counters in total count estimation, but they do not provide detailed outputs such as object locations and sizes. The latter are however important in many downstream tasks such as bio-medical analysis [35, 41], where explainability is crucial for human expert verification as well as for subsequent analyses. There is thus a large applicability gap between the density-based and detection-based low-shot counters.</p> <p>P3  \u5171\u540c\u7684\u7f3a\u70b9 \u5047\u9633\u6027\u8fc7\u9ad8</p> <p>Furthermore, both density-based and detection-based counters are prone to failure in scenes with several object types (Figure 1). The reason lies in the specificity  generalization tradeoff. Obtaining a high recall requires generalizing over the potentially diverse appearances of the selected object type instances in the image. However, this also leads to false activations on objects of other categories (false positives), leading to a reduced precision and count overestimation. A possible solution is to train on multiple-class images [22], however, this typically leads to a reduced recall and underestimated counts.</p> <p>P4-P5 \u8d21\u732e</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#2-related-work","title":"2. Related Work","text":"<p>\u7b2c\u4e00\u6bb5</p> <p>Object counting emerged as detection-based counting of objects belonging to specific classes, such as vehicles [5], cells [8], people [17], and polyps [41]. To address poor performance in densely populated regions, density-based methods [3, 4, 29\u201331] emerged as an alternative.</p> <p>\u76ee\u6807\u8ba1\u6570\u3001\u5c31\u6709\u68c0\u6d4b\u7684\u65b9\u6cd5\u3001\u7279\u5b9a\u7c7b\u522b\uff1b\u8ba1\u6570\u51c6\u786e\u6027\uff0c\u57fa\u4e8e\u5bc6\u5ea6\u56de\u5f52\u56fe\u7684\u8ba1\u6570\u65b9\u6cd5</p> <p>\u7b2c\u4e8c\u6bb5</p> <p>All these methods rely on the availability of large datasets to train category-specific models, which, however are not available in many applications.\u6570\u636e\u96c6</p> <p>\u7b2c\u4e09\u6bb5</p> <p>Class-agnostic approaches addressed this issue by test-time adaptation to various object categories with minimal supervision.</p> <p>\u7c7b\u65e0\u5173\u8ba1\u6570\u65b9\u6cd5\u3001\u6700\u5c11\u7684\u76d1\u7763\u4fe1\u53f7\u3001\u6d4b\u8bd5\u9636\u6bb5\u8c03\u6574</p> <p>Early representatives [19] and [37] proposed predicting the density map by applying a siamese matching network \u5b6a\u751f\u5339\u914d\u7f51\u7edc to compare image and exemplar features. \u6bd4\u8f83\u56fe\u50cf&amp;\u6837\u4f8b\u6846\u7279\u5f81</p> <ul> <li>Recently, the FSC147 dataset [26] was proposed to encourage the development of few-shot counting methods. Famnet [26] proposed a test-time adaptation of the backbone to improve density map estimation. FSC147\u6570\u636e\u96c6\u3001 Famnet  || FSC147\u6570\u636e\u96c6[ 26 ]\u7684\u63d0\u51fa\u9f13\u52b1\u4e86\u5c0f\u6837\u672c\u8ba1\u6570\u65b9\u6cd5\u7684\u53d1\u5c55\u3002Famnet [ 26 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u9aa8\u5e72\u6d4b\u8bd5\u65f6\u95f4\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u5bc6\u5ea6\u56fe\u4f30\u8ba1\u3002</li> <li>BMNet+ [28] improved localization by jointly learning representation and a non-linear similarity metric. A self-attention mechanism was applied to reduce the intra-class appearance variability. BMNet + [ 28 ]\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u8868\u793a\u548c\u975e\u7ebf\u6027\u76f8\u4f3c\u6027\u5ea6\u91cf\u6765\u6539\u8fdb\u5b9a\u4f4d\u3002\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u51cf\u5c11\u7c7b\u5185\u5916\u89c2\u53d8\u5f02\u6027\u3002</li> <li>SAFECount [38] introduced a feature enhancement module, improving generalization capabilities. SAFECount [ 38 ]\u5f15\u5165\u4e86\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002</li> <li>CounTR [16] used a vision transformer [7] for image feature extraction and a convolutional encoder to extract exemplar features. An interaction module based on cross-attention was proposed to fuse both, image and exemplar features. Coun TR [ 16 ]\u4f7f\u7528\u89c6\u89c9\u8f6c\u6362\u5668[ 7 ]\u8fdb\u884c\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\uff0c\u4f7f\u7528\u5377\u79ef\u7f16\u7801\u5668\u63d0\u53d6\u6837\u672c\u7279\u5f81\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u4ea4\u4e92\u6a21\u5757\u6765\u878d\u5408\u56fe\u50cf\u7279\u5f81\u548c\u6837\u672c\u7279\u5f81\u3002</li> <li>LOCA [6] proposed an object prototype extraction module, which combined exemplar appearance and shape with an iterative adaptation.LOCA [ 6 ]\u63d0\u51fa\u4e86\u4e00\u4e2a\u5bf9\u8c61\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5c06\u6837\u672c\u5916\u89c2\u548c\u5f62\u72b6\u4e0e\u8fed\u4ee3\u81ea\u9002\u5e94\u76f8\u7ed3\u5408\u3002</li> </ul> <p>Note</p> <p>Summary   5\u4e2a\u6a21\u578b   \u7c7b\u522b\u4e0d\u654f\u611f\u8ba1\u6570\u65b9\u6cd5\u3001\u6d4b\u8bd5\u9636\u6bb5\u81ea\u9002\u5e94\u3001\u5c11\u91cf\u76d1\u7763\u4fe1\u53f7\uff08\u4f60\u53bb\u770bGeCo\uff1a\u76f8\u5173\u5de5\u4f5c\uff0c\u4e5f\u662f\u8fd95\u4e2a\u6a21\u578b\u7684 \u7814\u7a76\u73b0\u72b6</p> <ul> <li>FamNet  </li> <li>BMNet+  </li> <li>SAFECount   </li> <li>CounTR  </li> <li>LOCA  </li> </ul> <p>\u7b2c\u56db\u6bb5</p> <p>All few-shot counting methods require few annotated exemplars to specify the object class. With the recent development of large language models (e.g. [23]) text-prompt based counting methods emerged.</p> <p>\u8f93\u5165\u4fe1\u53f7\u7684\u53d1\u5c55\uff1a\u4ece\u524d\u662f\u6807\u6ce8\u7684\u6837\u4f8b\u6846\u6307\u5b9a\u7c7b\u522b $\\rightarrow $  \u73b0\u5728\u57fa\u4e8e\u6587\u672c\u7684\u8ba1\u6570\u65b9\u6cd5\uff08\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff09</p> <p>Instead of specifying exemplars by bounding box annotations, these methods use text descriptions of the target object class. </p> <p>\u2764\ufe0f\u8fd9\u6bb5\u7684\u6587\u732e\u6982\u8ff0\uff1a\u5173\u4e8e\u7684\u662f\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u6307\u5b9a\u76ee\u6807\u7c7b\u522b </p> <p>ZeroCLIP [36] proposed text-based construction of prototypes, which are used to select relevant image patches acting as exemplars for counting.</p> <p>ZeroCLIP [ 36 ]\u63d0\u51fa\u4e86\u57fa\u4e8e\u6587\u672c\u7684\u539f\u578b\u6784\u9020\uff0c\u7528\u4e8e\u9009\u62e9\u76f8\u5173\u7684\u56fe\u50cf\u5757\u4f5c\u4e3a\u8ba1\u6570\u7684\u8303\u4f8b\u3002</p> <p>CLIPCount [15] leveraged CLIP [23] for image-text alignment and introduced patch-text contrastive loss for learning the visual representations used for density prediction. </p> <p>CLIPCount [ 15 ]\u5229\u7528CLIP [ 23 ]\u8fdb\u884c\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u5757-\u6587\u672c\u5bf9\u6bd4\u635f\u5931\u6765\u5b66\u4e60\u7528\u4e8e\u5bc6\u5ea6\u9884\u6d4b\u7684\u89c6\u89c9\u8868\u793a\u3002</p> <p>Several works [13, 25] address the extreme case in which no exemplars are provided and the task is to count the majority class objects (i.e., zero-shot counting).</p> <p>\u4e00\u4e9b\u5de5\u4f5c[ 13\u300125]\u89e3\u51b3\u4e86\u6ca1\u6709\u63d0\u4f9b\u6837\u4f8b\u7684\u6781\u7aef\u60c5\u51b5\uff0c\u5176\u4efb\u52a1\u662f\u5bf9\u591a\u6570\u7c7b\u5bf9\u8c61\u8fdb\u884c\u8ba1\u6570(\u5373\u96f6\u6837\u672c\u8ba1\u6570)</p> <p>Note</p> <p>summary  1. ZeroCLIP  2. CLIPCount   </p> <p>Info</p> <p>DAVE  \u2460\u57fa\u4e8e\u68c0\u6d4b   \u2461text-prompt  \u2462zero-shot</p> <p>\u7b2c\u4e94\u6bb5</p> <p>With minimal architectural changes, the recent few-shot methods [6, 16] also demonstrated a remarkable zero-shot counting performance. A common drawback of densitybased counters is that they do not provide object locations.</p> <p>\u5728\u7ed3\u6784\u53d8\u5316\u5f88\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u8fd1\u7684\u5c11\u6837\u672c\u65b9\u6cd5[ 6\u300116 ]\u4e5f\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u96f6\u6837\u672c\u8ba1\u6570\u6027\u80fd\u3002\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u5668\u7684\u4e00\u4e2a\u5171\u540c\u7f3a\u70b9\u662f\u5b83\u4eec\u4e0d\u63d0\u4f9b\u5bf9\u8c61\u4f4d\u7f6e\u3002</p> <p></p> <p></p> <p>[6]  LOCA  [16]CounTR</p> <p>\u5f00\u59cb\u5f15\u51fa\u53ef\u4ee5\u63d0\u4f9b\u5b9a\u4f4d\u7684\u8ba1\u6570 \u65b9\u6cd5</p> <p>\u7b2c\u516d\u6bb5</p> <p>To address the aforementioned limitation of density based counters, the first few shot counting and detection method [22] has been recently proposed by extending a transformer-based object detector [2] with an ability to detect objects specified by exemplars.</p> <p>\u6587\u732e22 \u7b2c\u4e00\u4e2a\u57fa\u4e8e\u68c0\u6d4b\u7684FSC\u65b9\u6cd5 </p> <p></p> <p>However, the detection based counter falls far behind in total count estimation compared with the best density-based counters.</p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u6700\u5927\u7684\u5f0a\u7aef\uff1a\u8ba1\u6570\u51c6\u786e\u6027\u4e0d\u9ad8(241117)</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#3-counting-by-detection-and-verification","title":"3. Counting by detection and verification","text":"<p>todo</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/","title":"rank4 CACViT","text":"<p>Info</p> <p>\u672c\u6587\u6982\u62ec  1.\u57fa\u4e8eViT \u540c\u65f6\u8fdb\u884c\u63d0\u53d6\u548c\u5339\u914d\uff0cdecoupled view  \uff08\u89e3\u51b3\u4e86\u4ee5\u524d\uff1a\u5148\u63d0\u53d6\u540e\u5339\u914d\u7684\u6a21\u5f0f\uff09  2.\u7eb5\u6a2a\u6bd4\u611f\u77e5\u7684\u5c3a\u5ea6\u5d4c\u5165 \u548c \u6570\u91cf\u7ea7\u5d4c\u5165  3.\u6570\u636e\u96c6\uff1aFSC147 &amp; CARPK  </p> <p>\u5f15\u7528\uff1a</p> <pre><code>@inproceedings{wang2024vision,\n  title={Vision transformer off-the-shelf: A surprising baseline for few-shot class-agnostic counting},\n  author={Wang, Zhicheng and Xiao, Liwen and Cao, Zhiguo and Lu, Hao},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  year={2024}\n}\n</code></pre> <p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>arxiv\u65e5\u671f\uff1a2024\u5e743\u67084\u65e5</p> <p>today\uff1a 241117</p> <p>\u6b63\u5f0f\u53d1\u8868\u9875\u9762 \u63d0\u4f9b\u89c6\u9891\u8bb2\u89e3</p> <p></p> <p>\u4f5c\u8005\uff1a\u534e\u4e2d\u79d1\u6280\u5927\u5b66</p> <p></p> <p>\u6807\u9898\uff1aVision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting</p> <p>\u57fa\u4e8e\u73b0\u6709\u7684ViT\u67b6\u6784\uff1aFSC\u8ba1\u6570\u65b9\u6cd5</p> <p></p> <p>\u4f5c\u8005</p> <ul> <li>\u6559\u80b2\u90e8\u56fe\u50cf\u5904\u7406\u548c\u667a\u80fd\u63a7\u5236\u91cd\u70b9\u5b9e\u9a8c\u5ba4</li> <li>\u534e\u4e2d\u79d1\u6280\u5927\u5b66 \u4eba\u5de5\u667a\u80fd\u4e0e\u81ea\u52a8\u5316\u5b66\u9662</li> </ul>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#abstract","title":"Abstract","text":"<p>Class-agnostic counting (CAC) aims to count objects of interest from a query image given few exemplars. </p> <p>\u95ee\u9898\u5b9a\u4e49\uff1aFSC few shot counting</p> <p>CAC\uff1aclass-agnostic counting</p> <p>\u5408\u8d77\u6765\u81ea\u5df1\u9020\u4e2a\u5c31\u662f\uff1aFSCAC few-shot class-agnostic counting\u5c0f\u6837\u672c\u7c7b\u65e0\u5173\u8ba1\u6570\u65b9\u6cd5\u7814\u7a76</p> <p>This task is typically addressed by extracting the features of query image and exemplars respectively and then matching their feature similarity, leading to an extract-then-match paradigm.</p> <p>\u56fe\u7247\u7279\u5f81  \u548c \u6837\u4f8b\u6846\u7279\u5f81  \u5206\u522b\u63d0\u53d6 \u7136\u540e\u8fdb\u884c\u5339\u914d</p> <p>\u6307\u51fa\u7814\u7a76\u73b0\u72b6\uff1a\u5148\u63d0\u53d6\u518d\u5339\u914d  extract-then-match paradigm</p> <p>In this work, we show that CAC can be simplified in an extract-and-match manner, particularly using a vision transformer (ViT) where feature extraction and similarity matching are executed simultaneously within the self-attention. </p> <p>ViT\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5339\u914d</p> <p>We reveal the rationale of such simplification from a decoupled view of the self-attention. The resulting model, termed CACViT, simplifies the CAC pipeline into a single pretrained plain ViT. </p> <p>Further, to compensate the loss of the scale and the order-of-magnitude information due to resizing and normalization in plain ViT, we present two effective strategies for scale and magnitude embedding.</p> <p>\u4e3a\u4e86\u5f25\u8865\u7f3a\u5931\u7684\u4fe1\u606f\uff1a\u5c3a\u5ea6\u5d4c\u5165 &amp; \u6570\u91cf\u7ea7\u5d4c\u5165</p> <p>Extensive experiments on the FSC147 and the CARPK datasets show that CACViT significantly outperforms state-of-the-art CAC approaches in both effectiveness (23.60% error reduction) and generalization, which suggests CACViT provides a concise and strong baseline for CAC. Code will be available.</p> <p>\u672c\u6587\u7684\u6570\u636e\u96c6\uff1a the FSC147 and the CARPK datasets</p> <p>Note</p> <p>\u672c\u6587\u7684\u521b\u65b0\u70b9\uff1a1+2</p> <p>1:CACViT:\u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u51fa\u548c\u5339\u914d\uff0c\u5229\u7528ViT\u67b6\u6784\uff0cdecoupled view</p> <p>2:\u5c3a\u5ea6\u5d4c\u5165\u3001\u6570\u91cf\u7ea7\u5d4c\u5165</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#introduction-contribution","title":"Introduction contribution","text":"<p>In a nutshell, our contributions are three-fold: </p> <ul> <li>A novel extract-and-match paradigm: we show that simultaneous feature extraction and matching can be made possible in CAC; \u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5339\u914d</li> <li>CACViT: a simple and strong ViT-based baseline, sets the new state-of-the-art on the FSC-147 benchmark; \u57fa\u4e8eViT</li> <li>We introduce two effective strategies to embed scale, aspect ratio, and order of magnitude information tailored to CACViT. \u57fa\u4e8e\u7eb5\u6a2a\u6bd4\u611f\u77e5\u7684\u5c3a\u5ea6\u5d4c\u5165\u3001\u6570\u91cf\u7ea7\u5d4c\u5165</li> </ul>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#conclusions","title":"Conclusions","text":"<p>In this work, we propose a simple yet efficient ViT-based model CACViT for CAC. \u57fa\u4e8eViT\u67b6\u6784\uff0c\u89e3\u51b3CAC\u95ee\u9898\uff0c\u53d6\u540d\uff1aCACViT</p> <p>Specifically, we show that the ViT is naturally suitable for the CAC task from a decoupled view. \u89e3\u8026\u89c6\u89d2\u4e0b\uff0c\u89e3\u8bfbViT</p> <p>And we propose a ViT-based extract-and-match paradigm for CAC. \u57fa\u4e8eViT\u540c\u65f6\u8fdb\u884c\u63d0\u53d6&amp;\u5339\u914d</p> <p>Then we introduce aspect-ratio-aware scale embedding and magnitude embedding to compensate for the information loss. \u7eb5\u6a2a\u6bd4\u611f\u77e5\u7684\u5c3a\u5ea6\u5d4c\u5165\u548c\u6570\u91cf\u7ea7\u5d4c\u5165\uff0c\u5f25\u8865\u4e22\u5931\u7684\u4fe1\u606f</p> <p>Our CACViT achieves stat-of-the-art results on FSC147, and we also verify the generality on CARPK.</p> <p>\u6570\u636e\u96c6\uff1aFSC147&amp;CARPK</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#introduction","title":"Introduction","text":"<p>P1 \u76ee\u6807\u8ba1\u6570\uff0c\u6700\u5f00\u59cb\u9488\u5bf9\u7279\u5b9a\u9886\u57df</p> <p>Object counting aims to estimate the number of objects from a query image. Most prior object counting approaches target a specific domain, e.g., crowd (Zhang et al. 2015; Shu et al. 2022; Zou et al. 2021), plant (Lu et al. 2017; Madec et al. 2019), and car (Onoro-Rubio and L \u0301 opez-Sastre 2016). </p> <p>They often require numerous class-specific training data to learn a good model (Wang et al. 2020). </p> <p>\u8fc7\u6e21\u5230\u7c7b\u65e0\u5173\u8ba1\u6570\u65b9\u6cd5</p> <p>In contrast, Class-Agnostic Counting (CAC), whose goal is to estimate the counting value of arbitrary categories given only few exemplars \u7ed9\u5b9a\u793a\u4f8b\u6846\u8ba1\u6570\u4efb\u610f\u7c7b\u522b, has recently received much attention due to its potential to generalize to unseen scenes and reduced reliance on class-specific training data (Lu, Xie, and Zisserman 2019; Ranjan et al. 2021; Shi et al. 2022; Liu et al. 2022).</p> <p>P2 \u8bf4\u7684\u662f  extract-then-match paradigm</p> <p>CAC is first introduced by Lu et al. (Lu, Xie, and Zisserman 2019), which is by default formulated as a template matching problem, leading to an extract-then-match paradigm. </p> <p>CAC\u4efb\u52a1\u6700\u65e9\u5f15\u5165\uff1a Lu et al. (Lu, Xie, and Zisserman 2019)\uff0c\u5b9a\u4e49\u4e3a\u6a21\u677f\u5339\u914d\u95ee\u9898\uff0c\u5148\u63d0\u53d6\u540e\u5339\u914d\u7684\u6a21\u5f0f</p> <p>Previous models (Ranjan et al. 2021; Shi et al. 2022; Lin et al. 2022) use shared CNN for query images and exemplars feature extraction, as the bottom-up feature extraction approach of the CNN can adapt to images of entirely different sizes. CNN\u81ea\u5e95\u5411\u4e0a\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u5f0f\u53ef\u4ee5\u9002\u5e94\u5927\u5c0f\u5b8c\u5168\u4e0d\u540c\u7684\u56fe\u50cf\u3002</p> <p>\uff08CounTR\uff09  Witnessing the ability of marking the responses on the attention map by cross-attention mechanism, some models such as CounTR (Liu et al. 2022) employs cross-attention to match the features of query image and exemplars.However, in CounTR the query feature and exemplar feature are embedded separately by a ViT and a CNN, and the matching part is achieved by an extra cross-attention stage. This strategy introduces much redundancy and task-specific designs, which is not in line with the trend of task-agnostic foundation models.</p> <p>\u4e00\u4e9b\u6a21\u578b\u5982CounTR ( Liu et al 2022)\u7b49\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728\u6ce8\u610f\u529b\u56fe\u4e0a\u6807\u8bb0\u54cd\u5e94\u7684\u80fd\u529b\u6765\u5339\u914d\u67e5\u8be2\u56fe\u50cf\u548c\u793a\u4f8b\u7684\u7279\u5f81\u3002\u7136\u800c\uff0c\u5728CounTR\u4e2d\uff0c\u67e5\u8be2\u7279\u5f81\u548c\u6837\u4f8b\u7279\u5f81\u5206\u522b\u7531\u4e00\u4e2aViT\u548c\u4e00\u4e2aCNN\u5d4c\u5165\uff0c\u5339\u914d\u90e8\u5206\u7531\u989d\u5916\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u9636\u6bb5\u5b9e\u73b0\u3002\u8fd9\u79cd\u7b56\u7565\u5f15\u5165\u4e86\u5927\u91cf\u7684\u5197\u4f59\u548c\u4efb\u52a1\u76f8\u5173\u7684\u8bbe\u8ba1\uff0c\u4e0d\u7b26\u5408\u4efb\u52a1\u65e0\u5173\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u8d8b\u52bf\u3002</p> <p>P3 ViT\u7684\u53d1\u5c55</p> <p>Recently, the computer vision community has witnessed great success with plain ViT in large multi-modal architectures (Touvron et al. 2023; Yu et al. 2022). Soon much work emerges for better adaptation of ViT on downstream vision tasks, such as object detection (Li et al. 2022; Lin et al. 2023), pose estimation (Xu et al. 2022, 2023) and image matting (Yao et al. 2023). As a template matching task, CAC is essentially suitable for using ViT with its attention mechanism; however, there is little focus on the adaptation of ViT on CAC task.</p> <p>P4 \u5f15\u51faViT\u548cCAC\u7684\u5173\u7cfb</p> <p>In this work, we share insights that the attention mechanism in plain ViT has the ability to extract the features for both the query image and the exemplars and perform feature matching for them. By grouping the query and exemplar tokens into concatenation and feeding them to a plain ViT, the self-attention process in ViT can be divide into two groups of self-attention, and two groups of cross-attention. The former self-attentions are to extract features for the query image and the exemplars, while the latter cross-attentions contains the matching process between the query image and the exemplars. Therefore, without multiple feature extractors or extra post-matching, it produces a novel extract-andmatch paradigm. Compared with prior arts, the extra attention from the query feature to the exemplars would further provide additional class information to the query image in this paradigm, enabling better perception of objects.Based on this idea, we propose a framework for CAC that mainly contains a single pretrained ViT, which verifies the feasibility of plain ViT for CAC task.</p> <p>P5</p> <p>For better adaptation of ViT to the specific CAC task, we introduce more insights closely related to CAC task in our model design. Specifically, we observe that certain restrictions or functions such as resizing and softmax normalization within this architecture can result in the loss of scale information and the order of magnitude of counting values. First, the exemplars must be resized to fit the ViT input, which introduces size ambiguity during matching. Prior CNN-based models (Shi et al. 2022) attempt to compensate for the scale information with scale embedding for exemplars; however, they neglect the information of aspect ratios, which is crucial for classes with abnormal ratios. This is largely overlooked in the existing literature. Second, the attention map with softmax function can represent the relative distribution of objects in the query image and therefore weakens the awareness of the model to the number of objects. We address this by restoring the magnitude order in the normalized attention map. Both the proposed scale embedding and magnitude embedding are easy to implement. By infusing the scale and the magnitude information into the plain ViT architecture, we acquire a surprisingly simple yet highly effective ViT baseline for CAC. The resulting model, termed CACViT, fully leverages the self-attention mechanism in ViT while also being tuned to mitigate the defects of this architecture in this task.</p> <p>P6</p> <p>Experiments on the public benchmark FSC147 (Ranjan et al. 2021) show that CACVit outperforms the previous best approaches by large margins, with relative error reductions of 19.04% and 23.60% on the validation and test sets, respectively, in terms of mean absolute error. Its cross-dataset generalization is also demonstrated on a car counting dataset CARPK (Hsieh, Lin, and Hsu 2017). We also provide extensive ablation studies to justify our propositions.</p> <p>P7 \u8d21\u732e</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#related-work","title":"Related Work","text":"<p>P1 </p> <p>The task of CAC is composed of two main components: feature extraction and feature matching. We first review each component in previous counting models, then discuss jointly feature extraction and matching in the other fields.</p> <p>Note</p> <p>Feature Extraction in Class-Agnostic Counting. CAC \u7684\u7279\u5f81\u63d0\u53d6 Feature Matching in Class-Agnostic Counting CAC\u4e2d\u7684\u7279\u5f81\u5339\u914d\u95ee\u9898 Jointly Feature Extraction and Matching. \u540c\u65f6\u8fdb\u884c\u63d0\u53d6 &amp; \u5339\u914d </p> <p>P2 CAC\u95ee\u9898\u4e2d\u7684\u7279\u5f81\u63d0\u53d6</p> <p>Feature Extraction in Class-Agnostic Counting.</p> <p>The investigation of feature extraction in counting first began with class-specific counting (Abousamra et al. 2021; Cao et al. 2018; He et al. 2021; Idrees et al. 2018; Laradji et al. 2018; Cheng et al. 2022). In class-specific counting, most works are designed to address the challenges posed by quantity variance and scale variance. (\u7279\u5b9a\u7c7b\u522b)</p> <p>Abousamra et al. 2021; </p> <p></p> <p>Cao et al. 2018;</p> <p></p> <p>He et al. 2021;</p> <p></p> <p>Idrees et al. 2018; </p> <p></p> <p>Laradji et al. 2018;</p> <p></p> <p>Cheng et al. 2022</p> <p></p> <p>\u90fd\u662f\u4e00\u4e9b \u6211\u6ca1\u4e86\u89e3\u8fc7\u7684\u53c2\u8003\u6587\u732e\uff0c\u7b11)</p> <p>For class-agnostic counting\uff08\u7c7b\u65e0\u5173\uff09, the core of feature extraction include unified matching space apart from challenges as above. To obtain a unified matching space, most previous work (Ranjan et al. 2021; Shi et al. 2022; You et al. 2023) uses the shared CNN-based feature extractors for query images and exemplars. CounTR (Liu et al. 2022), which first introduces the ViT for feature extraction in CAC, uses different feature extractors for the query images (a ViT) and exemplars (a CNN). Hence, a two-stage training scheme is used for unifying the feature space.</p> <p>Ranjan et al. 2021;  FamNet FSC147\u6570\u636e\u96c6\u3001CAC\u4efb\u52a1\u7684\u7b2c\u4e00\u7bc7\u561b\uff1fanyway\u7ecf\u5178\u6587\u732e\u4e86\u5c5e\u4e8e</p> <p></p> <p>Shi et al. 2022;   BMNet</p> <p></p> <p>You et al. 2023</p> <p></p> <p>CounTR (Liu et al. 2022) \u8001\u719f\u4eba\u60f9\u5012\u662f</p> <p></p> <p>P3 CAC\u95ee\u9898\u4e2d\u7684\u7279\u5f81\u5339\u914d</p> <p>Feature Matching in Class-Agnostic Counting.</p> <p>Compared with feature extraction, matching strategies in CAC have garnered more attention. \uff08\u7279\u5f81\u5339\u914d\u5b9e\u9645\u4e0a\u6709\u66f4\u591a\u7684\u5173\u6ce8\u5ea6\uff09</p> <p>The key points of the matching include the following two:  \u5339\u914d\u95ee\u9898\u4e2d\u4e3b\u8981\u5173\u6ce8\u7684\u4e24\u4e2a\u65b9\u9762</p> <p>1) robustness to appearance variance, and  \u5bf9\u4e8e\u5916\u89c2\u7684\u591a\u53d8 \u4f9d\u7136\u4fdd\u6301\u7a33\u5065\u6027 2) ability to characterize quantity levels.  \u5bf9\u4e8e\u6570\u91cf\u53d8\u5316\u7684\u7a33\u5065\u6027</p> <p>In the early attempt, naive inner product (Ranjan et al. 2021; Yang et al. 2021) is used, which is not robust to the appearance variance of objects to be counted. </p> <p>Ranjan et al. 2021  FamNet </p> <p></p> <p>Yang et al. 2021  </p> <p> </p> <p>Shi et al. (Shi et al. 2022) developed a bilinear matching network (BMNet) that expands the fixed inner product to a learnable bilinear similarity metric, which improves the robustness compared with the inner product. </p> <p>The recent ViT-based model CounTR (Liu et al. 2022) uses cross-attention for matching, which seems a natural choice for a transformer-based solution at first glance. However, we show that, in our plain ViT model CACViT, we can perform feature matching at the same time of extracting features by self-attention.</p> <p>CounTR</p> <p>CACViT</p> <p>P4</p> <p>Jointly Feature Extraction and Matching. </p> <p>For template matching and multi-modal tasks, feature extraction and matching are two main components. In tracking and detection tasks, MixFormer network (Chen et al. 2022) and FCT network (Han et al. 2022) were proposed to enhance the correlation between the target object and the image, thereby obtaining enhanced features for localization head.</p> <p>In multimodal tasks, ViLT (Kim, Son, and Kim 2021) strengthens the interaction between text and image during the feature extraction stage, resulting in efficient multi-modal features that benefit the performance of downstream tasks.  \u591a\u6a21\u6001</p> <p>To the best of our knowledge, we are the first to simultaneously consider feature extraction and matching in CAC, and we provide a decoupled analysis of the feasibility of this paradigm in the CAC, thereby streamlining the workflow of CAC task.\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6&amp;\u5339\u914d\u7684|241117</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#class-agnostic-counting-vision-transformer","title":"Class-Agnostic Counting Vision Transformer","text":"<p>Todo</p>"},{"location":"literature/ObejectCounting/rank5%20SSD/","title":"rank5 SSD","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>arxiv\u65e5\u671f\uff1a2024\u5e745\u670820\u65e5</p> <p></p> <p>\u4f5c\u8005\uff1a\u5357\u4eac\u7406\u5de5\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u5b66\u9662</p> <p>\u5f15\u7528</p> <pre><code>@inproceedings{ijcai2024p167,\ntitle = {Learning Spatial Similarity Distribution for Few-shot Object Counting},\nauthor = {Xu, Yuanwu and Song, Feifan and Zhang, Haofeng},\nbooktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI-24}},\npublisher = {International Joint Conferences on Artificial Intelligence Organization},\npages = {1507--1515},\nyear = {2024},\ndoi = {10.24963/ijcai.2024/167},\nurl = {https://doi.org/10.24963/ijcai.2024/167},\n}\n</code></pre> <p>\ud83d\udcdd \u5b66\u4e60\u5c11\u6837\u672c\u76ee\u6807\u8ba1\u6570\u7684\u7a7a\u95f4\u76f8\u4f3c\u5ea6\u5206\u5e03.</p> <p></p> <p>\u4e2d\u6587\u89e3\u91ca\uff1a</p> <p></p> <p></p> <p>\u4e0d\u662f\u6211\u60f3\u4ed4\u7ec6\u7814\u7a76\u7684\u65b9\u5411\uff0cpass</p>"},{"location":"literature/ObejectCounting/rank6%20LOCA/","title":"rank6 LOCA","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>\u5f15\u7528\uff1a</p> <pre><code>@InProceedings{Dukic_2023_ICCV,\n    author    = {{\\DJ}uki\\'c, Nikola and Luke\\v{z}i\\v{c}, Alan and Zavrtanik, Vitjan and Kristan, Matej},\n    title     = {A Low-Shot Object Counting Network With Iterative Prototype Adaptation},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month     = {October},\n    year      = {2023},\n    pages     = {18872-18881}\n}\n</code></pre> <p>arxiv\u65e5\u671f\uff1a2023\u5e749\u670828\u65e5</p> <p>\u6b63\u5f0f\u53d1\u8868\u9875\u9762 ICCV2023</p> <p>today 241117</p> <p></p> <p>\u6807\u9898\uff1aA Low-Shot Object Counting Network With Iterative Prototype Adaptation</p> <p>\u5c0f\u6837\u672c\u8ba1\u6570\u795e\u7ecf\u7f51\u7edc\uff1a\u8fed\u4ee3\u539f\u578b\u9002\u5e94\u6a21\u5757</p> <p>\u2764\ufe0f\uff1a\u4e22\u5931\u5f62\u72b6\u4fe1\u606f\uff08size or aspect\uff09\uff0c\u56e0\u6b64\u672c\u6587\u63d0\u51fa\u4e86OPE\uff08\u76ee\u6807\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff09\u6765\u628a\u5f62\u72b6\u4fe1\u606f\u8fdb\u884c\u8fed\u4ee3\u878d\u5408</p> <p>\u672c\u6587\u505a\u4e86\u4ec0\u4e48\u4e8b\uff1f</p> <ul> <li>\u6458\u8981</li> <li>\u5f15\u5165\u2014\u8d21\u732e</li> <li>\u7ed3\u8bba</li> </ul> <p>\u5177\u4f53\u600e\u4e48\u5b9e\u73b0\uff1amethod</p> <p>\u7ed3\u679c\uff1aExperiment</p> <p>\u7814\u7a76\u95ee\u9898\u7684\u52a8\u673a\uff1aIntroduction</p> <p>\u7814\u7a76\u73b0\u72b6\uff1arelated work</p>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#_1","title":"\u6458\u8981","text":"<p>We consider low-shot counting of arbitrary semantic categories in the image using only few annotated exemplars (few-shot) or no exemplars (no-shot). \u91c7\u7528\u5c0f\u6837\u672c\u6216\u80050\u6837\u672c \u8ba1\u6570\u4efb\u610f\u8bed\u4e49\u7c7b\u522b</p> <p>The standard few-shot pipeline follows extraction of appearance queries from exemplars and matching them with image features to infer the object counts. </p> <p>\u6807\u51c6\u7684\u5c0f\u6837\u672c \u9075\u5faa\u4ece\u6837\u672c\u4e2d\u63d0\u53d6\u5916\u89c2\u67e5\u8be2\uff0c\u5e76\u5c06\u5176\u4e0e\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u5339\u914d\uff0c\u4ee5\u63a8\u65ad\u7269\u4f53\u8ba1\u6570\u3002</p> <p>Existing methods extract queries by feature pooling which neglects the shape information (e.g., size and aspect) and leads to a reduced object localization accuracy and count estimates. \u4e22\u5931\u4e86\u5f62\u72b6\u4fe1\u606f\u548c\u5b9a\u4f4d\u4fe1\u606f\u3001\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u6c60\u5316\u63d0\u53d6\u67e5\u8be2\uff0c\u5ffd\u7565\u4e86\u5f62\u72b6\u4fe1\u606f(\u4f8b\u5982,\u5927\u5c0f\u548c\u7eb5\u6a2a\u6bd4)\uff0c\u5bfc\u81f4\u76ee\u6807\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u6570\u4f30\u8ba1\u503c\u964d\u4f4e\u3002</p> <p>We propose a L ow-shot  O bject C ounting network with iterative prototype A daptation (LOCA). </p> <p>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u8fed\u4ee3\u539f\u578b\u81ea\u9002\u5e94( LOCA )\u7684\u5c0f\u6837\u672c\u7269\u4f53\u8ba1\u6570\u7f51\u7edc\u3002</p> <p>Our main contribution is the new object prototype extraction module, which iteratively fuses the exemplar shape and appearance information with image features. </p> <p>\u76ee\u6807\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff0c\u8fed\u4ee3\u878d\u5408\u793a\u4f8b\u6846\u5f62\u72b6\u548c\u5916\u89c2\u4fe1\u606f</p> <p>\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u662f\u65b0\u7684\u76ee\u6807\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5c06\u6837\u672c\u5f62\u72b6\u548c\u5916\u89c2\u4fe1\u606f\u4e0e\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u8fed\u4ee3\u878d\u5408\u3002</p> <p>The module is easily adapted to zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot counting problems. \u8be5\u6a21\u5757\u53ef\u4ee5\u9002\u7528\u4e8e0-shot\u573a\u666f\uff0c\u4f7f\u5f97LOCA\u80fd\u591f\u8986\u76d6\u6574\u4e2a\u4f4e\u6837\u672c\u8ba1\u6570\u95ee\u9898\u3002</p> <p>LOCA outperforms all recent state-of-the-art methods on FSC147 benchmark by 20-30% in RMSE on one-shot and fewshot and achieves state-of-the-art on zero-shot scenarios, while demonstrating better generalization capabilities. The code and models are available.</p>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#-","title":"\u5f15\u5165-\u8d21\u732e","text":"<p>Introduction</p> <p>\u5012\u6570\u7b2c\u4e8c\u6bb5</p> <p>\uff08\u7b2c\u4e00\u4e2a\u8d21\u732e\uff0c\u63d0\u51faLOCA\uff09We propose a Low-shot Object Counting network with iterative prototype Adaptation (LOCA).</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u8fed\u4ee3\u539f\u578b\u81ea\u9002\u5e94( LOCA )\u7684\u5c0f\u6837\u672c\u7269\u4f53\u8ba1\u6570\u7f51\u7edc\u3002</p> <p>\uff08LOCA\u7684\u4e3b\u8981\u8d21\u732e\u662fOPE\u6a21\u5757\uff0cOPE\u6a21\u5757\u7684\u7a81\u51fa\u7279\u70b9\uff09Our main contribution is the new object prototype extraction module, which separately extracts the exemplar shape and appearance queries. The shape queries are gradually adapted into object prototypes by considering the exemplar appearance as well as the appearance of non-annotated objects, obtaining excellent localization properties and leading to highly accurate counts (Figure 1). </p> <ul> <li>OPE\u6a21\u5757\uff0c\u5206\u522b\u63d0\u53d6\u6837\u4f8b\u6846\u5f62\u72b6\u548c\u5916\u89c2\u67e5\u8be2</li> <li>\u5f62\u72b6\u7279\u5f81 \u662f\u9010\u6e10\u7684\u878d\u5408\u5230 \u76ee\u6807\u539f\u578b\u7684\u3002\uff08\u901a\u8fc7\u8003\u8651\u6837\u4f8b\u5916\u89c2\u548c\u975e\u6807\u6ce8\u5bf9\u8c61\u7684\u5916\u89c2\uff09 $\\rightarrow $  \u83b7\u5f97\u826f\u597d\u7684\u5b9a\u4f4d\u5c5e\u6027\uff0c\u5e76\u5f97\u5230\u9ad8\u5ea6\u51c6\u786e\u7684\u8ba1\u6570(\u56fe1 )\u3002</li> </ul> <p>\uff08\u4eae\u70b9\uff1aLOCA\u7b2c\u4e00\u4e2a\u4f7f\u7528\u6837\u4f8b\u6846\u5f62\u72b6\u4fe1\u606f\u6307\u5bfc\u8ba1\u6570)     To the best of our knowledge, LOCA is the first low-shot counting method that explicitly uses exemplars shape information for counting.</p> <p>\u5012\u6570\u7b2c\u4e00\u6bb5 \uff08\u5f15\u5165\u6700\u540e\u4e00\u6bb5 \u7ed3\u679c\u6bb5\uff09\u6307\u51fa\u672c\u6587\u6240\u7528\u6570\u636e\u96c6\uff1a</p> <ul> <li>FSC148\u6570\u636e\u96c6</li> <li>CARPK</li> </ul>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#_2","title":"\u7ed3\u8bba","text":"<p>P1  \u7b2c\u4e00\u70b9\uff1a\u63d0\u51fa\u4e86LOCA\uff0c\u5206\u522b\u8003\u8651\u793a\u4f8b\u7684\u5f62\u72b6\u7279\u5f81\u548c\u5916\u89c2\u7279\u5f81\uff0c\u5e76\u5206\u522b\u901a\u8fc7OPE\u6a21\u5757\u8fdb\u884c\u8fed\u4ee3\u878d\u5408</p> <p>\u56e0\u6b64\uff0c\u539f\u578b\u6cdb\u5316\u5230\u56fe\u50cf\u4e2d\u7684\u975e\u6807\u6ce8\u5bf9\u8c61\uff0c\u4ece\u800c\u5f97\u5230\u66f4\u597d\u7684\u5b9a\u4f4d\u6027\u8d28\u548c\u8ba1\u6570\u4f30\u8ba1\u3002</p> <p>We presented a new low-shot counting method LOCA, that addresses the limitations of the current state-of-the-art methods. LOCA considers the exemplar shape and appearance properties separately and iteratively adapts these into object prototypes by a new object prototype extraction (OPE) module considering the image-wide features. The prototypes thus generalize to the non-annotated objects in the image, leading to better localization properties and count estimates.</p> <p>P2 \u8bf4\u660e\u7ed3\u679c</p> <p>P3 \u6307\u51fa\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411</p> <p>We envision several possible future research directions. </p> <ul> <li>Additional supervision levels such as introducing negative exemplar annotations could be introduced in LOCA for better specification of the selected object class. This could lead to interactive tools for accurate object counting. </li> </ul> <p>\u4e3a\u4e86\u66f4\u597d\u5730\u89c4\u8303\u6240\u9009\u5bf9\u8c61\u7c7b\uff0c\u53ef\u4ee5\u5728LOCA\u4e2d\u5f15\u5165\u989d\u5916\u7684\u76d1\u7763\u7ea7\u522b\uff0c\u5982\u5f15\u5165\u8d1f\u4f8b\u6ce8\u91ca\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7528\u4e8e\u7cbe\u786e\u7269\u4f53\u8ba1\u6570\u7684\u4ea4\u4e92\u5f0f\u5de5\u5177\u3002</p> <ul> <li>Furthermore, a gap between low-shot counters and object detectors could be further narrowed by enabling bounding box or segmentation mask prediction in LOCA to output additional statistics about the counted objects such as average size, etc., which is useful for many practical applications such as biomedical analysis.  </li> </ul> <p>\u6b64\u5916\uff0c\u901a\u8fc7\u5728LOCA\u4e2d\u5b9e\u73b0\u8fb9\u754c\u6846\u6216\u5206\u5272\u63a9\u7801\u9884\u6d4b\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4f4e\u6837\u672c\u8ba1\u6570\u5668\u548c\u76ee\u6807\u68c0\u6d4b\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u8f93\u51fa\u5173\u4e8e\u8ba1\u6570\u5bf9\u8c61\u7684\u989d\u5916\u7edf\u8ba1\u4fe1\u606f\uff0c\u4f8b\u5982\u5e73\u5747\u5927\u5c0f\u7b49\uff0c\u8fd9\u5bf9\u4e8e\u751f\u7269\u533b\u5b66\u5206\u6790\u7b49\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u662f\u6709\u7528\u7684\u3002</p>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#_3","title":"\u5f15\u5165","text":"<p>P1 \u7279\u5b9a\u7269\u4f53\u8ba1\u6570\uff1b\u4eba\u7fa4\u3001\u6c7d\u8f66\u3001\u7269\u79cd\uff1b\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u6570\u636e \u2192 \u5c0f\u6837\u672c\u8ba1\u6570\uff08\u7531\u6837\u4f8b\u6846\u6807\u51fa\u76ee\u6807\uff09\u30010\u6837\u672c\u8ba1\u6570\uff08\u8ba1\u6570\u591a\u6570\u7c7b\u522b\uff09</p> <p>Object counting considers estimation of the number of specific objects in the image. Solutions based on object detectors have been extensively explored for categories such as people [1, 33], cars [20, 12] or animal species [2, 32]. However, these methods require huge annotated training datasets and are not applicable to counting new, previously unobserved, classes with potentially only few annotations. The latter problem is explored by low-shot counting, which encompasses few-shot and zero-shot counting. Few-shot counters count all present objects of some class with only few of them annotated by bounding boxes (exemplars), while zero-shot counters consider counting the most frequent class without annotations.</p> <p>P2   \u89d2\u5ea61\uff1a\u5c0f\u6837\u672c\u8ba1\u6570&amp;\u56de\u5f52\u5bc6\u5ea6\u56fe\uff1b\u89d2\u5ea62\uff1a0-shot</p> <ul> <li> <p>Few-shot counters have recently gained momentum with the emergence of a challenging dataset [24] and follow a common pipeline [18, 24, 13, 26, 31]\uff08\u5c0f\u6837\u672c\u8ba1\u6570\uff0c\u968f\u7740\u6570\u636e\u96c6\u7684\u51fa\u73b0\uff09. Image and exemplar features are extracted into object prototypes, which are matched to the image by correlation.\uff08\u56fe\u50cf\u548c\u6837\u4f8b\u7279\u5f81\u88ab\u63d0\u53d6\u5230\u5bf9\u8c61\u539f\u578b\u4e2d\uff0c\u5bf9\u8c61\u539f\u578b\u901a\u8fc7\u76f8\u5173\u6027\u4e0e\u56fe\u50cf\u8fdb\u884c\u5339\u914d\u3002\uff09 Finally, the obtained intermediate image representation is regressed into a 2D object density map, whose values sum to the object count estimate.\uff08\u6700\u540e\uff0c\u5c06\u5f97\u5230\u7684\u4e2d\u95f4\u56fe\u50cf\u8868\u793a\u56de\u5f52\u4e3a2D\u7269\u4f53\u5bc6\u5ea6\u56fe\uff0c\u5c06\u503c\u76f8\u52a0\u5f97\u5230\u76ee\u6807\u8ba1\u6570\u7684\u4f30\u8ba1\u503c\uff09 The methods primarily differ in the intermediate image representation construction method\uff08\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e \u4e2d\u95f4\u56fe\u50cf\u8868\u793a\u7684\u6784\u5efa\u8fc7\u7a0b\uff09\uff08\u4e3e\u4f8b\u5b50\u8bf4\u660e\uff09, which is based either on Siamese similarity [18, 24], cross-attention [16, 13] or feature and similarity fusion [26, 31].</p> </li> <li> <p>While receiving much less attention, zero-shot counters follow a similar principle, but either identify possible exemplars by majority vote from region proposals [22] or implicitly by attention modules [11].\u867d\u7136\u96f6\u6837\u672c\u8ba1\u6570\u5668\u53d7\u5230\u7684\u5173\u6ce8\u8f83\u5c11\uff0c\u4f46\u9075\u5faa\u7c7b\u4f3c\u7684\u539f\u5219\uff0c\u6216\u8005\u901a\u8fc7\u533a\u57df\u63d0\u6848\u7684\u591a\u6570\u6295\u7968\u6765\u8bc6\u522b\u53ef\u80fd\u7684\u6837\u672c[ 22 ]\uff0c\u6216\u8005\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u9690\u5f0f\u5730\u8bc6\u522b\u53ef\u80fd\u7684\u6837\u672c[ 11 ]\u3002</p> </li> </ul> <p>P3   \u5f15\u51fa\u672c\u6587\u8981\u8ba8\u8bba\u7684\u95ee\u9898\uff1a</p> <p>All few-shot counters construct object prototypes by pooling image features extracted from the exemplars into fixed-sized correlation filters. The prototypes thus fail to encode the object shape information (i.e., width, height and aspect), resulting in a reduced accuracy of the density map. Recent works have shown that this information loss can be partially addressed by complex architectures for learning a nonlinear similarity function [26]. Nevertheless, we argue that a much simpler counting architecture can be used instead, by explicitly addressing the exemplar shape and by applying an appropriate object prototype adaptation method.</p> <p>P4\u3001P5\uff1b\u8d21\u732e </p> <p>structure</p> <ol> <li>\u7279\u5b9a\u7269\u4f53\u8ba1\u6570</li> <li>\u5c0f\u6837\u672c\u8ba1\u6570 &amp;  \u56de\u5f52\u5bc6\u5ea6\u56fe</li> <li>\u5f15\u51fa\u95ee\u9898\uff1a\u4e22\u5931\u5f62\u72b6\u4fe1\u606f\uff0c\u5c3d\u7ba1\u73b0\u5728\u4e5f\u6709\u7814\u7a76\u65b9\u6cd5\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u662f\u7ed3\u6784\u6bd4\u8f83\u590d\u6742</li> <li> <ol> <li>\u8d21\u732e\u3001\u7ed3\u679c</li> </ol> </li> </ol>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#_4","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>P1 \u7279\u5b9a\u7269\u4f53\u8ba1\u6570</p> <p>Historically, object counting has been addressed by class-specific detectors for people [1, 33], cars [20, 12] and animals [2], but these methods do not cope well with extremely crowded scenes. In a jellyfish polyp counting scenario, [32] thus proposed to segment the image and interpret the segmentation as a collection of circular objects. Alternatively, [1, 6] framed counting as a regression of object density map, whose summation predicts the number of objects. A major drawback of these methods is that they require large annotated training datasets for each object class, which is often an unrealistic requirement.</p> <p>P2 class-agnostic counters  \u7c7b\u65e0\u5173\u8ba1\u6570\u7684\u53d1\u5c55</p> <p>In response, class-agnostic counters have been explored, that specialize to the object category at test-time using only a few user-provided object exemplars. </p> <p>\u6587\u732e\u6982\u8ff0</p> <ol> <li>An early representative [18] proposed a two-stream Generic Matching Network, that extracts the image and exemplar object features, concatenates them and regresses the representation into the final density map.  \u65e9\u671f\u7684\u4ee3\u8868[ 18 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6d41\u7684\u901a\u7528\u5339\u914d\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u63d0\u53d6\u56fe\u50cf\u548c\u6837\u672c\u5bf9\u8c61\u7279\u5f81\uff0c\u5e76\u5c06\u5b83\u4eec\u4e32\u8054\u8d77\u6765\uff0c\u5c06\u8868\u793a\u56de\u5f52\u5230\u6700\u7ec8\u7684\u5bc6\u5ea6\u56fe\u4e2d\u3002</li> <li>CFOCNet [30] noted that a mere concatenation leads to unreliable localization and proposed a Siamese correlation network(\u5b6a\u751f\u5339\u914d\u7f51) inspired by the tracking literature [3] to improve the localization and counts. CFOCNet [ 30 ]\u6307\u51fa\u7b80\u5355\u7684\u7ea7\u8054\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u5b9a\u4f4d\uff0c\u5e76\u53d7\u8ddf\u8e2a\u5347\u7684\u542f\u53d1\u63d0\u51fa\u4e86\u5b6a\u751f\u76f8\u5173\u7f51\u7edc\u6539\u8fdb\u4e86\u5b9a\u4f4d\u548c\u8ba1\u6570\u3002</li> <li>Ranjan et al. [24] proposed a further improvement of correlation robustness by test-time Siamese backbone adaptation. Ranjan\u7b49\u4eba[ 24 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6d4b\u8bd5\u65f6\u5b6a\u751f\u9aa8\u5e72\u7f51\u81ea\u9002\u5e94\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u76f8\u5173\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002</li> <li>Shi et al. [26] proposed an alternative approach for jointly learning the representation as well as a nonlinear similarity metric for improved localization and applied self-attention to reduce the within-class appearance variability in the test image. Shi\u7b49\u4eba[ 26 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u8868\u793a\u7684\u66ff\u4ee3\u65b9\u6cd5\u4ee5\u53ca\u6539\u8fdb\u5b9a\u4f4d\u7684\u975e\u7ebf\u6027\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5e76\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u6765\u51cf\u5c11\u6d4b\u8bd5\u56fe\u50cf\u4e2d\u7684\u7c7b\u5185\u5916\u89c2\u53d8\u5f02\u6027\u3002</li> <li>You et al. [31] combined the similarity map with the image features before applying location regression to improve count accuracy and proposed a learnable similarity metric to guide the fusion of exemplar and image features.You\u7b49[ 31 ]\u5728\u5e94\u7528\u4f4d\u7f6e\u56de\u5f52\u4e4b\u524d\u5c06\u76f8\u4f3c\u6027\u56fe\u4e0e\u56fe\u50cf\u7279\u5f81\u7ed3\u5408\u4ee5\u63d0\u9ad8\u8ba1\u6570\u7cbe\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u6765\u6307\u5bfc\u793a\u4f8b\u548c\u56fe\u50cf\u7279\u5f81\u7684\u878d\u5408\u3002</li> <li>Liu et al. [16] adopted a vision transformer [7] for image feature extraction and a convolutional encoder to extract the exemplars. Cross-attention is used to fuse image and exemplar features and a convolutional decoder regresses the density map. Liu\u7b49[ 16 ]\u91c7\u7528\u89c6\u89c9\u8f6c\u6362\u5668[ 7 ]\u8fdb\u884c\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\uff0c\u91c7\u7528\u5377\u79ef\u7f16\u7801\u5668\u63d0\u53d6\u6837\u672c\u3002\u4ea4\u53c9\u6ce8\u610f\u529b\u7528\u4e8e\u878d\u5408\u56fe\u50cf\u548c\u6837\u672c\u7279\u5f81\uff0c\u5377\u79ef\u89e3\u7801\u5668\u5bf9\u5bc6\u5ea6\u56fe\u8fdb\u884c\u56de\u5f52\u3002</li> <li>Recently, few-shot counting has been extended to few-shot detection [21] by adopting the transformer-based object detector [29] to predict also the object bounding box in addition to location.\u6700\u8fd1\uff0c\u5c0f\u6837\u672c\u8ba1\u6570\u5df2\u7ecf\u6269\u5c55\u5230\u5c0f\u6837\u672c\u68c0\u6d4b[ 21 ]\uff0c\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u76ee\u6807\u68c0\u6d4b\u5668[ 29 ]\uff0c\u9664\u4e86\u4f4d\u7f6e\u5916\uff0c\u8fd8\u9884\u6d4b\u76ee\u6807\u8fb9\u754c\u6846\u3002</li> </ol> <p>P3  3-shot\u2192 fewer shot</p> <p>\uff08\u8fd9\u6bb5\u6587\u732e\u7efc\u8ff0\u7684\u4e3b\u9898\uff09While most works addressed situations with several (typically three) exemplars available, only few recent works considered reducing this number. </p> <ul> <li>Lin et al. [13] proposed a counting method that requires only a single exemplar. Their method is based on a transformer architecture and formulates correlation between image and exemplar features by several self- and cross-attention blocks. </li> </ul> <p>Lin\u7b49\u4eba[ 13 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u53ea\u9700\u8981\u5355\u4e2a\u6837\u672c\u7684\u8ba1\u6570\u65b9\u6cd5\u3002\u4ed6\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u4e00\u79cd\u8f6c\u6362\u5668\u7ed3\u6784\uff0c\u901a\u8fc7\u51e0\u4e2a\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\u6765\u5efa\u7acb\u56fe\u50cf\u548c\u6837\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002</p> <p>\uff08\u6781\u7aef\u60c5\u51b5 0-shot\uff09An extreme case of zero-shot counting [22, 11] has been explored as well. </p> <ul> <li>Ranjan and Hoai [22] proposed RepRPN-Counter, which combines a region proposal network [25] that also predicts a repetition score of each proposal.Ranjan\u548cHoai</li> </ul> <p>[ 22 ]\u63d0\u51fa\u4e86RepRPN - Counter\uff0c\u5b83\u7ed3\u5408\u4e86\u4e00\u4e2a\u533a\u57df\u63d0\u6848\u7f51\u7edc[ 25 ]\uff0c\u8be5\u7f51\u7edc\u4e5f\u9884\u6d4b\u6bcf\u4e2a\u63d0\u6848\u7684\u91cd\u590d\u8bc4\u5206\u3002</p> <ul> <li>Proposals with the highest repetition scores are used as exemplars and sent through FamNet [24] to predict multiple density maps. </li> </ul> <p>\u91cd\u590d\u5f97\u5206\u6700\u9ad8\u7684\u63d0\u6848\u4f5c\u4e3a\u793a\u4f8b\uff0c\u901a\u8fc7Fam Net [ 24 ]\u53d1\u9001\uff0c\u9884\u6d4b\u591a\u4e2a\u5bc6\u5ea6\u56fe\u3002</p> <ul> <li>On the other hand, Hobley and Prisacariu [11] developed a weakly supervised method that implicitly identifies object category most likely to be counted and predicts a density map for that category. </li> </ul> <p>\u53e6\u4e00\u65b9\u9762\uff0cHobley\u548cPrisacariu [ 11 ]\u53d1\u5c55\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u5b83\u9690\u5f0f\u5730\u8bc6\u522b\u6700\u6709\u53ef\u80fd\u88ab\u8ba1\u6570\u7684\u5bf9\u8c61\u7c7b\u522b\uff0c\u5e76\u9884\u6d4b\u8be5\u7c7b\u522b\u7684\u5bc6\u5ea6\u56fe\u3002</p> <ul> <li>Vision transformer with a unsupervised training stage [16] has also shown success in zero-shot counting. </li> </ul> <p>\u5177\u6709\u65e0\u76d1\u7763\u8bad\u7ec3\u9636\u6bb5\u7684\u89c6\u89c9\u8f6c\u6362\u5668[ 16 ]\u5728\u96f6\u6837\u672c\u8ba1\u6570\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u6210\u529f\u3002</p> <p>241118</p> <p>todo\uff1a\u540e\u9762\u7684\u65b9\u6cd5</p>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/","title":"rank7 SemAug CountTR","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p></p> <p>\u6807\u9898\uff1aSemantic Generative Augmentations for Few-Shot Counting  \u5c0f\u6837\u672c\u8ba1\u6570 \u8bed\u4e49\u751f\u6210\u589e\u5f3a</p> <p>\u672c\u6587\u2764\ufe0f\uff1a</p> <ul> <li>\u4e3a\u4e86\u4f7f\u751f\u6210\u7684\u56fe\u50cf \u7684\u76ee\u6807\u6570\u91cf\u548c\u539f\u56fe\u50cf\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f7f\u7528 stable diffusion\u5408\u6210\u56fe\u50cf\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u53cc\u6761\u4ef6\uff1aprompt &amp; \u5bc6\u5ea6\u56fe</li> <li>\u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u56fe\u50cf\u603b\u662f\u8ddf\u8bad\u7ec3\u56fe\u50cf\u76f8\u540c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u589e\u5f3a\u56fe\u50cf\u591a\u6837\u6027\u7684\u7b56\u7565\uff1a\u968f\u673a\u6253\u4e71\u5b57\u5e55\u63cf\u8ff0\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u521b\u5efa\u6ca1\u89c1\u8fc7\u4f46\u662f\u5408\u7406\u7684 \u5bf9\u8c61\u7c7b\u578b\u548c\u7a7a\u95f4\u5206\u5e03</li> </ul> <p>\u5408\u6210\u56fe\u50cf &amp; \u591a\u6837\u5316\u7b56\u7565</p>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#abstract","title":"Abstract","text":"<p>\uff08\u6587\u751f\u56fe\u6269\u6563\u6a21\u578b\uff09</p> <p>With the availability of powerful text-to-image diffusion models, recent works have explored the use of synthetic data to improve image classification performances. </p> <p>\u968f\u7740\u5f3a\u5927\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u53ef\u7528\u6027\uff0c\u6700\u8fd1\u7684\u5de5\u4f5c\u63a2\u7d22\u4e86\u4f7f\u7528\u5408\u6210\u6570\u636e\u6765\u63d0\u9ad8\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002</p> <p>These works show that it can effectively augment or even replace real data.</p> <p>\u8fd9\u4e9b\u5de5\u4f5c\u8868\u660e\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u589e\u5f3a\u751a\u81f3\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u3002 </p> <p>In this work, we investigate how synthetic data can benefit few-shot class-agnostic counting. </p> <p>\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5408\u6210\u6570\u636e\u5982\u4f55\u6709\u5229\u4e8e\u5c0f\u6837\u672c\u7c7b\u522b\u65e0\u5173\u8ba1\u6570\u3002</p> <p>\u5408\u6210\u56fe\u50cf\u7684\u7b2c\u4e00\u4e2a\u8981\u6c42\uff1a\u5408\u6210\u7684\u56fe\u7247 \u76ee\u6807\u6570\u91cf\u662f\u76f8\u7b49\u7684</p> <p>This requires to generate images that correspond to a given input number of objects. </p> <p>\u8fd9\u5c31\u9700\u8981\u751f\u6210\u4e0e\u7ed9\u5b9a\u7684\u8f93\u5165\u7269\u4f53\u6570\u91cf\u76f8\u5bf9\u5e94\u7684\u56fe\u50cf\u3002</p> <p>However, text-to-image models struggle to grasp the notion of count.</p> <p>\u7136\u800c\uff0c\u6587\u672c\u5230\u56fe\u50cf\u7684\u6a21\u578b\u5f88\u96be\u628a\u63e1\u8ba1\u6570\u7684\u6982\u5ff5\u3002</p> <p>\u5408\u6210\u56fe\u50cf\u7684\u76d1\u7763\u4fe1\u53f7\uff1aprompt\u548c\u5bc6\u5ea6\u56fe\uff1b\u5177\u4f53\u7528\u7684\u6a21\u578b\uff1aStable Diffusion </p> <p>We propose to rely on a double conditioning of Stable Diffusion with both a prompt and a density map in order to augment a training dataset for few-shot counting. </p> <p>\u6211\u4eec\u63d0\u51fa\u4f7f\u7528\u7a33\u5b9a\u6269\u6563( Stable Diffusion )\u7684\u63d0\u793a\u56fe\u548c\u5bc6\u5ea6\u56fe\u7684\u53cc\u91cd\u6761\u4ef6\u6765\u589e\u52a0\u5c11\u6837\u672c\u8ba1\u6570\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002</p> <p>Due to the small dataset size, the fine-tuned model tends to generate images close to the training images. </p> <p>\u7531\u4e8e\u6570\u636e\u96c6\u89c4\u6a21\u8f83\u5c0f\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u63a5\u8fd1\u8bad\u7ec3\u56fe\u50cf\u7684\u56fe\u50cf\u3002</p> <p>\u4e3a\u4e86\u89e3\u51b3 \u56fe\u50cf\u603b\u662f\u63a5\u8fd1\u8bad\u7ec3\u56fe\u50cf\u7684\u95ee\u9898\uff0c\u63d0\u51fa \u968f\u673a\u6253\u4e71\u56fe\u50cf\u4e4b\u95f4\u7684\u5b57\u5e55</p> <p>We propose to enhance the diversity of synthesized images by exchanging captions between images thus creating unseen configurations of object types and spatial layout. </p> <p>\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u5728\u56fe\u50cf\u4e4b\u95f4\u4ea4\u6362\u5b57\u5e55\u6765\u589e\u5f3a\u5408\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u521b\u5efa\u770b\u4e0d\u89c1\u7684\u5bf9\u8c61\u7c7b\u578b\u548c\u7a7a\u95f4\u5e03\u5c40\u914d\u7f6e\u3002</p> <p>\uff08\u7ed3\u679c\uff09Our experiments show that our diversified generation strategy significantly improves the counting accuracy of two recent and performing few-shot counting models on FSC147 and CARPK.</p> <p>\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u591a\u6837\u5316\u751f\u6210\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86FSC147\u548cCARPK\u4e0a\u6700\u8fd1\u6267\u884c\u7684\u4e24\u4e2a\u5c11\u6837\u672c\u8ba1\u6570\u6a21\u578b\u7684\u8ba1\u6570\u51c6\u786e\u7387\u3002</p> <p>\u6570\u636e\u96c6\uff1a</p> <ul> <li>FSC147</li> <li>CARPK</li> </ul>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#-","title":"\u5f15\u5165-\u8d21\u732e","text":"<p>To tackle few-shot counting, we propose to synthesize unseen data with Stable Diffusion conditioned by both a textual prompt and a density map.</p> <p>\u4e3a\u4e86\u89e3\u51b3\u5c0f\u6837\u672c\u8ba1\u6570\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4f7f\u7528\u7a33\u5b9a\u6269\u6563\u6765\u5408\u6210\u770b\u4e0d\u89c1\u7684\u6570\u636e\uff0c\u5176\u6761\u4ef6\u662f\u6587\u672c\u63d0\u793a\u548c\u5bc6\u5ea6\u56fe\u3002</p> stable diffusion\uff1f <p>Note</p> <p>\u2460 \u4f7f\u7528stable diffusion\u5408\u6210\u6570\u636e  \u2461 \u76d1\u7763\u4fe1\u53f7\uff1a\u6587\u672c\u63d0\u793a\u548c\u5bc6\u5ea6\u56fe       </p> <p>We thus build an augmented FSC dataset that is used to train a deep counting network. </p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u589e\u5e7f\u7684FSC\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u8ba1\u6570\u7f51\u7edc\u3002</p> <p>The double conditioning, implemented with ControlNet [42], allows us to generate novel synthetic images with a precise control, preserving the ground truth for the counting task. </p> <p>\u7528controlnet\u7f51\u7edc[ 42 ]\u5b9e\u73b0\u7684\u53cc\u91cd\u6761\u4ef6\u5316\uff0c\u53ef\u4ee5\u4f7f\u6211\u4eec\u5728\u7cbe\u786e\u63a7\u5236\u4e0b\u751f\u6210\u65b0\u7684\u5408\u6210\u56fe\u50cf\uff0c\u4ece\u800c\u4e3a\u8ba1\u6570\u4efb\u52a1\u4fdd\u7559\u57fa\u672c\u7684\u771f\u503c\u3002</p> <p>It deals well with large numbers of objects, while current methods fail in such cases [19, 27]. </p> <p>\u5b83\u53ef\u4ee5\u5f88\u597d\u5730\u5904\u7406\u5927\u91cf\u7684\u5bf9\u8c61\uff0c\u800c\u76ee\u524d\u7684\u65b9\u6cd5\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5931\u6548[ 19\u300127]\u3002</p> <p>To increase the diversity of the augmented training set, we swap image descriptions between the n available training samples, leading to $\\frac{n(n\u22121)}{2} $ novel couples, each being the source of several possible synthetic images.</p> <p>\u4e3a\u4e86\u589e\u52a0\u6269\u5145\u8bad\u7ec3\u96c6\u7684\u591a\u6837\u6027\uff0c\u6211\u4eec\u5728n\u4e2a\u53ef\u7528\u7684\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u4ea4\u6362\u56fe\u50cf\u63cf\u8ff0\uff0c\u5f97\u5230n ( n-1 ) 2\u4e2a\u65b0\u7684\u5bf9\u5b50\uff0c\u6bcf\u4e2a\u5bf9\u5b50\u90fd\u662f\u82e5\u5e72\u53ef\u80fd\u7684\u5408\u6210\u56fe\u50cf\u7684\u6765\u6e90\u3002</p> <p>However, we show that some combinations do not make sense and lead to poor quality samples. </p> <p>\u7136\u800c\uff0c\u6211\u4eec\u8868\u660e\u4e00\u4e9b\u7ec4\u5408\u6ca1\u6709\u610f\u4e49\uff0c\u5e76\u5bfc\u81f4\u8d28\u91cf\u8f83\u5dee\u7684\u6837\u672c\u3002</p> <p>Therefore, we only select plausible pairs, resulting in improved augmentation quality. </p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u53ea\u9009\u62e9\u4e86\u4f3c\u662f\u800c\u975e\u7684\u914d\u5bf9\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u589e\u5f3a\u8d28\u91cf\u3002</p> <p>We evaluate our approach on two class-agnostic counting networks, namely SAFECount [41] and CounTR [6]. We show that it significantly improves the performances on the benchmark dataset FSC147 [28] and allow for a better generalization on the CARPK dataset [14].</p> <p>\u6211\u4eec\u5728SAFECount [ 41 ]\u548cCoun TR [ 6 ]\u4e24\u4e2a\u7c7b\u4e0d\u53ef\u77e5\u8ba1\u6570\u7f51\u7edc\u4e0a\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u5b83\u5728\u57fa\u51c6\u6570\u636e\u96c6FSC147 [ 28 ]\u4e0a\u7684\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff0c\u5e76\u4e14\u5728CARPK\u6570\u636e\u96c6[ 14 ]\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</p> <ul> <li> <p>\u5bf9\u6bd4\u6a21\u578b\uff1aSAFECount [41] and CounTR [6]</p> </li> <li> <p>benchmark\uff1aFSC147\u3001CARPK</p> </li> <li>\u672c\u6587\u63d0\u51fa\u7684\u662f \u5bf9\u6570\u636e\u8f93\u5165\u7684\u591a\u6837\u6027\u8fdb\u884c\u6269\u5145</li> </ul>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#_1","title":"\u7ed3\u8bba","text":"<p>7 Conclusion</p> <p>\u7531\u6269\u6563\u6a21\u578b\u5408\u6210\u6570\u636e\u63d0\u9ad8FSC\u8ba1\u6570\u6027\u80fd</p> <p>We show that synthetic data generated by diffusion models improve deep models for few-shot counting. </p> <p>\u4ee5\u5bc6\u5ea6\u56fe\u4e3a\u6761\u4ef6\uff0c\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u6587\u751f\u56fe\u6a21\u578b</p> <p>We adapt a pretrained text-to-image model with a density map conditioning and </p> <p>\u6211\u4eec\u63d0\u51fa\u7684\u591a\u6837\u5316\u7b56\u7565\uff1a\u5229\u7528\u5b57\u5e55\u76f8\u4f3c\u6027\uff0c\u751f\u6210\u5408\u7406\u7684\u4f46\u662f \u6df7\u5408\u4e86\u4e0d\u540c\u8bad\u7ec3\u56fe\u50cf\u548c\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f</p> <p>we propose a diversification strategy that exploits caption similarities to generate unseen but plausible data that mixes the semantics and the geometry of different training images. </p> <p>\u5c55\u793a\u4e86\u9009\u62e9  compatible images \uff08\u76f8\u5bb9\u7684\u56fe\u50cf\uff1f\uff09\u5408\u6210\u56fe\u50cf\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd|| \u6211\u8bb0\u5f97\u6709\u4e00\u4e2a\u6a21\u578b\u7684\u62fc\u63a5\u56fe\u50cf\u6765\u7740\uff0c\u54ea\u7bc7\u8bba\u6587\u6765\u7740\uff1f</p> <p>We show that selecting compatible images improves synthetic image quality with beneficial effects on model performance. </p> <p>\u6211\u4eec\u63d0\u51fa\u7684\u591a\u6837\u6027\u6570\u636e\u5408\u6210\u7b56\u7565\u63d0\u9ad8\u4e86\u8ba1\u6570\u6027\u80fd\uff0cFSC147 \u548c CARPK</p> <p>We demonstrate that learning with our diverse synthetic data leads to improved counting accuracy on FSC147 and state of the art generalization on CARPK.</p> <p>\u6211\u4eec\u63d0\u51fa\u7684\u6570\u636e\u5408\u6210\u7b56\u7565\u7ecf\u8fc7\u5fae\u8c03\u53ef\u4ee5\u7528\u4e8e\u5176\u4ed6\u9886\u57df\uff1a\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272</p> <p>This strategy could be adapted to other tasks requiring fine grained compositionality, such as object detection and semantic segmentation. </p> <p>\u6211\u4eec\u7684\u591a\u6837\u5316\u7b56\u7565\uff1a\u901a\u8fc7\u5728\u5bc6\u5ea6\u56fe\u5f15\u5165\u5408\u9002\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u3001\u901a\u8fc7\u6587\u672c\u4e4b\u95f4\u76f8\u4e92\u4ea4\u6362\u548c\u5bc6\u5ea6\u56fe\u7684\u63a7\u5236\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u6269\u5c55</p> <p>Our diversification scheme could be further extended by swapping both the captions and the density controls, by introducing a suitable similarity metric that operates on the density maps.</p>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#_2","title":"\u5f15\u5165","text":"<p>1 Introduction</p> <p>P1 </p> <p>\u76ee\u6807\u8ba1\u6570\u7684\u5e94\u7528\u9886\u57df</p> <p>Counting objects is a task with applications in many domains e.g. manufacturing, medicine, monitoring, that involve different types of objects.</p> <p>\u7279\u5b9a\u76ee\u6807\u8ba1\u6570 \u2192FSC CAC\u8ba1\u6570</p> <p>\u4e24\u4e2a\u7a81\u51fa\u7279\u70b9\uff1a\u2460 bounding boxes (cf. Fig. 2),  \u2461 an extract-then-match manner [21].</p> <p>(\u540e\u9762\u6709\u4eba\u521b\u65b0\uff0c\u5c31\u628a\u8fd9\u4e2athen\u6539\u6210 and)</p> <p>While earlier works focused on learning specialized networks [2, 7, 14, 16], Few-Shot object Counting (FSC) [31] was recently introduced to train models that can count any object, including from categories outside the training data. Methods tackling FSC rely on exemplar objects annotated with bounding boxes (cf. Fig. 2), in an extract-then-match manner [21]. </p> <p>\u5982\u4f55\u5bf9\u56fe\u50cf\u7279\u5f81 \u548c \u6837\u4f8b\u6846\u7279\u5f81 \u8fdb\u884c\u5339\u914d\uff1a\u2460 correlation maps [31, 41]   \u2461attention [6, 9]</p> <p>The features of the exemplars and query image are compared using e.g. correlation maps [31, 41] or attention [6, 9]. Matched features are then transformed into a density map indicating at each location in the image the density of the objects of interest. The density map is then summed to obtain the predicted count.</p> <p>P2</p> <p>\u6570\u636e\u96c6\u751f\u6210\uff1a\u4eceGAN \u2192 \u6269\u6563\u6a21\u578b</p> <p>\u63d0\u51fachallenge  FSC147\u6570\u636e\u96c6\u6709\u9650 The reference dataset for FSC, namely FSC147 [31], contains a limited amount of data (3659 train images) thus bounding\u9650\u5236 the performances of counting networks [30]. </p> <p>\u6269\u5145\u6570\u636e\u96c6\u5f88\u9ebb\u70e6 Expanding such a dataset is costly as the annotation process requires pinpointing the center of each object present in a query image, with a potentially high number of occurrences. </p> <p>solutions To overcome the small dataset size, Ranjan et al. [30] augment FSC147 using a GAN to diversify the image styles.solutions**\u2460  Ranjan et al. [30]\u8fd9\u4e2a\u4eba\u7528GAN \u591a\u6837\u5316\u56fe\u50cf\u7684\u683c\u5f0f**</p> <p>Diffusion models have now surpassed GANs owing to their training stability and lower sensitivity to mode collapse. \u73b0\u72b6\uff1a\u6269\u6563\u6a21\u578b\ud83d\udd25\u4e86</p> <p>These models produce more effective and diverse augmentations [12, 37, 39]. Recent works mostly aim at augmenting classification datasets e.g. ImageNet [8], where augmentations are generated by prompting the models with the image labels. \u6269\u6563\u6a21\u578b\ud83d\udd25\u7684\u8bc1\u636e\uff0c\u4e14\u4e3b\u8981\ud83d\udd25\u5728\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u56fe\u50cf\u6807\u7b7e\u6765\u63d0\u793a\u6a21\u578b</p> <p>**motivation \u5230\u4e86\u6211\u4eec\u8981\u8ba8\u8bba\u7684\u95ee\u9898\uff1a\u6ca1\ud83d\udd25\u5230\u8ba1\u6570\u6570\u636e\u96c6**This fails to produce satisfying images for counting datasets as text-to-image models struggle to generate the correct number of objects [26]. \u56e0\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u5f88\u96be\u4ea7\u751f \u5bf9\u8c61\u6570\u91cf\u6b63\u786e\u7684\u6570\u636e\u96c6</p> <p>\u505a\u7684\u4e00\u4e9b\u52aa\u529b Some works tackle improving compositionality in vision-language models [19, 25, 27] but are limited to small numbers of objects. \u4e00\u4e9b\u5de5\u4f5c\u81f4\u529b\u4e8e\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b[ 19\u300125\u300127]\u7684\u7ec4\u5408\u6027\uff0c\u4f46\u4ec5\u9650\u4e8e\u5c11\u91cf\u5bf9\u8c61\u3002</p> <p>Other works add more control to pre-trained text-to-image models [15, 23, 42].\u5176\u4ed6\u5de5\u4f5c\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b[ 15\u300123\u300142]\u4e2d\u6dfb\u52a0\u4e86\u66f4\u591a\u7684\u63a7\u5236\u3002</p> <p>P3 \u672c\u6587\u7684\uff1aStable Diffusion &amp; ControlNet [42]</p> <p>\u6211\u4eec\u7684\u5de5\u4f5c \u2b50\ufe0f**To tackle few-shot counting, **we propose to synthesize unseen data with Stable Diffusion conditioned by both a textual prompt and a density map. </p> <p>\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u548c\u5bc6\u5ea6\u56fe \u4f7f\u7528\u6269\u6563\u6a21\u578b \u751f\u6210\u6570\u636e</p> <p>We thus build an augmented FSC dataset that is used to train a deep counting network. </p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u589e\u5e7f\u7684FSC\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u8ba1\u6570\u7f51\u7edc\u3002</p> <p>The double conditioning, implemented with ControlNet [42], allows us to generate novel synthetic images with a precise control, preserving the ground truth for the counting task. </p> <p>\u7528controlnet\u7f51\u7edc[ 42 ]\u5b9e\u73b0\u7684\u53cc\u91cd\u6761\u4ef6\u5316\uff0c\u53ef\u4ee5\u4f7f\u6211\u4eec\u5728\u7cbe\u786e\u63a7\u5236\u4e0b\u751f\u6210\u65b0\u7684\u5408\u6210\u56fe\u50cf\uff0c\u4ece\u800c\u4e3a\u8ba1\u6570\u4efb\u52a1\u4fdd\u7559\u57fa\u672c\u7684\u771f\u503c\u3002</p> <p>It deals well with large numbers of objects, while current methods fail in such cases [19, 27].</p> <p>\u5b83\u53ef\u4ee5\u5f88\u597d\u5730\u5904\u7406\u5927\u91cf\u7684\u5bf9\u8c61\uff0c\u800c\u76ee\u524d\u7684\u65b9\u6cd5\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5931\u6548[ 19\u300127]\u3002</p> <p>To increase the diversity of the augmented training set, we swap image descriptions between the \\(n\\) available training samples, leading to $\\frac{n(n\u22121)}{2} $ novel couples, each being the source of several possible synthetic images. </p> <p>\u4e3a\u4e86\u589e\u52a0\u6269\u5145\u8bad\u7ec3\u96c6\u7684\u591a\u6837\u6027\uff0c\u6211\u4eec\u5728n\u4e2a\u53ef\u7528\u7684\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u4ea4\u6362\u56fe\u50cf\u63cf\u8ff0\uff0c\u5f97\u5230n ( n-1 ) 2\u4e2a\u65b0\u7684\u5bf9\u5b50\uff0c\u6bcf\u4e2a\u5bf9\u5b50\u90fd\u662f\u82e5\u5e72\u53ef\u80fd\u7684\u5408\u6210\u56fe\u50cf\u7684\u6765\u6e90\u3002</p> <p>However, we show that some combinations do not make sense and lead to poor quality samples. </p> <p>\u7136\u800c\uff0c\u6211\u4eec\u8868\u660e\u4e00\u4e9b\u7ec4\u5408\u6ca1\u6709\u610f\u4e49\uff0c\u5e76\u5bfc\u81f4\u8d28\u91cf\u8f83\u5dee\u7684\u6837\u672c\u3002</p> <p>Therefore, we only select plausible pairs, resulting in improved augmentation quality. </p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u53ea\u9009\u62e9\u4e86\u4f3c\u662f\u800c\u975e\u7684\u914d\u5bf9\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u589e\u5f3a\u8d28\u91cf\u3002</p> <p>We evaluate our approach on two class-agnostic counting networks, namely SAFECount [41] and CounTR [6].</p> <p>\u6211\u4eec\u5728SAFECount [ 41 ]\u548cCoun TR [ 6 ]\u4e24\u4e2a\u7c7b\u4e0d\u53ef\u77e5\u8ba1\u6570\u7f51\u7edc\u4e0a\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002</p> <p>We show that it significantly improves the performances on the benchmark dataset FSC147 [28] and allow for a better generalization on the CARPK dataset [14].</p> <p>\u6211\u4eec\u8bc1\u660e\u4e86\u5b83\u5728\u57fa\u51c6\u6570\u636e\u96c6FSC147 [ 28 ]\u4e0a\u7684\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff0c\u5e76\u4e14\u5728CARPK\u6570\u636e\u96c6[ 14 ]\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</p>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#_3","title":"\u603b\u7ed3","text":"<p>\u672c\u6587\u7684\u5f15\u5165\u662f\u4e09\u6bb5\uff1a</p> <p>\u7b2c\u4e00\u6bb5\uff1a\u76ee\u6807\u8ba1\u6570\u7684\u5e94\u7528\u9886\u57df \u7ecf\u5386\u4e86\u4ece\u7279\u5b9a\u7269\u4f53 \u5230 \u901a\u7528\u7269\u4f53</p> <p>\u7b2c\u4e8c\u6bb5\uff1a\u4ecb\u7ecd\u6570\u636e\u96c6\u5408\u6210\u65b9\u6cd5\uff1a\u4eceGAN \u5230 Stable Diffusion</p> <p>\u7b2c\u4e09\u6bb5\uff1a\u6307\u51fa\u672c\u6587\u8d21\u732e\uff0c\u518d\u6b21\u5f3a\u8c03</p> <ul> <li>\u751f\u6210\u56fe\u50cf\uff1aStable diffusion</li> <li>\u5408\u6210\u56fe\u50cf\u7684\u76ee\u6807\u6570\u91cf  \u548c \u53c2\u8003\u56fe\u50cf \u662f\u76f8\u540c\u7684</li> <li>prompt \u548c density map\u540c\u65f6\u6307\u5bfc\u56fe\u50cf\u5408\u6210</li> <li>\u5408\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u7b56\u7565\uff1a</li> <li>swap image descriptions \uff1b\u968f\u673a\u4ea4\u6362\u56fe\u50cf\u63cf\u8ff0</li> </ul>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#_4","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>\u603b\u7ed3\uff1a\u672c\u6587\u7684\u76f8\u5173\u5de5\u4f5c\u4ece\u4e24\u65b9\u9762\u5c55\u5f00\uff1a</p> <p>Learning with Generated Data</p> <p>Few-shot Object Counting</p> <p>\u751f\u6210\u6570\u636e\u7684\u5b66\u4e60 \u548c \u5c0f\u6837\u672c\u8ba1\u6570</p> <p>\u7b2c\u4e00\u6bb5\uff1a</p> <p>\u7b2c\u4e00\u90e8\u5206\uff1aLearning with Generated Data</p> <p>\uff08\u73b0\u72b6\uff1a\uff09</p> <p>Improvements in image synthesis using generative models have sparked great interest in generating fake images to train deep neural networks. </p> <p>\u4f7f\u7528\u751f\u6210\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u5408\u6210\u7684\u6539\u8fdb\u6fc0\u53d1\u4e86\u4eba\u4eec\u5bf9\u751f\u6210\u5047\u56fe\u50cf\u4ee5\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6781\u5927\u5174\u8da3\u3002 </p> <p>\uff08\u4eceGAN\u5f00\u59cb\uff09</p> <p>GANs were the first popular models to synthesize data for image classification [1, 5, 17], crowd counting [40] and image segmentation [43]. </p> <p>GANs\u662f\u7b2c\u4e00\u4e2a\u6d41\u884c\u7684\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b[ 1\u30015\u300117]\u3001\u4eba\u7fa4\u8ba1\u6570[ 40 ]\u548c\u56fe\u50cf\u5206\u5272[ 43 ]\u7684\u6570\u636e\u5408\u6210\u6a21\u578b\u3002 </p> <p>\uff08\u5230\u73b0\u5728\u7684\u6269\u6563\u6a21\u578b\uff1aDDPM\u3001 Latent Diffusion \uff09</p> <p>Nowadays, diffusion models such as DDPM [13] or Latent Diffusion [32] seem to outperform GANs, demonstrating more stable training, better coverage of the training distribution and higher image quality. </p> <p>\u5982\u4eca\uff0c\u6269\u6563\u6a21\u578b\u5982DDPM [ 13 ]\u6216Latent Diffusion [ 32 ]\u4f3c\u4e4e\u4f18\u4e8eGANs\uff0c\u663e\u793a\u51fa\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\uff0c\u66f4\u597d\u7684\u8bad\u7ec3\u5206\u5e03\u8986\u76d6\u7387\u548c\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\u3002 </p> <p>\uff08\u6269\u6563\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ee5\u6587\u672c\u4e3a\u6761\u4ef6\u7684\u6269\u6563\u6a21\u578b\uff09</p> <p>The availability of powerful text-conditioned diffusion models \u6587\u672c\u6761\u4ef6\u6269\u6563\u6a21\u578b [24, 29, 32, 33] has led to many works exploring how to leverage synthetic data for computer vision \u5229\u7528\u751f\u6210\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1, e.g. image classification in low-data regime [12], zero/few-shot learning [37, 39], ImageNet classification [3, 4, 34] and self-supervised learning [38]. </p> <p>\u5f3a\u5927\u7684\u6587\u672c\u6761\u4ef6\u6269\u6563\u6a21\u578b[ 24\u300129\u300132\u300133]\u7684\u51fa\u73b0\uff0c\u5f15\u53d1\u4e86\u8bb8\u591a\u7814\u7a76\u5982\u4f55\u5229\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5de5\u4f5c\u3002</p> <p>These works focus on how to reduce domain gap \u51cf\u5c11\u9886\u57df\u9e3f\u6c9f [12], improve the prompts \u6539\u8fdb\u63d0\u793a using e.g. text-to-sentence model [12] or WordNet [34] and increase diversity \u589e\u52a0\u591a\u6837\u6027 by optimizing the guidance scale [3, 34, 37].   \u4f18\u5316\u6307\u5bfc\u5c3a\u5ea6</p> <p>\u8fd9\u4e9b\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u4f55\u51cf\u5c11\u9886\u57df\u9e3f\u6c9f[ 12 ]\uff0c\u4f7f\u7528\u6587\u672c\u5230\u53e5\u5b50\u6a21\u578b[ 12 ]\u6216\u8bcd\u7f51[ 34 ]\u6765\u6539\u8fdb\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6307\u5bfc\u5c3a\u5ea6[ 3,34,37]\u6765\u589e\u52a0\u591a\u6837\u6027\u3002</p> <p>This body of literature consistently demonstrates how generated data allow deep networks to learn more robust representations and improve generalization for image classification. </p> <p>\u8fd9\u7ec4\u6587\u732e\u4e00\u81f4\u5730\u5c55\u793a\u4e86\u751f\u6210\u6570\u636e\u5982\u4f55\u8ba9\u6df1\u5ea6\u7f51\u7edc\u5b66\u4e60\u66f4\u9c81\u68d2\u7684\u8868\u793a\uff0c\u5e76\u63d0\u9ad8\u56fe\u50cf\u5206\u7c7b\u7684\u6cdb\u5316\u6027\u3002 </p> <p>\u6211\u4eec\u5728\u56fe\u50cf\u5408\u6210\u9886\u57df\u7684\u5de5\u4f5c\uff1a</p> <p>\u2764\ufe0f\uff1aTo bring the power of synthetic data to counting,  we propose to condition diffusion models  not only on text prompts but also on counting density maps to generate images with the correct number of objects in the desired spatial configuration. </p> <ul> <li>\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff1a\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u548c\u5bc6\u5ea6\u56fe</li> <li>\u5728\u671f\u671b\u7684\u7a7a\u95f4\u4f4d\u7f6e\u4e0a\uff0c\u751f\u6210\u6709\u6b63\u786e\u6570\u91cf\u7684\u56fe\u7247</li> </ul> <p>We focus more specifically on few-shot class-agnostic object counting. Compared to image classification, this task involves small datasets and local spatial understanding, as objects can be small and follow complex layouts.</p> <p>\u6211\u4eec\u66f4\u4e13\u6ce8\u4e8e\u5c11\u6837\u672c\u7c7b\u65e0\u5173\u7269\u4f53\u8ba1\u6570\u3002\u4e0e\u56fe\u50cf\u5206\u7c7b\u76f8\u6bd4\uff0c\u8fd9\u9879\u4efb\u52a1\u6d89\u53ca\u5c0f\u578b\u6570\u636e\u96c6\u548c\u5c40\u90e8\u7a7a\u95f4\u7406\u89e3\uff0c\u56e0\u4e3a\u5bf9\u8c61\u53ef\u4ee5\u662f\u5c0f\u578b\u7684\uff0c\u5e76\u4e14\u9075\u5faa\u590d\u6742\u7684\u5e03\u5c40\u3002</p> <p>The generated data needs a level of compositionality that current generative models, including diffusion models \u6269\u6563\u6a21\u578b, struggle to achieve. </p> <p>\u751f\u6210\u7684\u6570\u636e\u9700\u8981\u4e00\u79cd \u7ec4\u5408\u6027\u6c34\u5e73 \uff0c\u8fd9\u662f\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\uff0c\u5305\u62ec**\u6269\u6563\u6a21\u578b**\uff0c\u5728\u5b9e\u73b0\u4e0a\u5b58\u5728\u56f0\u96be\u7684\u3002</p> <p>\u751f\u6210\u6570\u636e\u9700\u8981\u7ec4\u5408\uff0c\u8fd9\u662f\u73b0\u5728\u7684\u751f\u6210\u6a21\u578b\uff0c\u5305\u62ec\u6269\u6563\u6a21\u578b\uff0c\u96be\u4ee5\u5b9e\u73b0\u7684\u3002</p> <p>To bring the power of synthetic data to counting,  we propose to condition diffusion models \u6761\u4ef6\u6269\u6563\u6a21\u578b not only on text prompts but also on counting density maps \u6587\u672c\u63d0\u793a+\u5bc6\u5ea6\u56fe to generate images with the correct number of objects \u751f\u6210\u6709\u6b63\u786e\u5bf9\u8c61\u6570\u91cf\u7684\u56fe\u7247 in the desired spatial configuration. \u5728\u671f\u671b\u7684\u7a7a\u95f4\u4f4d\u7f6e\u4e0a </p> <p>\u4e3a\u4e86\u5c06\u5408\u6210\u6570\u636e\u7684\u80fd\u529b\u7528\u4e8e\u8ba1\u6570\uff0c\u6211\u4eec\u63d0\u51fa\u4e0d\u4ec5\u5728\u6587\u672c\u63d0\u793a\u4e0a\uff0c\u800c\u4e14\u5728\u8ba1\u6570\u5bc6\u5ea6\u56fe\u4e0a\u5bf9\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4ee5\u751f\u6210\u6240\u9700\u7a7a\u95f4\u914d\u7f6e\u4e2d\u5177\u6709\u6b63\u786e\u6570\u91cf\u5bf9\u8c61\u7684\u56fe\u50cf\u3002</p> <p>We exploit this double control to generate diversified unseen data by prompting the model with novel combinations of the controls.</p> <p>\u6211\u4eec\u5229\u7528\u8fd9\u79cd\u53cc\u91cd\u63a7\u5236\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u672a\u89c1\u6570\u636e\uff0c\u901a\u8fc7\u4f7f\u7528\u65b0\u7684\u63a7\u5236\u7ec4\u5408\u6765\u4fc3\u4f7f\u6a21\u578b\u3002</p> <p>Tip</p> <p>\u6211\u5199\u8fd9\u90e8\u5206\u6587\u732e\u7efc\u8ff0\u7684\u65f6\u5019\uff0c\u4e5f\u4eceGAN\u5f00\u59cb\uff0c\u5199\u5230diffusion\uff0c\u6700\u540e\u5230\u5173\u4e8e\u76ee\u6807\u8ba1\u6570</p> <p>\u7b2c\u4e8c\u6bb5</p> <p>\u7b2c\u4e8c\u90e8\u5206\uff1aFew-shot Object Counting \u5c0f\u6837\u672c\u8ba1\u6570\u7684\u53d1\u5c55</p> <p>CAC\u4efb\u52a1\u7684\u5b9a\u4e49</p> <p>The goal of few-shot class agnostic object counting is to count how many instances of objects of any arbitrary category there are in a given image, by leveraging only a few exemplars of the category of interest. </p> <p>\u6587\u732e1\uff1aFamNet</p> <p>This was initially formulated as matching exemplars and image patch features [21]. FSC147 [31] was later put forward as the main dataset for this task, with an open set train and test split to evaluate generalization to unseen object categories. Its authors introduced FamNet, a deep net trained to infer density maps from feature similarities.</p> <p>\u6587\u732e2\uff1aBMNet</p> <p>In the same lineage, BMNet [36] refines the similarity map by learning the similarity metric jointly with the counting network. </p> <p>\u6587\u732e3\uff1aSAFECount</p> <p>In SAFECount [41], the similarities are used to fuse exemplars features into the query image features. The density map is then predicted from the enhanced features.</p> <p>\u6587\u732e4\u30015\uff1aCounTR [6] and LOCA [9]</p> <p>Other works e.g. CounTR [6] and LOCA [9] focus on improving the feature representations using a Transformer backbone as the visual encoder and injecting information about the exemplars\u2019 shape in the network [9]. </p> <p>\u6587\u732e6\uff1a\u4e0e\u6211\u4eec\u5de5\u4f5c\u6700\u76f8\u5173\u7684\u6587\u732e Vicinal Couting Network from Rajan et al. [30]  \u2192 \u8bf4\u660e \u56fe\u50cf\u589e\u5f3a\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8ba1\u6570\u6027\u80fd</p> <p>The closest comparison to our work is the Vicinal Couting Network from Rajan et al. [30]. It augments FSC147 with generated data by training a conditional GAN jointly with the counting network, producing augmentations that preserve the image content while modifying its visual appearance. While outperformed by later models, it introduced the idea that well-chosen augmentations can significantly boost counting accuracy. </p> <p>\u6211\u4eec\u7684\u5de5\u4f5c\uff1a\u591a\u6837\u6027\u5408\u6210\u7b56\u7565\uff1a\u4e0d\u4ec5\u5408\u6210\u5916\u89c2\uff0c\u8fd8\u53ef\u4ee5\u6539\u53d8\u5185\u5bb9\uff1b\u4f7f\u7528\u7684\u6587\u751f\u56fe\u7684\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b</p> <p>In this work, we leverage large pre-trained text-to-image diffusion models to produce diverse augmentations that not only alter the appearance, but are also able to change the content, to synthesize augmentations with a variety of object semantics.</p> <p>24\u00b711\u00b718</p> <p>todo\uff1amethod</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/","title":"rank8 CounTR","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>\u671f\u520a\uff1aBMVC</p> <p></p> <ul> <li>\u53d1\u8868\u65f6\u95f4\uff1aBMVC2022 \u3010British Machine Vision Conference\u3011</li> <li>\u4f5c\u8005\uff1a\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\uff1bVGG \u89c6\u89c9\u51e0\u4f55\u7ec4 \u82f1\u56fd\u54c8\u4f5b\u5927\u5b66</li> <li> <p>arxiv\u65e5\u671f\uff1a2023\u5e742\u67082\u65e5\uff08\u5148\u53d1\u8868\u3001\u540e\u6302\u7684arxiv\uff09</p> </li> <li> <p>\u5f15\u7528\uff1a</p> </li> </ul> <pre><code>@inproceedings{liu2022countr,\n  author = {Chang, Liu and Yujie, Zhong and Andrew, Zisserman and Weidi, Xie},\n  title = {CounTR: Transformer-based Generalised Visual Counting},\n  booktitle={British Machine Vision Conference (BMVC)},\n  year = {2022}\n}\n</code></pre> <ul> <li>\u6587\u7ae0\u6807\u9898\uff1aCounTR: Transformer-based Generalised Visual Counting</li> </ul> <p>\u57fa\u4e8eTransformer\u7684\u901a\u7528\u3001\u6cdb\u5316\u89c6\u89c9\u8ba1\u6570</p> <ul> <li>\u672c\u6587\u53d1\u73b0\u4ec0\u4e48\u95ee\u9898\uff0c\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898\uff1f</li> </ul> <p></p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_1","title":"\u2764 \u5168\u6587\u6982\u62ec","text":"<p>\u8bbe\u8ba1\u6a21\u5757\u3011\uff08\u5520\u5520\u53e8\u53e8\u8bf4\u4e86\u4e09\u904d\uff1b\u6458\u8981\u8bf4\u3001\u8d21\u732e\u8bf4\u3001\u7ed3\u8bba\u8bf4\uff09\uff1a</p> <ul> <li> <p>CountTR\uff1b</p> </li> <li> <p>\u4e24\u9636\u6bb5\u8bad\u7ec3\u3010why\uff1f\u3011\uff1b</p> </li> <li> <p>\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u3010input\u7684\u521b\u65b0\u3011\uff1b</p> </li> <li> <p>\u7ed3\u679csota</p> </li> </ul> <p>\u3010\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898? \u3011\uff1a\u8fd9\u7bc7\u6587\u7ae0\u597d\u50cf\u66f4\u5173\u6ce8\u76ee\u6807\u8ba1\u6570\u7684\u6cdb\u5316\u6027\u3002</p> <p>\u6838\u5fc3\u7684\u4e00\u53e5\u8bdd\uff1a</p> <p>\u2b50\ufe0f the generalised visual object counting problem of counting the number of objects from arbitrary semantic categories using arbitrary number of \u201cexemplars\u201d</p> <p>\u2b50\ufe0f Any shot counting</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_2","title":"\u6458\u8981","text":"<p>\u672c\u6587\u6838\u5fc3\uff1a</p> <ul> <li> <p>\u901a\u7528\u89c6\u89c9\u7269\u4f53\u8ba1\u6570</p> </li> <li> <p>\u5bf9\u4e8e\u4efb\u610f\u7c7b\u522b\u3001\u7ed9\u5b9a\u4efb\u610f\u793a\u4f8b</p> </li> </ul> <p>In this paper, we consider the problem of generalised visual object counting, with the goal of developing a computational model for counting the number of objects from arbitrary semantic categories, using arbitrary number of \u201cexemplars\u201d, i.e. zero-shot or few shot counting. </p> <p>\u56db\u4e2a\u8d21\u732e\uff1a</p> <p>To this end, we make the following four contributions:</p> <ol> <li>We introduce a novel transformer-based architecture for generalised visual object counting, termed as Counting TRansformer (CounTR), which explicitly captures the similarity between image patches or with given \u201cexemplars\u201d using the attention mechanism; </li> <li>We adopt a two-stage training regime, that first pre-trains the model with self-supervised learning, and followed by supervised fine-tuning; </li> <li>We propose a simple, scalable pipeline for synthesizing training images with a large number of instances or that from different semantic categories, explicitly forcing the model to make use of the given \u201cexemplars\u201d; </li> <li>We conduct thorough ablation studies on the large-scale counting benchmark, e.g. FSC147, and demonstrate state-of-the-art performance on both zero and few-shot settings. Project page: https://verg-avesta.github.io/CounTR_Webpage/.</li> </ol> <p>\u672c\u6587\u7f3a\u9677\uff1a\u540e\u9762\u7684CACViT\u5bf9\u6bd4\u8fd9\u7bc7\u8bba\u6587</p> <p>\u4e24\u9636\u6bb5 extract-then-match  \u2192 \u5355\u4e00\u9636\u6bb5\uff0c\u540c\u65f6 extract-and-match</p> <p>\u56db\u4e2a\u8d21\u732e\uff1a</p> <ol> <li>\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u8ba1\u6570\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6355\u6349\u56fe\u50cf\u5757\u548c\u793a\u4f8b\u7684\u76f8\u4f3c\u6027</li> <li>\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a</li> <li>\u7b2c\u4e00\u9636\u6bb5\uff1a\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b</li> <li>\u7b2c\u4e8c\u9636\u6bb5\uff1a\u76d1\u7763\u3001\u5fae\u8c03</li> <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u3001\u53ef\u4f38\u7f29\u7684\u3001\u80fd\u591f\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u7684\uff0c\u5927\u91cf\u5b9e\u4f8b\u3001\u6765\u81ea\u4e0d\u540c\u7684\u8bed\u4e49\u7c7b\u522b\u3001\u4f7f\u5f97\u6a21\u578b\u5145\u5206\u5229\u7528\u7ed9\u5b9a\u793a\u4f8b</li> <li>\u6d88\u878d\u5b9e\u9a8c</li> </ol> <p>\u5bf9\uff01\u5c31\u662f\u8fd9\u7bc7\u8bba\u6587\uff0c\u56db\u5f20\u56fe\u7247\u62fc\u63a5\u8fdb\u884c\u56fe\u50cf\u5408\u6210\uff01\u628a\u8fd9\u7bc7\u8bba\u6587\u548cSemAug CounTR\u533a\u5206\uff1b</p> <ul> <li>\u90fd\u662f\u5408\u6210\u56fe\u50cf\uff0c\u672c\u6587\u7528\u7684\u662f\u88c1\u526a\u3001\u62fc\u63a5\u3001\u6df7\u5408\u3001\u7f29\u653e</li> <li>\u8bed\u4e49\u589e\u5f3a\u7684SemAug CounTR\u7528\u7684\u662f\uff1aStable duffusion</li> </ul> <p></p> <p>\u7ffb\u8bd1\u4e00\u4e0b\u8fd9\u91cc\u7684\u6587\u5b57\uff1a\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u7684\u5904\u7406\u6d41\u7a0b</p> <p>(1) \u4ee3\u8868\u88c1\u526a\u548c\u7f29\u653e</p> <p>(2) \u4ee3\u8868\u62fc\u8d34\u548c\u6df7\u5408</p> <p>\u5728\u63a5\u4e0b\u6765\u7684\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u5c06\u88c1\u526a\u3001\u7f29\u653e\u548c\u62fc\u8d34\u5408\u5e76\u4e3a\u62fc\u8d34\u9636\u6bb5\u3002Type A \u4f7f\u7528\u56db\u5f20\u4e0d\u540c\u7684\u56fe\u50cf\u6765\u63d0\u9ad8\u80cc\u666f\u591a\u6837\u6027\uff0c\u800c Type B \u53ea\u4f7f\u7528\u4e00\u5f20\u56fe\u50cf\u6765\u589e\u52a0\u56fe\u50cf\u4e2d\u5305\u542b\u7684\u7269\u4f53\u6570\u91cf\u3002\u767d\u8272\u9ad8\u5149\u533a\u57df\u662f\u7ecf\u8fc7\u9ad8\u65af\u6ee4\u6ce2\u540e\u7684\u70b9\u6ce8\u91ca\u5bc6\u5ea6\u56fe\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u3002</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_3","title":"\u603b\u7ed3\u6458\u8981","text":"<ul> <li> <p>\u8003\u8651\u7684\u95ee\u9898\uff1a\u89c6\u89c9\u76ee\u6807\u8ba1\u6570\u7684\u6cdb\u5316\u6027</p> </li> <li> <p>\u76ee\u7684\u662f\u4e3a\u4e86\u8bbe\u8ba1\u4e00\u4e2a\u6a21\u578b\uff1a\u7ed9\u5b9a\u4efb\u610f\u6570\u91cf\u7684\u793a\u4f8b\u6846\uff0c\u8ba1\u6570 \u4efb\u610f\u7684\u8bed\u4e49\u7269\u4f53   </p> </li> </ul> <p>\u6211\u4eec\u7684\u6a21\u578b\u65e2\u9002\u7528\u4e8e0-shot\u60c5\u5f62 \u4e5f\u9002\u5408 few-shot\u60c5\u5f62</p> <ul> <li> <p>4\u4e2a\u8d21\u732e\uff1a</p> </li> <li> <p>\u8bbe\u8ba1\u4e00\u4e2a\u65b0\u7684Transformer\u67b6\u6784 \u2192 \u6355\u6349\u56fe\u50cfpatch \u4e0e \u793a\u4f8b\u6846\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027</p> </li> <li> <p>\u91c7\u7528\u4e24\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3</p> <pre><code>\u7b2c\u4e00\u6b65\uff1a\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u6a21\u578b\n\u7b2c\u4e8c\u6b65\uff1a\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\n</code></pre> </li> <li> <p>\u8bbe\u8ba1\u4e86\u4e00\u4e2apipeline\uff0c\u7528\u4e8e\u5408\u6210\u8bad\u7ec3\u56fe\u50cf</p> <ol> <li> <p>\u3010\u4ec0\u4e48\u6837\u7684\u8bad\u7ec3\u56fe\u50cf\uff1f\u3011</p> <pre><code>\u6709\u5927\u91cf\u5b9e\u4f8b\n \u6765\u81ea\u4e0d\u540c\u7684\u8bed\u4e49\u7c7b\u522b\n</code></pre> </li> <li> <p>\u3010\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u505a\uff1f\u3011</p> <ol> <li>\u5f3a\u8feb\u6a21\u578b\u4f7f\u7528 \u7ed9\u5b9a\u7684\u793a\u4f8b\u6846</li> </ol> <p>\u3010\u793a\u4f8b\u6846\u4e3a\u4ec0\u4e48\u52a0\u5f15\u53f7\uff1f\u3011</p> </li> </ol> </li> <li> <p>\u3010\u7ed3\u679c\u3011\u5728FSC147\u6570\u636e\u96c6\u4e0a\u505a\u6d88\u878d\u5b9e\u9a8c\uff0c\u57280-shot  &amp; 1-shot\u4e0a\u90fd\u8fbe\u5230\u4e86SOTA</p> </li> <li> <p>\u4ee3\u7801\u516c\u5f00\u3001\u81ea\u5df1\u505a\u4e86\u4e2a\u7f51\u9875\uff1ahttps://verg-avesta.github.io/CounTR_Webpage/</p> </li> </ul>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#-","title":"\u5f15\u5165-\u8d21\u732e","text":"<p>\uff08\u539f\u6587\uff09To summarise, in this paper, we make four contributions: </p> <p>First, we introduce an architecture for generalised visual object counting based on transformer, termed as CounTR (pronounced as counter). It exploits the attention mechanisms to explicitly capture the similarity between image patches, or with the few-shot instance \u201cexemplars\u201d provided by the end user; </p> <p>Second, we adopt a two-stage training regime (self-supervised pre-training, followed by supervised fine-tuning) and show its effectiveness for the task of visual counting; </p> <p>Third, we propose a simple yet scalable pipeline for synthesizing training images with a large number of instances, and demonstrate that it can significantly improve the performance on images containing a large number of object instances; </p> <p>Fourth, we conduct thorough ablation studies on the large-scale counting benchmark, e.g. FSC-147 [24], and demonstrate state-of-the-art performance on both zero-shot and few-shot settings, improving the previous best approach by over 18.3% on the mean absolute error of the test set.</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#-_1","title":"\u603b\u7ed3 \u5f15\u5165-\u8d21\u732e","text":"<p>\u6211\u4eec\u7684\u6a21\u578b\u67094\u4e2a\u8d21\u732e\uff1a</p> <ol> <li> <p>\u3010\u65b0\u7684\u7f51\u7edc\u67b6\u6784\u3011CounTR : \u57fa\u4e8eTransformer\uff1b\u5173\u6ce8\u53ef\u89c1\u7269\u4f53\u7684\u6cdb\u5316 \u8ba1\u6570\u80fd\u529b\uff0c\u8fd9\u8868\u660e\uff1a</p> </li> <li> <p>\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6e05\u695a\u5730\u6355\u6349\u5230image patches\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027</p> </li> <li> <p>\u6216\u8005 few-shot\u4efb\u52a1\u4e2d\uff0c\u7ed9\u5b9a\u793a\u4f8b\u6846\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027</p> </li> <li> <p>\u3010\u4e24\u9636\u6bb5\u8bad\u7ec3\u3011\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5177\u4f53\u6765\u8bf4\u5c31\u662f</p> </li> <li> <p>\u81ea\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u9884\u8bad\u7ec3</p> </li> <li>\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03</li> </ol> <p>\u5e76\u9a8c\u8bc1\u4e86\u8fd9\u4e2a\u65b9\u6cd5 \u5bf9\u4e8e\u89c6\u89c9\u8ba1\u6570\u4efb\u52a1\u7684\u6709\u6548\u6027</p> <ol> <li> <p>\u3010\u5408\u6210\u56fe\u7247\u3011\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355 \u5e76\u4e14\u5c3a\u5ea6\u53ef\u53d8\u7684 pipeline \u6765\u5408\u6210\u6709\u5927\u91cf\u5b9e\u4f8b\u7684 \u8bad\u7ec3\u56fe\u7247\uff1b\u5e76\u8868\u660e \u5b83\u80fd\u663e\u8457\u63d0\u9ad8 \u6a21\u578b  \u56fe\u50cf\u5728\u6709\u5927\u91cf\u8ba1\u6570\u5bf9\u8c61\u7684\u60c5\u51b5\u4e0b\u7684 \u8ba1\u6570\u6027\u80fd</p> </li> <li> <p>\u3010\u7ed3\u679c\u3011\u6211\u4eec\u5728\u5927\u89c4\u6a21\u8ba1\u6570benchmark\u4e0b\uff0c\u8fdb\u884c\u4e86\u6574\u4e2a\u6d88\u878d\u5b9e\u9a8c\uff0c\u6bd4\u5982FSC147\u6570\u636e\u96c6</p> </li> <li> <p>\u6211\u4eec\u7684\u7ed3\u679c\uff1a0-shot\u30011-shot \u90fd\u662fsota</p> </li> <li>\u5177\u4f53\u6765\u8bf4\uff1aMAE\uff1b test set \uff1b\u4e4b\u524d\u6700\u597d\u7684\u65b9\u6cd5 \u63d0\u5347\u8d8518.3%</li> </ol>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_4","title":"\u7ed3\u8bba","text":"<p>\u539f\u6587\uff1a</p> <p>In this work, we aim at the generalised visual object counting problem of counting the number of objects from arbitrary semantic categories using arbitrary number of \u201cexemplars\u201d. We propose a novel transformer-based architecture for it, termed as CounTR. It is first pretrained with self-supervised learning, and followed by supervised fine-tuning. We also propose a simple, scalable pipeline for synthesizing training images that can explicitly force the model to make use of the given \u201cexemplars\u201d. Our model achieves state-of-the-art performance on both zero-shot and few-shot settings.</p> <p>\u3010\u518d\u6b21\u91cd\u7533\u672c\u6587\u7684\u7814\u7a76target\u3011</p> <p>\u6458\u8981\u548c\u7ed3\u8bba\u90fd\u5728\u5f3a\u8c03</p> <p>\u6211\u4eec\u7684\u6a21\u578b\u81f4\u529b\u4e8e\uff1a\u4e00\u822c\u5316\u7684\u89c6\u89c9\u7269\u4f53\u8ba1\u6570\u95ee\u9898</p> <p>\u5177\u4f53\u6765\u8bf4\u5c31\u662f\uff1a  \u4f7f\u7528\u4efb\u610f\u6570\u91cf\u7684\u6837\u4f8b \u8ba1\u6570 \u4efb\u610f\u8bed\u4e49\u7c7b\u522b\u7684\u5bf9\u8c61\u6570\u91cf</p> <p>\u56e0\u6b64\uff0c</p> <p>\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a \u65b0\u7684 \u57fa\u4e8eTransformer\u7684\u67b6\u6784 \u53eb CountTR\uff0c</p> <p>CountTR =\u300b \u4e24\u9636\u6bb5\u8bad\u7ec3 = \u81ea\u76d1\u7763\u5b66\u4e60 \u9884\u8bad\u7ec3  + \u76d1\u7763\u5b66\u4e60 \u5fae\u8c03</p> <p>pipeline\uff1a\u5408\u6210\u8bad\u7ec3\u56fe\u50cf  =\u300b\u662f\u7684\u6a21\u578b\u80fd\u591f\u5229\u7528\u7ed9\u5b9a\u7684 \u6837\u4f8b</p> <p>0-shot\u3001few-shot \u90fd\u662f sota</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_5","title":"\u968f\u624b\u8bb0","text":"<p>scalable\uff1a\u53ef\u4f38\u7f29\u7684</p> <p>pipeline\uff1a\u6574\u4e2a\u6570\u636e\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u6d41\u7a0b\u3001\u7aef\u5230\u7aef\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6db5\u76d6\u4e86\u4ece\u539f\u59cb\u6570\u636e\u5230\u6700\u7ec8\u7ed3\u679c\u7684\u5168\u8fc7\u7a0b</p> <p>pipeline\u662f\u6574\u4e2a\u6d41\u7a0b\u7684\u6982\u8ff0</p> <p>backbone\u662f\u6a21\u578b\u4e2d\u7684\u5173\u952e\u7279\u5f81\u63d0\u53d6\u90e8\u5206</p> <p>benchmark\u662f\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u7684\u6807\u51c6\u6d4b\u8bd5\u548c\u6307\u6807</p> <p>mosaic training data generation, namely, collage and blending</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_6","title":"\u5f15\u5165","text":"<p>\u7b2c\u4e00\u6bb5\uff1a\u5c0f\u4e8e5\u7684\u80fd\u5feb\u901f\u8ba1\u6570\uff0c\u6570\u91cf\u589e\u591a\u8ba1\u6570\u80fd\u529b\u6025\u5267\u4e0b\u964d</p> <p>Despite all the exceptional abilities, the human visual system is particularly weak in counting objects in the image. In fact, given a visual scene with a collection of objects, one can only make a rapid, accurate, and confident judgment if the number of items is below five, with an ability termed as subitizing [16]. While for scenes with an increasing number of objects, the accuracy and confidence of the judgments tend to decrease dramatically. Until at some point, counting can only be accomplished by calculating estimates or enumerating the instances, which incurs low accuracy or tremendous time cost.</p> <p>\u7b2c\u4e8c\u6bb5\uff1a\u6211\u4eec\u6587\u7ae0\u7684\u76ee\u6807 \u901a\u7528\u89c6\u89c9\u7269\u4f53\u8ba1\u6570\u7cfb\u7edf</p> <p>In this paper, our goal is to develop a generalised visual object counting system, that augments humans\u2019 ability for recognising the number of objects in a visual scene. .......</p> <p>To this end, we propose a novel architecture that transforms the input image (with the few-shot annotations if any) into a density map, and the final count can be obtained by simply summing over the density map.</p> <p>\u7b2c\u4e09\u6bb5\uff1a</p> <p>Specifically, we take inspiration from Lu et al. [19] that self-similarity is a strong prior in visual object counting, and introduce a transformer-based architecture where the self-similarity prior can be explicitly captured by the built-in attention mechanisms, both among the input image patches and with the few-shot annotations (if any).</p> <p>\u89c6\u89c9\u7269\u4f53\u7684\u81ea\u76f8\u4f3c\u6027\uff1bTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6355\u6349\u56fe\u50cf\u5757\u548c\u793a\u4f8b\u7684\u81ea\u76f8\u4f3c\u6027</p> <p>We propose a two-stage training scheme, with the transformer-based image encoder being firstly pre-trained with self-supervision via masked image modeling [11], followed by supervised fine-tuning for the task at hand.</p> <p>\u4e24\u9636\u6bb5\u8bad\u7ec3\u6a21\u5f0f\uff1b</p> <p>\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u7f16\u7801\u5668\u9996\u5148\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3</p> <p>\u7136\u540e\u5bf9\u624b\u5934\u7684\u4efb\u52a1\u8fdb\u884c\u6709\u76d1\u7763\u7684\u5fae\u8c03\u3002</p> <p>We demonstrate that self-supervised pre-training can effectively learn the visual representation for counting, thus significantly improving the performance. </p> <p>\u6211\u4eec\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u80fd\u6709\u6548\u5b66\u4e60\u8ba1\u6570\u7684\u89c6\u89c9\u8868\u793a\u3001\u5e76\u63d0\u9ad8\u6027\u80fd</p> <p>Additionally, to tackle the long-tailed challenge in existing generalised visual object counting datasets, where the majority of images only contain a small number of objects, we propose a simple, yet scalable pipeline for synthesizing training images with a large number of instances, as a consequence, establishing reliable data sources for model training, to condition the user provided instance exemplars.\uff08\u53e5\u5b50\u662f\u771f\u957f\uff09</p> <p>\u2b50\ufe0f \u4e3a\u4e86\u89e3\u51b3 \u73b0\u6709\u901a\u7528\u89c6\u89c9\u76ee\u6807\u8ba1\u6570\u6570\u636e\u96c6\u7684 \u4e00\u76f4\u5b58\u5728\u7684\u95ee\u9898\uff0c\u90a3\u5c31\u662f \u5927\u591a\u6570\u8bad\u7ec3\u56fe\u50cf\u53ea\u5305\u542b\u4e00\u5c0f\u90e8\u5206\u6570\u91cf\u7684\u7269\u4f53\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b80\u5355\u7684\u3001\u53ef\u4f38\u7f29\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u5408\u6210\u542b\u6709\u5927\u91cf\u5b9e\u4f8b\u7684\u8bad\u7ec3\u56fe\u7247\u2192\u5efa\u7acb\u4e86\u53ef\u9760\u7684\u6570\u636e\u96c6 \u7528\u4e8e\u6a21\u578b\u8bad\u7ec3</p> <p>\u7b2c\u56db\u6bb5\uff1a\u8d21\u732e</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_7","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>\u76f8\u5173\u5de5\u4f5c\u5c31\u4e24\u6bb5\uff1a</p> <ul> <li>\u89c6\u89c9\u7269\u4f53\u8ba1\u6570</li> <li>\u7c7b\u65e0\u5173\u76ee\u6807\u8ba1\u6570</li> </ul> <p>\u7b2c\u4e00\u6bb5\uff1aVisual object counting. </p> <p>\u9996\u5148\u6307\u51fa\uff0c\u76ee\u6807\u8ba1\u6570\u95ee\u9898\u7684\u65b9\u6cd5\u5206\u6210\u4e24\u7c7b\uff1a\u57fa\u4e8e\u68c0\u6d4b\u7684\u548c\u57fa\u4e8e\u56de\u5f52\u7684</p> <p>In the literature, object counting approaches can generally be cast into two categories: detection-based counting [3, 5, 13] or regression-based counting [1, 2, 4, 15, 17, 20, 29]. </p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684......</p> <p>The former relies on a visual object detector that can localize object instances in an image. This method, however, requires training individual detectors for different objects, and the detection problem remains challenging if only a small number of annotations are given. </p> <p>\u57fa\u4e8e\u56de\u5f52\u7684......\u5c06\u56fe\u50cf\u6620\u5c04\u6210\u6807\u91cf\uff0c\u6216\u8005\u5c06\u56fe\u50cf\u6620\u5c04\u6210\u5bc6\u5ea6\u56fe</p> <p>The latter avoids solving the hard detection problem, instead, methods are designed to learn either a mapping from global image features to a scalar (number of objects), or a mapping from dense image features to a density map, achieving better results on counting overlapping instances. </p> <p>\u4f46\u662f\u6307\u51fa\u95ee\u9898\uff0c\u5148\u524d\u7684\u8ba1\u6570\u65b9\u6cd5\u4e0d\u7ba1\u662f\u57fa\u4e8e\u68c0\u6d4b\u7684\u8fd8\u662f\u57fa\u4e8e\u56de\u5f52\u7684\uff0c\u90fd\u53ea\u80fd\u8ba1\u6570\u7279\u5b9a\u7c7b\u522b\u7684\u7269\u4f53\uff0c\u6240\u4ee5\u5f15\u51fa\u7b2c\u4e8c\u6bb5\uff0c\u7c7b\u65e0\u5173\u8ba1\u6570</p> <p>However, previous methods from both lines (detection, regression) have only been able to count objects of one particular class (e.g. cars, cells).</p> <p>\u7b2c\u4e8c\u6bb5 Class-agnostic object counting. </p> <p>\u7ed9\u51fa\u95ee\u9898\u5b9a\u4e49</p> <p>Recently, class-agnostic few-shot counting [19, 24, 30] has witnessed a rise in research interest in the community. Unlike the class-specific models that could only count objects of specific classes like cars, cells, or people, class-agnostic counting aims to count the objects in an image based on a few given \u201cexemplar\u201d instances,thus is also termed as few-shot counting. </p> <p>\u76ee\u6807\uff1a\u8bad\u7ec3\u9636\u6bb5\uff0c\u6316\u6398\u4e0d\u540c\u7c7b\u522b\u76ee\u6807\u7684\u5171\u6027</p> <p>Generally speaking, class-agnostic few-shot counting models need to mine the commonalities between the counts of different classes of objects during training. </p> <p>\u6587\u732e GMN</p> <p>In [19], the authors propose a generic matching network (GMN), which regresses the density map by computing the similarity between the CNN features from image and exemplar shots; </p> <p>FamNet </p> <p>FamNet [24] utilizes feature correlation for prediction and uses adaptation loss to update the model\u2019s parameters at test time; </p> <p>SAFECount</p> <p>SAFECount [30] uses the support feature to enhance the query feature, making the extracted features more refined and then regresses to obtain density maps; </p> <p>[12]</p> <p>In a very recent work [12], the authors exploit a pre-trained DINO [21] model and a lightweight regression head to count without exemplars. </p> <p></p> <p>\u672c\u6587\uff1a\u57fa\u4e8eTransformer\u7684\uff0cCounTR</p> <p>In this paper, we also use transformer-based architecture, however, train it from scratch, and augment it with the ability to count the objects with any shot.</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#3-methods","title":"3 Methods","text":"<p>\u672c\u6587\u4e3b\u8981\u8003\u8651 \u6cdb\u5316\u7269\u4f53\u8ba1\u6570</p> <p>\u8ba1\u6570\u56fe\u7247\u4e2d\u4efb\u610f\u7c7b\u522b\u7684\u7269\u4f53\uff0c\u4f7f\u7528\u4efb\u610f\u6570\u91cf\u7684\u6837\u4f8b\u6846\uff0c\u6837\u4f8b\u6846\u7684\u6570\u91cf\u4ece0~few</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#overview","title":"Overview","text":"<p>\u7b26\u53f7\u8bf4\u660e\uff1a</p> <ul> <li> <p>\\(\\mathcal{D_{train}}={(\\mathcal{X_1,S_1,y_1}),...,(\\mathcal{X_N,S_N,y_N})}\\)</p> </li> <li> <p>\u8f93\u5165\u56fe\u7247\uff1a \\(\\mathcal{X_i} \\in \\mathbb{R}^{H \\times W \\times3}\\)</p> </li> <li> <p>\u6837\u4f8b\u6846\u7b26\u53f7\u8bf4\u660e\uff1a</p> <ul> <li>\\(\\mathcal{S_i} = \\{b_i\\}^K\\) exemplar \u793a\u4f8b\u6846</li> <li>\\(b_i^k \\in \\mathbb{R}^4\\) </li> <li>\\(K \\in \\{0,1,2,3....\\}\\)</li> </ul> </li> <li> <p>\\(y_i \\in \\mathbb{R}^{H \\times W \\times 1}\\)</p> </li> <li> <p>\\(\\mathcal{D_{test}}={(\\mathcal{X_{N+1},S_{N+1}}),...,(\\mathcal{X_M,S_M})}\\)</p> </li> <li> <p>disjoint</p> </li> </ul> <p></p> <ul> <li>\u57fa\u4e8eTransformer\u7684\u8ba1\u6570\u6a21\u578b\uff0c\u6240\u4ee5\u53ebCounTR</li> <li>Transformer\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u663e\u5f0f\u5730\u6bd4\u8f83\u4efb\u4f55\u5176\u4ed6\u7a7a\u95f4\u4f4d\u7f6e\u548c\"\u6837\u4f8b\"\u4e4b\u95f4\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u662f\u7531\u6700\u7ec8\u7528\u6237\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e2d\u63d0\u4f9b\u7684</li> <li>3.2\u8282 \u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u5373\u9996\u5148\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u91cd\u5efa( MAE )\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4e0b\u6e38\u8ba1\u6570\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5c55\u793a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5bf9\u901a\u7528\u89c6\u89c9\u5bf9\u8c61\u8ba1\u6570\u7684\u6709\u6548\u6027\u7684\u5de5\u4f5c</li> <li>3.3\u8282  \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u5408\u6210\u67b6\u6784\uff1b\u4ee5\u89e3\u51b3\u73b0\u6709\u7269\u4f53\u8ba1\u6570\u6570\u636e\u96c6\u4e2d\u957f\u5c3e\u5206\u5e03(\u4e5f\u5c31\u662f\u8bf4,\u5177\u6709\u5927\u91cf\u5b9e\u4f8b\u7684\u56fe\u50cf\u5f80\u5f80\u662f\u4e0d\u9891\u7e41\u7684)\u7684\u6311\u6218</li> <li>3.4\u8282 \u5c06\u4ecb\u7ecd\u6d4b\u8bd5\u65f6\u95f4\u6b63\u89c4\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u6d4b\u8bd5\u65f6\u95f4\u6b63\u89c4\u5316\u548c\u6d4b\u8bd5\u65f6\u95f4\u88c1\u526a(?)</li> </ul>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#31-architecture","title":"3.1 Architecture","text":"<ul> <li>\u89c6\u89c9\u7279\u5f81\u7f16\u7801\u5668  \\(\\Phi_{VIT-ENC}(\\cdot)\\) \u3001\\(\\Phi_{CNN-ENC}(\\cdot)\\)</li> <li>\u7279\u5f81\u4ea4\u4e92\u6a21\u5757</li> <li>\u89e3\u7801\u5668</li> </ul> <ul> <li>\u56fe\u50cf\u7279\u5f81\uff1aquery</li> <li> <p>\u793a\u4f8b\u6846\u7279\u5f81\uff1akey &amp; value</p> </li> <li> <p>\u6ca1\u6709\u793a\u4f8b\u6846\uff0c\u5c31\u6709\u4e2a\u53ef\u5b66\u4e60\u7684token\u4f5c\u4e3akey &amp; value</p> </li> </ul>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#311-visual-encoder","title":"3.1.1 Visual Encoder","text":"<p>\u89c6\u89c9\u7f16\u7801\u5668\u5305\u62ec\u4e24\u4e2a\u90e8\u5206</p> <ul> <li>\u57fa\u4e8eViT\u7684\u7f16\u7801\u5668\uff0c\u5904\u7406\u8f93\u5165\u56fe\u50cf\u7279\u5f81\uff0c\u6620\u5c04\u4e3a\u9ad8\u7ef4\u7279\u5f81\u56fe</li> <li>\u8ba1\u7b97\u793a\u4f8b\u6846\u7684\u89c6\u89c9\u7279\u5f81</li> <li>\u5bf9\u4e8eViT\u6765\u8bf4\uff0c\u8f93\u5165\u56fe\u7247\u5212\u5206\u621016\u00d716\u7684patches</li> <li> <p>12\u5c42\uff0c\u4f7f\u7528\u4e86\u4f4d\u7f6e\u7f16\u7801\uff0c\u6ca1\u6709cls token</p> </li> <li> <p>ViT\u7684\u8f93\u51fa\u662f\u4e00\u4e2ad\u7ef4\u5411\u91cf</p> </li> </ul> <p></p> <p></p> <p>CNN\u63d0\u53d6\u793a\u4f8b\u6846\u7279\u5f81</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#312-feature-interaction-module","title":"3.1.2 Feature Interaction Module","text":"<ul> <li>\u7279\u5f81\u4ea4\u4e92\u6a21\u5757 \u878d\u5408\u4fe1\u606f</li> <li>\u7531\u4e00\u7cfb\u5217\u89e3\u7801\u5668\u7ec4\u6210</li> <li>\u56fe\u50cf\u7279\u5f81\u4f5c\u4e3aquery\u3001\u6837\u672c\u7279\u5f81 \u7ecf\u8fc7\u4e24\u4e2a\u4e0d\u540c\u7684\u7ebf\u6027\u5c42 \u5904\u7406\uff0c\u5f97\u5230 key \u548c value</li> <li>FIM\u7684\u8f93\u51fa \u548c \\(\\mathcal{F_{VIT}}\\) \u7684\u8f93\u51fa\u4fdd\u6301\u76f8\u540c\u7684\u7ef4\u5ea6</li> </ul> <p>\u603b\u7ed3\u4e00\u4e0b\uff0c\u516c\u5f0f\uff1a</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#313-decoder","title":"3.1.3 Decoder","text":"<ul> <li> <p>\u7279\u5f81\u4ea4\u4e92\u6a21\u5757\u7684\u8f93\u51fa\u7ed3\u679c\u88ab\u8fdb\u4e00\u6b65\u91cd\u5851\u56de\u4e8c\u7ef4\u7279\u5f81\u56fe\uff0c\u5e76\u6062\u590d\u5230\u539f\u59cb\u5206\u8fa8\u7387\u4f5c\u4e3a\u8f93\u5165\u56fe\u50cf</p> </li> <li> <p>\u91c7\u7528\u6e10\u8fdb\u5f0f\u4e0a\u91c7\u6837\u8bbe\u8ba1\uff0c\u9996\u5148\u5c06\u5411\u91cf\u5e8f\u5217\u91cd\u5851\u4e3a\u7a20\u5bc6\u7684\u7279\u5f81\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8eConvNet\u7684\u89e3\u7801\u5668\u8fdb\u884c\u5904\u7406\u3002</p> </li> </ul> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u4f7f\u7528\u4e864\u4e2a\u4e0a\u91c7\u6837\u5757\uff0c\u6bcf\u4e2a\u4e0a\u91c7\u6837\u5757\u7531\u4e00\u4e2a\u5377\u79ef\u5c42\u548c\u4e00\u4e2a2 \u00d7\u53cc\u7ebf\u6027\u63d2\u503c\u7ec4\u6210\u3002\u5728\u6700\u540e\u4e00\u6b21\u4e0a\u91c7\u6837\u4e4b\u540e\uff0c\u91c7\u7528\u7ebf\u6027\u5c42\u4f5c\u4e3a\u5bc6\u5ea6\u56de\u5f52\u5668\uff0c\u8f93\u51fa\u5355\u901a\u9053\u7684\u5bc6\u5ea6\u70ed\u56fe</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#32-two-stage-training-scheme","title":"3.2 Two-stage Training Scheme","text":"<p>\u6307\u51fa\u95ee\u9898\uff1a\u89c6\u89c9\u4fe1\u53f7\u662f\u9ad8\u5ea6\u5197\u4f59\u7684\uff0c\u7269\u4f53\u901a\u5e38\u4ee5\u76f8\u540c\u7684\u5f62\u5f0f\u591a\u6b21\u51fa\u73b0</p> <p>\u56e0\u6b64\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668 \\(\\Phi_{VIT-ENC}(\\cdot)\\)</p> <p>\u91c7\u7528Masked\u81ea\u7f16\u7801\u5668( MAE )\u7684\u601d\u60f3\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u90e8\u5206\u89c2\u6d4b\u503c\u7684\u56fe\u50cf\u91cd\u5efa\u6765\u8bad\u7ec3\u6a21\u578b\u3002</p> <p></p> <p>\u5408\u6210\u8bad\u7ec3\u56fe\u50cf</p> <p>\u7b2c\u4e00\u6b65\uff0c\u88c1\u526a &amp; \u7f29\u653e</p> <p>\u7b2c\u4e8c\u6b65 \uff0c\u62fc\u63a5 &amp; \u6df7\u5408</p> <p>\u878d\u5408\u624d\u53eb\u3001\u7f29\u653e\u3001\u62fc\u63a5</p> <p>A\u7c7b\u4f7f\u75284\u5e45\u4e0d\u540c\u7684\u56fe\u50cf\u6765\u63d0\u9ad8\u80cc\u666f\u591a\u6837\u6027\uff0cB\u7c7b\u53ea\u4f7f\u7528\u4e00\u5e45\u56fe\u50cf\u6765\u589e\u52a0\u4e00\u5e45\u56fe\u50cf\u4e2d\u5305\u542b\u7684\u7269\u4f53\u6570\u91cf\u3002\u767d\u8272\u9ad8\u5149\u662f\u9ad8\u65af\u6ee4\u6ce2\u540e\u7528\u4e8e\u53ef\u89c6\u5316\u7684\u70b9\u6807\u6ce8\u5bc6\u5ea6\u56fe\u3002</p> <p></p> <p>\u81ea\u76d1\u7763\u9884\u8bad\u7ec3 \u63a9\u7801\u81ea\u7f16\u7801\u5668</p> <p>\u9996\u5148\u5c06\u56fe\u50cf\u5212\u5206\u4e3a\u89c4\u5219\u7684\u975e\u91cd\u53e0\u5757\uff0c\u5e76\u4e14\u53ea\u91c7\u6837\u5757( 50 %)\u7684\u5b50\u96c6\u4f5c\u4e3aViT\u7f16\u7801\u5668\u7684\u8f93\u5165\u3002</p> <p>\u8ba1\u7b97\u5f97\u5230\u7684\u7279\u5f81\u8fdb\u4e00\u6b65\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u4f20\u9012\uff0c\u8be5\u89e3\u7801\u5668\u7531\u82e5\u5e72\u4e2a\u8f6c\u6362\u5668\u89e3\u7801\u5668\u5c42\u7ec4\u6210\uff0c\u5176\u4e2d\u53ef\u5b66\u4e60\u7684\u63a9\u7801token\u548c\u4f4d\u7f6e\u7f16\u7801\u7684\u7ec4\u5408\u88ab\u7528\u4f5cQuery\uff0c\u4ec5\u4ece\u89c2\u5bdf\u5230\u7684patch\u91cd\u5efa\u8f93\u5165\u56fe\u50cf\u3002</p> <p>\u8bad\u7ec3\u635f\u5931\u7b80\u5355\u5b9a\u4e49\u4e3a\u91cd\u5efa\u56fe\u50cf\u4e0e\u8f93\u5165\u56fe\u50cf\u5728\u50cf\u7d20\u7a7a\u95f4\u7684\u5747\u65b9\u8bef\u5dee( MSE )\u3002</p> <p></p> <p>\u6709\u76d1\u7763\u7684\u5fae\u8c03</p> <ul> <li>\u4f7f\u7528\u9884\u8bad\u7ec3\u7684ViT\u7684\u521d\u59cb\u6743\u91cd\uff0c\u521d\u59cb\u5316\u56fe\u50cf\u7f16\u7801\u5668</li> <li>\u5e76\u5bf9\u6211\u4eec\u63d0\u51fa\u7684\u901a\u7528\u7269\u4f53\u8ba1\u6570\u67b6\u6784\u8fdb\u884c\u5fae\u8c03</li> <li>\u6a21\u578b\u7684\u8f93\u5165\uff1a\u539f\u59cb\u56fe\u50cf \\(\\mathcal{X}_i\\) + \\(K\\) \u4e2a\u793a\u4f8b\u6846 \\(\\mathcal{S}_i =\\{b_i\\}\\)</li> <li>\u8f93\u51fa\u5bc6\u5ea6\u56fe \\(\\hat{y}_i \\in \\mathbb{R}^{H\u00d7W\u00d71}\\)</li> <li>\u56fe\u50cf \\(C_i \\in \\mathbb{R}\\) \u4e2d\u663e\u8457\u7269\u4f53\u7684\u7edf\u8ba1\u6570\u91cf \u901a\u8fc7\u5bf9 \u79bb\u6563\u5bc6\u5ea6\u56fe  \\(y_i\\) \u6c42\u548c\u5f97\u5230</li> <li>\u6211\u4eec\u4f7f\u7528\u6bcf\u4e2a\u50cf\u7d20\u7684\u5747\u65b9\u8bef\u5dee \u8ba1\u6570\u4f30\u8ba1\u5bc6\u5ea6\u56fe \\(\\hat{y_i}\\) \u548c\u771f\u5b9e \u5bc6\u5ea6\u56fe  \\(y_i\\) \u5dee\u5f02</li> <li>\u771f\u5b9e\u5bc6\u5ea6\u56fe\u7684\u751f\u6210\uff1a\u57fa\u4e8e\u70b9\u6ce8\u91ca</li> </ul>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#33-scalable-mosaicing","title":"3.3 Scalable Mosaicing","text":"<p>\u7f29\u653e\u62fc\u63a5\u6846\u67b6\u3001\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u3001\u5904\u7406\u957f\u5c3e\u6548\u5e94</p> <p>\u73b0\u6709\u7684\u6570\u636e\u96c6 \u76ee\u6807\u7269\u4f53\u7684\u6570\u91cf\u8f83\u5c11</p> <ul> <li>\u5728FSC - 147\u6570\u636e\u96c6\u4e2d\uff0c\u5728\u8bad\u7ec3\u96c6\u4e2d\u76843659\u5f20\u56fe\u50cf\u4e2d\uff0c\u4ec5\u67096\u5f20\u56fe\u50cf\u5305\u542b\u8d85\u8fc71000\u4e2a\u7269\u4f53\u3002\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u63d0\u4f9b\u4eba\u5de5\u6ce8\u91ca\u7684\u6602\u8d35\u7a0b\u5e8f\u9020\u6210\u7684</li> <li>\u63a5\u4e0b\u6765\uff0c\u4ecb\u7ecd\u63d0\u51fa\u7684 \u5408\u6210\u8bad\u7ec3\u6570\u636e\u751f\u6210\u7684\u4e24\u4e2a\u6b65\u9aa4</li> </ul> <p></p> <p></p> <p>\u62fc\u63a5</p> <p>\u9996\u5148\u4ece\u56fe\u50cf\u4e2d\u88c1\u526a\u4e00\u4e2a\u968f\u673a\u5927\u5c0f\u7684\u6b63\u65b9\u5f62\u533a\u57df\uff0c\u5e76\u5c06\u5176\u7f29\u653e\uff0c\u4f8b\u5982\u539f\u59cb\u56fe\u50cf\u5927\u5c0f\u7684\u56db\u5206\u4e4b\u4e00\u3002\u5728\u591a\u6b21\u91cd\u590d\u533a\u57df\u88c1\u526a\u540e\uff0c\u6211\u4eec\u5c06\u88c1\u526a\u7684\u533a\u57df\u62fc\u8d34\u5728\u4e00\u8d77\u5e76\u66f4\u65b0\u76f8\u5e94\u7684\u5bc6\u5ea6\u56fe\u3002\u5b83\u6709\u4e24\u79cd\u4e0d\u540c\u7684\u5f62\u5f0f\uff1a\u53ea\u4f7f\u7528\u4e00\u5f20\u56fe\u50cf\u6216\u4f7f\u7528\u56db\u5f20\u4e0d\u540c\u7684\u56fe\u50cf\u3002</p> <p>\u5982\u679c\u6211\u4eec\u53ea\u4f7f\u7528\u4e00\u5f20\u56fe\u50cf\uff0c\u6211\u4eec\u53ef\u4ee5\u589e\u52a0\u56fe\u50cf\u4e2d\u5305\u542b\u7684\u5bf9\u8c61\u6570\u91cf\uff0c\u8fd9\u5728\u89e3\u51b3\u957f\u5c3e\u95ee\u9898\u4e0a\u975e\u5e38\u6709\u5e2e\u52a9\u3002\u5982\u679c\u6211\u4eec\u4f7f\u7528\u56db\u5f20\u4e0d\u540c\u7684\u56fe\u50cf\uff0c\u6211\u4eec\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u56fe\u50cf\u7684\u80cc\u666f\u591a\u6837\u6027\uff0c\u5e76\u589e\u5f3a\u6a21\u578b\u533a\u5206\u4e0d\u540c\u7c7b\u522b\u5bf9\u8c61\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u5145\u5206\u5229\u7528\u8fd9\u4e24\u4e2a\u4f18\u52bf\uff0c\u6211\u4eec\u5236\u5b9a\u4e86\u4ee5\u4e0b\u8bbe\u7f6e\u3002\u5982\u679c\u56fe\u50cf\u4e2d\u5305\u542b\u7684\u5bf9\u8c61\u6570\u91cf\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\uff0c\u6211\u4eec\u4f7f\u7528\u76f8\u540c\u7684\u56fe\u50cf\u8fdb\u884c\u62fc\u8d34\uff1b\u5982\u679c\u6ca1\u6709\uff0c\u6211\u4eec\u4f7f\u7528\u56db\u5f20\u4e0d\u540c\u7684\u56fe\u50cf\u3002\u8bf7\u6ce8\u610f\uff0c\u5982\u679c\u4f7f\u7528\u4e86\u56db\u5f20\u4e0d\u540c\u7684\u56fe\u50cf\uff0c\u6211\u4eec\u53ea\u80fd\u4f7f\u7528\u5c11\u91cf\u6837\u672c\uff08few-shot\uff09\u8bbe\u7f6e\u8fdb\u884c\u63a8\u7406\uff0c\u5426\u5219\u6a21\u578b\u5c06\u4e0d\u77e5\u9053\u8981\u8ba1\u6570\u54ea\u4e2a\u5bf9\u8c61\u3002\u5982\u679c\u6211\u4eec\u4f7f\u7528\u76f8\u540c\u7684\u56fe\u50cf\uff0c\u62fc\u8d34\u540e\u7684\u56fe\u50cf\u53ef\u4ee5\u7528\u6765\u8bad\u7ec3\u5c11\u91cf\u6837\u672c\u8bbe\u7f6e\u548c\u96f6\u6837\u672c\uff08zero-shot\uff09\u8bbe\u7f6e\u3002</p> <p></p> <p></p> <p>\u56fe3. \u6d4b\u8bd5\u65f6\u5f52\u4e00\u5316\u8fc7\u7a0b\u7684\u53ef\u89c6\u5316\u3002\u5728\u6d4b\u8bd5\u65f6\u5f52\u4e00\u5316\u4e2d\uff0c\u5982\u679c\u5bc6\u5ea6\u56fe\u4e2d\u793a\u4f8b\u4f4d\u7f6e\u7684\u5e73\u5747\u603b\u548c\u8d85\u8fc71.8\uff0c\u5219\u5bc6\u5ea6\u56fe\u7684\u603b\u548c\u5c06\u9664\u4ee5\u8fd9\u4e2a\u5e73\u5747\u503c\u4ee5\u6210\u4e3a\u6700\u7ec8\u9884\u6d4b\u3002\u5728\u6d4b\u8bd5\u65f6\u88c1\u526a\u4e2d\uff0c\u5982\u679c\u81f3\u5c11\u6709\u4e00\u4e2a\u793a\u4f8b\u7684\u8fb9\u957f\u5c0f\u4e8e10\u50cf\u7d20\uff0c\u56fe\u50cf\u5c06\u88ab\u88c1\u526a\u62109\u4e2a\u90e8\u5206\uff0c\u6a21\u578b\u5c06\u5206\u522b\u5904\u7406\u8fd99\u4e2a\u56fe\u50cf\u3002\u6700\u7ec8\u9884\u6d4b\u5c06\u662f\u8fd99\u4e2a\u56fe\u50cf\u7ed3\u679c\u7684\u603b\u548c\u3002</p> <p></p> <p>\u6df7\u5408</p> <p>\u6df7\u5408\u3002\u7b80\u5355\u5730\u88c1\u526a\u548c\u62fc\u63a5\u5e76\u4e0d\u80fd\u5408\u6210\u5b8c\u7f8e\u7684\u56fe\u50cf\uff0c\u56e0\u4e3a\u8fb9\u754c\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u660e\u663e\u7684\u4eba\u5de5\u75d5\u8ff9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u4eba\u5de5\u75d5\u8ff9\uff0c\u6211\u4eec\u5229\u7528\u56fe\u50cf\u8fde\u63a5\u5904\u7684\u6df7\u5408\u6280\u672f\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u88c1\u526a\u7684\u56fe\u50cf\u5c3a\u5bf8\u7565\u5927\u4e8e\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8\u7684\u56db\u5206\u4e4b\u4e00\uff0c\u8fd9\u6837\u6211\u4eec\u53ef\u4ee5\u5728\u8fb9\u754c\u7559\u51fa\u7279\u5b9a\u7684\u7a7a\u95f4\u7528\u4e8e\u03b1\u901a\u9053\u6df7\u5408\u3002\u6211\u4eec\u4f7f\u7528\u968f\u673a\u7684\u03b1\u901a\u9053\u8fb9\u754c\u5bbd\u5ea6\uff0c\u8fd9\u4f7f\u5f97\u56fe\u50cf\u7684\u5408\u6210\u66f4\u52a0\u771f\u5b9e\u3002\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u53ea\u6df7\u5408\u539f\u59cb\u56fe\u50cf\u800c\u4e0d\u662f\u5bc6\u5ea6\u56fe\uff0c\u4ee5\u4fdd\u6301\u70b9\u6ce8\u91ca\u7684\u5f62\u5f0f\uff08\u53ea\u67090\u548c1\uff09\u3002\u7531\u4e8e\u6df7\u5408\u8fb9\u754c\u5185\u7684\u5bf9\u8c61\u5f88\u5c11\uff0c\u4e14\u4f7f\u7528\u4e00\u5f20\u56fe\u50cf\u5236\u4f5c\u7684\u9a6c\u8d5b\u514b\u4ec5\u9002\u7528\u4e8e\u5bf9\u8c61\u6570\u91cf\u975e\u5e38\u591a\u7684\u56fe\u50cf\uff0c\u56e0\u6b64\u6df7\u5408\u5f15\u8d77\u7684\u8bef\u5dee\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#34-test-time-normalisation","title":"3.4 Test-time Normalisation","text":"<p>\u5bf9\u4e8e\u5c11\u91cf\u6837\u672c\u8ba1\u6570\uff0c\u6211\u4eec\u5728\u6b63\u6587\u4e2d\u5f15\u5165\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u7684\u5f52\u4e00\u5316\u7b56\u7565\u6765\u6821\u51c6\u8f93\u51fa\u5bc6\u5ea6\u56fe\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u63a8\u7406\u65f6\uff0c\u6211\u4eec\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\uff0c\u5373\u793a\u4f8b\u4f4d\u7f6e\u7684\u5bf9\u8c61\u8ba1\u6570\u5e94\u8be5\u6070\u597d\u662f1.0\uff0c\u56e0\u6b64\u4efb\u4f55\u9884\u6d4b\u504f\u5dee\u90fd\u53ef\u4ee5\u901a\u8fc7\u5c06\u5bc6\u5ea6\u56fe\u9664\u4ee5\u793a\u4f8b\u4f4d\u7f6e\u5f53\u524d\u9884\u6d4b\u7684\u8ba1\u6570\u6765\u6821\u51c6\u3002\u6211\u4eec\u91c7\u53d6\u8fd9\u79cd\u65b9\u6cd5\u662f\u56e0\u4e3a\u7531\u4e8e\u8fb9\u754c\u6846\u7684\u6b67\u4e49\uff0c\u6a21\u578b\u6709\u65f6\u9009\u62e9\u5bf9\u8c61\u7684\u6700\u5c0f\u81ea\u76f8\u4f3c\u5355\u5143\u8fdb\u884c\u8ba1\u6570\uff0c\u800c\u4e0d\u662f\u6574\u4e2a\u5bf9\u8c61\uff0c\u5982\u56fe3\uff08a\uff09\u6240\u793a\u3002\u56e0\u6b64\uff0c\u5982\u679c\u5bf9\u5e94\u4e8e\u8fb9\u754c\u6846\u7684\u5bc6\u5ea6\u56fe\u533a\u57df\u7684\u5e73\u5747\u603b\u548c\u8d85\u8fc7\u4e00\u4e2a\u9608\u503c\uff0c\u4f8b\u59821.8\uff0c\u6211\u4eec\u5c06\u5229\u7528\u8fd9\u79cd\u6d4b\u8bd5\u65f6\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002</p> <p></p> <p></p> <p>\u6b64\u5916\uff0c\u5bf9\u4e8e\u5305\u542b\u5fae\u5c0f\u7269\u4f53\u7684\u56fe\u50cf\uff08\u4e00\u4e2a\u793a\u4f8b\u7684\u8fb9\u957f\u5c0f\u4e8e10\u50cf\u7d20\uff09\uff0c\u6211\u4eec\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u9884\u6d4b\u65b9\u6cd5\uff0c\u5c06\u56fe\u50cf\u7b49\u5206\u4e3a\u4e5d\u4e2a\u90e8\u5206\uff0c\u5e76\u5c06\u5176\u7f29\u653e\u56de\u539f\u59cb\u5927\u5c0f\uff0c\u4ee5\u4fbf\u6211\u4eec\u7684\u6a21\u578b\u5206\u522b\u5904\u7406\u3002\u7269\u4f53\u7684\u603b\u6570\u662f\u8fd9\u4e5d\u4e2a\u56fe\u50cf\u7684\u5355\u72ec\u8ba1\u6570\u7ed3\u679c\u7684\u603b\u548c\u3002</p>"},{"location":"literature/ObejectCounting/rank9%20SemAug_SAFECount/","title":"rank9 SemAug SAFECount","text":"<p>SemAug \u63d0\u51fa\u7684\u662f\u4e00\u4e2a\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5206\u522b\u5bf9\u4e24\u4e2a\u4e0d\u540c\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u6570\u636e\u589e\u5f3a\uff1aCounTR\u548cSAFECount\uff0c\u5237\u7684\u540c\u4e00\u4e2a\u6570\u636e\u96c6\u90fd\u662fFSC147\u6570\u636e\u96c6\uff0c\u91cd\u70b9\u5728\u4e8e\uff0c\u7ecf\u8fc7\u6570\u636e\u589e\u5f3a\u540e\u7684\u6a21\u578b\uff0c\u6548\u679c\u90fd\u6bd4\u539f\u6765\u7684\u597d</p> <p>\u91cd\u70b9\u5728\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u7b56\u7565\uff1a</p> <ul> <li>\u6269\u6563\u6a21\u578b\uff0c\u76d1\u7763\u4fe1\u53f7\uff1aprompt \u548c density map</li> </ul> <p></p>"},{"location":"literature/ObjectDetection/","title":"Index","text":"<ul> <li> DINO \u8bba\u6587\u7b80\u4ecb</li> <li> Transformer DINO\u548cGrounding DINO\u8bb2\u89e3</li> <li> IDEA\u7814\u7a76\u9662CVR\u53d1\u5e03DINO-X\u76ee\u6807\u68c0\u6d4b\u89c6\u89c9\u5927\u6a21\u578b\uff1a\u5e26\u6765\u5168\u573a\u666f\u611f\u77e5\u7684\u65b0\u9ad8\u5ea6</li> <li> \u76ee\u6807\u68c0\u6d4b\uff1a\u4eceR-CNN\u3001YOLO\u5230DETR\u3001DINO</li> </ul>"},{"location":"literature/ObjectDetection/1/","title":"DETR\u8bba\u6587\u7cfb\u5217","text":"<ul> <li>DETR\u8bba\u6587\u7cfb\u5217<ul> <li>DETR\u8bba\u6587<ul> <li>DETR\u662f\u4ec0\u4e48\uff1f</li> </ul> </li> <li>Deformable DETR \u53ef\u53d8\u5f62</li> <li>Conditional DETR \u6709\u6761\u4ef6\u7684DETR</li> <li>Anchor DETR</li> <li>DAB-DETR</li> <li>DN-DETR \u53bb\u566aDETR</li> <li>DINO</li> <li>Lite DETR</li> <li>Focus DETR</li> <li>H-DETR</li> </ul> </li> </ul>"},{"location":"literature/ObjectDetection/1/#detr_1","title":"DETR\u8bba\u6587","text":"<p>rpn\u9636\u6bb5\u3001roi\u9636\u6bb5\u4e24\u6b21sample\u6b63\u8d1f\u6837\u672c\u7684\u6570\u91cf</p> <p>\u9996\u5148\u5728rpn\u9636\u6bb5\uff0c\u9996\u5148\u751f\u6210\u4e0d\u540c\u6bd4\u4f8b\u7684\u5bbd\u9ad8\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u6bd4\u4f8b\u7684\u7279\u5f81\u56fe\u7684\u6bd4\u4f8b\u4e0a\u751f\u6210</p> <p>\u9700\u8981\u5b9a\u4e49anchor\u7684\u5bbd\u9ad8\u6bd4\u3001\u6570\u91cf</p> <p>fast RCNN\u9700\u8981\u5728rpn\u9636\u6bb5\u3001roi\u9636\u6bb5\u8fdb\u884c\u4e24\u6b21NMS\u7684\u8ba1\u7b97\uff0c\u6d88\u9664\u76f8\u540c\u4f4d\u7f6e\u91cd\u590d\u533a\u57df\u7684\u9884\u6d4b</p> <p></p> <p></p> <p>DETR\u79fb\u9664\u4e86Anchor\u7684\u751f\u6210\uff0c\u5728fast rcnn\u4e2d\uff0c\u9700\u8981\u5728\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u9884\u8bbe\u7684\u6570\u91cf\u4ee5\u53ca\u5bbd\u9ad8\u6bd4\uff0c\u4e00\u4e9banchor\uff0c\u8fd9\u4e9banchor\u5728\u540e\u7eed\u7684\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5728\u7f51\u7edc\u7684\u8f93\u51fa\u4e2d\u8fdb\u884c\u5fae\u8c03\uff0cDETR\u5b8c\u5168\u6452\u5f03\u4e86\u8fd9\u79cd\u65b9\u5f0f</p> <p></p> <p>DETR\u5c5e\u4e8e\u96c6\u5408\u9884\u6d4b\u7684\u6a21\u578b</p> <p>\u76ee\u6807\u68c0\u6d4bNMS\u7684\u8fc7\u7a0b\u5c31\u662f\u6392\u9664\u6389\u5728\u4e00\u4e9b\u76f8\u8fd1\u7684\u4f4d\u7f6e\u4e0a\u5bf9\u4e8e\u540c\u4e00\u4e2a\u7c7b\u522b\u91cd\u590d\u7684\u9884\u6d4b</p> <p>\u5728DETR\u4e2d\uff0c\u662f\u6ca1\u6709\u4e86\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u7f51\u7edc\u6240\u7ed9\u51fa\u7684\u8f93\u51fa\u5728\u5b9a\u4e49\u4e2d\u5c31\u662f100\u4e2a\u7ed3\u679c\uff0c\u8fd9100\u4e2a\u7ed3\u679c\u4e5f\u662f\u6700\u7ec8\u7684\u7f51\u7edc\u9884\u6d4b\u7684\u7ed3\u679c\uff0c\u5305\u62ec\u56fe\u7247\u4e2d\u9690\u5f0f\u7c7b\u7684\u7ed3\u679c\uff0c\u6216\u8005\u67d0\u4e9b\u9884\u6d4b\u7ed3\u679c\u662f\u6ca1\u6709\u76ee\u6807\u7684\uff0c\u8fd9\u4e2a\u9884\u6d4b\u503c100\u6307\u7684\u662f\u5728\u6240\u6709\u53ef\u80fd\u7684\u9884\u6d4b\u7ed3\u679c\u4e2d\u7684\u6700\u5927\u503c\uff0c\u4e00\u822c\u6765\u8bf4\u4e00\u5f20\u56fe\u7247\u4e5f\u4e0d\u4f1a\u5b9a\u4e49100\u4e2agt\uff0c\u5728\u5176\u4ed6\u7684\u76ee\u6807\u68c0\u6d4b\u4e2d\uff0c\u5728\u6700\u540e\u4e00\u4e2a\u9636\u6bb5\u8f93\u51fa\u7684\u7f51\u7edc\u7ed3\u679c\u7684\u76ee\u6807\u6846\uff0c\u5728\u6ca1\u6709\u8fdb\u884cNMS\u4e4b\u524d\uff0c\u8fd9\u4e2a\u6570\u91cf\u662f\u591a\u8fc7\u4e8e100\u7684</p> <p>DETR\u7684\u4e24\u4e2a\u6838\u5fc3\u5185\u5bb9\uff1a</p> <p></p> <p>\u4e00\u4e2a\u662fTransformer\u7684\u7ed3\u679c\uff0c\u4ece\u6807\u9898\u4e2d\u5c31\u53ef\u4ee5\u770b\u51fa\uff1b\u7b2c\u4e8c\u4e2a\u662f\u6307\u7684\u4e8c\u5206\u56fe\u5339\u914d</p> <p>\u9996\u5148\u4ecb\u7ecd\u4e8c\u5206\u56fe\u5339\u914d</p> <p></p> <p>\u4e8c\u5206\u56fe\u5c31\u662f\u5b9a\u4e49\u4e86\u4e24\u4e2a\u96c6\u5408\uff0c\u5728\u6bcf\u4e00\u4e2a\u96c6\u5408\uff0c\u5728\u5404\u81ea\u7684\u96c6\u5408\u5185\uff0c\u6bcf\u4e00\u4e2a\u70b9\u6ca1\u6709\u8fde\u63a5\uff0c\u5728\u4e24\u4e2a\u96c6\u5408\u95f4\uff0c\u6709\u8fb9\u8fdb\u884c\u8fde\u63a5\uff1b\u4e8c\u5206\u56fe\u5339\u914d\u4e0e\u5308\u7259\u5229\u7b97\u6cd5\uff0c\u5c31\u662f\u5982\u4f55\u8ba1\u7b97\u4e24\u4e2a\u96c6\u5408\u5f97\u5230\u6700\u4f18\u5206\u914d\u7684\u95ee\u9898</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>\u8f85\u52a9\u635f\u5931</p> <p></p> <p></p> <p>\u4f46\u662f\u5bf9\u4e8e\u5c0f\u76ee\u6807\u7684\u7ed3\u679c\u4e0d\u592a\u597d\uff0c\u53ef\u80fdTransformer\u4e0d\u592a\u9002\u5408\u5c0f\u76ee\u6807</p> <p></p> <p></p>"},{"location":"literature/ObjectDetection/1/#detr_2","title":"DETR\u662f\u4ec0\u4e48\uff1f","text":""},{"location":"literature/ObjectDetection/1/#deformable-detr","title":"Deformable DETR \u53ef\u53d8\u5f62","text":""},{"location":"literature/ObjectDetection/1/#conditional-detr-detr","title":"Conditional DETR \u6709\u6761\u4ef6\u7684DETR","text":""},{"location":"literature/ObjectDetection/1/#anchor-detr","title":"Anchor DETR","text":""},{"location":"literature/ObjectDetection/1/#dab-detr","title":"DAB-DETR","text":""},{"location":"literature/ObjectDetection/1/#dn-detr-detr","title":"DN-DETR \u53bb\u566aDETR","text":""},{"location":"literature/ObjectDetection/1/#dino","title":"DINO","text":""},{"location":"literature/ObjectDetection/1/#lite-detr","title":"Lite DETR","text":""},{"location":"literature/ObjectDetection/1/#focus-detr","title":"Focus DETR","text":""},{"location":"literature/ObjectDetection/1/#h-detr","title":"H-DETR","text":""},{"location":"literature/ObjectDetection/2/","title":"\u76ee\u6807\u68c0\u6d4b\u57fa\u7840\u77e5\u8bc6","text":""},{"location":"literature/ObjectDetection/2/#iou","title":"IOU","text":""},{"location":"literature/ObjectDetection/2/#iou_1","title":"IOU\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6587\u5b57\uff1aIntersection over Union\u3001\u4e24\u4e2abox\u533a\u57df\u7684\u4ea4\u96c6\u6bd4\u4e0a\u5e76\u96c6\u3001\u4ea4\u5e76\u6bd4</p> <p>\u56fe\u793a\uff1a</p>"},{"location":"literature/ObjectDetection/2/#iou_2","title":"IOU\u600e\u4e48\u5b9e\u73b0\uff1f","text":"<p>\u601d\u8def\uff1a\uff08\u6ce8\u610f\u7ef4\u5ea6\u4e00\u81f4\uff09</p> <ul> <li> <p>\u9996\u5148\u8ba1\u7b97\u4e24\u4e2abox\u5de6\u4e0a\u89d2\u70b9\u5750\u6807\u7684\u6700\u5927\u503c\u548c\u53f3\u4e0b\u89d2\u5750\u6807\u7684\u6700\u5c0f\u503c </p> </li> <li> <p>\u7136\u540e\u8ba1\u7b97\u4ea4\u96c6\u9762\u79ef </p> </li> <li> <p>\u6700\u540e\u628a\u4ea4\u96c6\u9762\u79ef\u9664\u4ee5\u5bf9\u5e94\u7684\u5e76\u96c6\u9762\u79ef</p> </li> </ul> <p>Note</p> <p>\u5de6\u4e0a\u89d2\u5750\u6807\u548c\u53f3\u4e0b\u89d2\u5750\u6807\u786e\u5b9a\u4e00\u4e2a\u6846</p>"},{"location":"literature/ObjectDetection/2/#api","title":"\u5b98\u65b9api","text":"<p>pytorch\u6e90\u7801\uff1a</p> <p>\uff08\u6ce8\u610f\u77e9\u9635\u7ef4\u5ea6\u7684\u53d8\u5316\uff09</p>"},{"location":"literature/ObjectDetection/2/#box_iou","title":"box_iou","text":"<pre><code>def box_iou(boxes1: Tensor, boxes2: Tensor) -&gt; Tensor:\n    \"\"\"\n    Return intersection-over-union (Jaccard index) between two sets of boxes.\n\n    Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with\n    ``0 &lt;= x1 &lt; x2`` and ``0 &lt;= y1 &lt; y2``.\n\n    Args:\n        boxes1 (Tensor[N, 4]): first set of boxes\n        boxes2 (Tensor[M, 4]): second set of boxes\n\n    Returns:\n        Tensor[N, M]: the NxM matrix containing the pairwise IoU values for every element in boxes1 and boxes2\n    \"\"\"\n    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n        _log_api_usage_once(box_iou)\n    inter, union = _box_inter_union(boxes1, boxes2)\n    iou = inter / union\n    return iou\n</code></pre> <p>\u61c2\u4e86\uff1aboxes1\u6709N\u4e2a\uff0cboxes2\u6709M\u4e2a\uff0c\u8fd4\u56de\u7684N\u00d7M\u662fboxes1\u4e2d\u7684\u6240\u6709\u68c0\u6d4b\u6846\u4e0eboxes2\u4e2d\u6240\u6709\u68c0\u6d4b\u6846\u7684IOU\u503c\uff0cboxes\uff1a\u96c6\u5408\u3001\u6846\u7684\u96c6\u5408</p>"},{"location":"literature/ObjectDetection/2/#_box_inter_union","title":"_box_inter_union","text":"<pre><code>def _box_inter_union(boxes1: Tensor, boxes2: Tensor) -&gt; Tuple[Tensor, Tensor]:\n    area1 = box_area(boxes1) # (x1, y1, x2, y2)\n    area2 = box_area(boxes2)\n\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2] \u627e\u5230\u5de6\u4e0a\u89d2\u7684\u6700\u5927\u503c\n    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2] \u627e\u5230\u53f3\u4e0b\u89d2\u7684\u6700\u5c0f\u503c\n\n    wh = _upcast(rb - lt).clamp(min=0)  # [N,M,2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n\n    union = area1[:, None] + area2 - inter\n\n    return inter, union\n</code></pre>"},{"location":"literature/ObjectDetection/2/#box_area","title":"box_area","text":"<p>\u662f\u4ec0\u4e48\uff1f</p> <p>\u7ed9\u5b9a\u4e00\u7cfb\u5217\u8fb9\u754c\u6846\u5750\u6807\u96c6\u5408\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u8fb9\u754c\u6846\u7684\u9762\u79ef  </p> <p>\u8fd4\u56de\u503c\uff1aN\u7684\u8fb9\u754c\u6846\u7684\u9762\u79ef</p> <p>\u600e\u4e48\u5b9e\u73b0\u7684\uff1f \u9762\u79ef\u7684\u8ba1\u7b97\u516c\u5f0f\uff1a<code>(boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])</code></p> <p>\u8fb9\u754c\u6846\u7684\u5750\u6807\u96c6\u5408\uff1f <code>(x1, y1, x2, y2)</code> </p> <pre><code>def box_area(boxes: Tensor) -&gt; Tensor:\n    \"\"\"\n    Computes the area of a set of bounding boxes, which are specified by their\n    (x1, y1, x2, y2) coordinates.\n\n    Args:\n        boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n            are expected to be in (x1, y1, x2, y2) format with\n            ``0 &lt;= x1 &lt; x2`` and ``0 &lt;= y1 &lt; y2``.\n\n    Returns:\n        Tensor[N]: the area for each box\n    \"\"\"\n    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n        _log_api_usage_once(box_area)\n    boxes = _upcast(boxes)\n    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n</code></pre>"},{"location":"literature/ObjectDetection/2/#reference","title":"Reference  \u4ee3\u7801\u5b9e\u73b0","text":"<pre><code>import torch\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# IOU\u8ba1\u7b97\n# \u5047\u8bbebox1\u7ef4\u5ea6\u4e3a[N,4]   box2\u7ef4\u5ea6\u4e3a[M,4]\ndef iou(box1, box2):\n    N = box1.size(0)\n    M = box2.size(0)\n\n    lt = torch.max(  # \u5de6\u4e0a\u89d2\u7684\u70b9\n        box1[:, :2].unsqueeze(1).expand(N, M, 2),   # [N,2]-&gt;[N,1,2]-&gt;[N,M,2]\n        box2[:, :2].unsqueeze(0).expand(N, M, 2),   # [M,2]-&gt;[1,M,2]-&gt;[N,M,2]\n    )\n    print(\"lt\",lt)\n    print(\"lt shape\",lt.shape)\n\n    rb = torch.min(\n        box1[:, 2:].unsqueeze(1).expand(N, M, 2),\n        box2[:, 2:].unsqueeze(0).expand(N, M, 2),\n    )\n    print(\"rb\",rb)\n    print(\"rb shape\",rb.shape)\n\n    wh = rb - lt  # [N,M,2]\n    print(\"wh\",wh)\n    wh[wh &lt; 0] = 0   # \u4e24\u4e2abox\u6ca1\u6709\u91cd\u53e0\u533a\u57df\n    inter = wh[:,:,0] * wh[:,:,1]   # [N,M]\n\n    area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # (N,)\n    area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # (M,)\n    area1 = area1.unsqueeze(1).expand(N,M)  # (N,M)\n    area2 = area2.unsqueeze(0).expand(N,M)  # (N,M)\n\n    iou = inter / (area1 + area2 - inter)\n    return iou\n\n# # \u6d4b\u8bd5\u4ee3\u78011\n# M=1\n# N=1\n# box1 = torch.tensor([[20, 30, 40, 50]], dtype=torch.float)  # \u624b\u52a8\u8bbe\u7f6e\u6709\u91cd\u53e0\u533a\u57df\u7684\u6846\n# box2 = torch.tensor([[30, 40, 50, 60]], dtype=torch.float)\n# # IOU: tensor([[0.1429]])\n\n# \u6d4b\u8bd5\u4ee3\u78012\nM=2\nN=1\nbox1 = torch.tensor([[20, 30, 40, 50]], dtype=torch.float)  # \u624b\u52a8\u8bbe\u7f6e\u6709\u91cd\u53e0\u533a\u57df\u7684\u6846\nprint(box1.shape)\nbox2 = torch.tensor([[30, 40, 50, 60], [15, 25, 35, 45]], dtype=torch.float)\nprint(box2.shape)\n\n# IOU: tensor([[0.1429, 0.3913]])\n\n# \u6d4b\u8bd5\u4ee3\u78013\n# N=2\n# M=3\n# box1 = torch.tensor([[20, 30, 40, 50], [60, 70, 80, 90]], dtype=torch.float)  # \u624b\u52a8\u8bbe\u7f6e\u6709\u91cd\u53e0\u533a\u57df\u7684\u6846\n# print(box1.shape)\n# box2 = torch.tensor([[30, 40, 50, 60], [70, 80, 90, 100], [15, 25, 35, 45]], dtype=torch.float)\n# print(box2.shape)\n\n# print(\"Box1:\", box1)\n# print(\"Box2:\", box2)\n# print(\"IOU:\", iou(box1, box2))\n\n# # \u7ed8\u5236\u8fb9\u754c\u6846\n# fig, ax = plt.subplots(1)\n\n# # \u7ed8\u5236box1\n# for i in range(N):\n#     rect = patches.Rectangle((box1[i, 0], box1[i, 1]), box1[i, 2] - box1[i, 0], box1[i, 3] - box1[i, 1], linewidth=1, edgecolor='r', facecolor='none')\n#     ax.add_patch(rect)\n\n# # \u7ed8\u5236box2\n# for i in range(M):\n#     rect = patches.Rectangle((box2[i, 0], box2[i, 1]), box2[i, 2] - box2[i, 0], box2[i, 3] - box2[i, 1], linewidth=1, edgecolor='b', facecolor='none')\n#     ax.add_patch(rect)\n\n# plt.xlim(0, 100)\n# plt.ylim(0, 100)\n# plt.gca().set_aspect('equal', adjustable='box')\n# plt.show()\n</code></pre> <p>\u5176\u4e2d\uff1a</p> <p>torch.unsqueeze(1) \u8868\u793a\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u589e\u52a0\u4f4d\u7f6e\u4e3a\u7ef4\u5ea61</p> <p>torch.squeeze(1) \u8868\u793a\u51cf\u5c11\u4e00\u4e2a\u7ef4\u5ea6</p> <p>\ud83c\udf30\uff1a</p> <p></p> <p>\\(\\frac{1}{7}\\)</p>"},{"location":"literature/ObjectDetection/2/#nms","title":"NMS","text":""},{"location":"literature/ObjectDetection/2/#nms_1","title":"NMS\u662f\u4ec0\u4e48\uff1f","text":"<p>5\u4e2a\u5b57\uff1a\u8fc7\u6ee4\u5197\u4f59\u6846</p> <p>\u6587\u5b57\u63cf\u8ff0\u3001\u6570\u5b66\u5b9e\u4f8b</p> <p>\u6587\u5b57\uff1a</p> <ul> <li>Non-maximum suppression\u3001\u975e\u6781\u5927\u503c\u6291\u5236\u7b97\u6cd5</li> </ul> <p>\u6570\u5b66\u5b9e\u4f8b</p> <p>\u56fe\u793a\uff1a</p> <p>\u9700\u8981\u7ed9\u5b9a\u7684\u8f93\u5165\uff1a\u9884\u6d4b\u7684\u8fb9\u754c\u6846\u53ca\u7f6e\u4fe1\u5ea6</p> <p>Note</p> <p>\u975e\u6781\u5927\u503c\u6291\u5236\uff08NMS\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u53bb\u9664\u91cd\u53e0\u7684\u8fb9\u754c\u6846    :param bboxes: \u8fb9\u754c\u6846\u5217\u8868\uff0c\u6bcf\u4e2a\u8fb9\u754c\u6846\u683c\u5f0f\u4e3a[xmin, ymin, xmax, ymax]     :param scores: \u8fb9\u754c\u6846\u5bf9\u5e94\u7684\u7f6e\u4fe1\u5ea6\u5217\u8868   :param iou _thresh: IOU\uff08\u4ea4\u5e76\u6bd4\uff09\u9608\u503c\uff0c\u7528\u4e8e\u5224\u65ad\u4e24\u4e2a\u8fb9\u754c\u6846\u662f\u5426\u91cd\u53e0                :return: \u7ecf\u8fc7NMS\u5904\u7406\u540e\u7684\u8fb9\u754c\u6846\u548c\u7f6e\u4fe1\u5ea6\u5217\u8868</p> <p></p>"},{"location":"literature/ObjectDetection/2/#nms_2","title":"\u4e3a\u4ec0\u4e48NMS\uff1f","text":"<p>NMS\u7684\u4f5c\u7528\u662f\u53bb\u9664\u591a\u4e2a\u9884\u6d4b\u540c\u4e00\u7269\u4f53\u7684\u5197\u4f59\u68c0\u6d4b\u6846</p> <p>\u54ea\u91cc\u9700\u8981\u7528\uff1f</p> <p>\u4ee5RCNN\u7cfb\u5217\u3001Yolo\u7cfb\u5217\u4e3a\u9996\u7684\u4e00\u4e9b\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5df2\u7ecf\u53d6\u5f97\u4e86\u975e\u5e38\u6210\u719f\u7684\u5e94\u7528\u6548\u679c\uff0c\u4f46\u4e0d\u7ba1\u662f\u5355\u9636\u6bb5\u6a21\u578b\uff08\u5982Yolo\u3001SSD\uff09\u8fd8\u662f\u4e24\u9636\u6bb5\u6a21\u578b\uff08\u5982RCNN\uff09\uff0c\u90fd\u9700\u8981\u8fdb\u884c\u975e\u6781\u5927\u503c\u6291\u5236\uff08Non-Maximum Suppression\uff0c\u7b80\u79f0NMS\uff09\u7684\u540e\u5904\u7406\u64cd\u4f5c\u3002</p> <p>\u7f3a\u70b9</p> <p>\u2460 O(n)\u7684\u7b97\u6cd5\uff0c\u5f53\u6846\u5f88\u591a\u65f6\uff0c\u6bd4\u8f83\u8d39\u65f6\uff1b</p> <p>\u2461 \u5728\u8fdb\u884cNMS\u65f6\uff0c\u5e76\u6ca1\u6709\u4f7f\u7528\u5230\u56fe\u50cf\u7279\u5f81\uff0c\u800c\u662f\u4ec5\u4ec5\u4f7f\u7528\u4e86\u9884\u6d4b\u6846\u7684\u53c2\u6570\u3002\u4e5f\u5c31\u662f\u8bf4\uff0cNMS\u7b97\u6cd5\u662f\u6ca1\u6709\u201c\u770b\u5230\u201d\u56fe\u50cf\u7684\uff0c\u8fd9\u5c31\u4f1a\u5bfc\u81f4\u4e00\u4e2a\u95ee\u9898\uff0c\u6709\u4e9b\u79bb\u5f97\u975e\u5e38\u8fd1\u751a\u81f3\u91cd\u5408\u7684\u7269\u4f53\uff08\u6bd4\u5982\u4eba\u7fa4\u4e2d\u7684\u4e24\u4e2a\u4eba\uff09\u5bf9\u5e94\u7684\u68c0\u6d4b\u6846\u4f1a\u88abNMS\u7b97\u6cd5\u7ed9\u53bb\u9664\u6389\u800c\u53ea\u4fdd\u7559\u4e00\u4e2a\uff0c\u8fd9\u662f\u56e0\u4e3aNMS\u7b97\u6cd5\u6ca1\u6709\u201c\u770b\u5230\u201c\u56fe\u7247\uff0c\u5e76\u4e0d\u77e5\u9053\u8fd9\u91cc\u9762\u6709\u4e24\u4e2a\u4eba\uff0c\u53ea\u77e5\u9053\u4e24\u4e2a\u6846\u7684IoU\uff08intersection over union\uff09\u975e\u5e38\u5927\u800c\u5df2\uff1b</p> <p>\u2462 NMS\u9700\u8981\u8bbe\u7f6e\u8d85\u53c2\u6570\u9608\u503c\uff0c\u8fd9\u9700\u8981\u4e00\u5b9a\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u800c\u4e14\u5728\u8bbe\u8ba1\u65f6\u4f1a\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1case by case\uff0c\u8fd9\u4f7f\u5f97\u65b9\u6cd5\u4e0d\u591fgeneral</p> <p>\u4e3a\u4ec0\u4e48\u4f1a\u6709\u5197\u4f59\u6846\uff1f</p> <p>\u4e3b\u8981\u7684\u539f\u56e0\uff1a\u5728\u5bf9\u6bcf\u4e2aanchor\u8fdb\u884c\u56de\u5f52\u7684\u65f6\u5019\uff0c\u662f\u72ec\u7acb\u8fdb\u884c\u7684\u3002</p> <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u5047\u8bbeanchor A\u548canchor B\u9884\u6d4b\u7684\u662f\u540c\u4e00\u4e2a\u4eba\uff0c\u7136\u800canchor A\u548canchor B\u4e4b\u95f4\u5e76\u6ca1\u6709\u4fe1\u606f\u4ea4\u6362\uff0c\u56e0\u6b64\u5b83\u4eec\u4f1a\u5206\u522b\u9884\u6d4b\u51fa\u76f8\u4f3c\u7684\u7ed3\u679c\uff08\u751a\u81f3\u53ef\u80fd\u7f6e\u4fe1\u5206\u6570\u90fd\u5f88\u9ad8\uff09\u3002\u56e0\u6b64\uff0c\u5982\u679c\u80fd\u591f\u8ba9anchor B\u77e5\u9053\uff0c\u5df2\u7ecf\u6709anchor A\u5728\u9884\u6d4b\u8fd9\u4e2a\u7269\u4f53\u4e86\uff0c\u5c31\u53ef\u4ee5\u907f\u514danchor B\u53bb\u9884\u6d4b\u91cd\u590d\u7684\u5197\u4f59\u6846\u4e86\u3002</p>"},{"location":"literature/ObjectDetection/2/#nms_3","title":"NMS\u600e\u4e48\u5b9e\u73b0\uff1f","text":"<p>\u4ee3\u7801</p>"},{"location":"literature/ObjectDetection/2/#api_1","title":"\u5b98\u65b9api","text":"<p>source</p> <p></p> <p>\u6211\u6252\u4e0d\u5230\u6e90\u7801</p>"},{"location":"literature/ObjectDetection/2/#_2","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<ol> <li> <p>\u9009\u53d6\u8fd9\u7c7bbox\u4e2dscores\u6700\u5927\u7684\u54ea\u4e00\u4e2a\uff0c\u8bb0\u4e3abox_best\uff0c\u5e76\u4fdd\u7559\u5b83</p> </li> <li> <p>\u8ba1\u7b97box_best\u4e0e\u5176\u4f59\u7684box\u7684IOU</p> </li> <li> <p>\u5982\u679c\u5176IOU&gt;0.5\u4e86\uff0c\u90a3\u4e48\u5c31\u820d\u5f03\u8fd9\u4e2abox</p> </li> </ol> <p>\uff08\u7531\u4e8e\u53ef\u80fd\u8fd9\u4e24\u4e2abox\u8868\u793a\u540c\u4e00\u76ee\u6807\uff0c\u6240\u4ee5\u4fdd\u7559\u5206\u6570\u9ad8\u7684\u54ea\u4e00\u4e2a\uff09</p> <ol> <li>\u4ece\u6700\u540e\u5269\u4f59\u7684boxes\u4e2d\uff0c\u518d\u627e\u51fa\u6700\u5927scores\u7684\u54ea\u4e00\u4e2a\uff0c\u5982\u6b64\u5faa\u73af\u5f80\u590d</li> </ol> <p></p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# NMS\u7b97\u6cd5\n# bboxes\u7ef4\u5ea6\u4e3a[N,4]\uff0cscores\u7ef4\u5ea6\u4e3a[N,], \u5747\u4e3atensor\ndef nms(bboxes, scores, threshold=0.5):\n    x1 = bboxes[:,0]\n    y1 = bboxes[:,1]\n    x2 = bboxes[:,2]\n    y2 = bboxes[:,3]\n    areas = (x2-x1)*(y2-y1)   # [N,] \u6bcf\u4e2abbox\u7684\u9762\u79ef\n    _, order = scores.sort(0, descending=True)    # \u964d\u5e8f\u6392\u5217\n\n    keep = []\n    while order.numel() &gt; 0:       # torch.numel()\u8fd4\u56de\u5f20\u91cf\u5143\u7d20\u4e2a\u6570\n        if order.numel() == 1:     # \u4fdd\u7559\u6846\u53ea\u5269\u4e00\u4e2a\n            i = order.item()\n            keep.append(i)\n            break\n        else:\n            i = order[0].item()    # \u4fdd\u7559scores\u6700\u5927\u7684\u90a3\u4e2a\u6846box[i]\n            keep.append(i)\n\n        # \u8ba1\u7b97box[i]\u4e0e\u5176\u4f59\u5404\u6846\u7684IOU(\u601d\u8def\u5f88\u597d)\n        xx1 = x1[order[1:]].clamp(min=x1[i])   # [N-1,]\n        yy1 = y1[order[1:]].clamp(min=y1[i])\n        xx2 = x2[order[1:]].clamp(max=x2[i])\n        yy2 = y2[order[1:]].clamp(max=y2[i])\n        inter = (xx2-xx1).clamp(min=0) * (yy2-yy1).clamp(min=0)   # [N-1,]\n\n        iou = inter / (areas[i]+areas[order[1:]]-inter)  # [N-1,]\n        idx = (iou &lt;= threshold).nonzero().squeeze() # \u6ce8\u610f\u6b64\u65f6idx\u4e3a[N-1,] \u800corder\u4e3a[N,]\n        if idx.numel() == 0:\n            break\n        order = order[idx+1]  # \u4fee\u8865\u7d22\u5f15\u4e4b\u95f4\u7684\u5dee\u503c\n    return torch.LongTensor(keep)   # Pytorch\u7684\u7d22\u5f15\u503c\u4e3aLongTensor\n\n# \u6d4b\u8bd5\u4ee3\u7801\nbboxes = torch.tensor([\n    [20, 30, 40, 50],\n    [22, 32, 42, 52],\n    [100, 100, 120, 130]\n], dtype=torch.float)\n\nscores = torch.tensor([0.9, 0.75, 0.6], dtype=torch.float)\n\n# \u8c03\u7528NMS\u51fd\u6570\nkeep_indices = nms(bboxes, scores, threshold=0.5)\nprint(\"\u4fdd\u7559\u7684\u6846\u7d22\u5f15:\", keep_indices)\n\n# # \u53ef\u89c6\u5316\u7ed3\u679c\n# fig, ax = plt.subplots(1)\n\n# # \u7ed8\u5236\u6240\u6709\u8fb9\u754c\u6846\n# for i in range(bboxes.size(0)):\n#     rect = patches.Rectangle((bboxes[i, 0], bboxes[i, 1]), bboxes[i, 2] - bboxes[i, 0], bboxes[i, 3] - bboxes[i, 1], linewidth=1, edgecolor='r', facecolor='none', label='All Boxes' if i == 0 else \"\")\n#     ax.add_patch(rect)\n\n# # \u7ed8\u5236\u4fdd\u7559\u7684\u8fb9\u754c\u6846\n# for i in keep_indices:\n#     rect = patches.Rectangle((bboxes[i, 0], bboxes[i, 1]), bboxes[i, 2] - bboxes[i, 0], bboxes[i, 3] - bboxes[i, 1], linewidth=2, edgecolor='b', facecolor='none', label='Kept Boxes' if i == keep_indices[0] else \"\")\n#     ax.add_patch(rect)\n\n# # \u6dfb\u52a0\u56fe\u4f8b\n# handles, labels = ax.get_legend_handles_labels()\n# by_label = dict(zip(labels, handles))\n# ax.legend(by_label.values(), by_label.keys())\n\n# plt.xlim(0, 150)\n# plt.ylim(0, 150)\n# plt.gca().set_aspect('equal', adjustable='box')\n# plt.show()\n</code></pre> <p>torch.numel() \u8868\u793a\u4e00\u4e2a\u5f20\u91cf\u603b\u5143\u7d20\u7684\u4e2a\u6570   </p> <p>torch.clamp(min, max) \u8bbe\u7f6e\u4e0a\u4e0b\u9650</p> <p>tensor.item() \u628atensor\u5143\u7d20\u53d6\u51fa\u4f5c\u4e3anumpy\u6570\u5b57</p> <p>\u53e6\u8865\u5145\u4ee3\u7801\u793a\u4f8b</p>"},{"location":"literature/ObjectDetection/2/#bounding-box-regression","title":"Bounding box regression","text":""},{"location":"literature/ObjectDetection/2/#anchor","title":"Anchor","text":""},{"location":"literature/ObjectDetection/3/","title":"\uff08DETR\uff09End-to-End Object Detection with Transformer","text":"<p>\u8bba\u6587</p> <p>\u6e90\u7801</p> <p></p> <p>\u9898\u76ee\uff1a\u7aef\u5230\u7aef\u7684\u3001\u57fa\u4e8eTransformer\u7684\u76ee\u6807\u68c0\u6d4b</p> <p>\u4f5c\u8005\uff1aFacebook</p> <p>\u65f6\u95f4\uff1a2020\u5e745\u670828\u65e5</p> <p>\u671f\u520a\uff1a</p>"},{"location":"literature/ObjectDetection/3/#_1","title":"\u2b50\ufe0f \u6458\u8981","text":"<p>We present a new method that views object detection as a direct set prediction problem. </p> <p>\u5c06\u76ee\u6807\u68c0\u6d4b\u770b\u505a\u662f\u4e00\u4e2a\u9884\u6d4b\u95ee\u9898</p> <p>Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. </p> <p>\u7b80\u5316\u4e86\u68c0\u6d4b\u8fc7\u7a0b\u3001\u53bb\u6389\u4e86\u5f88\u591a\u4eba\u5de5\u6b65\u9aa4\uff1aNMS\u3001\u751f\u6210\u951a\u6846</p> <p>The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. </p> <p>\u6211\u4eec\u7684\u6a21\u578b\u540d\u5b57\uff1aDETR\u3001\u5168\u5c40\u635f\u5931\u3001\u5308\u7259\u5229\u4e8c\u5206\u5339\u914d\u7b97\u6cd5\u3001\u57fa\u4e8eTransformer Encoder decoder\u7684\u68c0\u6d4b\u7ed3\u6784</p> <p>Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel.</p> <p>\u7ed9\u5b9alearned object queries\uff1f\uff0cDETR\u63a8\u7406 \u5bf9\u8c61\u548c\u5168\u5c40\u56fe\u50cf\u7684\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u5e76\u884c\u7684\u8f93\u51fa\u6700\u7ec8\u7684\u9884\u6d4b\u96c6\u5408</p> <p>The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. </p> <p>DETR\u6982\u5ff5\u4e0a\u7b80\u5355\u3001\u4e0d\u9700\u8981\u4e13\u95e8\u7684\u5e93\u3001\u8ddf\u5176\u4ed6\u68c0\u6d4b\u5668\u4e0d\u592a\u4e00\u6837</p> <p>\uff08\u7ed3\u679c\uff09DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. </p> <p>DETR\u7684\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u95f4\u6027\u80fd \u90fd\u53ef\u4ee5\u5ab2\u7f8e \u5f88\u6210\u719f\u7684\u3001\u4f18\u5316\u5f88\u597d\u7684 faster RCNN</p> <p>\u68c0\u6d4b\u6570\u636e\u96c6\u7684baseline\uff1aCOCO</p> <p>Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines.</p> <p>\u4efb\u52a1\u7684\u6cdb\u5316\u6027\u80fd\uff0cDETR\u53ef\u4ee5\u63a8\u5e7f\u5230\u5168\u666f\u5206\u5272\u4efb\u52a1</p>"},{"location":"literature/ObjectDetection/3/#introduction","title":"\u2b50\ufe0f Introduction","text":""},{"location":"literature/ObjectDetection/3/#modern-detectors","title":"\u7b2c\u4e00\u6bb5 Modern detectors","text":"<p>The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest.</p> <p>\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u5b9a\u4e49</p> <p>Modern detectors address this set prediction task in an indirect way, by defining surrogate regression and classification problems on a large set of proposals [37,5], anchors [23], or window centers [53,46]. </p> <p>\u73b0\u5728\u7684\u68c0\u6d4b\u65b9\u6cd5\uff1a\u95f4\u63a5\u7684\u65b9\u6cd5\u8fdb\u884c\u68c0\u6d4b</p> <ul> <li> Their performances are significantly influenced by postprocessing steps to collapse near-duplicate predictions, by the design of the anchor sets and by the heuristics that assign target boxes to anchors [52]. </li> </ul> <p>\u73b0\u5728\u7684\u68c0\u6d4b\u65b9\u6cd5\uff1a\u8868\u73b0\u53d7\u5230\u540e\u5904\u7406\u6b65\u9aa4\u7684\u663e\u8457\u5f71\u54cd\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u5305\u62ec\u5408\u5e76\u8fd1\u91cd\u590d\u7684\u9884\u6d4b\u3001\u951a\u70b9\u96c6\u7684\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u5c06\u76ee\u6807\u6846\u5206\u914d\u7ed9\u951a\u70b9\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5[52]\u3002</p> <p>To simplify these pipelines, we propose a direct set prediction approach to bypass the surrogate tasks. </p> <p>\u4e3a\u4e86\u7b80\u5316\u8fd9\u4e9b\u6d41\u7a0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u7684\u96c6\u5408\u9884\u6d4b\u65b9\u6cd5</p> <p>This end-to-end philosophy has led to significant advances in complex structured prediction tasks such as machine translation or speech recognition, but not yet in object detection: previous attempts [43,16,4,39] either add other forms of prior knowledge, or have not proven to be competitive with strong baselines on challenging benchmarks.</p> <p>\u8fd9\u79cd\u7aef\u5230\u7aef\u7684\u7406\u5ff5\u5728\u590d\u6742\u7684\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u6bd4\u5982\u673a\u5668\u7ffb\u8bd1\u6216\u8bed\u97f3\u8bc6\u522b\uff0c\u4f46\u5728\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5c1a\u672a\u5b9e\u73b0\uff1a\u4e4b\u524d\u7684\u5c1d\u8bd5[43,16,4,39]\u8981\u4e48\u589e\u52a0\u4e86\u5176\u4ed6\u5f62\u5f0f\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8981\u4e48\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u672a\u80fd\u8bc1\u660e\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\u76f8\u7ade\u4e89\u3002</p> <p>This paper aims to bridge this gap.</p>"},{"location":"literature/ObjectDetection/3/#1","title":"\u56fe1","text":"<ul> <li>DETR\u76f4\u63a5\u3001\u5e76\u884c\u7684\u9884\u6d4b\u6700\u7ec8\u7684\u68c0\u6d4b\u96c6\u5408</li> <li>DETR\uff1aCNN\u548cTransformer\u67b6\u6784\u7ec4\u5408</li> <li>\u8bad\u7ec3\u9636\u6bb5\uff1a\u4e8c\u90e8\u56fe\u5339\u914d\u5c06\u9884\u6d4b\u548c\u771f\u5b9e\u6846\u5173\u8054\u8d77\u6765\u3001\u6ca1\u6709\u5339\u914d\u7684\u9884\u6d4b\u80fd\u591f\u4ea7\u751f\u4e00\u4e2a\u6ca1\u6709\u5bf9\u8c61\u7684\u7c7b\u522b\u9884\u6d4b</li> </ul>"},{"location":"literature/ObjectDetection/3/#based-on-transformers-self-attention-mechanisms","title":"\u7b2c\u4e8c\u6bb5 based on transformers &amp; self-attention mechanisms","text":"<p>We streamline the training pipeline by viewing object detection as a direct set prediction problem. </p> <p>\u5c06\u76ee\u6807\u68c0\u6d4b\u89c6\u4e3a\u4e00\u4e2a\u76f4\u63a5\u7684\u96c6\u5408\u9884\u6d4b\u95ee\u9898\u6765\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b</p> <p>We adopt an encoder-decoder architecture based on transformers [47], a popular architecture for sequence prediction. </p> <p>\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u7ed3\u6784</p> <p>The self-attention mechanisms of transformers, which explicitly model all pairwise interactions between elements in a sequence, make these architectures particularly suitable for specific constraints of set prediction such as removing duplicate predictions.</p> <p>Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff1a\u660e\u786e\u7ed9\u51fa\u5e8f\u5217\u4e2d\u6240\u6709\u5143\u7d20\u7684\u6210\u5bf9\u4ea4\u4e92\uff0c\u7279\u522b\u9002\u5408\u4e8e\u96c6\u5408\u9884\u6d4b\u7684\u7279\u5b9a\u7ea6\u675f\uff0c\u4f8b\u5982\u53bb\u9664\u91cd\u590d\u9884\u6d4b</p> <p>\uff08\u5c31\u662f\u8bf4\u660eTransformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f88\u9002\u5408\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\uff0c\u7ed9\u51fa\u6210\u5bf9\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff09</p>"},{"location":"literature/ObjectDetection/3/#trained-end-to-end","title":"\u7b2c\u4e09\u6bb5 trained end-to-end","text":"<p>Our DEtection TRansformer (DETR, see Figure 1) predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects. </p> <ul> <li>\u4e00\u6b21\u6027\u3001\u7aef\u5230\u6bb5\u9884\u6d4b</li> <li>\u901a\u8fc7\u4e00\u4e2a\u96c6\u5408\u635f\u5931\u51fd\u6570\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u8be5\u51fd\u6570\u5728\u9884\u6d4b\u5bf9\u8c61\u548c\u771f\u5b9e\u5bf9\u8c61\u4e4b\u95f4\u6267\u884c\u4e8c\u5206\u56fe\u5339\u914d</li> </ul> <p>DETR simplifies the detection pipeline by dropping multiple hand-designed components that encode prior knowledge, like spatial anchors or non-maximal suppression. </p> <p>DETR\u901a\u8fc7\u820d\u5f03\u591a\u4e2a\u7f16\u7801\u5148\u9a8c\u77e5\u8bc6\u7684\u624b\u5de5\u8bbe\u8ba1\u7ec4\u4ef6\uff0c\u5982\u7a7a\u95f4\u951a\u70b9\u6216\u975e\u6781\u5927\u503c\u6291\u5236\uff0c\u7b80\u5316\u4e86\u68c0\u6d4b\u6d41\u7a0b\u3002</p> <p>Unlike most existing detection methods, DETR doesn\u2019t require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes.1</p> <p>\u4e0e\u5927\u591a\u6570\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u540c\uff0cDETR\u4e0d\u9700\u8981\u4efb\u4f55\u5b9a\u5236\u5c42\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728\u5305\u542b\u6807\u51c6CNN\u548c\u53d8\u6362\u5668\u7c7b\u7684\u4efb\u4f55\u6846\u67b6\u4e2d\u8f7b\u677e\u590d\u73b0\u3002</p>"},{"location":"literature/ObjectDetection/3/#bipartite-matching-loss","title":"\u7b2c\u56db\u6bb5 bipartite matching loss","text":"<p>Compared to most previous work on direct set prediction, the main features of DETR are the conjunction of the bipartite matching loss and transformers with (non-autoregressive) parallel decoding [29,12,10,8]. </p> <p>DETR\u7684\u7279\u70b9\uff1a\u7ed3\u5408\u4e8c\u5206\u56fe\u5339\u914d\u635f\u5931\u548cTransformer\u7684\u5e76\u884c\u89e3\u7801\u635f\u5931</p> <p>\ud83d\udce2 transformers with (non-autoregressive) \u975e\u81ea\u56de\u5f52</p> <p>In contrast, previous work focused on autoregressive decoding with RNNs [43,41,30,36,42]. </p> <p>\u5148\u524d\u7684\u635f\u5931\u662f\uff1aRNN\u89e3\u7801\u5668\u7684\u81ea\u56de\u5f52\u635f\u5931</p> <p>Our matching loss function uniquely assigns a prediction to a ground truth object, and is invariant to a permutation of predicted objects, so we can emit them in parallel.</p> <p>\u5339\u914d\u635f\u5931\u51fd\u6570\u552f\u4e00\u5730\u5c06\u9884\u6d4b\u5206\u914d\u7ed9\u771f\u5b9e\u5bf9\u8c61\uff0c\u5e76\u4e14\u5bf9\u9884\u6d4b\u5bf9\u8c61\u7684\u6392\u5217\u662f\u4e0d\u53d8\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5e76\u884c\u5730\u8f93\u51fa\u5b83\u4eec\u3002</p>"},{"location":"literature/ObjectDetection/3/#baseline","title":"\u7b2c\u4e94\u6bb5 baseline","text":"<p>We evaluate DETR on one of the most popular object detection datasets, COCO [24], against a very competitive Faster R-CNN baseline [37]. </p> <p>\u6570\u636e\u96c6\uff1aobject detection datasets, COCO</p> <p>\u6a21\u578b\uff1aFaster R-CNN baseline</p> <p>More precisely, DETR demonstrates significantly better performance on large objects, a result likely enabled by the non-local computations of the transformer. </p> <p>DETR\u5728\u5927\u76ee\u6807\u7684\u68c0\u6d4b\u6027\u80fd\u6bd4\u8f83\u597d</p> <p>\u53ef\u80fd\u7684\u539f\u56e0\uff1aTransformer\u7684\u975e\u5c40\u90e8\u8ba1\u7b97\uff08\u662f\u7684\uff0cTransformer\u662f\u5bf9\u6240\u6709\u8bcd\uff0c\u4e24\u4e24\u4e4b\u95f4\u4efb\u610f\u53ef\u80fd\u5f97\u5173\u7cfb\u90fd\u8fdb\u884c\u5efa\u6a21\uff0c\u662f\u4e00\u79cd\u5168\u5c40\u5efa\u6a21\u65b9\u6cd5\uff0c\u95ee\u9898\u5c31\u662f \u4f1a\u5bf9\u4e0d\u90a3\u4e48\u91cd\u8981\u7684\u8bcd \u4e5f\u8fdb\u884c\u4e86\u5173\u6ce8\uff09</p> <p>It obtains, however, lower performances on small objects. We expect that future work will improve this aspect in the same way the development of FPN [22] did for Faster R-CNN.</p> <p>\u4f46\u662f\uff0c\u5c0f\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u5c31\u4e0d\u90a3\u4e48\u597d</p>"},{"location":"literature/ObjectDetection/3/#differ-from-standard-object-detectors","title":"\u7b2c\u516d\u6bb5 differ from standard object detectors","text":"<p>The new model requires extra-long training schedule and benefits from auxiliary decoding losses in the transformer. </p> <p>\u65b0\u6a21\u578b\uff08DETR\uff09\u9700\u8981\u4e00\u4e2a\u8d85\u957f\u7684\u8bad\u7ec3\u65f6\u95f4\u8868\uff0c\u5e76\u4e14\u4ecetransformer\u4e2d\u7684\u8f85\u52a9\u89e3\u7801\u635f\u5931\u4e2d\u53d7\u76ca</p>"},{"location":"literature/ObjectDetection/3/#extend-to-more-complex-tasks","title":"\u7b2c\u4e03\u6bb5 extend to more complex tasks","text":"<p>The design ethos of DETR easily extend to more complex tasks. </p> <p>\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u4efb\u52a1</p> <p>In our experiments, we show that a simple segmentation head trained on top of a pretrained DETR outperfoms competitive baselines on Panoptic Segmentation [19], a challenging pixel-level recognition task that has recently gained popularity.</p> <p>\uff08\u5168\u666f\u5206\u5272\uff09\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u5206\u5272\u5934\uff0c\u5b83\u5728\u9884\u8bad\u7ec3\u7684DETR\u4e4b\u4e0a\u8bad\u7ec3\uff0c\u5176\u6027\u80fd\u5728Panoptic\u5206\u5272[19]\u4e0a\u8d85\u8d8a\u4e86\u7ade\u4e89\u57fa\u7ebf\uff0cPanoptic\u5206\u5272\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u50cf\u7d20\u7ea7\u8bc6\u522b\u4efb\u52a1\uff0c\u6700\u8fd1\u53d8\u5f97\u6d41\u884c\u8d77\u6765\u3002</p>"},{"location":"literature/ObjectDetection/3/#related-work","title":"\u2b50\ufe0f Related work","text":"<p>Our work build on prior work in several domains: bipartite matching losses for set prediction, encoder-decoder architectures based on the transformer, parallel decoding, and object detection methods.</p> <p>DETR\u6d89\u53ca\u5230\u7684\u76f8\u5173\u9886\u57df\uff1a</p> <ul> <li>bipartite matching losses for set prediction \u5bf9\u4e8e\u96c6\u5408\u9884\u6d4b\u7684\u4e8c\u5206\u56fe\u5339\u914d\u635f\u5931</li> <li>encoder-decoder architectures based on the transformer </li> <li>parallel decoding \u5e76\u884c\u89e3\u7801</li> <li>object detection methods.</li> </ul>"},{"location":"literature/ObjectDetection/3/#1set-prediction-rightarrow","title":"\u76f8\u5173\u9886\u57df\u5de5\u4f5c1\uff1aSet Prediction $\\rightarrow  $ \u5308\u7259\u5229\u635f\u5931","text":"<p>There is no canonical\uff08\u6807\u51c6\u7684\uff09 deep learning model to directly predict sets. </p> <p>\u5bf9\u4e8e\u96c6\u5408\u9884\u6d4b\uff0c\u6ca1\u6709\u6807\u51c6\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b</p> <p>The basic set prediction task is multilabel classification for which the baseline approach, one-vs-rest, does not apply to problems such as detection where there is an underlying structure between elements (i.e., near-identical boxes). </p> <p>\u96c6\u5408\u9884\u6d4b\u4efb\u52a1\u662f\u4e00\u79cd\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\uff0c\u4e00\u5bf9\u591a\uff0c\u4e0d\u9002\u5408\u68c0\u6d4b\u4efb\u52a1</p> <p>\u68c0\u6d4b\u4efb\u52a1\u7684\u7279\u70b9\uff1a\u5143\u7d20\u4e4b\u95f4\u6709\u7ed3\u6784\u5173\u7cfb\uff0c\u4f8b\u5982\uff0c\u68c0\u6d4b\u4e2d\u7684\u8fd1\u76f8\u540c\u6846</p> <p>The first difficulty in these tasks is to avoid near-duplicates.\uff08\u907f\u514d\u8fd1\u4f3c\u91cd\u590d\uff09 </p> <p>\u68c0\u6d4b\u4efb\u52a1\u7684\u7b2c\u4e00\u4e2a\u96be\u70b9\uff1a\u53bb\u91cd</p> <p>Most current detectors use postprocessings such as non-maximal suppression to address this issue, but direct set prediction are postprocessing-free. </p> <p>\u68c0\u6d4b\u4efb\u52a1\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff1aNMS</p> <p>\u4f46\u662f\uff0c\u96c6\u5408\u9884\u6d4b\u662f \u4e0d\u9700\u8981\u8fdb\u884c\u540e\u5904\u7406\u7684</p> <p>They need global inference schemes that model interactions between all predicted elements to avoid redundancy. </p> <p>\u96c6\u5408\u9884\u6d4b\u9700\u8981 \u5168\u5c40\u63a8\u7406</p> <p>\uff08\u8bf4\u7684\u662fIOU\u3001NMS\uff09\u9700\u8981\u5168\u5c40\u63a8\u7406\u65b9\u6848\uff0c\u8fd9\u4e9b\u65b9\u6848\u6a21\u62df\u6240\u6709\u9884\u6d4b\u5143\u7d20\u4e4b\u95f4\u7684\u4ea4\u4e92\u4ee5\u907f\u514d\u5197\u4f59</p> <p>For constant-size set prediction, dense fully connected networks [9] are sufficient but costly. A general approach is to use auto-regressive sequence models such as recurrent neural networks [48].</p> <p>\u5bf9\u4e8e\u56fa\u5b9a\u5927\u5c0f\u7684\u96c6\u5408\u9884\u6d4b\u95ee\u9898\uff1a\u5168\u8fde\u63a5\u7f51\uff0c\u7f3a\u70b9\uff1a\u6210\u672c\u9ad8</p> <p>\u89e3\u51b3\uff1a\u81ea\u56de\u5f52\u6a21\u578b RNN</p> <p>In all cases, the loss function should be invariant by a permutation of the predictions. </p> <p>\u96c6\u5408\u9884\u6d4b\u7684\u635f\u5931\u51fd\u6570 \u5e94\u8be5\u4e0e\u9884\u6d4b\u7684\u987a\u5e8f \u65e0\u5173</p> <p>The usual solution is to design a loss based on the Hungarian algorithm [20], to find a bipartite matching between ground-truth and prediction. </p> <p>\u5982\u4f55\u5b9e\u73b0 \u635f\u5931\u51fd\u6570\u4e0e\u987a\u5e8f\u65e0\u5173\uff1f \u5308\u7259\u5229\u7b97\u6cd5</p> <p>\u5bfb\u627egt\u548c\u9884\u6d4b\u7684\u4e8c\u5206\u5339\u914d</p> <p>This enforces permutation-invariance, and guarantees that each target element has a unique match. </p> <p>\u7279\u70b9\uff1a</p> <ul> <li>\u987a\u5e8f\u6392\u5217\u4e0d\u53d8\u5f62</li> <li>\u552f\u4e00\u5339\u914d</li> </ul> <p>We follow the bipartite matching loss approach. In contrast to most prior work however, we step away from autoregressive models and use transformers with parallel decoding.</p> <ul> <li>DETR \u4f7f\u7528 \u4e8c\u5206\u5339\u914d\u635f\u5931\u65b9\u6cd5</li> <li>\u4f7f\u7528Transformer\u7684\u5e76\u884c\u89e3\u7801\u6a21\u578b</li> <li>\u6ca1\u6709\u7528\u81ea\u56de\u5f52\u6a21\u578b</li> </ul>"},{"location":"literature/ObjectDetection/3/#2transformers-and-parallel-decoding","title":"\u76f8\u5173\u5de5\u4f5c2\uff1aTransformers and Parallel Decoding","text":""},{"location":"literature/ObjectDetection/3/#transformer","title":"\u7b2c\u4e00\u6bb5 \u4ecb\u7ecdTransformer\u662f\u4ec0\u4e48\uff0c\u4ee5\u53ca\u4f18\u70b9","text":"<p>Transformers  were introduced by Vaswani et al . [47] as a new attention-based building block for machine translation. </p> <p>Attention mechanisms  [2] are neural network layers that aggregate information from the entire input sequence. </p> <p>Transformers introduced self-attention layers, which, similarly to Non-Local Neural Networks [49], scan through each element of a sequence and update it by aggregating information from the whole sequence. </p> <p>\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u7c7b\u4f3c\u975e\u5c40\u90e8\u795e\u7ecf\u7f51\u7edc</p> <p>\u626b\u63cf\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5143\u7d20\uff0c\u805a\u5408\u6574\u4e2a\u5e8f\u5217\u7684\u4fe1\u606f\u5e76\u8fdb\u884c\u66f4\u65b0</p> <p>One of the main advantages  of attention-based models is their global computations and perfect memory, which makes them more suitable than RNNs on long sequences. Transformers are now replacing RNNs in many problems in natural language processing, speech processing and computer vision [8,27,45,34,31].</p>"},{"location":"literature/ObjectDetection/3/#tr","title":"\u7b2c\u4e8c\u6bb5  tr\u7684\u7f3a\u70b9 \u4ee5\u53ca \u6211\u4eec","text":"<p>Transformers were first used in auto-regressive models, following early sequence-to-sequence models [44], generating output tokens one by one. </p> <p>Transforme\u7684\u5e94\u7528\u9886\u57df\uff1a\u81ea\u56de\u5f52\u6a21\u578b\uff0cseq2seq\u6a21\u578b\uff0cone by one\u8f93\u51fa</p> <p>However, the prohibitive inference cost (proportional to output length, and hard to batch) lead to the development of parallel sequence generation, in the domains of audio [29], machine translation [12,10], word representation learning [8], and more recently speech recognition [6]. </p> <p>Transformer\u7684\u7f3a\u70b9\uff1a\u5e73\u65b9\u590d\u6742\u5ea6\uff0c\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c</p> <p>We also combine transformers and parallel decoding for their suitable trade-off between computational cost and the ability to perform the global computations required for set prediction.</p> <p>\u6211\u4eec\u7684\u505a\u6cd5\uff1aTransformer+\u5e76\u884c\u89e3\u7801</p> <p>\u6743\u8861 \u8ba1\u7b97\u6210\u672c \u548c \u5168\u5c40\u8ba1\u7b97\u80fd\u529b</p>"},{"location":"literature/ObjectDetection/3/#3-object-detection","title":"\u76f8\u5173\u5de5\u4f5c3 Object detection","text":""},{"location":"literature/ObjectDetection/3/#_2","title":"\u7b2c\u4e00\u6bb5 \u662f\u4ec0\u4e48","text":"<p>Most  modern object detection  methods make predictions relative to some initial guesses. </p> <p>\u73b0\u5728\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5 \u9700\u8981\u7ed9\u51fa \u521d\u59cb\u731c\u6d4b</p> <p>Two-stage detectors  [37,5] predict boxes w.r.t. proposals, whereas  single-stage methods  make predictions w.r.t. anchors [23] or a grid of possible object centers [53,46]. </p> <p>\u4e24\u9636\u6bb5\u68c0\u6d4b\u65b9\u6cd5 &amp; \u5355\u9636\u6bb5\u68c0\u6d4b\u65b9\u6cd5</p> <p>\u4e24\u9636\u6bb5\u9884\u6d4b \u9884\u6d4b\u6846</p> <p>\u5355\u9636\u6bb5\u9884\u6d4b \u9884\u6d4b\u951a\u70b9 or \u76ee\u6807\u4e2d\u5fc3\u7684\u7f51\u683c</p> <p>Recent work [52] demonstrate that the final performance of these systems heavily depends on the exact way these initial guesses are set.</p> <p>\u7f3a\u70b9\uff1a\u4f9d\u8d56\u521d\u503c</p> <p>In our model we are able to remove this hand-crafted process and streamline the detection process by directly predicting the set of detections with absolute box prediction w.r.t. the input image rather than an anchor.</p> <p>\u6211\u4eec\uff1a</p> <p>how\uff1f</p> <ul> <li>\u79fb\u9664\u624b\u5de5\u8fc7\u7a0b</li> <li>\u7b80\u5316\u68c0\u6d4b\u8fc7\u7a0b</li> </ul> <p>what\uff1f</p> <ul> <li>\uff08\u76f4\u63a5\u8bf4\u662f\u4ec0\u4e48\uff09\u76f4\u63a5\u9884\u6d4b\u68c0\u6d4b\u96c6\uff1a\u76f4\u63a5\u9884\u6d4b\u6846</li> <li>\uff08\u7528\u76f8\u5bf9\u5173\u7cfb\u8bf4\u662f\u4ec0\u4e48\uff09\u662f\u56fe\u50cf\u800c\u4e0d\u662f\u951a\u70b9</li> </ul>"},{"location":"literature/ObjectDetection/3/#set-based-loss","title":"\u7b2c\u4e00\u90e8\u5206 Set-based loss.\u57fa\u4e8e\u96c6\u5408\u7684\u635f\u5931","text":""},{"location":"literature/ObjectDetection/3/#_3","title":"\u7b2c\u4e8c\u6bb5","text":"<p>Several object detectors [9,25,35] used the bipartite matching loss. However, in these early deep learning models, the relation between different prediction was modeled with convolutional or fully-connected layers only and a hand-designed NMS post-processing can improve their performance. More recent detectors [37,23,53] use non-unique assignment rules between ground truth and predictions together with an NMS.</p>"},{"location":"literature/ObjectDetection/3/#_4","title":"\u7b2c\u4e09\u6bb5","text":"<p>Learnable NMS methods [16,4] and relation networks [17] explicitly model relations between different predictions with attention. Using direct set losses, they do not require any post-processing steps. However, these methods employ additional hand-crafted context features like proposal box coordinates to model relations between detections efficiently, while we look for solutions that reduce the prior knowledge encoded in the model.</p>"},{"location":"literature/ObjectDetection/3/#recurrent-detectors","title":"\u7b2c\u4e8c\u90e8\u5206 Recurrent detectors.  \u68c0\u6d4b\u65b9\u6cd5","text":""},{"location":"literature/ObjectDetection/3/#_5","title":"\u7b2c\u56db\u6bb5","text":"<p>Closest to our approach are end-to-end set predictions for object detection [43] and instance segmentation [41,30,36,42]. Similarly to us, they use bipartite-matching losses with encoder-decoder architectures based on CNN activations to directly produce a set of bounding boxes. These approaches, however, were only evaluated on small datasets and not against modern baselines. In particular, they are based on autoregressive models (more precisely RNNs), so they do not leverage the recent transformers with parallel decoding.</p>"},{"location":"literature/ObjectDetection/3/#_6","title":"\u2b50\ufe0f \u7ed3\u8bba","text":""},{"location":"literature/ObjectDetection/3/#_7","title":"\u7b2c\u4e00\u6bb5","text":"<p>We presented DETR, a new design for object detection systems based on transformers and bipartite matching loss for direct set prediction. The approach achieves comparable results to an optimized Faster R-CNN baseline on the challenging COCO dataset. DETR is straightforward to implement and has a flexible architecture that is easily extensible to panoptic segmentation, with competitive results. In addition, it achieves significantly better performance on large objects than Faster R-CNN, likely thanks to the processing of global information performed by the self-attention. </p>"},{"location":"literature/ObjectDetection/3/#_8","title":"\u7b2c\u4e8c\u6bb5","text":"<p>This new design for detectors also comes with new challenges, in particular regarding training, optimization and performances on small objects. Current detectors required several years of improvements to cope with similar issues, and we expect future work to successfully address them for DETR.</p>"},{"location":"literature/ObjectDetection/3/#the-detr-model","title":"\u2b50\ufe0f The DETR model","text":"<p>Two ingredients are essential for direct set predictions in detection: (1) a set prediction loss that forces unique matching between predicted and ground truth boxes; (2) an architecture that predicts (in a single pass) a set of objects and models their relation. We describe our architecture in detail in Figure 2.</p> <p>Object detection set prediction loss</p>"},{"location":"literature/Reproduction/","title":"Index","text":"<p>null</p>"},{"location":"logs/","title":"\u6742","text":"<p>\u542c\u8bf4\u6bcf\u4e2ap\u4eba\u5fc5\u5907\u7684\u6742\uff0c\u7b11\uff09</p> <p></p> <p>\ud83e\udd63weibo</p> <p>\ud83d\udcddcsdn</p> <ul> <li> <p>2024\u5e7412\u670812\u65e5 \u5f00\u9898\u62a5\u544a</p> </li> <li> <p>2024\u5e7412\u670813\u65e5 \u5f00\u9898\u62a5\u544a</p> </li> <li> <p>2024\u5e7412\u670816\u65e5 \u5f00\u9898\u7b54\u8fa9</p> </li> </ul>"},{"location":"logs/diary/","title":"\u4e50\u89c2 &amp; \u575a\u5f3a","text":"<p>\ud83d\udc95\ud83c\udf08\ud83d\udc3e \u4f46\u884c\u597d\u4e8b\uff0c\u4e0d\u95ee\u524d\u7a0b</p> <p>\ud83e\ude90\ud83d\udcab \u6361\u5783\u573e\u7684\u4eba\u4e0d\u4f1a\u5ac9\u5992\u5f00\u5b9d\u9a6c\u8f66\u7684\u4eba\uff0c\u4f46\u4f1a\u5ac9\u5992\u5783\u573e\u6bd4\u5b83\u6361\u5f97\u591a\u7684\u4eba</p> <p>\ud83c\udf3a\ud83d\udc0b\u2728 \u9762\u671d\u5927\u6d77\uff0c\u6625\u6696\u82b1\u5f00</p> <p>\uff08241219\uff09\u4e00\u6574\u4e2a\u5927\u6446\u70c2\uff0c\u4f60\u52aa\u529b\u5427\uff0c\u6211\u5f00\u5fc3\u5c31\u597d\u4e86\u3002\u751f\u547d\u4e4b\u6811\u5373\u5c06\u67af\u840e\uff0c\u7075\u9b42\u9a6c\u4e0a\u7a92\u606f\uff0c\u6211\u88c5\u4e0d\u53bb\u4e0b\u4e86</p> <ul> <li>241115 \u5c0f\u7ea2\u4e66\u4e0a\u53d1\u4e86\u4e2a\u8d34\uff0c\u7fa4\u8d77\u5632\u4e4b\uff1a\u522b\u9a82\u4e86\u522b\u9a82\u4e86\uff0c\u6211\u9519\u4e86</li> <li>241117 \u6765\u4e86</li> <li>241118 \u6765\u54af</li> <li> \u6587\u732e\u9605\u8bfb\u7b14\u8bb0</li> <li> \u597d\u6d88\u606f\uff1a\u6587\u7ae0\u65f6\u95f4\u6233\u6539\u5bf9\u4e86</li> <li>241119 \u5f00\u5de5\uff0c\u6162\u6162\u6765\u4e5f\u633a\u597d\u7684\uff0c\u662f\u7684</li> <li>241125 \u5e72\u6d3b</li> <li>241126 \u6765\u4e86</li> <li>241127 \u6765\u4e86</li> <li>241128 \u661f\u671f\u56db \u6765\u4e86 \u6674\u5929</li> <li>241129 \u661f\u671f\u4e94 \u6765\u4e86 \u9634\u5929</li> </ul> <p>\u2b50\ufe0f Week 1</p> <p>2024\u5e7412\u67081\u65e5 \u661f\u671f\u65e5 \u4f11</p> <p>2024\u5e7412\u67082\u65e5 \u661f\u671f\u4e00 \u6765\u4e86 \u6674\u5929</p> <ul> <li> \u67e5\uff1a\u5f52\u4e00\u5316</li> <li> \u6539\uff1a\u5f52\u4e00\u5316\u3001\u6587\u732e\u9605\u8bfb COUNTGD\u3001COUNTR</li> <li> \u589e\uff1aGAN</li> </ul> <p>8h9min</p> <p>2024\u5e7412\u67083\u65e5 \u661f\u671f\u4e8c \u6765\u4e86 \u6674\u5929</p> <ul> <li> \u6539\uff1aGAN</li> </ul> <p>6h34min</p> <p>2024\u5e7412\u67084\u65e5 \u661f\u671f\u4e09 \u6765\u4e86 \u6674\u5929</p> <ul> <li> \u6539\uff1aGAN\uff08DONE\uff09</li> <li> \u6539\uff1aViT\uff08DONE\uff09</li> <li> \u589e\uff1aBert</li> <li> \u589e\uff1avision transformer\u4ee3\u7801\uff08DONE\uff09</li> <li> \u589e\uff1aclip</li> </ul> <p>10h45min</p> <p>2024\u5e7412\u67085\u65e5 \u661f\u671f\u56db \u6674\u5929 \u6765\u4e86</p> <ul> <li> \u6539\uff1aWeightNorm</li> </ul> <p>2h21min</p> <p>2024\u5e7412\u67086\u65e5 \u661f\u671f\u4e94 \u6559\u8d44\u9762\u8bd5</p> <p>2024\u5e7412\u67087\u65e5 \u661f\u671f\u516d \u6559\u8d44\u9762\u8bd5</p> <p>2024\u5e7412\u67088\u65e5 \u661f\u671f\u65e5 \u9634\u5929 \u6765\u4e86</p> <ul> <li> \u6539\uff1aWeightNorm</li> <li> \u589e\uff1aGAN\u7684\u53d8\u4f53\uff1a\u6700\u5c0f\u4e8c\u4e58GAN</li> </ul> <p>3h45min</p> <p>\u2b50Week2</p> <p>2024\u5e7412\u67089\u65e5 \u661f\u671f\u4e00 \u9634\u5929 \u6765\u4e86</p> <ul> <li> \u6539\uff1aGAN\u53d8\u4f53\uff1acGAN\uff08DONE\uff09</li> <li> \u589e\uff1apytorch\u8bfb\u53d6csv\u3001excel\u6587\u4ef6\u8f6c\u6362\u6210tensor</li> <li> \u589e\uff1aResNet\u9879\u76ee\u5b9e\u6218</li> <li> \u6539\uff1aDiffusion models</li> </ul> <p>8h59min</p> <p>2024\u5e7412\u670810\u65e5 \u661f\u671f\u4e8c \u4e0b\u96ea \u6765\u4e86</p> <ul> <li> \u67e5\uff1aGAN\u53d8\u4f53</li> <li> \u6539\uff1aDDPM</li> <li> \u589e\uff1aKL\u6563\u5ea6\uff08DONE\uff09</li> </ul> <p>6h44min</p> <p>2024\u5e7412\u670811\u65e5 \u661f\u671f\u4e09 \u9634\u5929 \u6765\u4e86 \u51b2\uff01</p> <ul> <li> \u6539\uff1aDDPM\u63a8\u5bfc</li> </ul> <p>2024\u5e7412\u670812\u65e5 \u661f\u671f\u56db \u9634\u5929 \u6765\u4e86 </p> <ul> <li> \u589e\uff1alatex\u6349\u866b</li> </ul> <p>2024\u5e7412\u670813\u65e5 \u661f\u671f\u4e94 \u6674\u5929 \u6765\u4e86</p> <ul> <li> \u6539\uff1alatex\u6349\u866b</li> </ul> <p>\u2b50Week3</p> <p>2024\u5e7412\u670819\u65e5 \u661f\u671f\u56db \u9634\u5929 \u6765\u4e86</p> <ul> <li> \u6539\uff1a\u6269\u6563\u6a21\u578b\u63a8\u5bfc</li> </ul> <p>2024\u5e7412\u670820\u65e5 \u661f\u671f\u4e94 \u65e9\u4e0a\u597d</p> <ul> <li> \u6539\uff1aVAE\u63a8\u5bfc</li> <li> \u589e\uff1aRNN</li> </ul> <p>2024 \u5e74 12 \u6708 21 \u65e5 \u661f\u671f\u516d \u4e0a\u5348\u597d</p> <ul> <li> \u589e\uff1a\u5c0f\u8bb2\u5802\uff08ViT\u3001CLIP\uff09</li> <li> \u67e5\uff1aRNN</li> </ul> <p>\u8fd1\u671f\u9ed8\u5ff5\uff1a\u300c\u6c38\u8fdc\u6446\u8131\u53d7\u5bb3\u8005\u53d9\u4e8b\u300d</p> <p>\u505c\u6b62\u5411\u4e16\u754c\u63cf\u8ff0\u6211\u7684\u76d1\u72f1</p> <p>\u66f4\u91cd\u8981\u7684\u662f\uff1a\u505c\u6b62\u5411\u81ea\u5df1\u63cf\u8ff0</p> <p>\u66f4\u91cd\u8981\u7684\u662f\uff1a\u73b0\u5728\u4ece\u76d1\u72f1\u91cc\u7ad9\u8d77\u8eab\uff0c\u8d70\u51fa\u53bb</p> <p>\u56e0\u4e3a\u76d1\u72f1\u6ca1\u6709\u4e0a\u9501\uff0c\u4e5f\u6ca1\u6709\u95e8</p> <p>\u6700\u540e\uff0c\u6839\u672c\u6ca1\u6709\u76d1\u72f1</p> <p>\u6c38\u8fdc\u4e0d\u8981\u8bd5\u56fe\u5c06\u81ea\u5df1\u6253\u9020\u6210\u4e00\u4e2a\u53d7\u5bb3\u8005\u6765\u83b7\u5f97\u522b\u4eba\u7684\u7406\u89e3\u548c\u7231\uff0c\u4e0d\u8981\u89c9\u5f97\u4e16\u754c\u5bf9\u4f60\u4e0d\u516c\uff0c\u4e0d\u8981\u89c9\u5f97\u8c01\u5bf9\u4e0d\u8d77\u4f60</p> <p>\u628a\u81ea\u5df1\u5f53\u6210\u53d7\u5bb3\u8005\uff0c\u5c31\u6c38\u8fdc\u8981\u627e\u51f6\u624b\uff1b</p> <p>\u628a\u81ea\u5df1\u5f53\u6210\u524d\u8fdb\u8005\uff0c\u5c31\u6c38\u8fdc\u5728\u627e\u52a9\u624b</p> <p>2024 \u5e74 12 \u6708 22 \u65e5 \u661f\u671f\u65e5</p> <p>\u2b50Week4</p> <p>2024 \u5e74 12 \u6708 23 \u65e5 \u661f\u671f\u4e00 </p>"},{"location":"sticks/","title":"\u4fbf\u7b7e","text":"<p>\u4e00\u4e9b\u96f6\u96f6\u6563\u6563</p>"},{"location":"sticks/DONE/","title":"DONE","text":"<ul> <li> \u672c\u5730\u6587\u4ef6\u548c\u5728\u7ebf\u6587\u4ef6\u7684\u5b58\u50a8\u95ee\u9898\uff0c\u4e0a\u4f20\u4e0a\u53bb\u7684\u672c\u5730\u600e\u4e48\u7ba1\u7406\uff0c\u53c8\u4e0d\u80fd\u5b8c\u5168\u5728\u7ebf</li> </ul> <p>\u7b49\u4f60\u5199\u5f97\u591a\u5230\u5360\u7528\u672c\u5730\u592a\u591a\u7a7a\u95f4\u518d\u8bf4\u5427\uff0c\u7b11\uff09</p> <ul> <li> \u56fe\u5e8a &amp; typora&amp; vscode&amp;github</li> </ul> <p>typora \u53ef\u4ee5\u81ea\u52a8\u521b\u5efa\u56fe\u5e8a\u6587\u4ef6\u5939</p> <ul> <li> mkdocs material \u5185\u5bb9\u5dee\u53c2\u8003</li> <li> <p> \u65f6\u95f4\u6233\u663e\u793a\u6709\u95ee\u9898  github actions error</p> </li> <li> <p> \u7248\u672c\u4fee\u6539</p> </li> </ul> <p>\u5b98\u65b9\u94fe\u63a5</p> <p>\u7248\u672c\u63a7\u5236\u793a\u4f8b</p> <p>\u7248\u672c\u63a7\u5236\u6e90\u7801</p> <p>\u597d\u590d\u6742\uff0c\u518d\u8bf4\u5427</p> <ul> <li> \u6587\u6863\u6807\u9898\u52a0\u7f16\u53f7\uff08\u53ef\u4ee5\u4f46\u6ca1\u5fc5\u8981\uff0c\u65b0\u5efaCSS\u6587\u4ef6\uff0c\u7136\u540e\u5728yml\u914d\u7f6e\u6587\u4ef6\u4e2d\u5f15\u7528</li> <li> mkdocs\u7684\u6587\u4ef6\u7ec4\u7ec7\u7ed3\u6784</li> </ul> <p>docs/\u6587\u4ef6\u5939\uff08\u5bfc\u822a\u680f\uff09/\uff08\u8d77\u4e2a\u522b\u540d\uff09/\u6587\u4ef6\u5939/\u6587\u4ef6\u5939/md\u6587\u4ef6</p> <p>docs/\u6587\u4ef6\u5939\uff08\u5bfc\u822a\u680f\uff09/\u6587\u4ef6\u5939\uff08\u5de6\u4fa7\u680f\u4e0b\u62c9\u6761\uff09/md\u6587\u4ef6</p> <p>docs/\u6587\u4ef6\u5939\uff08\u5bfc\u822a\u6a2a\u680f\uff09/md\u6587\u4ef6\uff08\u5de6\u4fa7\u680f\uff09/\u4e00\u7ea7\u6807\u9898\uff08\u6807\u9898\u5904\uff09/\u4e8c\u7ea7\u6807\u9898\uff08\u76ee\u5f55\u4ece\u4e8c\u7ea7\u6807\u9898\u5f00\u59cb\u663e\u793a\uff09</p> <p>\u4e00\u7ea7\u6807\u9898\u76f4\u63a5\u4f1a\u663e\u793a\u5728\u5de6\u4fa7\u680f\uff0c\u6216\u8005\u5728yml\u6587\u4ef6\u4e2d\u8d77\u522b\u540d</p> <ul> <li> \u82f1\u6587\u6587\u672c \u4e24\u7aef\u5bf9\u9f50(\u4ee5\u540e\u518d\u8bf4\u5427\uff0c\u4eba\u5bb6\u90fd\u6ca1\u5f04\uff0c\u6211\u4e5f\u4e0d\u6298\u817e\u4e86)</li> <li> \u8fd9\u4e2a\u4e3b\u9898\u8d85\u597d\u770b\uff0c\u6709\u7a7a\u6298\u817e\u4e00\u4e0b</li> <li> git push origin main\u6bcf\u6b21push\u5c31\u4f1a\u628a\u6240\u6709\u6587\u4ef6\u7684\u65f6\u95f4\u5168\u90e8\u66f4\u6539\u4e86</li> </ul> <p>\u6539\u5bf9\u4e86\uff01\u91cd\u65b0\u628a\u6574\u4e2a \u5de5\u4f5c\u6d41\u6587\u4ef6\u590d\u5236\u4e86\u522b\u4eba\u7684\u4e00\u4efd\u3002</p> <ul> <li> \u6587\u4ef6\u7ed3\u6784\u53d8\u4e86\uff0c\u8bb0\u5f97\u4fee\u6539yml\u7684\u8def\u5f84</li> </ul>"},{"location":"sticks/GitHub/","title":"GitHub","text":"<p>\u65b0\u5efa\u4ed3\u5e93\uff1a</p> <pre><code>echo \"# Rongerr.github.io\" &gt;&gt; README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/dearRongerr/Rongerr.github.io.git\ngit push -u origin main\n</code></pre> <p>\u5411\u5df2\u6709\u4ed3\u5e93\u63a8\u9001</p> <pre><code>git remote add origin https://github.com/dearRongerr/Rongerr.github.io.git\ngit branch -M main\ngit push -u origin main\n</code></pre>"},{"location":"sticks/MacOS/","title":"MacOS","text":""},{"location":"sticks/MacOS/#_1","title":"\u7ec8\u7aef\u547d\u4ee4","text":"<ul> <li><code>cd ..</code>  \u8fd4\u56de\u4e0a\u7ea7\u6587\u4ef6</li> <li><code>ls</code>  \u663e\u793a\u5f53\u524d\u76ee\u5f55\u6587\u4ef6</li> <li><code>ls -a</code> \u663e\u793a\u5f53\u524d\u76ee\u5f55\u7684\u6240\u6709\u6587\u4ef6\uff0c\u5305\u62ec\u9690\u85cf\u6587\u4ef6</li> </ul> <p>macOS \u7ec8\u7aef\u547d\u4ee4</p> <ul> <li> <p>\u663e\u793a\u6587\u4ef6\u6811</p> </li> <li> <p>\u6253\u5f00mac\u7ec8\u7aef</p> <ul> <li> <p>\u8f93\u5165 brew install tree</p> </li> <li> <p>\u4f7f\u7528\uff1a</p> </li> </ul> <p><code>tree</code> \u663e\u793a\u6587\u4ef6\u6811</p> <p><code>tree -a</code> \u663e\u793a\u6240\u6709\u6587\u4ef6\u6811\uff0c\u5305\u542b\u9690\u85cf\u6587\u4ef6</p> </li> <li> <p>vim\uff1f</p> </li> </ul>"},{"location":"sticks/TODO/","title":"TODO","text":""},{"location":"sticks/TODO/#_1","title":"\u6298\u817e","text":"<ul> <li> \u6362\u4e00\u4e0b\u7f51\u5740\uff0c\u6253\u5f00\u6709\u70b9\u6162</li> <li> typora\u516c\u5f0f\u7f16\u53f7\u53bb\u6389</li> <li> mac \u8fb9\u680f \u6700\u8fd1\u6253\u5f00\u7684\u8def\u5f84</li> </ul>"},{"location":"sticks/TODO/#_2","title":"\u516b\u80a1","text":"<ul> <li> <p>\u9762\u8bd5\u9898</p> </li> <li> <p>\u529b\u6263</p> </li> </ul>"},{"location":"sticks/TODO/#_3","title":"\u6bd5\u4e1a","text":"<ul> <li> <p>\u5f00\u9898\u62a5\u544a\uff0cppt</p> </li> <li> <p>\u6bd5\u4e1a\u8bba\u6587 latex\u6a21\u677f\uff0c\u516c\u5f0f\uff0c\u53c2\u8003\u6587\u732e</p> </li> </ul>"},{"location":"sticks/latex/","title":"LaTex","text":"<p>latex\u5bfc\u5165\u5305</p> <pre><code>\\usepackage{caption}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n</code></pre> <ul> <li>caption</li> <li>\u5bf9\u9f50</li> <li>\\mathbb</li> </ul>"},{"location":"sticks/markdwon_learn/","title":"markdown","text":""},{"location":"sticks/markdwon_learn/#_1","title":"\u951a\u70b9\u8bbe\u7f6e","text":"<p>\u4ece\u54ea\u513f\u8df3\uff1a</p> <pre><code>[\u8bf4\u660e\u6587\u5b57](#jump)\n</code></pre> <p>\u8df3\u5230\u54ea\u91cc\uff1a</p> <pre><code>&lt;span id = \"jump\"&gt;\u8df3\u8f6c\u5230\u7684\u4f4d\u7f6e&lt;/span&gt;\n</code></pre>"},{"location":"sticks/markdwon_learn/#_2","title":"\u7bad\u5934\u4e0a\u5199\u5b57","text":"<pre><code>X \\stackrel{F}{\\rightarrow} Y\n</code></pre>"},{"location":"sticks/markdwon_learn/#_3","title":"\u7bad\u5934\u4e0a\u67b6\u5b57\u7b26","text":"<pre><code>$\\vec{a}$  \u5411\u91cf\n$\\overline{a}$ \u5e73\u5747\u503c\n$\\widehat{a}$ (\u7ebf\u6027\u56de\u5f52\uff0c\u76f4\u7ebf\u65b9\u7a0b) \u5c16\n$\\widetilde{a}$ \n$\\dot{a}$   \u4e00\u9636\u5bfc\u6570\n$\\ddot{a}$  \u4e8c\u9636\u5bfc\u6570\n</code></pre>"},{"location":"sticks/markdwon_learn/#markdown_1","title":"markdown\u591a\u884c\u5927\u62ec\u53f7","text":""},{"location":"sticks/markdwon_learn/#_4","title":"\u5c45\u4e2d\u5bf9\u9f50\u7684\u5927\u62ec\u53f7","text":"\\[ f(i)= \\left\\{\\begin{matrix} 1,i\\in Q \\\\ -1,i\\notin Q \\end{matrix}\\right. \\] <pre><code>$$\nf(i)=\n\\left\\{\\begin{matrix}\n1,i\\in Q \\\\\n-1,i\\notin Q\n\\end{matrix}\\right.\n$$\n</code></pre>"},{"location":"sticks/markdwon_learn/#_5","title":"\u6807\u51c6\u5927\u62ec\u53f7","text":"<p>\u5de6\u5bf9\u9f50\u7684\u5927\u62ec\u53f7</p> <pre><code>$$\n\\begin{cases}\nx+y=5 \\\\\n2x+3y=12\n\\end{cases}\n$$\n</code></pre> \\[ \\begin{cases} x+y=5 \\\\ 2x+3y=12 \\end{cases} \\]"},{"location":"sticks/markdwon_learn/#_6","title":"\u6ce2\u6d6a\u53f7","text":"<pre><code>$\\sim$\n</code></pre> <p>\\(\\sim\\)</p> <p>\u6b63\u6bd4\u4e8e\u7b26\u53f7</p> <pre><code>$\\propto$\n</code></pre> <p>\\(\\propto\\)</p> <p>\u79ef\u5206\u7b26\u53f7</p> <pre><code>\\int\n</code></pre> <p>\\(\\int\\)</p> <p>\u4efb\u610f</p> <pre><code>${\\forall}$\n</code></pre> <p>\\({\\forall}\\)</p> <p>\u5b58\u5728</p> <pre><code>${\\exists}$\n</code></pre> <p>\\({\\exists}\\)</p> <p>\u7b49\u4ef7\u4e8e</p> <pre><code>$\\iff$\n</code></pre> <p>\\(\\iff\\)</p> <pre><code>$\\partial$\n</code></pre> <p>\\(\\partial\\)</p> <pre><code>\\mathbf{I}\n</code></pre> <p>\\(\\mathbf{I}\\) \u52a0\u7c97\u9ed1\u4f53\u8868\u793a\u5411\u91cf</p> <pre><code>$\\pi$\n</code></pre> <p>\\(\\pi\\)</p> <pre><code>$\\prod$\n$\\cdot$\n$\\times$\n</code></pre> <p>\\(\\prod\\)</p> <p>\\(\\cdot\\)</p> <p>\\(\\times\\)</p>"},{"location":"sticks/mkdocs_learn/","title":"MkDocs","text":"<p>\u4e3b\u9898\u914d\u7f6e\uff1aMaterial for MkDocs</p> <p>\u672c\u5730\u8c03\u8bd5\uff1a</p> <pre><code>(base) .. mkdocs-site % mkdocs -h\nUsage: mkdocs [OPTIONS] COMMAND [ARGS]...\n\n  MkDocs - Project documentation with Markdown.\n\nOptions:\n  -V, --version         Show the version and exit.\n  -q, --quiet           Silence warnings\n  -v, --verbose         Enable verbose output\n  --color / --no-color  Force enable or disable color and wrapping for the output. Default is auto-\n                        detect.\n  -h, --help            Show this message and exit.\n\nCommands:\n  build      Build the MkDocs documentation.\n  get-deps   Show required PyPI packages inferred from plugins in mkdocs.yml.\n  gh-deploy  Deploy your documentation to GitHub Pages.\n  new        Create a new MkDocs project.\n  serve      Run the builtin development server.\n</code></pre> <p>\u53c2\u8003\u6a21\u7248\u6e90\u7801</p> <p>\u53c2\u8003\u6a21\u7248\u5c55\u793a</p> <p>\u5b98\u65b9\u6587\u6863\uff1amkdocs\u914d\u7f6e </p> <p>mkdocs\u5165\u95e8\u6559\u7a0b</p>"},{"location":"sticks/mkdocs_learn/#_1","title":"\u6587\u4ef6\u7ec4\u7ec7\u5f62\u5f0f","text":"<pre><code>(base) ... docs % tree\n.\n\u251c\u2500\u2500 Error  # \u6587\u4ef6\u5939\n\u2502   \u2514\u2500\u2500 \u62a5\u9519.md   # markdown\u6587\u4ef6\n\u251c\u2500\u2500 Leecode\n\u2502   \u2514\u2500\u2500 \u529b\u6263.md\n\u251c\u2500\u2500 home\n\u2502   \u251c\u2500\u2500 page-1.md\n\u2502   \u2514\u2500\u2500 page-2.md\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 mkdocs\n\u2502   \u251c\u2500\u2500 css\n\u2502   \u2502   \u251c\u2500\u2500 no-footer.css\n\u2502   \u2502   \u2514\u2500\u2500 unordered-list-symbols.css\n\u2502   \u2514\u2500\u2500 javascripts\n\u2502       \u2514\u2500\u2500 katex.js\n\u2514\u2500\u2500 \u4fbf\u7b7e  # \u6587\u4ef6\u5939\n  \u251c\u2500\u2500 TODO  # \u56fe\u5e8a\n  \u2502   \u251c\u2500\u2500 1.png\n  \u2502   \u2514\u2500\u2500 image-20241115095446260.png\n  \u251c\u2500\u2500 TODO.md #markdown\u6587\u4ef6\n  \u251c\u2500\u2500 mkdocs_learn\n  \u2502   \u2514\u2500\u2500 image-20241115100605111-1636372-1636377.png\n  \u251c\u2500\u2500 mkdocs_learn.md\n  \u2514\u2500\u2500 \u5907\u5fd8.md\n\n10 directories, 14 files\n</code></pre> <p>\u524d\u6bb5\u4e0e\u540e\u7aef\u7684\u5bf9\u5e94</p> <p></p>"},{"location":"sticks/mkdocs_learn/#_2","title":"\u6dfb\u52a0\u9875\u9762\u521b\u5efa\u65f6\u95f4\u3001\u6700\u540e\u4e00\u6b21\u4fee\u6539\u65f6\u95f4","text":"<p>\u5b98\u65b9\u6587\u6863\u94fe\u63a5</p> <p></p>"},{"location":"sticks/mkdocs_learn/#_3","title":"\u5199\u4f5c","text":"<p>\u66f4\u591a\u5199\u4f5c</p> <pre><code>!!! note\n    This is a note.\n</code></pre> <pre><code>!!! tip\n    This is a tip.\n</code></pre> <pre><code>!!! warning\n    This is a warning.\n</code></pre> <pre><code>!!! danger\n    This is a danger.\n</code></pre> <pre><code>!!! success\n    This is a success.\n</code></pre> <pre><code>!!! info\n    This is a info.\n</code></pre> <pre><code>!!! quote\n    This is a quote.\n</code></pre> <pre><code>??? question \"What is the meaning of life, the universe, and everything?\"\n</code></pre> <p>Note</p> <p>This is a note.</p> <p>Tip</p> <p>This is a tip.</p> <p>Warning</p> <p>This is a warning.</p> <p>Danger</p> <p>This is a danger.</p> <p>Success</p> <p>This is a success.</p> <p>Info</p> <p>This is a info.</p> <p>Quote</p> <p>This is a quote.</p> What is the meaning of life, the universe, and everything?"},{"location":"sticks/mkdocs_learn/#mkdocs_1","title":"mkdocs\u547d\u4ee4","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> <p>Project layout</p> <pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"sticks/sticks/","title":"Sticks","text":""},{"location":"sticks/sticks/#_1","title":"\u8bba\u6587\u5e38\u89c1\u683c\u5f0f\u4fee\u6539\u5efa\u8bae","text":"<ul> <li> \u53c2\u8003\u6587\u732e\u7684\u5f15\u7528\u683c\u5f0f\uff1a</li> </ul> <p>\u4f5c\u8005\u540d\uff0c\u6587\u7ae0\u540d\uff0c\u671f\u520a\u540d\uff0c\u53d1\u8868\u5e74\u4efd\uff0c\u5377\u53f7\uff08\u671f\u53f7\uff09\uff0c\u8d77\u6b62\u9875\u7801. Wu, C.L.; Chau, K.W.; Li, Y.S. Methods to improve neural network performance in daily flows prediction. J. Hydrol. 2009, 372, 80\u201393. </p> <p>\u5982\u679c\u662f\u4e66\uff0c\u8981\u5199\u6210\uff1a\u4f5c\u8005\u540d\uff0c\u4e66\u540d\uff0c\u51fa\u7248\u793e\u7684\u540d\u79f0\uff0c\u51fa\u7248\u5e74\u4efd\u3002  \u4f8b\u5982\uff1aVapnik, V. Statistical Learning Theory; Wiley: New York, NY, USA, 1998.</p> <ul> <li> \u6587\u732e\u7efc\u8ff0\u90e8\u5206\uff1a</li> </ul> <p>\u53e6\u5916\uff0c\u6587\u732e\u7efc\u8ff0\u90e8\u5206\uff0c\u5f15\u7528\u7684\u65f6\u5019\u628a\u4f5c\u8005\u7b49\u540e\u9762\u7684\u4eba\u53bb\u6389</p>"}]}