# TSP

> [ML 领域三大顶刊](https://www.bilibili.com/video/BV1Nr421p7NV/?spm_id_from=333.788.top_right_bar_window_history.content.click&vd_source=ddd7d236ab3e9b123c4086c415f4939e) 
>
> - NIPS（暑假要开始，收稿量非常大，同时质量非常过硬）——OpenReview
> - ICLR（暑假刚结束）——OpenReview
> - ICML（寒假结束）
>
> 关于分类：
>
> - Oral：12 分钟口头陈述
> - Splotlights（特别关注）：4 分钟的口头演示
> - Posters（海报）：其余被接收的论文都是海报演示
> - ORALS >  Splotlights ＞ POSTERS

- [ ] 2019、LogTrans
- [ ] ICLR2020、Reformer

---

- [x] NeurIPS2021 、Autoformer、清华大学吴海旭
- [ ] AAAI2021、Informer

---

- [x] ICLR2022  (Oral)、Pyformer、上海交通大学、蚂蚁集团
- [x] ICML2022、Fedformer、阿里达摩院
- [ ] NeuraIPS2022、SCINet

---

- [ ] AAAI2023 、 Are Transformer Effective ？
- [ ] NeurIPS2023 、TLNets
- [ ] NeurIPS2023 (Spotlight)、WITRAN、北京交通大学万怀宇团队
- [ ] ICLR2023、Crossformer
- [ ] 2023、DLinear
- [ ] 2023、TimesNet

- [ ] ICLR2023、PatchTST

- [x] ICLR2023、SegRNN、华南理工大学

----

- [x] ICLR2024、TimeMixer、蚂蚁集团、清华大学吴海旭
- [ ] ICLR 2024、、iTransformer、
- [ ] 2024、UnetTSF、中国科学技术大学、中国科学院

----

PaperWithCode：[Time Series Forecasting ](https://paperswithcode.com/task/time-series-forecasting) 

- 特征之间的相关性
- 注意力计算的复杂度有点高
- 注意力机制

## 论文关键词

### WITRAN

水波纹信息传输循环加速网络

- 非逐点语义信息捕获
- 内存占用 &  时间复杂度

贡献：

（1）水波纹信息传输：WIT

（2）水平垂直 门控选择单元： HVGSU

（3）循环加速网络：RAN
