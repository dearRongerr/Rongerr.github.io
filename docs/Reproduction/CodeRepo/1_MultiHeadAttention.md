# å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å½¢çŠ¶å˜åŒ–

## QKV

å…³äº Transformer çš„ QKV åŒæºé—®é¢˜

$QK^TV = B_QL_QD_Q \cdot B_KD_KL_K \cdot B_VL_VD_V$

ï¼ˆ1ï¼‰å›ç­”åŒæºé—®é¢˜ï¼š

- è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ˆselfAï¼‰ï¼Œ$QKV$ æ¥è‡ªåŒä¸€ä¸ªåºåˆ—
- äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ˆcrossAï¼‰ï¼Œ$Q$ æ¥è‡ªä¸€ä¸ªåºåˆ—ï¼Œ$KV$ æ¥è‡ªå¦ä¸€ä¸ªåºåˆ—ï¼Œæ•æ‰ä¸¤ä¸ªä¸åŒåºåˆ—ä¹‹é—´çš„ä¾èµ–å…³ç³»

ï¼ˆ2ï¼‰è®¨è®ºå½¢çŠ¶é—®é¢˜ï¼š

- å¿…é¡»æ»¡è¶³çš„æ˜¯ï¼š $D_Q = D_K$ ã€ $L_K = L_V$
- ç®€åŒ–ç‰ˆæœ¬ï¼š $attn = L_Q \times L_K$

---

**å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼š**

çš„å½¢çŠ¶å˜åŒ–ï¼šBLD  $\stackrel{D=H\cdot d}{\rightarrow}$  BLHd 

---

## [**å½¢çŠ¶å˜åŒ–**](#forward) 

```python
def forward(self, q, k, v, mask=None):
    batch_size, sequence_length, model_dim = q.shape

    # [B, L, d_model] â†’ çº¿æ€§æŠ•å½± â†’ [B, L, H*d_k] â†’ view â†’ [B, L, H,d_k] â†’ transpose  â†’ [B, H, L, d_k]
    q = self.q_proj(q).view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)
    # [B, L, d_model] â†’ çº¿æ€§æŠ•å½± â†’ [B, L, H*d_k] â†’ view â†’ [B, L, H,d_k] â†’ transpose  â†’ [B, H, L, d_k]
    k = self.k_proj(k).view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)
    # [B, L, d_model] â†’ çº¿æ€§æŠ•å½± â†’ [B, L, H*d_v] â†’ view â†’ [B, L, H,d_v] â†’ transpose  â†’ [B, H, L, d_v]
    v = self.v_proj(v).view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)

    # [B, H, L, d_k] Ã— [B, H, d_k, S] â†’ çŸ©é˜µä¹˜æ³•(torch.matmul) â†’ [B, H, L, S]
    scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)

    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e09)

    # [B, H, L, S] â†’ softmax â†’ [B, H, L, S]
    prob = F.softmax(scores, dim=-1)

    prob = self.dropout(prob)

    # [B, H, L, S] Ã— [B, H, S, d_v] â†’ çŸ©é˜µä¹˜æ³•(torch.matmul) â†’ [B, H, L, d_v] â†’ è½¬ç½®(.transpose) â†’ [B, L, H, d_v] â†’ é‡å¡‘(.view) â†’ [B, L, d_model]
    attn_weights = torch.matmul(prob, v).transpose(1, 2).contiguous().view(batch_size, sequence_length, model_dim)

    # æ¢å¤åŸå§‹ç»´åº¦ï¼š[B, L, d_model] â†’ çº¿æ€§æŠ•å½± â†’ [B, L, d_model]
    output = self.o_proj(attn_weights)
    return output
```

<a name="forward">å½¢çŠ¶åˆ†æ </a> 

ï¼ˆ1ï¼‰**æŸ¥è¯¢ï¼ˆqueriesï¼‰**ï¼š

- `[B, L, d_model] â†’ çº¿æ€§æŠ•å½±(nn.Linear) â†’ [B, L, H*d_k] â†’ é‡å¡‘(.view) â†’ [B, L, H, d_k] â†’ è½¬ç½®(.transpose) â†’ [B, H, L, d_k]`
- é€šè¿‡çº¿æ€§æŠ•å½±å°†æŸ¥è¯¢å‘é‡æŠ•å½±åˆ°å¤šå¤´æ³¨æ„åŠ›ç©ºé—´ï¼Œå¹¶é‡å¡‘ä¸ºå¤šå¤´çš„å½¢çŠ¶ã€‚

```python
q = self.q_proj(q).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)
```



ï¼ˆ2ï¼‰**é”®ï¼ˆkeysï¼‰**ï¼š  $D_Q = D_K \triangleq d_k$    

- `[B, S, d_model] â†’ çº¿æ€§æŠ•å½±(nn.Linear) â†’ [B, L, H*d_k] â†’ é‡å¡‘(.view) â†’ [B, L, H, d_k] â†’ è½¬ç½®(.transpose) â†’ [B, H, L, d_k]`
- é€šè¿‡çº¿æ€§æŠ•å½±å°†é”®å‘é‡æŠ•å½±åˆ°å¤šå¤´æ³¨æ„åŠ›ç©ºé—´ï¼Œå¹¶é‡å¡‘ä¸ºå¤šå¤´çš„å½¢çŠ¶ã€‚
- æ›´å‡†ç¡®çš„è®°å·ï¼Œè¡¨ç¤º QKçš„ç»´åº¦å¿…é¡»ç›¸åŒ $\triangleq d_k$ ï¼ŒKV çš„é•¿åº¦å¿…é¡»ç›¸åŒ $\triangleq S$

> ç®€åŒ–ç‰ˆ
>
> -  $Q \triangleq BLD_kã€  K \triangleq BSD_k ã€V \triangleq BSD_v$ 
> - ==linear ã€viewã€transpose==  $Qï¼šBLD_k -> BLHd_k -> BHLd_k$ 
> - $Kï¼šBSD_k -> BSHd_k -> BHSd_k$ 
> - $Vï¼šBSD_v -> BSHd_v -> BHSd_v$
> -  ==torch.matmal==  $QK^T: BHLd_k \cdot BHd_kS -> BHLS$
> -  ==softmax ã€dropoutã€torch.matmul== $sonftmax(QK^T)V ->BHLS \cdot BHSd_v -> BHLd_v -> transpose -> view -> BLD$  

ï¼ˆ3ï¼‰**å€¼ï¼ˆvaluesï¼‰**ï¼š

- `[B, L, d_model] â†’ çº¿æ€§æŠ•å½±(nn.Linear) â†’ [B, L, H*d_v] â†’ é‡å¡‘(.view) â†’ [B, L, H, d_v] â†’ è½¬ç½®(.transpose) â†’ [B, H, L, d_v]`
- é€šè¿‡çº¿æ€§æŠ•å½±å°†å€¼å‘é‡æŠ•å½±åˆ°å¤šå¤´æ³¨æ„åŠ›ç©ºé—´ï¼Œå¹¶é‡å¡‘ä¸ºå¤šå¤´çš„å½¢çŠ¶ã€‚

ï¼ˆ4ï¼‰**è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†**ï¼š

```python
scores = torch.matmul(q,k.transpose(-1,-2))//math.sqrt(self.head_dim)
```

- `[B, H, L, d_k] Ã— ([B, H, L, d_k] â†’ transpose â†’ [B, H, d_k, L]) â†’ çŸ©é˜µä¹˜æ³•(torch.matmul) â†’ [B, H, L, L]`  
- é€šè¿‡çŸ©é˜µä¹˜æ³•è®¡ç®—æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„ç‚¹ç§¯æ³¨æ„åŠ›å¾—åˆ†ï¼Œå¹¶è¿›è¡Œç¼©æ”¾ã€‚

ï¼ˆ5ï¼‰**è®¡ç®—æ³¨æ„åŠ›åŠ æƒå€¼**ï¼š

- `[B, H, L, L] â†’ softmax â†’ [B, H, L, L]`
- é€šè¿‡softmaxè®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚

ï¼ˆ6ï¼‰**åº”ç”¨æ³¨æ„åŠ›æƒé‡**ï¼š

- `[B, H, L, L] Ã— [B, H, L, d_v] â†’ çŸ©é˜µä¹˜æ³• â†’ [B, H, L, d_v] â†’ è½¬ç½® â†’ [B, L, H, d_v] â†’ é‡å¡‘ â†’ [B, L, d_model]`
- é€šè¿‡çŸ©é˜µä¹˜æ³•å°†æ³¨æ„åŠ›æƒé‡åº”ç”¨äºå€¼å‘é‡ï¼Œå¹¶é‡å¡‘ä¸ºåŸå§‹å½¢çŠ¶ã€‚

ï¼ˆ7ï¼‰**è¾“å‡º**ï¼š è¿˜åŸç»´åº¦



- `[B, L, d_model] â†’ çº¿æ€§æŠ•å½± â†’ [B, L, d_model]`
- é€šè¿‡çº¿æ€§æŠ•å½±å°†åˆå¹¶çš„è¾“å‡ºæ¢å¤åˆ°åŸå§‹ç»´åº¦ã€‚

ğŸ“¢ ç®€åŒ–ç‰ˆ

-  $Q \triangleq BLD_kã€  K \triangleq BSD_k ã€V \triangleq BSD_v$ 
- ==linear ã€viewã€transpose==  $Qï¼šBLD_k -> BLHd_k -> BHLd_k$ 
- $Kï¼šBSD_k -> BSHd_k -> BHSd_k$ 
- $Vï¼šBSD_v -> BSHd_v -> BHSd_v$
-  ==torch.matmal==  $QK^T: BHLd_k \cdot BHd_kS -> BHLS$
-  ==softmax ã€dropoutã€torch.matmul== $sonftmax(QK^T)V ->BHLS \cdot BHSd_v -> BHLd_v -> transpose -> view -> BLD$  

å®Œæ•´çš„ä»£ç ï¼Œå¾—ä¼šå‘€

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    def __init__(self,model_dim,num_heads,dropout=0.1):
        super().__init__()
        self.model_dim = model_dim
        self.num_heads = num_heads
        self.head_dim = model_dim // num_heads

        self.q_proj = nn.Linear(model_dim,model_dim)
        self.k_proj = nn.Linear(model_dim,model_dim)
        self.v_proj = nn.Linear(model_dim,model_dim)

        self.dropout = nn.Dropout(dropout)

        self.o_proj = nn.Linear(model_dim,model_dim)
    
    def forward(self,q,k,v,mask=None):

        batch_size,sequence_length,model_dim = q.shape

        q = self.q_proj(q).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)
        k = self.k_proj(k).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)
        v = self.v_proj(v).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)

        scores = torch.matmul(q,k.transpose(-1,-2))//math.sqrt(self.head_dim)

        if mask is not None:
            scores = scores.masked_fill(mask==0,-1e09)

        prob = F.softmax(scores,dim=-1)

        prob = self.dropout(prob)

        attn_weights = torch.matmul(prob,v).transpose(1,2).contiguous().view(batch_size,sequence_length,model_dim)

        output = self.o_proj(attn_weights)
        return output

model_dim = 512 
num_heads = 8
mha = MultiHeadAttention(model_dim=model_dim, num_heads=num_heads, dropout=0.1)
batch_size = 10
sequence_length = 60
q = torch.randn(batch_size, sequence_length, model_dim)
k = torch.randn(batch_size, sequence_length, model_dim)
v = torch.randn(batch_size, sequence_length, model_dim)

mask = None
output = mha(q, k, v, mask)
print(output.shape)  # è¾“å‡ºçš„å½¢çŠ¶åº”è¯¥æ˜¯(batch_size, sequence_length, model_dim)
```

