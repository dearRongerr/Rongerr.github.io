# 4月

## 2025-04-02 Wednesday 

加油。

## 2025-04-04 Friday 

### 文献阅读

Long time series forecasting aims to utilize historical information to forecast future states over extended horizons. Traditional RNN-based series forecasting methods struggle to effectively address long-term dependencies and gradient issues in long time series problems. Recently, SegRNN has emerged as a leading RNN-based model tailored for long-term series forecasting, demonstrating state-of-the-art performance while maintaining a streamlined architecture through innovative segmentation and parallel decoding techniques. Nevertheless, SegRNN has several limitations: its fixed segmentation disrupts data continuity and fails to effectively leverage information across different segments, the segmentation strategy employed by SegRNN does not fundamentally address the issue of information loss within the recurrent structure. To address these issues, we propose the ISMRNN method with three key enhancements: we introduce an implicit segmentation structure to decompose the time series and map it to segmented hidden states, resulting in denser information exchange during the segmentation phase. Additionally, we incorporate residual structures in the encoding layer to mitigate information loss within the recurrent structure. To extract information more effectively, we further integrate the Mamba architecture to enhance time series information extraction. Experiments on several real-world long time series forecasting datasets demonstrate that our model surpasses the performance of current state-of-the-art models.

长期时间序列预测旨在利用历史信息来预测未来在较长时间范围内的状态。传统的基于RNN的时间序列预测方法在处理长期时间序列问题时，难以有效解决长期依赖性和梯度问题。最近，SegRNN作为一种针对长期序列预测的领先RNN模型出现，通过创新的分段和并行解码技术，以精简的架构实现了最先进的性能。然而，SegRNN存在几个局限性：其固定的分段方式破坏了数据的连续性，无法有效利用不同分段之间的信息，SegRNN采用的分段策略也未能从根本上解决循环结构中的信息丢失问题。为了解决这些问题，我们提出了ISMRNN方法，包含三个关键增强点：我们引入了一种隐式分段结构，将时间序列分解并映射到分段隐藏状态，在分段阶段实现了更密集的信息交换。此外，我们在编码层中引入残差结构，以减轻循环结构中的信息丢失。为了更有效地提取信息，我们进一步整合了Mamba架构来增强时间序列信息提取。在几个真实世界长期时间序列预测数据集上的实验表明，我们的模型超越了当前最先进的模型性能。

In this work, we propose a novel model named ISMRNN to address the issues associated with SegRNN. Specifically, ISMRNN introduces an implicit segmentation structure that decomposes and maps the time series into encoded vectors through two linear transformations. This method facilitates more continuous processing during segmentation and enhances the utilization of information between different segments. Additionally, ISMRNN incorporates a residual structure with a linear layer, allowing some information to bypass the recurrent encoding structure, thus reducing information loss within the recurrent framework. Furthermore, we employ the Mamba structure[Gu and Dao, 2023] for preprocessing the time series, which aids in capturing long-term dependencies more effectively. The main contributions of ISMRNN can be summarized as follows:  

• Utilizing implicit segmentation for denser information exchange during the segmentation phase. 

 • Incorporating the Mamba structure to improve information preprocessing.  

• The residual structure reduces information loss within the recurrent structure.

在这项工作中，我们提出了一种名为 ISMRNN 的新型模型，以解决与 SegRNN 相关的问题。具体来说，ISMRNN 引入了一种隐式分段结构，通过两次线性变换将时间序列分解并映射为编码向量。这种方法在分段过程中实现了更连续的处理，并增强了不同分段之间信息的利用。此外，ISMRNN 在编码结构中引入了带有线性层的残差结构，使得部分信息能够绕过循环编码结构，从而减少循环框架内的信息丢失。此外，我们还采用 Mamba 结构 [Gu and Dao, 2023] 对时间序列进行预处理，这有助于更有效地捕捉长期依赖性。ISMRNN 的主要贡献可以总结如下：
- 利用隐式分段，在分段阶段实现更密集的信息交换。
- 引入 Mamba 结构以改进信息预处理。
- 残差结构减少了循环结构内的信息丢失。



==输入：==

x.shape=torch.Size([16, 720, 321])  batchSize SequenceLength FeatureDim

==reshape== 

x.reshape(-1, self.seg_num_x, self.seg_len) → [16×321=5136, 15,48]

==self.valueEmbedding== 

```python
# build model
self.valueEmbedding = nn.Sequential(
    nn.Linear(self.seg_len, self.d_model),
    nn.ReLU()
)

Sequential(
  (0): Linear(in_features=48, out_features=512, bias=True)
  (1): ReLU()
)
```

→ [5136, 15,512]

- 本文认为SegRNN直接分段破坏了序列之间的相关关系，因此采用UnetTSF 进行隐式分段 

- 其次 SegRNN 在预测序列长度远远大于输入序列长度时，模型性能不佳，因此本文提出
- SegRNN 在编码阶段并没有用到频域信息，造成信息丢失，因此
- SegRNN 在进行分段时，重塑形式时，将特征维度与batch 维度进行混合，忽略了特征之间的相关性，并没有充分考虑到特征对于预测结果的影响，因此，（注意力机制）
- SegRNN 在进行建模时，忽略了时间序列中存在的高频成分和低频成分信息，因此

## 2025-04-05 Saturday 

time Unet forward 接收 x，输入 x 的形状是(32,720,7)，

`x = self.revin_layer(x, 'norm')` 进行可逆实例归一化 ，形状不变(32,720,7)

`x1 = x.permute(0,2,1)` 接下来交换维度，形状由(32,720,7)变为(32,7,720)

```python
i = 0
for down_block in self.down_blocks:
    e_out.append(down_block(x1))
    x1 = self.Maxpools[i](x1)
    i = i+1
```

==self.down_blocks== 

```
ModuleList(
  (0): block_model(
    (Linear_channel): ModuleList(
      (0-6): 7 x Linear(in_features=720, out_features=720, bias=True)
    )
    (ln): LayerNorm((720,), eps=1e-05, elementwise_affine=True)
    (relu): ReLU(inplace=True)
  )
  (1): block_model(
    (Linear_channel): ModuleList(
      (0-6): 7 x Linear(in_features=359, out_features=359, bias=True)
    )
    (ln): LayerNorm((359,), eps=1e-05, elementwise_affine=True)
    (relu): ReLU(inplace=True)
  )
  (2): block_model(
    (Linear_channel): ModuleList(
      (0-6): 7 x Linear(in_features=179, out_features=179, bias=True)
    )
    (ln): LayerNorm((179,), eps=1e-05, elementwise_affine=True)
    (relu): ReLU(inplace=True)
  )
  (3): block_model(
    (Linear_channel): ModuleList(
      (0-6): 7 x Linear(in_features=89, out_features=89, bias=True)
    )
    (ln): LayerNorm((89,), eps=1e-05, elementwise_affine=True)
    (relu): ReLU(inplace=True)
  )
)
```

==self.Maxpools== 

```python
ModuleList(
  (0-3): 4 x AvgPool1d(kernel_size=(3,), stride=(2,), padding=(0,))
)
```

==第1次循环== 

输入 x1 形状: [32, 7, 720]

`down_block(x1)` 输出形状: [32, 7, 720]

`Maxpools[0](x1)` 输出形状: [32, 7, 359]

池化参数: kernel_size=3, stride=2, padding=0

计算: `(720 + 2*0 - 3)/2 + 1 = 359`

==第2次循环==

输入 x1 形状: [32, 7, 359]

down_block(x1) 输出形状: [32, 7, 359]

`Maxpools[1](x1)` 输出形状: [32, 7, 179]
计算: `(359 + 2*0 - 3)/2 + 1 = 179`

==第3次循环== 

输入 x1 形状: [32, 7, 179]

`down_block(x1)` 输出形状: [32, 7, 179]

`Maxpools[2](x1)` 输出形状: [32, 7, 89]

计算: `(179 + 2*0 - 3)/2 + 1 = 89`

==第4次循环== 

输入 x1 形状: [32, 7, 89]

down_block(x1) 输出形状: [32, 7, 89]
`Maxpools[3](x1)`输出形状: [32, 7, 44]

计算: (89 + 2*0 - 3)/2 + 1 = 44

---

循环结束后，`e_out` 列表包含`4`个张量，对应`4`个尺度的特征：

e_out[0]: [32, 7, 720] (原始尺度)

e_out[1]: [32, 7, 359] (第一次下采样)

e_out[2]: [32, 7, 179] (第二次下采样)

e_out[3]: [32, 7, 89] (第三次下采样)

- 随着层次加深，时间维度逐渐减小，每层捕获不同时间尺度的特征
- `down_block` 操作不改变通道维度和批次维度，只在时间维度上进行映射
- 每次池化操作会使时间维度大约减半 (取决于精确的池化参数)

(cv中，通过卷积通道数翻倍，尺度不变；池化成通道数不变，尺度减半)

---

`e_last = e_out[self.stage_num - 1]` 其中 `self.stage_num=4`，获取第三次下采样层输出[32, 7, 89] ，也就是 `e_last.shape = [32, 7, 89]` 

<details>
<summary>怎么从论文中扒模块？</summary>
<p>
	（1）找到原论文，提出的模块，给出的代码
    （2）准备自己的测试文件
    分析，原模块的 init、forward 分别需要什么参数，形状的含义
    分析，自己为什么需要这个模块。自己的数据是什么形状的，将自己的数据维度拿出来，分析和原模块的维度对齐。测试对象是自己所需要的数据。测试成功。
</p>
</details>

```python
e_last = e_out[self.stage_num - 1]
for i in range(self.stage_num - 1):
    e_last = torch.cat((e_out[self.stage_num - i - 2], e_last), dim=2)
    e_last = self.up_blocks[i](e_last)
e_last = e_last.permute(0,2,1)
e_last = self.revin_layer(e_last, 'denorm')
return e_last
```

初始条件：

- stage_num = 4 (默认设置)
- e_out 包含4个不同尺度的特征图:
  * e_out[0]: [32, 7, 720]
  * e_out[1]: [32, 7, 359]
  * e_out[2]: [32, 7, 179]
  * e_out[3]: [32, 7, 89]

==第1次循环 (i=0)== 

e_last 初始值为 e_out[3]: [32, 7, 89]

`e_last = torch.cat((e_out[self.stage_num - i -2], e_last), dim=2)`

计算 `self.stage_num - i - 2 = 4 - 0 - 2 = 2`

<details>
<summary>
说明：拼接 e_out[2] 和 e_last: [32, 7, 179] 和 [32, 7, 89] 
</summary>
<p>
来，读， [32, 7, 179] ，32 个 7，7 个 179，现在拼接沿着 dim=2，现在是 32 个 7，7 个 179+89
</p>
</details>

沿 dim=2 (时间维度) 拼接，得到 [32, 7, 179+89] = [32, 7, 268]

通过 up_blocks[0] 映射，输出形状变为 [32, 7, 179]

<details>
<summary>说明：self.up_blocks</summary>
<p>
```python
    ModuleList(
      (0): block_model(
        (Linear_channel): ModuleList(
          (0-6): 7 x Linear(in_features=268, out_features=179, bias=True)
        )
        (ln): LayerNorm((179,), eps=1e-05, elementwise_affine=True)
        (relu): ReLU(inplace=True)
      )
      (1): block_model(
        (Linear_channel): ModuleList(
          (0-6): 7 x Linear(in_features=538, out_features=359, bias=True)
        )
        (ln): LayerNorm((359,), eps=1e-05, elementwise_affine=True)
        (relu): ReLU(inplace=True)
      )
      (2): block_model(
        (Linear_channel): ModuleList(
          (0-6): 7 x Linear(in_features=1079, out_features=720, bias=True)
        )
        (ln): LayerNorm((720,), eps=1e-05, elementwise_affine=True)
        (relu): ReLU(inplace=True)
      )
    )
```
</p>
</details>
==第2次循环 (i=1)==

e_last 当前值: [32, 7, 179]  

`e_last = torch.cat((e_out[self.stage_num - i -2], e_last), dim=2)`

计算 `self.stage_num - i - 2 = 4 - 1 - 2 = 1`

拼接 `e_out[1]` 和 `e_last`: [32, 7, 359] 和 [32, 7, 179]

沿 dim=2 拼接，得到 [32, 7, 359+179] = [32, 7, 538]

通过 up_blocks[1] 映射，输出形状变为 [32, 7, 359]

==第3次循环 (i=2)==

e_last 当前值: [32, 7, 359]

计算 self.stage_num - i - 2 = 4 - 2 - 2 = 0

拼接 e_out[0] 和 e_last: [32, 7, 720] 和 [32, 7, 359]

沿 dim=2 拼接，得到 [32, 7, 720+359] = [32, 7, 1079]

通过 up_blocks[2] 映射，输出形状变为 [32, 7, 720]

---

 `e_last = e_last.permute(0,2,1)`

调整维度: e_last.permute(0,2,1) 将 [32, 7, 720] 变为 [32, 720, 7]

应用逆归一化: `revin_layer(e_last, 'denorm')` 形状不变，仍为 `[32, 720, 7]`

返回最终输出: 形状为 `[32, 720, 7]`


