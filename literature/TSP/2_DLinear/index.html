<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://dearrongerr.github.io/Rongerr.github.io/literature/TSP/2_DLinear/">
      
      
        <link rel="prev" href="../1_SegRNN/">
      
      
        <link rel="next" href="../3_TimesNet/">
      
      
      <link rel="icon" href="../../../assets/images/logo2.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>2022„ÄÅLTSF-Linear - Ê∫∂err</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
  
  <style>:root{--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20576%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M288%2064a32%2032%200%201%200%200%2064%2032%2032%200%201%200%200-64m-96%2032c0-53%2043-96%2096-96s96%2043%2096%2096c0%2041.8-26.7%2077.4-64%2090.5v257.9c62.9-14.3%20110.2-69.7%20111.9-136.5L415.8%20322c-10%208.7-25.1%207.7-33.9-2.3s-7.7-25.1%202.3-33.9l64-56c9-7.9%2022.6-7.9%2031.6%200l64%2056c10%208.7%2011%2023.9%202.3%2033.9s-23.9%2011-33.9%202.3L496%20307.9C493.9%20421%20401.6%20512%20288%20512S82.1%20421%2080%20307.9l-16.2%2014.2c-10%208.7-25.1%207.7-33.9-2.3s-7.7-25.1%202.3-33.9l64-56c9-7.9%2022.6-7.9%2031.6%200l64%2056c10%208.7%2011%2023.9%202.3%2033.9s-23.9%2011-33.9%202.3L144.1%20308c1.8%2066.8%2049.1%20122.2%20111.9%20136.5V186.6c-37.3-13.2-64-48.7-64-90.5z%22/%3E%3C/svg%3E');--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22m368.5%2018.3-50.1%2050.1%20125.3%20125.3%2050.1-50.1c21.9-21.9%2021.9-57.3%200-79.2l-46.1-46.1c-21.9-21.9-57.3-21.9-79.2%200m-89.2%2078.9-.5.1-144.1%2043.2c-19.9%206-35.7%2021.2-42.3%2041L3.8%20445.8c-2.9%208.7-1.9%2018.2%202.5%2026l155.4-155.4c-1.1-4-1.6-8.1-1.6-12.4%200-26.5%2021.5-48%2048-48s48%2021.5%2048%2048-21.5%2048-48%2048c-4.3%200-8.5-.6-12.4-1.6L40.3%20505.7c7.8%204.4%2017.2%205.4%2026%202.5l264.3-88.6c19.7-6.6%2035-22.4%2041-42.3l43.2-144.1.1-.5L279.4%2097.2z%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M40%2048c-13.3%200-24%2010.7-24%2024v48c0%2013.3%2010.7%2024%2024%2024h48c13.3%200%2024-10.7%2024-24V72c0-13.3-10.7-24-24-24zm152%2016c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032h288c17.7%200%2032-14.3%2032-32s-14.3-32-32-32zm0%20160c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032h288c17.7%200%2032-14.3%2032-32s-14.3-32-32-32zm0%20160c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032h288c17.7%200%2032-14.3%2032-32s-14.3-32-32-32zM16%20232v48c0%2013.3%2010.7%2024%2024%2024h48c13.3%200%2024-10.7%2024-24v-48c0-13.3-10.7-24-24-24H40c-13.3%200-24%2010.7-24%2024m24%20136c-13.3%200-24%2010.7-24%2024v48c0%2013.3%2010.7%2024%2024%2024h48c13.3%200%2024-10.7%2024-24v-48c0-13.3-10.7-24-24-24z%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20384%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M292.9%20384c7.3-22.3%2021.9-42.5%2038.4-59.9C364%20289.7%20384%20243.2%20384%20192%20384%2086%20298%200%20192%200S0%2086%200%20192c0%2051.2%2020%2097.7%2052.7%20132.1%2016.5%2017.4%2031.2%2037.6%2038.4%2059.9h201.7zm-4.9%2048H96v16c0%2044.2%2035.8%2080%2080%2080h32c44.2%200%2080-35.8%2080-80zM184%20112c-39.8%200-72%2032.2-72%2072%200%2013.3-10.7%2024-24%2024s-24-10.7-24-24c0-66.3%2053.7-120%20120-120%2013.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M434.8%2070.1c14.3%2010.4%2017.5%2030.4%207.1%2044.7l-256%20352c-5.5%207.6-14%2012.3-23.4%2013.1s-18.5-2.7-25.1-9.3l-128-128c-12.5-12.5-12.5-32.8%200-45.3s32.8-12.5%2045.3%200l101.5%20101.5%20234-321.7c10.4-14.3%2030.4-17.5%2044.7-7.1z%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m0-336c-17.7%200-32%2014.3-32%2032%200%2013.3-10.7%2024-24%2024s-24-10.7-24-24c0-44.2%2035.8-80%2080-80s80%2035.8%2080%2080c0%2047.2-36%2067.2-56%2074.5v3.8c0%2013.3-10.7%2024-24%2024s-24-10.7-24-24v-8.1c0-20.5%2014.8-35.2%2030.1-40.2%206.4-2.1%2013.2-5.5%2018.2-10.3%204.3-4.2%207.7-10%207.7-19.6%200-17.7-14.3-32-32-32zm-32%20192a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%200c14.7%200%2028.2%208.1%2035.2%2021l216%20400c6.7%2012.4%206.4%2027.4-.8%2039.5S486.1%20480%20472%20480H40c-14.1%200-27.1-7.4-34.4-19.5s-7.5-27.1-.8-39.5l216-400c7-12.9%2020.5-21%2035.2-21m0%20168c-13.3%200-24%2010.7-24%2024v112c0%2013.3%2010.7%2024%2024%2024s24-10.7%2024-24V192c0-13.3-10.7-24-24-24m26.7%20216a26.7%2026.7%200%201%200-53.3%200%2026.7%2026.7%200%201%200%2053.3%200%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2024%2024%22%3E%3Cpath%20d%3D%22M12%203c4.97%200%209%203.58%209%208s-6%2010-9%2010-9-5.58-9-10%204.03-8%209-8m-1.69%207.93C9.29%209.29%207.47%208.58%206.25%209.34s-1.38%202.71-.36%204.35c1.03%201.64%202.85%202.35%204.07%201.59%201.22-.78%201.37-2.71.35-4.35m3.38%200c-1.02%201.64-.87%203.57.35%204.35%201.22.76%203.04.05%204.07-1.59%201.02-1.64.86-3.59-.36-4.35s-3.04-.05-4.06%201.59M12%2017.75c-2%200-2.5-.75-2.5-.75%200%20.03.5%202%202.5%202s2.5-2%202.5-2-.5.75-2.5.75%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M296%2040c0-22.1-17.9-40-40-40s-40%2017.9-40%2040c0%2044.1-53.3%2066.1-84.5%2035-15.6-15.6-40.9-15.6-56.6%200s-15.6%2040.9%200%2056.6c31.2%2031.2%209.1%2084.5-35%2084.5-22.1%200-40%2017.9-40%2040s17.9%2040%2040%2040c44.1%200%2066.1%2053.3%2035%2084.5-15.6%2015.6-15.6%2040.9%200%2056.6s40.9%2015.6%2056.6%200c31.2-31.2%2084.5-9.1%2084.5%2035%200%2022.1%2017.9%2040%2040%2040s40-17.9%2040-40c0-44.1%2053.3-66.1%2084.5-35%2015.6%2015.6%2040.9%2015.6%2056.6%200s15.6-40.9%200-56.6c-31.2-31.2-9.1-84.5%2035-84.5%2022.1%200%2040-17.9%2040-40s-17.9-40-40-40c-44.1%200-66.1-53.3-35-84.5%2015.6-15.6%2015.6-40.9%200-56.6s-40.9-15.6-56.6%200c-31.2%2031.1-84.5%209.1-84.5-35M160%20224a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200m160%2032a32%2032%200%201%201%200%2064%2032%2032%200%201%201%200-64%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M352%200c0-17.7-14.3-32-32-32s-32%2014.3-32%2032v64h-96c-53%200-96%2043-96%2096v224c0%2053%2043%2096%2096%2096h256c53%200%2096-43%2096-96V160c0-53-43-96-96-96h-96zM160%20368c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24M224%20176a48%2048%200%201%201%200%2096%2048%2048%200%201%201%200-96m144%2048a48%2048%200%201%201%2096%200%2048%2048%200%201%201-96%200m-304%200c0-17.7-14.3-32-32-32S0%20206.3%200%20224v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32zm544-32c-17.7%200-32%2014.3-32%2032v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32v-96c0-17.7-14.3-32-32-32%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M288%200H128c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032v151.5L7.5%20426.3C2.6%20435%200%20444.7%200%20454.7%200%20486.4%2025.6%20512%2057.3%20512h333.4c31.6%200%2057.3-25.6%2057.3-57.3%200-10-2.6-19.8-7.5-28.4L320%20215.5V64c17.7%200%2032-14.3%2032-32S337.7%200%20320%200zm-96%20215.5V64h64v151.5c0%2011.1%202.9%2022.1%208.4%2031.8L306%20320H142l41.6-72.7c5.5-9.7%208.4-20.6%208.4-31.8%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20576%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M419.5%2096c-16.6%200-32.7%204.5-46.8%2012.7-15.8-16-34.2-29.4-54.5-39.5%2028.2-24%2064.1-37.2%20101.3-37.2C505.9%2032%20576%20102%20576%20188.5c0%2041.5-16.5%2081.3-45.8%20110.6l-71.1%2071.1C429.8%20399.5%20390%20416%20348.5%20416%20262.1%20416%20192%20346%20192%20259.5c0-1.5%200-3%20.1-4.5.5-17.7%2015.2-31.6%2032.9-31.1s31.6%2015.2%2031.1%2032.9v2.6c0%2051.1%2041.4%2092.5%2092.5%2092.5%2024.5%200%2048-9.7%2065.4-27.1l71.1-71.1c17.3-17.3%2027.1-40.9%2027.1-65.4%200-51.1-41.4-92.5-92.5-92.5zm-144.3%2077.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5%200-48%209.7-65.4%2027.1l-71.1%2071.1C73.8%20275.5%2064%20299.1%2064%20323.6c0%2051.1%2041.4%2092.5%2092.5%2092.5%2016.5%200%2032.6-4.4%2046.7-12.6%2015.8%2016%2034.2%2029.4%2054.6%2039.5-28.2%2023.9-64%2037.2-101.3%2037.2C70.1%20480.2%200%20410.2%200%20323.7c0-41.5%2016.5-81.3%2045.8-110.6l71.1-71.1c29.3-29.3%2069.1-45.8%20110.6-45.8%2086.6%200%20156.5%2070.6%20156.5%20156.9v3.9c-.4%2017.7-15.1%2031.6-32.8%2031.2s-31.6-15.1-31.2-32.8v-2.3c0-33.7-18-63.3-44.8-79.6z%22/%3E%3C/svg%3E');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=LXGW+WenKai+Screen+GB+Screen:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"LXGW WenKai Screen GB Screen";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/timeline.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
      <link rel="stylesheet" href="../../../mkdocs/css/no-footer.css">
    
      <link rel="stylesheet" href="../../../mkdocs/css/unordered-list-symbols.css">
    
      <link rel="stylesheet" href="../../../assets/stylesheets/cards.css">
    
      <link rel="stylesheet" href="../../../assets/stylesheets/flink.css">
    
      <link rel="stylesheet" href="../../../assets/stylesheets/rounded_corner.css">
    
      <link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css">
    
      <link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css">
    
      <link rel="stylesheet" href="https://cdn.tonycrane.cc/jbmono/jetbrainsmono.css">
    
      <link rel="stylesheet" href="https://cdn.tonycrane.cc/lxgw/lxgwscreen.css">
    
      <link rel="stylesheet" href="../../../assets/stylesheets/custom.css">
    
      <link rel="stylesheet" href="../../../assets/stylesheets/tasklist.css">
    
      <link rel="stylesheet" href="../../../assets/stylesheets/fold_toc.css">
    
      <link rel="stylesheet" href="../../../assets/stylesheets/changelog_extra.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/fonts/material-icons.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/tippy/light.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/tippy/material.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/tippy/shift-away.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/tippy/scale.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/tippy/backdrop.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/tippy/tippy.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/core/core.css">
    
      <link rel="stylesheet" href="../../../assets/document_dates/user.config.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style><style>/* Ëá™ÂÆö‰πâÂàÜÈöîÁ∫øÊ†∑Âºè */
hr.custom-hr {
    position: relative;
    text-align: center;
    border: 0;
    height: 1px;
    background: none;
    /* ‰ΩøÁî®‰∏ªÈ¢òÁöÑËæπÊ°ÜÈ¢úËâ≤ */
    border-top: 2px solid var(--md-default-fg-color--lighter, currentColor);
    margin: 100px 0 !important;
}

hr.custom-hr:before {
    content: attr(data-content);
    position: absolute;
    /* ‰ΩøÁî®‰∏ªÈ¢òÁöÑËÉåÊôØËâ≤Ôºåfallback Âà∞È°µÈù¢ËÉåÊôØËâ≤ */
    background: var(--md-default-bg-color, var(--md-primary-bg-color, inherit));
    /* ‰ΩøÁî®‰∏ªÈ¢òÁöÑÊñáÊú¨È¢úËâ≤ */
    color: var(--md-default-fg-color, currentColor);
    padding: 0 0.5em;
    line-height: 1;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    font-size: 2em;
    font-weight: bold;
}

/* ÁõÆÂΩï‰∏≠ÂàÜÈöîÁ∫øÊù°ÁõÆÁöÑÊ†∑Âºè */
.md-nav__item.hr-nav-item {
    text-align: center;
    margin: 1.16em 0;
    /* Â¢ûÂä†‰∏ä‰∏ãÈó¥Ë∑ù */
}

.md-nav__item.hr-nav-item .md-nav__link {
    justify-content: center;
    font-weight: bold;
    font-size: 1.1em;
    color: var(--md-typeset-color) !important;
}

/* ÊÇ¨ÂÅúÊïàÊûú */
.md-nav__item.hr-nav-item .md-nav__link:hover {
    color: var(--md-accent-fg-color) !important;
}

/* ‰øÆÊîπÂàÜÈöîÁ∫øÁöÑÊ†∑Âºè */
.md-content__inner hr.custom-hr {
    border: none !important;
    height: 2px !important;
    background-color: var(--md-typeset-color) !important;
    margin: 8em 0 !important;
    /* Â¢ûÂä†ÂàÜÈöîÁ∫øÁöÑ‰∏ä‰∏ãÈó¥Ë∑ù */
}

/* Á°Æ‰øùÂàÜÈöîÁ∫øÊñáÂ≠óÂèØËßÅ */
.md-content__inner hr.custom-hr[data-content]::before {
    color: var(--md-typeset-color) !important;
}</style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="green" data-md-color-accent="light-green">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#2022ltsf-linear" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Ê∫∂err" class="md-header__button md-logo" aria-label="Ê∫∂err" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 8C8 10 5.9 16.17 3.82 21.34l1.89.66.95-2.3c.48.17.98.3 1.34.3C19 20 22 3 22 3c-1 2-8 2.25-13 3.25S2 11.5 2 13.5s1.75 3.75 1.75 3.75C7 8 17 8 17 8"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ê∫∂err
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2022„ÄÅLTSF-Linear
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="green" data-md-color-accent="light-green" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/dearRongerr/Rongerr.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
  </div>
  <div class="md-source__repository">
    Rongerr.github.io
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  üçÉ Welcome

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../sticks/5_preReproduction/" class="md-tabs__link">
          
  
  
    
  
  ‰æøÁ≠æ

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../bagu/" class="md-tabs__link">
          
  
  
    
  
  Èù¢ËØï

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Error/github/" class="md-tabs__link">
          
  
  
    
  
  Êçâ‰∏™Ëô´

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../learning/" class="md-tabs__link">
          
  
  
    
  
  Ê∑±Â∫¶Â≠¶‰π†

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  
    
  
  ÊñáÁåÆ

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Reproduction/" class="md-tabs__link">
          
  
  
    
  
  ËÆ∫ÊñáÂ§çÁé∞

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../CodeRepo/" class="md-tabs__link">
          
  
  
    
  
  python

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Statistics/" class="md-tabs__link">
          
  
  
    
  
  ÁªüËÆ°Â≠¶

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../logs/" class="md-tabs__link">
          
  
  
    
  
  Êî∂Á∫≥ÁÆ±

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Ê∫∂err" class="md-nav__button md-logo" aria-label="Ê∫∂err" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 8C8 10 5.9 16.17 3.82 21.34l1.89.66.95-2.3c.48.17.98.3 1.34.3C19 20 22 3 22 3c-1 2-8 2.25-13 3.25S2 11.5 2 13.5s1.75 3.75 1.75 3.75C7 8 17 8 17 8"></path></svg>

    </a>
    Ê∫∂err
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dearRongerr/Rongerr.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
  </div>
  <div class="md-source__repository">
    Rongerr.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    üçÉ Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2">
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ‰æøÁ≠æ
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            ‰æøÁ≠æ
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1">
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ÈÄêÊ≠•Á≥ªÂàó
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            ÈÄêÊ≠•Á≥ªÂàó
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/5_preReproduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    (ÈÄêÊ≠•Á≥ªÂàó)Â§çÁé∞‰πãÂâç
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/6_0_docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    (ÈÄêÊ≠•Á≥ªÂàó)docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/1_github_v3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    (ÈÄêÊ≠•Á≥ªÂàó) git
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/mkdocs_learn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MkDocs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/markdwon_learn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    markdown
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/latex/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LaTex
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/MacOS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MacOS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/shell/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/linux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    linux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/screen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    screen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/writting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÂÜô‰Ωú
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/3_vscode/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VSCode
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/4_mkdocs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    mkdocs Â∏∏Áî®ÂëΩ‰ª§
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_13">
        
          
          <label class="md-nav__link" for="__nav_2_13" id="__nav_2_13_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Git
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_13">
            <span class="md-nav__icon md-icon"></span>
            Git
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/GitHub/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GitÂ≠¶‰π†
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/1_github_v0/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    git init
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/1_github_v1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Git(clone‰ª•Âêé)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/1_github_v2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    gitÂàÜÊîØ‰∏éÂäüËÉΩÊµãËØï
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../bagu/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Èù¢ËØï
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Èù¢ËØï
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2">
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    È¢òÁõÆ
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            È¢òÁõÆ
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/questions/1_questions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Èù¢ËØïÈóÆÈ¢ò
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../bagu/leetcode/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    ÂäõÊâ£
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            ÂäõÊâ£
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/leetcode/1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1 ‰∏§Êï∞‰πãÂíå
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/leetcode/2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2 ‰∏§Êï∞Áõ∏Âä†
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_4">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../bagu/deeplearning/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Ê∑±Â∫¶Â≠¶‰π†
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Ê∑±Â∫¶Â≠¶‰π†
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/deeplearning/transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÊâãÊíïTransformer‰ª£Á†Å
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/deeplearning/pytorch_shape_function/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    pytorchÁöÑÁª¥Â∫¶ÂèòÊç¢ÂáΩÊï∞
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/deeplearning/1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    visionTransformer‰ª£Á†Å
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_5">
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Êú∫Âô®Â≠¶‰π†
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            Êú∫Âô®Â≠¶‰π†
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/machinelearning/kmeans/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÊâãÊíïkmeans
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/machinelearning/2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÊâãÊíïÂèçÂêë‰º†Êí≠
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4">
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Êçâ‰∏™Ëô´
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Êçâ‰∏™Ëô´
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/github/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    github
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/latex/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latex
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/macos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    macOS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/Typora_1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PicGo+Typora+GithubÂõæÂ∫äËÆæÁΩÆ
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../learning/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Ê∑±Â∫¶Â≠¶‰π†
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Ê∑±Â∫¶Â≠¶‰π†
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/17_1_SENet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SENet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/17_ManBa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mamba
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/16_KAN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KAN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/15_CausualConv/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Âõ†ÊûúÂç∑ÁßØ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/3_ViT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ViT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/1_clip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/2_MOCO/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MOCO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/2_python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ‰∏ãÂàíÁ∫øÊñπÊ≥ï &amp; ËôöÊãüÁéØÂ¢É
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÂõæËß£LayerNorm &amp; BatchNorm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5ÁßçÂΩí‰∏ÄÂåñÊñπÊ≥ï
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/vit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vision TransformerÁöÑÂéüÁêÜ‰∏éÈöæÁÇπÊ∫êÁ†ÅÂÆûÁé∞
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/pe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4Áßç‰ΩçÁΩÆÁºñÁ†Å
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/convs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Âç∑ÁßØ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÊùéÊ≤ê ÁõÆÊ†áÊ£ÄÊµãÈÉ®ÂàÜ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/4_GAN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GAN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/5_Bert/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BERT‰ªéÈõ∂ËØ¶ÁªÜËß£ËØª
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/6_Diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/6_Diffusion1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/8_WeightNorm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    WeightNorm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/9_cGAN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GAN Âèò‰Ωì
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/12_KLdivergence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KL divergence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/13_RNN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/14_LSTM/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LSTM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/0_pdfNotes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    üìí
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    ÊñáÁåÆ
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            ÊñáÁåÆ
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµã
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_2" id="__nav_6_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6_2">
            <span class="md-nav__icon md-icon"></span>
            Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµã
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0_note/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Êó∂Â∫èÈ¢ÑÊµãÂü∫Á°Ä
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_SegRNN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2023„ÄÅSegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    2022„ÄÅLTSF-Linear
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    2022„ÄÅLTSF-Linear
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    <span class="md-ellipsis">
      Abstract
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      1. Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contributions" class="md-nav__link">
    <span class="md-ellipsis">
      contributions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-preliminaries-tsf-problem-formulation" class="md-nav__link">
    <span class="md-ellipsis">
      2. Preliminaries: TSF Problem Formulation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-transformer-based-ltsf-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      3. Transformer-Based LTSF Solutions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-an-embarrassingly-simple-baseline" class="md-nav__link">
    <span class="md-ellipsis">
      4. An Embarrassingly Simple Baseline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      5. Experiments
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-experimental-settings" class="md-nav__link">
    <span class="md-ellipsis">
      5.1. Experimental Settings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-comparison-with-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      5.2. Comparison with Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-more-analyses-on-ltsf-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      5.3. More Analyses on LTSF-Transformers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.3. More Analyses on LTSF-Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      (1)ÂõûÊ∫ØÁ™óÂè£ÁöÑÈïøÂ∫¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2close-input-far-input" class="md-nav__link">
    <span class="md-ellipsis">
      (2)close input &amp; far input
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      (3)Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊòØÂê¶ÊúâÁî®
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      (4)‰ΩçÁΩÆ‰ø°ÊÅØÁöÑ‰øùÁïô
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      (5)ËÆ®ËÆ∫‰∏çÂêåÁöÑÂµåÂÖ•Á≠ñÁï•
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      (6)Êï∞ÊçÆÈõÜÁöÑËßÑÊ®°
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      (7)ÊïàÁéáÁúüÁöÑÂæàÈáçË¶ÅÂêóÔºü
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-conclusion-and-future-work" class="md-nav__link">
    <span class="md-ellipsis">
      6. Conclusion and Future Work
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Appendix">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-related-work-non-transformer-based-tsf-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      A. Related Work: Non-Transformer-Based TSF Solutions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-experimental-details" class="md-nav__link">
    <span class="md-ellipsis">
      B. Experimental Details
    </span>
  </a>
  
    <nav class="md-nav" aria-label="B. Experimental Details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#b1-data-descriptions" class="md-nav__link">
    <span class="md-ellipsis">
      B.1. Data Descriptions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b2-implementation-details" class="md-nav__link">
    <span class="md-ellipsis">
      B.2. Implementation Details
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-additional-comparison-with-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      C. Additional Comparison with Transformers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="C. Additional Comparison with Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#c1-comparison-of-univariate-forecasting" class="md-nav__link">
    <span class="md-ellipsis">
      C.1. Comparison of Univariate Forecasting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c2-comparison-under-different-look-back-windows" class="md-nav__link">
    <span class="md-ellipsis">
      C.2. Comparison under Different Look-back Windows
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-ablation-study-on-the-ltsf-linear" class="md-nav__link">
    <span class="md-ellipsis">
      D. Ablation study on the LTSF-Linear
    </span>
  </a>
  
    <nav class="md-nav" aria-label="D. Ablation study on the LTSF-Linear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#d1-motivation-of-nlinear" class="md-nav__link">
    <span class="md-ellipsis">
      D.1. Motivation of NLinear
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d2-the-features-of-ltsf-linear" class="md-nav__link">
    <span class="md-ellipsis">
      D.2. The Features of LTSF-Linear
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d3-interpretability-of-ltsf-linear" class="md-nav__link">
    <span class="md-ellipsis">
      D.3. Interpretability of LTSF-Linear
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_TimesNet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2023„ÄÅTimesNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_Informer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2021„ÄÅ Informer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_Autoformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2021„ÄÅAutoformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_UnetTSF/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2024„ÄÅUnetTSF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_SCINet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2022„ÄÅSCINet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_PatchTST/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2023„ÄÅPatchTST
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9_Pyraformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2022„ÄÅPyraformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_TimesMixer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2024„ÄÅTimeMixer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_Fedformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2022„ÄÅFEDformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_WITRAN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2023„ÄÅWITRAN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_TFB/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TFB
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_TSLib/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TSLib
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6_3">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../ObejectCounting/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    ÁõÆÊ†áËÆ°Êï∞
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_3" id="__nav_6_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_3">
            <span class="md-nav__icon md-icon"></span>
            ÁõÆÊ†áËÆ°Êï∞
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank1%20CountGD/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank1 CountGD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank2%20GeCo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank2 GeCo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank3%20DAVE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank3 DAVE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank4%20CACViT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank4 CACViT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank5%20SSD/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank5 SSD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank6%20LOCA/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank6 LOCA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank7%20SemAug_CountTR/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank7 SemAug CountTR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank8%20CounTR/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank8 CounTR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank9%20SemAug_SAFECount/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank9 SemAug SAFECount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank10%20SPDCN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank10 SPDCN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank11%20GCA_SUN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank11 GCA SUN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank12%20SAFECount/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    rank12 SAFECount
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
          
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6_4">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../ObjectDetection/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    ÁõÆÊ†áÊ£ÄÊµã
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_4" id="__nav_6_4_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_4">
            <span class="md-nav__icon md-icon"></span>
            ÁõÆÊ†áÊ£ÄÊµã
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObjectDetection/2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÁõÆÊ†áÊ£ÄÊµãÂü∫Á°ÄÁü•ËØÜ
    
  </span>
  

      </a>
    </li>
  

              
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObjectDetection/1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DETRËÆ∫ÊñáÁ≥ªÂàó
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObjectDetection/3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÔºàDETRÔºâEnd-to-End Object Detection with Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Reproduction/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    ËÆ∫ÊñáÂ§çÁé∞
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_7" id="__nav_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            ËÆ∫ÊñáÂ§çÁé∞
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/7_summary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ê±áÊÄªÂ§çÁé∞Ë∞ÉÁî®Âõæ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/DAVE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DAVEÂ§çÁé∞
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/5_SegRNN_index/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/5_SegRNN_v1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Â§çÁé∞SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/5_SegRNN_v2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÔºàË°•ÂÖÖÔºâÂ§çÁé∞ SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/5_SegRNN_v3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    (ÊâãÂÜôÁ¨îËÆ∞)SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/5_SegRNN_v4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SegRNNÂÆûÈ™åËøáÁ®ã
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/6_AutoFormer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/6_AutoFormer_v1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    (Áª≠) Autoformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/6_AutoFormer_v2_eg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoFormer ÂèØËßÜÂåñÁªìÊûú
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/8_ThuML/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    thumlÂ§çÁé∞
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/9_WITRAN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    WITRAN‰ª£Á†ÅÂ§çÁé∞
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/10_UNetTSF/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNetTSF
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../CodeRepo/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    python
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_8" id="__nav_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            python
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CodeRepo/4_PreTrained/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    È¢ÑËÆ≠ÁªÉÊùÉÈáç
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CodeRepo/3_args/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Á±ªÁöÑÂàùÂßãÂåñ‰º†ÂÖ•ÂëΩ‰ª§Ë°åÂèÇÊï∞
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CodeRepo/2_SHHtensorboard/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ËøúÁ®ãÊúçÂä°Âô®&amp;tensorboard
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CodeRepo/2_0_tensorboad/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    tensorboardÂèØËßÜÂåñ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CodeRepo/4_PreTrained/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    È¢ÑËÆ≠ÁªÉÊùÉÈáç
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CodeRepo/1_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Êü•Áúã pytorch ÁΩëÁªúÁªìÊûÑ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/CodeRepo/4_self_/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ëá™ÊÄªÔºöÊâíÊ®°Âùó
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ‰∏Ä‰∫õÊ®°Âùó
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÁâπÂæÅËûçÂêàÊñπÂºè
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ‰∏Ä‰∫õÊÑüÊÇü
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/CodeRepo/1_MultiHeadAttention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÂΩ¢Áä∂ÂèòÂåñ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/CodeRepo/2_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ‰ªéÁé∞ÂÆûÁîüÊ¥ªÁöÑËßíÂ∫¶Áúã Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/CodeRepo/3_fourier/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DFT‰æãÈ¢ò
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reproduction/CodeRepo/1_0_Autoformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autoformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Statistics/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    ÁªüËÆ°Â≠¶
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_9" id="__nav_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            ÁªüËÆ°Â≠¶
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Statistics/1_FFT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FourierÁ∫ßÊï∞
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Statistics/2_FFT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FourierÂü∫Á°ÄÁü•ËØÜ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Statistics/1_0_fourier/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Â§çÂπ≥Èù¢ÊóãËΩ¨&amp;DFT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Statistics/1_1_fourier/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Áõ¥ËßÇÁêÜËß£ÂÇÖÈáåÂè∂ÂèòÊç¢
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Statistics/1_2_signal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ‰ø°Âè∑ÁöÑÂêàÊàê‰∏éÂàÜËß£
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Statistics/1_4_signal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FS„ÄÅFT„ÄÅDTFS„ÄÅDTFT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Statistics/1_3_complexExp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Â§çÊåáÊï∞ÁöÑÊÄßË¥®
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_10">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../logs/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Êî∂Á∫≥ÁÆ±
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10" id="__nav_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Êî∂Á∫≥ÁÆ±
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/1_date/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ‰∏Ä‰∫õÊó•Êúü
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/2_updatelog/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Êõ¥Êñ∞Êó•Âøó
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/4_flink/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Êõ¥Â§öÈìæÊé•
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/6_waline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÁïÖÊâÄÊ¨≤Ë®Ä
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/7_backup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_10_7">
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../logs/1_0_diary/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    daily
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10_7" id="__nav_10_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_7">
            <span class="md-nav__icon md-icon"></span>
            daily
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/1_0_diary/2025/0_Question/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ÈóÆÈ¢òÊ∏ÖÂçï
    
  </span>
  

      </a>
    </li>
  

              
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/1_0_diary/2025/03/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3 Êúà
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/1_0_diary/2025/04/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4Êúà
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/diary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Êõ¥Êó©‰ª•Ââç
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    <span class="md-ellipsis">
      Abstract
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      1. Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contributions" class="md-nav__link">
    <span class="md-ellipsis">
      contributions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-preliminaries-tsf-problem-formulation" class="md-nav__link">
    <span class="md-ellipsis">
      2. Preliminaries: TSF Problem Formulation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-transformer-based-ltsf-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      3. Transformer-Based LTSF Solutions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-an-embarrassingly-simple-baseline" class="md-nav__link">
    <span class="md-ellipsis">
      4. An Embarrassingly Simple Baseline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      5. Experiments
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-experimental-settings" class="md-nav__link">
    <span class="md-ellipsis">
      5.1. Experimental Settings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-comparison-with-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      5.2. Comparison with Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-more-analyses-on-ltsf-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      5.3. More Analyses on LTSF-Transformers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.3. More Analyses on LTSF-Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      (1)ÂõûÊ∫ØÁ™óÂè£ÁöÑÈïøÂ∫¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2close-input-far-input" class="md-nav__link">
    <span class="md-ellipsis">
      (2)close input &amp; far input
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      (3)Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊòØÂê¶ÊúâÁî®
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      (4)‰ΩçÁΩÆ‰ø°ÊÅØÁöÑ‰øùÁïô
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      (5)ËÆ®ËÆ∫‰∏çÂêåÁöÑÂµåÂÖ•Á≠ñÁï•
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      (6)Êï∞ÊçÆÈõÜÁöÑËßÑÊ®°
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      (7)ÊïàÁéáÁúüÁöÑÂæàÈáçË¶ÅÂêóÔºü
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-conclusion-and-future-work" class="md-nav__link">
    <span class="md-ellipsis">
      6. Conclusion and Future Work
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Appendix">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-related-work-non-transformer-based-tsf-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      A. Related Work: Non-Transformer-Based TSF Solutions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-experimental-details" class="md-nav__link">
    <span class="md-ellipsis">
      B. Experimental Details
    </span>
  </a>
  
    <nav class="md-nav" aria-label="B. Experimental Details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#b1-data-descriptions" class="md-nav__link">
    <span class="md-ellipsis">
      B.1. Data Descriptions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b2-implementation-details" class="md-nav__link">
    <span class="md-ellipsis">
      B.2. Implementation Details
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-additional-comparison-with-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      C. Additional Comparison with Transformers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="C. Additional Comparison with Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#c1-comparison-of-univariate-forecasting" class="md-nav__link">
    <span class="md-ellipsis">
      C.1. Comparison of Univariate Forecasting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c2-comparison-under-different-look-back-windows" class="md-nav__link">
    <span class="md-ellipsis">
      C.2. Comparison under Different Look-back Windows
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-ablation-study-on-the-ltsf-linear" class="md-nav__link">
    <span class="md-ellipsis">
      D. Ablation study on the LTSF-Linear
    </span>
  </a>
  
    <nav class="md-nav" aria-label="D. Ablation study on the LTSF-Linear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#d1-motivation-of-nlinear" class="md-nav__link">
    <span class="md-ellipsis">
      D.1. Motivation of NLinear
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d2-the-features-of-ltsf-linear" class="md-nav__link">
    <span class="md-ellipsis">
      D.2. The Features of LTSF-Linear
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d3-interpretability-of-ltsf-linear" class="md-nav__link">
    <span class="md-ellipsis">
      D.3. Interpretability of LTSF-Linear
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                

                  
  
    <a href="https://github.com/dearRongerr/Rongerr.github.io/edit/main/docs/literature/TSP/2_DLinear.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/dearRongerr/Rongerr.github.io/raw/main/docs/literature/TSP/2_DLinear.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  




<h1 id="2022ltsf-linear">2022„ÄÅLTSF-Linear<a class="headerlink" href="#2022ltsf-linear" title="Permanent link">¬∂</a></h1>
<div class="document-dates-plugin-wrapper document-dates-top"><div class="document-dates-plugin" locale="zh"><span data-tippy-content data-tippy-raw="2025-03-04"><span class="material-icons" data-icon="doc_created"></span><time datetime="2025-03-04T21:01:00+08:00">2025-03-04 21:01:00</time></span><span data-tippy-content data-tippy-raw="2025-09-28"><span class="material-icons" data-icon="doc_modified"></span><time datetime="2025-09-28T13:09:55.983169+00:00">2025-09-28 13:09:55</time></span><span class="material-icons" data-icon="doc_author"></span><div class="avatar-group"><div class="avatar-wrapper" data-name="dearr" data-tippy-content data-tippy-raw="<a href=&quot;mailto:1939472345@qq.com&quot;>dearr</a>"><span class="avatar-text"></span><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://avatars.githubusercontent.com/dearRongerr" data-desc-position="bottom"><img class="avatar" src="https://avatars.githubusercontent.com/dearRongerr"></a></div></div></div></div>
<div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;">
<p><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> Á∫¶ 20999 ‰∏™Â≠ó <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 17H7V3h14m0-2H7a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2M3 5H1v16a2 2 0 0 0 2 2h16v-2H3m12.96-10.71-2.75 3.54-1.96-2.36L8.5 15h11z"></path></svg></span> 20 Âº†ÂõæÁâá <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> È¢ÑËÆ°ÈòÖËØªÊó∂Èó¥ 105 ÂàÜÈíü</p>
</div>
<p>ÂéüÊñáÔºö<a href="https://arxiv.org/pdf/2205.13504">Are Transformers Effective for Time Series Forecasting?</a> </p>
<p>Ê∫êÁ†ÅÔºö<a href="https://github.com/cure-lab/LTSF-Linear">https://github.com/cure-lab/LTSF-Linear</a></p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="../images/image-20250302165234615.png" data-desc-position="bottom"><img alt="image-20250302165234615" src="../images/image-20250302165234615.png"></a></p>
<p><a href="https://zhuanlan.zhihu.com/p/682796181">Êó∂Â∫èÈ¢ÑÊµãÁØá-DLinear&amp;NLinearÈòÖËØªÁ¨îËÆ∞</a></p>
<h2 id="abstract">Abstract<a class="headerlink" href="#abstract" title="Permanent link">¬∂</a></h2>
<p>Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. </p>
<p>ËøëÊúüÔºåÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÂú®ÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâ‰ªªÂä°‰∏≠ÂëàÁé∞Âá∫ËøÖÁåõÂèëÂ±ïÁöÑÊÄÅÂäø„ÄÇÂ∞ΩÁÆ°ËøáÂéªÂá†Âπ¥ÂÖ∂ÊÄßËÉΩ‰∏çÊñ≠ÊèêÂçáÔºå‰ΩÜÊàë‰ª¨Âú®Êú¨Á†îÁ©∂‰∏≠ÂØπËØ•Á†îÁ©∂ÊñπÂêëÁöÑÊúâÊïàÊÄßÊèêÂá∫‰∫ÜË¥®Áñë„ÄÇ</p>
<p>Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. </p>
<p>ÂÖ∑‰ΩìËÄåË®ÄÔºåTransformerÊó†ÁñëÊòØÊèêÂèñÈïøÂ∫èÂàó‰∏≠ÂÖÉÁ¥†‰πãÈó¥ËØ≠‰πâÂÖ≥ËÅîÊúÄ‰∏∫ÊàêÂäüÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÂú®Êó∂Èó¥Â∫èÂàóÂª∫Ê®°‰∏≠ÔºåÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØ‰ªéËøûÁª≠ÁÇπÁöÑÊúâÂ∫èÈõÜÂêà‰∏≠ÊèêÂèñÊó∂Èó¥ÂÖ≥Á≥ª„ÄÇ</p>
<p>While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss.</p>
<p>ËôΩÁÑ∂Âú®Transformer‰∏≠‰ΩøÁî®‰ΩçÁΩÆÁºñÁ†Å‰ª•ÂèäÂà©Áî®Ê†áËÆ∞ÂµåÂÖ•Â≠êÂ∫èÂàóÊúâÂä©‰∫é‰øùÁïôÈÉ®ÂàÜÈ°∫Â∫è‰ø°ÊÅØÔºå‰ΩÜÊéíÂàó‰∏çÂèòÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂõ∫ÊúâÁâπÊÄß‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÂØºËá¥Êó∂Èó¥‰ø°ÊÅØÁöÑ‰∏¢Â§±„ÄÇ</p>
<p>To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. </p>
<p>Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: <a href="https://github.com/cure-lab/LTSFLinear">https://github.com/cure-lab/LTSFLinear</a>.</p>
<p>‰∏∫‰∫ÜÈ™åËØÅÊàë‰ª¨ÁöÑËßÇÁÇπÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁªÑÊûÅÂÖ∂ÁÆÄÂçïÁöÑÂçïÂ±ÇÁ∫øÊÄßÊ®°ÂûãÔºåÂëΩÂêç‰∏∫ LTSF-LinearÔºåÁî®‰∫éÂØπÊØî„ÄÇÂú®‰πù‰∏™ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLTSF-Linear ÊÑèÂ§ñÂú∞Âú®ÊâÄÊúâÊÉÖÂÜµ‰∏ãÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÂ§çÊùÇÂü∫‰∫é Transformer ÁöÑ LTSF Ê®°ÂûãÔºå‰∏îÈÄöÂ∏∏‰ºòÂäøÊòæËëó„ÄÇ</p>
<p>Ê≠§Â§ñÔºåÊàë‰ª¨ËøòËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂÆûËØÅÁ†îÁ©∂Ôºå‰ª•Êé¢Á¥¢ LTSF Ê®°ÂûãÁöÑÂêÑÁßçËÆæËÆ°ÂÖÉÁ¥†ÂØπÂÖ∂Êó∂Èó¥ÂÖ≥Á≥ªÊèêÂèñËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨Â∏åÊúõËøô‰∏Ä‰ª§‰∫∫ÊÑèÂ§ñÁöÑÂèëÁé∞ËÉΩÂ§ü‰∏∫ LTSF ‰ªªÂä°ÂºÄËæüÊñ∞ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊàë‰ª¨ËøòÂÄ°ÂØºÂú®Êú™Êù•ÈáçÊñ∞ÂÆ°ËßÜÂü∫‰∫é Transformer ÁöÑËß£ÂÜ≥ÊñπÊ°àÂú®ÂÖ∂‰ªñÊó∂Èó¥Â∫èÂàóÂàÜÊûê‰ªªÂä°Ôºà‰æãÂ¶ÇÂºÇÂ∏∏Ê£ÄÊµãÔºâ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ‰ª£Á†ÅÂèØÂú®‰ª•‰∏ãÈìæÊé•Ëé∑ÂèñÔºö<a href="https://github.com/cure-lab/LTSFLinear">https://github.com/cure-lab/LTSFLinear</a>„ÄÇ</p>
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">¬∂</a></h2>
<p>Time series are ubiquitous in today‚Äôs data-driven world. Given historical data, time series forecasting (TSF) is a long-standing task that has a wide range of applications, including but not limited to traffic flow estimation, energy management, and financial investment. Over the past several decades, TSF solutions have undergone a progression from traditional statistical methods (e.g., ARIMA [1]) and machine learning techniques (e.g., GBRT [11]) to deep learning-based solutions, e.g., Recurrent Neural Networks [15] and Temporal Convolutional Networks [3, 17].</p>
<p>Âú®ÂΩì‰ªäÊï∞ÊçÆÈ©±Âä®ÁöÑ‰∏ñÁïå‰∏≠ÔºåÊó∂Èó¥Â∫èÂàóÊó†Â§Ñ‰∏çÂú®„ÄÇÂü∫‰∫éÂéÜÂè≤Êï∞ÊçÆÔºåÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâÊòØ‰∏ÄÈ°πÁî±Êù•Â∑≤‰πÖÁöÑ‰ªªÂä°ÔºåÂÖ∂Â∫îÁî®ËåÉÂõ¥ÊûÅ‰∏∫ÂπøÊ≥õÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é‰∫§ÈÄöÊµÅÈáè‰º∞ËÆ°„ÄÅËÉΩÊ∫êÁÆ°ÁêÜÂíåÈáëËûçÊäïËµÑÁ≠âÈ¢ÜÂüü„ÄÇÂú®ËøáÂéªÂá†ÂçÅÂπ¥Èó¥ÔºåÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑËß£ÂÜ≥ÊñπÊ°àÁªèÂéÜ‰∫Ü‰ªé‰º†ÁªüÁöÑÁªüËÆ°ÊñπÊ≥ïÔºà‰æãÂ¶ÇARIMAÔºâÂíåÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÔºà‰æãÂ¶ÇGBRTÔºâÂà∞Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºà‰æãÂ¶ÇÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÂíåÊó∂Èó¥Âç∑ÁßØÁΩëÁªúÔºâÁöÑÂèëÂ±ïÊºîÂèò„ÄÇ</p>
<p>Transformer [26] is arguably the most successful sequence modeling architecture, demonstrating unparalleled performances in various applications, such as natural language processing (NLP) [7], speech recognition [8], and computer vision [19, 29]. Recently, there has also been a surge of Transformer-based solutions for time series analysis, as surveyed in [27]. Most notable models, which focus on the less explored and challenging long-term time series forecasting (LTSF) problem, include LogTrans [16] (NeurIPS 2019), Informer [30] (AAAI 2021 Best paper), Autoformer [28] (NeurIPS 2021), Pyraformer [18] (ICLR 2022 Oral), Triformer [5] (IJCAI 2022) and the recent FEDformer [31] (ICML 2022).</p>
<p>TransformerÊó†ÁñëÊòØÂ∫èÂàóÂª∫Ê®°È¢ÜÂüüÊúÄ‰∏∫ÊàêÂäüÁöÑÊû∂ÊûÑÔºåÂú®ËØ∏Â§öÂ∫îÁî®‰∏≠Â±ïÁé∞Âá∫‰∫ÜÊó†‰∏é‰º¶ÊØîÁöÑÊÄßËÉΩÔºå‰æãÂ¶ÇËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ„ÄÅËØ≠Èü≥ËØÜÂà´‰ª•ÂèäËÆ°ÁÆóÊú∫ËßÜËßâÁ≠â„ÄÇËøëÊúüÔºåÂü∫‰∫éTransformerÁöÑÊó∂Èó¥Â∫èÂàóÂàÜÊûêËß£ÂÜ≥ÊñπÊ°à‰πüÂëàÁé∞Âá∫ËøÖÁåõÂèëÂ±ïÁöÑÊÄÅÂäøÔºåÁõ∏ÂÖ≥Á†îÁ©∂ÁªºËø∞ÂèØËßÅ‰∫éÊñáÁåÆ[27]„ÄÇÂÖ∂‰∏≠Ôºå‰∏ìÊ≥®‰∫éÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâËøô‰∏ÄÂ∞öÊú™ÂÖÖÂàÜÊé¢Á¥¢‰∏îÊûÅÂÖ∑ÊåëÊàòÊÄßÁöÑÈóÆÈ¢òÁöÑÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑÊ®°ÂûãÂåÖÊã¨LogTrans[16]ÔºàNeurIPS 2019Ôºâ„ÄÅInformer[30]ÔºàAAAI 2021ÊúÄ‰Ω≥ËÆ∫ÊñáÔºâ„ÄÅAutoformer[28]ÔºàNeurIPS 2021Ôºâ„ÄÅPyraformer[18]ÔºàICLR 2022Âè£Â§¥Êä•ÂëäÔºâ„ÄÅTriformer[5]ÔºàIJCAI 2022Ôºâ‰ª•ÂèäÊúÄËøëÁöÑFEDformer[31]ÔºàICML 2022Ôºâ„ÄÇ</p>
<p>The main working power of Transformers is from its multi-head self-attention mechanism, which has a remarkable capability of extracting semantic correlations among elements in a long sequence (e.g., words in texts or 2D patches in images). However, self-attention is permutationinvariant and ‚Äúanti-order‚Äù to some extent. While using various types of positional encoding techniques can preserve some ordering information, it is still inevitable to have temporal information loss after applying self-attention on top of them. </p>
<p>TransformerÁöÑ‰∏ªË¶ÅÂ∑•‰ΩúÂéüÁêÜÊ∫ê‰∫éÂÖ∂Â§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËØ•Êú∫Âà∂Âú®ÊèêÂèñÈïøÂ∫èÂàó‰∏≠ÂÖÉÁ¥†‰πãÈó¥ÁöÑËØ≠‰πâÂÖ≥ËÅîÔºà‰æãÂ¶ÇÊñáÊú¨‰∏≠ÁöÑËØçËØ≠ÊàñÂõæÂÉè‰∏≠ÁöÑ‰∫åÁª¥ÂùóÔºâÊñπÈù¢ÂÖ∑ÊúâÊòæËëóÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊòØÊéíÂàó‰∏çÂèòÁöÑÔºåÂπ∂‰∏îÂú®ÊüêÁßçÁ®ãÂ∫¶‰∏äÊòØ‚ÄúÂèçÂ∫è‚ÄùÁöÑ„ÄÇÂ∞ΩÁÆ°‰ΩøÁî®ÂêÑÁßçÁ±ªÂûãÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊäÄÊúØÂèØ‰ª•‰øùÁïô‰∏Ä‰∫õÈ°∫Â∫è‰ø°ÊÅØÔºå‰ΩÜÂú®Â∫îÁî®Ëá™Ê≥®ÊÑèÂäõ‰πãÂêéÔºå‰ªçÁÑ∂‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÂØºËá¥Êó∂Èó¥‰ø°ÊÅØÁöÑ‰∏¢Â§±„ÄÇ</p>
<p>This is usually not a serious concern for semanticrich applications such as NLP, e.g., the semantic meaning of a sentence is largely preserved even if we reorder some words in it. However, when analyzing time series data, there is usually a lack of semantics in the numerical data itself, and we are mainly interested in modeling the temporal changes among a continuous set of points. That is, the order itself plays the most crucial role. Consequently, we pose the following intriguing question: <strong>Are Transformers really effective for long-term time series forecasting?</strong></p>
<p>ÂØπ‰∫éËØ≠‰πâ‰∏∞ÂØåÁöÑÂ∫îÁî®ÔºàÂ¶ÇËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºâÔºåËøôÈÄöÂ∏∏‰∏çÊòØ‰∏•ÈáçÁöÑÈóÆÈ¢òÔºå‰æãÂ¶ÇÔºåÂç≥‰ΩøÊàë‰ª¨ÈáçÊñ∞ÊéíÂàóÂè•Â≠ê‰∏≠ÁöÑ‰∏Ä‰∫õËØçËØ≠ÔºåÂè•Â≠êÁöÑËØ≠‰πâÊÑè‰πâ‰ªçÁÑ∂Âæó‰ª•Â§ßÈÉ®ÂàÜ‰øùÁïô„ÄÇÁÑ∂ËÄåÔºåÂú®ÂàÜÊûêÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÊó∂ÔºåÊï∞ÂÄºÊï∞ÊçÆÊú¨Ë∫´ÈÄöÂ∏∏Áº∫‰πèËØ≠‰πâÔºåËÄåÊàë‰ª¨‰∏ªË¶ÅÂÖ≥Ê≥®ÁöÑÊòØÂª∫Ê®°ËøûÁª≠ÁÇπÈõÜ‰πãÈó¥ÁöÑÊó∂Èó¥ÂèòÂåñ„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÈ°∫Â∫èÊú¨Ë∫´Ëµ∑ÁùÄÊúÄÂÖ≥ÈîÆÁöÑ‰ΩúÁî®„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰ª•‰∏ãÂºï‰∫∫ÂÖ•ËÉúÁöÑÈóÆÈ¢òÔºöTransformerÊòØÂê¶ÁúüÁöÑÈÄÇÁî®‰∫éÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºü</p>
<p>Moreover, while existing Transformer-based LTSF solutions have demonstrated considerable prediction accuracy improvements over traditional methods, in their experiments, all the compared (non-Transformer) baselines perform autoregressive or iterated multi-step (IMS) forecasting [1, 2, 22, 24], which are known to suffer from significant error accumulation effects for the LTSF problem. Therefore, in this work, we challenge Transformer-based LTSF solutions with direct <strong>multi-step (DMS) forecasting strategies</strong> to validate their real performance.</p>
<p>Ê≠§Â§ñÔºåÂ∞ΩÁÆ°Áé∞ÊúâÁöÑÂü∫‰∫éTransformerÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâËß£ÂÜ≥ÊñπÊ°àÂ∑≤ÊòæÁ§∫Âá∫Áõ∏ËæÉ‰∫é‰º†ÁªüÊñπÊ≥ïÁöÑÊòæËëóÈ¢ÑÊµãÁ≤æÂ∫¶ÊèêÂçáÔºå‰ΩÜÂú®ÂÖ∂ÂÆûÈ™å‰∏≠ÔºåÊâÄÊúâË¢´ÊØîËæÉÁöÑÔºàÈùûTransformerÔºâÂü∫Á∫øÂùáÈááÁî®Ëá™ÂõûÂΩíÊàñËø≠‰ª£Â§öÊ≠•ÔºàIMSÔºâÈ¢ÑÊµãÁ≠ñÁï•„ÄÇËøô‰∫õÊñπÊ≥ïÂ∑≤Áü•Âú®LTSFÈóÆÈ¢ò‰∏≠Â≠òÂú®ÊòæËëóÁöÑËØØÂ∑ÆÁ¥ØÁßØÊïàÂ∫î„ÄÇÂõ†Ê≠§ÔºåÂú®Êú¨Â∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÈááÁî®Áõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÈ¢ÑÊµãÁ≠ñÁï•Êù•ÊåëÊàòÂü∫‰∫éTransformerÁöÑLTSFËß£ÂÜ≥ÊñπÊ°àÔºå‰ª•È™åËØÅÂÖ∂ÁúüÂÆûÊÄßËÉΩ„ÄÇ</p>
<blockquote>
<p>ËøòÊòØÂæóÂ§öÁúãÊñáÁåÆÔºåÁé∞Âú®ÈÉΩÊòØÂ§öÊ≠•Âπ∂Ë°åÈ¢ÑÊµã</p>
</blockquote>
<p>Not all time series are predictable, let alone long-term forecasting (e.g., for chaotic systems). </p>
<p>We hypothesize that long-term forecasting is only feasible for those time series with a relatively clear trend and periodicity. </p>
<p>As linear models can already extract such information, we introduce a set of embarrassingly simple models named LTSF-Linear as a new baseline for comparison. </p>
<p>LTSF-Linear regresses historical time series with a one-layer linear model to forecast future time series directly. </p>
<p>Âπ∂ÈùûÊâÄÊúâÊó∂Èó¥Â∫èÂàóÈÉΩÂÖ∑Â§áÂèØÈ¢ÑÊµãÊÄßÔºåÈïøÊúüÈ¢ÑÊµãÊõ¥ÊòØÂ¶ÇÊ≠§Ôºà‰æãÂ¶ÇÊ∑∑Ê≤åÁ≥ªÁªüÔºâ„ÄÇÊàë‰ª¨Êé®ÊµãÔºåÈïøÊúüÈ¢ÑÊµã‰ªÖÈÄÇÁî®‰∫éÈÇ£‰∫õË∂ãÂäøÂíåÂë®ÊúüÊÄßËæÉ‰∏∫ÊòéÊòæÁöÑÊó∂Èó¥Â∫èÂàó„ÄÇÁî±‰∫éÁ∫øÊÄßÊ®°ÂûãÂ∑≤ÁªèËÉΩÂ§üÊèêÂèñÊ≠§Á±ª‰ø°ÊÅØÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁªÑÊûÅ‰∏∫ÁÆÄÂçïÁöÑÊ®°ÂûãÔºåÂëΩÂêç‰∏∫ LTSF-LinearÔºå‰Ωú‰∏∫Êñ∞ÁöÑÊØîËæÉÂü∫Á∫ø„ÄÇLTSF-Linear ÈÄöËøáÂçïÂ±ÇÁ∫øÊÄßÊ®°ÂûãÂØπÂéÜÂè≤Êó∂Èó¥Â∫èÂàóËøõË°åÂõûÂΩíÔºåÁõ¥Êé•È¢ÑÊµãÊú™Êù•ÁöÑÂ∫èÂàó„ÄÇ</p>
<p>We conduct extensive experiments on nine widely-used benchmark datasets that cover various real-life applications: traffic, energy, economics, weather, and disease predictions. </p>
<p>Surprisingly, our results show that LTSF-Linear outperforms existing complex Transformerbased models in all cases, and often by a large margin (20% ‚àº 50%). </p>
<p>Moreover, we find that, in contrast to the claims in existing Transformers, most of them fail to extract temporal relations from long sequences, i.e., the forecasting errors are not reduced (sometimes even increased) with the increase of look-back window sizes. </p>
<p>Finally, we conduct various ablation studies on existing Transformer-based TSF solutions to study the impact of various design elements in them.</p>
<p>Êàë‰ª¨Âú®Ê∂µÁõñ‰∫§ÈÄö„ÄÅËÉΩÊ∫ê„ÄÅÁªèÊµé„ÄÅÂ§©Ê∞îÂíåÁñæÁóÖÈ¢ÑÊµãÁ≠âÂ§öÁßçÂÆûÈôÖÂ∫îÁî®ÁöÑ‰πù‰∏™ÂπøÊ≥õ‰ΩøÁî®ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂ§ßÈáèÂÆûÈ™å„ÄÇ‰ª§‰∫∫ÊÑèÂ§ñÁöÑÊòØÔºåÁªìÊûúÊòæÁ§∫ LTSF-Linear Âú®ÊâÄÊúâÊÉÖÂÜµ‰∏ãÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÂ§çÊùÇÂü∫‰∫é Transformer ÁöÑÊ®°ÂûãÔºå‰∏î‰ºòÂäøÊòæËëóÔºà20% Ëá≥ 50%Ôºâ„ÄÇ</p>
<p>Ê≠§Â§ñÔºåÊàë‰ª¨ÂèëÁé∞Ôºå‰∏éÁé∞Êúâ Transformer ‰∏≠ÁöÑ‰∏ªÂº†Áõ∏ÂèçÔºåÂ§ßÂ§öÊï∞ Transformer Êó†Ê≥ï‰ªéÈïøÂ∫èÂàó‰∏≠ÊèêÂèñÊó∂Èó¥ÂÖ≥Á≥ªÔºåÂç≥ÈöèÁùÄÂõûÈ°æÁ™óÂè£Â§ßÂ∞èÁöÑÂ¢ûÂä†ÔºåÈ¢ÑÊµãËØØÂ∑ÆÂπ∂Êú™Èôç‰ΩéÔºàÊúâÊó∂ÁîöËá≥Â¢ûÂä†Ôºâ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÂØπÁé∞ÊúâÁöÑÂü∫‰∫é Transformer ÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâËß£ÂÜ≥ÊñπÊ°àËøõË°å‰∫ÜÂêÑÁßçÊ∂àËûçÁ†îÁ©∂Ôºå‰ª•Êé¢Á©∂ÂÖ∂‰∏≠ÂêÑÁßçËÆæËÆ°ÂÖÉÁ¥†ÁöÑÂΩ±Âìç„ÄÇ</p>
<blockquote>
<p>ÊâÄ‰ª•ËØ¥ÔºåTransformer ÁöÑÂõûÊúõÁ™óÂè£ËæπÈïøËØØÂ∑Æ‰ºöÂ¢ûÂä†</p>
</blockquote>
<h2 id="contributions">contributions<a class="headerlink" href="#contributions" title="Permanent link">¬∂</a></h2>
<p>To sum up, the contributions of this work include:</p>
<ul>
<li>To the best of our knowledge, this is the first work to challenge the effectiveness of the booming Transformers for the long-term time series forecasting task.</li>
<li>ÊçÆÊàë‰ª¨ÊâÄÁü•ÔºåËøôÊòØÈ¶ñÊ¨°ÂØπÂú®ÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰ªªÂä°‰∏≠Ëì¨ÂãÉÂèëÂ±ïÁöÑTransformerÁöÑÊúâÊïàÊÄßÊèêÂá∫ÊåëÊàò„ÄÇ</li>
<li>To validate our claims, we introduce a set of embarrassingly simple one-layer linear models, named LTSF-Linear, and compare them with existing Transformer-based LTSF solutions on nine benchmarks. LTSF-Linear can be a new baseline for the LTSF problem.</li>
<li>‰∏∫‰∫ÜÈ™åËØÅÊàë‰ª¨ÁöÑËßÇÁÇπÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁªÑÊûÅÂÖ∂ÁÆÄÂçïÁöÑÂçïÂ±ÇÁ∫øÊÄßÊ®°ÂûãÔºåÂëΩÂêç‰∏∫LTSF-LinearÔºåÂπ∂Âú®‰πù‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂ∞ÜÂÖ∂‰∏éÁé∞ÊúâÁöÑÂü∫‰∫éTransformerÁöÑLTSFËß£ÂÜ≥ÊñπÊ°àËøõË°åÊØîËæÉ„ÄÇLTSF-LinearÂèØ‰ª•‰Ωú‰∏∫LTSFÈóÆÈ¢òÁöÑ‰∏Ä‰∏™Êñ∞ÁöÑÂü∫Á∫øÊ®°Âûã„ÄÇ</li>
<li>
<p>We conduct comprehensive empirical studies on various aspects of existing Transformer-based solutions, including <strong>the capability of modeling long inputs</strong>, <mark>the sensitivity to time series order</mark>, <strong>the impact of positional encoding</strong> and <mark>sub-series embedding</mark>, and <strong>efficiency comparisons.</strong> Our findings would benefit future research in this area.</p>
</li>
<li>
<p>Êàë‰ª¨ÂØπÁé∞ÊúâÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÁöÑÂêÑ‰∏™ÊñπÈù¢ËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂÆûËØÅÁ†îÁ©∂ÔºåÂåÖÊã¨ÂØπÈïøËæìÂÖ•ÁöÑÂª∫Ê®°ËÉΩÂäõ„ÄÅÂØπÊó∂Èó¥Â∫èÂàóÈ°∫Â∫èÁöÑÊïèÊÑüÊÄß„ÄÅ‰ΩçÁΩÆÁºñÁ†ÅÂíåÂ≠êÂ∫èÂàóÂµåÂÖ•ÁöÑÂΩ±Âìç‰ª•ÂèäÊïàÁéáÊØîËæÉÁ≠â„ÄÇÊàë‰ª¨ÁöÑÂèëÁé∞Â∞ÜÊúâÂä©‰∫éËØ•È¢ÜÂüüÊú™Êù•ÁöÑÁ†îÁ©∂„ÄÇ</p>
</li>
</ul>
<p>With the above, we conclude that ‚ë† the temporal modeling capabilities of Transformers for time series are exaggerated, at least for the existing LTSF benchmarks. At the same time, ‚ë° while LTSF-Linear achieves a better prediction accuracy compared to existing works, it merely serves as a simple baseline for future research on the challenging longterm TSF problem. </p>
<p>With our findings, we also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks in the future.</p>
<p>Âü∫‰∫é‰∏äËø∞Á†îÁ©∂ÔºåÊàë‰ª¨ÂæóÂá∫ÁªìËÆ∫ÔºöTransformerÂú®Êó∂Èó¥Â∫èÂàóÂª∫Ê®°ÊñπÈù¢ÁöÑËÉΩÂäõË¢´È´ò‰º∞‰∫ÜÔºåËá≥Â∞ëÂØπ‰∫éÁé∞ÊúâÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâÂü∫ÂáÜÊµãËØïËÄåË®ÄÊòØËøôÊ†∑„ÄÇ‰∏éÊ≠§ÂêåÊó∂ÔºåÂ∞ΩÁÆ°LTSF-LinearÁõ∏ËæÉ‰∫éÁé∞ÊúâÁ†îÁ©∂ÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÈ¢ÑÊµãÁ≤æÂ∫¶Ôºå‰ΩÜÂÆÉ‰ªÖ‰ªÖÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂü∫Á∫øÊ®°ÂûãÔºåÁî®‰∫éÊú™Êù•ÂØπÊûÅÂÖ∑ÊåëÊàòÊÄßÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâÈóÆÈ¢òÁöÑÁ†îÁ©∂„ÄÇÈâ¥‰∫éÊàë‰ª¨ÁöÑÂèëÁé∞ÔºåÊàë‰ª¨ËøòÂÄ°ÂØºÂú®Êú™Êù•ÈáçÊñ∞ÂÆ°ËßÜÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÂú®ÂÖ∂‰ªñÊó∂Èó¥Â∫èÂàóÂàÜÊûê‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ</p>
<h2 id="2-preliminaries-tsf-problem-formulation">2. Preliminaries: TSF Problem Formulation<a class="headerlink" href="#2-preliminaries-tsf-problem-formulation" title="Permanent link">¬∂</a></h2>
<p>For time series containing C variates, given historical data $ \mathcal{X} = {X_1^t, ..., X_C<sup>t}_{t=1}</sup>L $, wherein L is the look-back window size and $ X_i^t $ is the value of the i^th variate at the t^th time step. The time series forecasting task is to predict the values $ \hat{\mathcal{X}} = {\hat{X}_1^t, ..., \hat{X}_C<sup l_t="L+T">t}_{t=L+1}</sup> $ at the T future time steps. </p>
<p>When T &gt; 1, <strong>iterated multi-step (IMS) forecasting</strong> learns a single-step forecaster and iteratively applies it to obtain multi-step predictions. Alternatively, direct <strong>multi-step (DMS) forecasting</strong> directly optimizes the multi-step forecasting objective at once.</p>
<p>ÂØπ‰∫éÂåÖÂê´ C ‰∏™ÂèòÈáèÁöÑÊó∂Èó¥Â∫èÂàóÔºåÁªôÂÆöÂéÜÂè≤Êï∞ÊçÆ $ \mathcal{X} = {X_1^t, ..., X_C<sup>t}_{t=1}</sup>L <span class="arithmatex">\(ÔºåÂÖ∂‰∏≠ L ÊòØÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÔºå\)</span> X_i^t $ ÊòØÁ¨¨ i ‰∏™ÂèòÈáèÂú®Á¨¨ t ‰∏™Êó∂Èó¥Ê≠•ÁöÑÂÄº„ÄÇÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰ªªÂä°ÊòØÈ¢ÑÊµãÊú™Êù• T ‰∏™Êó∂Èó¥Ê≠•ÁöÑÂÄº $ \hat{\mathcal{X}} = {\hat{X}_1^t, ..., \hat{X}_C<sup l_t="L+T">t}_{t=L+1}</sup> $„ÄÇÂΩì T &gt; 1 Êó∂ÔºåËø≠‰ª£Â§öÊ≠•ÔºàIMSÔºâÈ¢ÑÊµãÂ≠¶‰π†‰∏Ä‰∏™ÂçïÊ≠•È¢ÑÊµãÂô®ÔºåÂπ∂Ëø≠‰ª£Âú∞Â∫îÁî®ÂÆÉÊù•Ëé∑ÂæóÂ§öÊ≠•È¢ÑÊµã„ÄÇÊàñËÄÖÔºåÁõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÈ¢ÑÊµãÁõ¥Êé•‰∏ÄÊ¨°ÊÄß‰ºòÂåñÂ§öÊ≠•È¢ÑÊµãÁõÆÊ†á„ÄÇ</p>
<p>Compared to DMS forecasting results, IMS predictions have smaller variance thanks to the autoregressive estimation procedure, but they inevitably suffer from error accumulation effects. </p>
<p>Consequently, IMS forecasting is preferable when there is a highly-accurate single-step forecaster, and <span class="arithmatex">\(T\)</span> is relatively small.</p>
<p>In contrast, DMS forecasting generates more accurate predictions when it is hard to obtain an unbiased single-step forecasting model, or <span class="arithmatex">\(T\)</span> is large.</p>
<p>‰∏éÁõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÈ¢ÑÊµãÁªìÊûúÁõ∏ÊØîÔºåËø≠‰ª£Â§öÊ≠•ÔºàIMSÔºâÈ¢ÑÊµãÁî±‰∫éËá™ÂõûÂΩí‰º∞ËÆ°ËøáÁ®ãËÄåÂÖ∑ÊúâËæÉÂ∞èÁöÑÊñπÂ∑ÆÔºå‰ΩÜÂÆÉ‰ª¨‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÈÅ≠ÂèóËØØÂ∑ÆÁ¥ØÁßØÊïàÂ∫îÁöÑÂΩ±Âìç„ÄÇÂõ†Ê≠§ÔºåÂΩìÂ≠òÂú®È´òÂ∫¶Á≤æÁ°ÆÁöÑÂçïÊ≠•È¢ÑÊµãÂô®Ôºå‰∏î <span class="arithmatex">\(T\)</span> Áõ∏ÂØπËæÉÂ∞èÊó∂ÔºåIMSÈ¢ÑÊµãÊõ¥‰∏∫ÂèØÂèñ„ÄÇÁõ∏ÂèçÔºåÂΩìÈöæ‰ª•Ëé∑ÂæóÊó†ÂÅèÁöÑÂçïÊ≠•È¢ÑÊµãÊ®°ÂûãÔºåÊàñËÄÖ <span class="arithmatex">\(T\)</span> ËæÉÂ§ßÊó∂ÔºåDMSÈ¢ÑÊµãËÉΩÂ§üÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÈ¢ÑÊµã„ÄÇ</p>
<blockquote>
<p>ËøôÈÉ®ÂàÜËÆ®ËÆ∫‰∫Ü‰ªÄ‰πàÊó∂ÂÄô‰ΩøÁî®Ëá™ÂõûÂΩíÈ¢ÑÊµãÔºå‰ªÄ‰πàÊó∂ÂÄôÈááÁî®Âπ∂Ë°åÈ¢ÑÊµã</p>
<p>ÈÇ£ÊàëÁöÑÈ¢ÑÊµãÈÄªËæëÊòØÊúâÁÇπÈóÆÈ¢ò</p>
<p>Êî∂Ëé∑ÔºöÂπ∂Ë°åÂ§öÊ≠•È¢ÑÊµãÔºõËøòÊúâ‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊòØÈááÁî®ÈÄöÈÅìÁã¨Á´ãÁ≠ñÁï•ËøòÊòØÊ∑∑Âêà</p>
</blockquote>
<h2 id="3-transformer-based-ltsf-solutions">3. Transformer-Based LTSF Solutions<a class="headerlink" href="#3-transformer-based-ltsf-solutions" title="Permanent link">¬∂</a></h2>
<p><mark>Transformer-based models</mark> [26] have achieved unparalleled performances in many long-standing AI tasks in natural language processing and computer vision fields, thanks to the effectiveness of the multi-head self-attention mechanism. </p>
<p>This has also triggered lots of research interest in Transformer-based time series modeling techniques [20, 27]. </p>
<p>In particular, a large amount of research works are dedicated to the LTSF task (e.g., [16, 18, 28, 30, 31]). Considering the ability to capture long-range dependencies with Transformer models, most of them focus on the less-explored long-term forecasting problem (<span class="arithmatex">\(T \gg 1\)</span>) [1].</p>
<p>Âü∫‰∫éTransformerÁöÑÊ®°Âûã[26]Áî±‰∫éÂ§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊúâÊïàÊÄßÔºåÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÁöÑËÆ∏Â§öÈïøÊúüÂ≠òÂú®ÁöÑAI‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊó†‰∏é‰º¶ÊØîÁöÑÊÄßËÉΩ„ÄÇËøô‰πüÊøÄÂèë‰∫ÜÂØπÂü∫‰∫éTransformerÁöÑÊó∂Èó¥Â∫èÂàóÂª∫Ê®°ÊäÄÊúØ[20, 27]ÁöÑÂ§ßÈáèÁ†îÁ©∂ÂÖ¥Ë∂£„ÄÇÁâπÂà´ÊòØÔºåÂ§ßÈáèÁöÑÁ†îÁ©∂Â∑•‰ΩúËá¥Âäõ‰∫éÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâ‰ªªÂä°Ôºà‰æãÂ¶ÇÔºå[16, 18, 28, 30, 31]Ôºâ„ÄÇËÄÉËôëÂà∞‰ΩøÁî®TransformerÊ®°ÂûãÊçïÊçâÈïøË∑ùÁ¶ª‰æùËµñÂÖ≥Á≥ªÁöÑËÉΩÂäõÔºå‰ªñ‰ª¨‰∏≠ÁöÑÂ§ßÂ§öÊï∞ÈÉΩÈõÜ‰∏≠Âú®Êé¢Á¥¢ËæÉÂ∞ëÁöÑÈïøÊúüÈ¢ÑÊµãÈóÆÈ¢òÔºà<span class="arithmatex">\(T \gg 1\)</span>Ôºâ[1]„ÄÇ</p>
<blockquote>
<p>ÊâÄ‰ª•ÈïøÊúüÈ¢ÑÊµãÈóÆÈ¢òÔºåÂèçËÄåÊòØ Transformer Âá∫Áé∞‰ª•ÂêéÂºÄÂßãÁöÑ</p>
</blockquote>
<p>When applying the vanilla Transformer model to the LTSF problem, it has some limitations, including the quadratic time/memory complexity with the original selfattention scheme and error accumulation caused by the autoregressive decoder design. Informer [30] addresses these issues and proposes a novel Transformer architecture with reduced complexity and a DMS forecasting strategy. Later, more Transformer variants introduce various time series features into their models for performance or efficiency improvements [18,28,31]. We summarize the design elements of existing Transformer-based LTSF solutions as follows (see Figure 1).</p>
<p>Â∞ÜÁªèÂÖ∏ÁöÑTransformerÊ®°ÂûãÂ∫îÁî®‰∫éÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâÈóÆÈ¢òÊó∂ÔºåÂ≠òÂú®‰∏Ä‰∫õÈôêÂà∂ÔºåÂåÖÊã¨ÂéüÂßãËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑ‰∫åÊ¨°Êó∂Èó¥ÂíåÂÜÖÂ≠òÂ§çÊùÇÂ∫¶‰ª•ÂèäÁî±Ëá™ÂõûÂΩíËß£Á†ÅÂô®ËÆæËÆ°ÂºïËµ∑ÁöÑËØØÂ∑ÆÁ¥ØÁßØ„ÄÇInformer [30] Ëß£ÂÜ≥‰∫ÜËøô‰∫õÈóÆÈ¢òÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ∑ÊúâÈôç‰ΩéÂ§çÊùÇÂ∫¶ÂíåÁõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÈ¢ÑÊµãÁ≠ñÁï•ÁöÑÊñ∞ÂûãTransformerÊû∂ÊûÑ„ÄÇÈöèÂêéÔºåÊõ¥Â§öÁöÑTransformerÂèò‰ΩìÂ∞ÜÂêÑÁßçÊó∂Èó¥Â∫èÂàóÁâπÂæÅÂºïÂÖ•‰ªñ‰ª¨ÁöÑÊ®°Âûã‰∏≠Ôºå‰ª•ÊèêÈ´òÊÄßËÉΩÊàñÊïàÁéá[18, 28, 31]„ÄÇÊàë‰ª¨Â¶Ç‰∏ãÊÄªÁªì‰∫ÜÁé∞ÊúâÂü∫‰∫éTransformerÁöÑLTSFËß£ÂÜ≥ÊñπÊ°àÁöÑËÆæËÆ°ÂÖÉÁ¥†ÔºàËßÅÂõæ1Ôºâ„ÄÇ</p>
<blockquote>
<ul>
<li>ÊåáÂá∫ Transformer ÂØπ‰∫éÊó∂Â∫è‰ªªÂä°ÁöÑÈóÆÈ¢òÔºö‰∫åÊ¨°Êó∂Èó¥Â§çÊùÇÂ∫¶&amp;Á©∫Èó¥Â§çÊùÇÂ∫¶</li>
<li>InformerÔºåÈôç‰ΩéÂ§çÊùÇÂ∫¶&amp;Áõ¥Êé•Â§öÊ≠•È¢ÑÊµãÔºàmarkÔºâ</li>
</ul>
</blockquote>
<p><mark>Time series decomposition</mark>: For data preprocessing, normalization with zero-mean is common in TSF. Besides, Autoformer [28] first applies seasonal-trend decomposition behind each neural block, which is a standard method in time series analysis to make raw data more predictable [6, 13]. Specifically, they use a moving average kernel on the input sequence to extract the trend-cyclical component of the time series. The difference between the original sequence and the trend component is regarded as the seasonal component. On top of the decomposition scheme of Autoformer, FEDformer [31] further proposes the mixture of experts‚Äô strategies to mix the trend components extracted by moving average kernels with various kernel sizes.</p>
<p>Êó∂Èó¥Â∫èÂàóÂàÜËß£ÔºöÂú®Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ‰∏≠ÔºåÈõ∂ÂùáÂÄºÂΩí‰∏ÄÂåñÂú®Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâ‰∏≠ÂæàÂ∏∏ËßÅ„ÄÇÊ≠§Â§ñÔºåAutoformer[28]È¶ñÊ¨°Âú®ÊØè‰∏™Á•ûÁªèÂùóÂêéÈù¢Â∫îÁî®Â≠£ËäÇË∂ãÂäøÂàÜËß£ÔºåËøôÊòØÊó∂Èó¥Â∫èÂàóÂàÜÊûê‰∏≠‰ΩøÂéüÂßãÊï∞ÊçÆÊõ¥ÂèØÈ¢ÑÊµãÁöÑÊ†áÂáÜÊñπÊ≥ï[6, 13]„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ªñ‰ª¨‰ΩøÁî®ÁßªÂä®Âπ≥ÂùáÊ†∏Âú®ËæìÂÖ•Â∫èÂàó‰∏äÊèêÂèñÊó∂Èó¥Â∫èÂàóÁöÑË∂ãÂäøÂë®ÊúüÊàêÂàÜ„ÄÇÂéüÂßãÂ∫èÂàó‰∏éË∂ãÂäøÊàêÂàÜ‰πãÈó¥ÁöÑÂ∑ÆÂÄºË¢´ËßÜ‰∏∫Â≠£ËäÇÊàêÂàÜ„ÄÇÂú®AutoformerÁöÑÂàÜËß£ÊñπÊ°àÂü∫Á°Ä‰∏äÔºåFEDformer[31]Ëøõ‰∏ÄÊ≠•ÊèêÂá∫‰∫Ü‰∏ìÂÆ∂Ê∑∑ÂêàÁ≠ñÁï•ÔºåÂ∞ÜÈÄöËøá‰∏çÂêåÊ†∏Â§ßÂ∞èÁöÑÁßªÂä®Âπ≥ÂùáÊ†∏ÊèêÂèñÁöÑË∂ãÂäøÊàêÂàÜËøõË°åÊ∑∑Âêà„ÄÇ</p>
<blockquote>
<p>Fedformer ÊòØÂØπ Autoformer ÁöÑÊîπËøõÔºåÂºïÂÖ•‰∫ÜÈ¢ëÂüü‰ø°ÊÅØ</p>
</blockquote>
<p><strong>Input embedding strategies:</strong> The self-attention layer in the Transformer architecture cannot preserve the positional information of the time series. </p>
<p>However, local positional information, i.e. the ordering of time series, is important. </p>
<p>Besides, global temporal information, such as hierarchical timestamps (week, month, year) and agnostic timestamps (holidays and events), is also informative [30]. </p>
<p>To enhance the temporal context of time-series inputs, a practical design in the SOTA Transformer-based methods is injecting several embeddings, like a <mark>fixed positional encoding, a channel projection embedding, and learnable temporal embeddings</mark> into the input sequence. </p>
<blockquote>
<p>ÊòØÔºö‰ΩçÁΩÆÁºñÁ†Å„ÄÅÈÄöÈÅìÂµåÂÖ•„ÄÅÊó∂Èó¥ÂµåÂÖ•</p>
</blockquote>
<p>Moreover, temporal embeddings with a temporal convolution layer [16] or learnable timestamps [28] are introduced.</p>
<blockquote>
<p>ËøôÈáåÊòØÂú®ËØ¥ÂèØÂ≠¶‰π†ÁöÑÊó∂Èó¥ÂµåÂÖ•</p>
</blockquote>
<p>ËæìÂÖ•ÂµåÂÖ•Á≠ñÁï•ÔºöTransformerÊû∂ÊûÑ‰∏≠ÁöÑËá™Ê≥®ÊÑèÂäõÂ±ÇÊó†Ê≥ï‰øùÁïôÊó∂Èó¥Â∫èÂàóÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇÁÑ∂ËÄåÔºåÂ±ÄÈÉ®‰ΩçÁΩÆ‰ø°ÊÅØÔºåÂç≥Êó∂Èó¥Â∫èÂàóÁöÑÈ°∫Â∫èÔºåÊòØÈáçË¶ÅÁöÑ„ÄÇÊ≠§Â§ñÔºåÂÖ®Â±ÄÊó∂Èó¥‰ø°ÊÅØÔºåÂ¶ÇÂ±ÇÊ¨°ÂåñÁöÑÊó∂Èó¥Êà≥ÔºàÂë®„ÄÅÊúà„ÄÅÂπ¥ÔºâÂíå‰∏çÂèØÁü•ÁöÑÊó∂Èó¥Êà≥ÔºàÂÅáÊúüÂíå‰∫ã‰ª∂ÔºâÔºå‰πüÂÖ∑Êúâ‰ø°ÊÅØ‰ª∑ÂÄº[30]„ÄÇ‰∏∫‰∫ÜÂ¢ûÂº∫Êó∂Èó¥Â∫èÂàóËæìÂÖ•ÁöÑÊó∂Èó¥‰∏ä‰∏ãÊñáÔºåÊúÄÂÖàËøõÁöÑÂü∫‰∫éTransformerÁöÑÊñπÊ≥ï‰∏≠‰∏Ä‰∏™ÂÆûÁî®ÁöÑËÆæËÆ°ÊòØÂ∞ÜÂá†ÁßçÂµåÂÖ•ÔºåÂ¶ÇÂõ∫ÂÆöÁöÑ‰ΩçÁΩÆÁºñÁ†Å„ÄÅÈÄöÈÅìÊäïÂΩ±ÂµåÂÖ•ÂíåÂèØÂ≠¶‰π†ÁöÑÊó∂Èó¥ÂµåÂÖ•Ê≥®ÂÖ•Âà∞ËæìÂÖ•Â∫èÂàó‰∏≠„ÄÇÊ≠§Â§ñÔºåËøòÂºïÂÖ•‰∫ÜÂÖ∑ÊúâÊó∂Èó¥Âç∑ÁßØÂ±Ç[16]ÊàñÂèØÂ≠¶‰π†Êó∂Èó¥Êà≥[28]ÁöÑÊó∂Èó¥ÂµåÂÖ•„ÄÇ</p>
<p><strong>Self-attention schemes:</strong> Transformers rely on the self-attention mechanism to extract the semantic dependencies between paired elements. Motivated by reducing the <span class="arithmatex">\(O(L^2)\)</span> time and memory complexity of the vanilla Transformer, recent works propose <strong>two strategies</strong> for efficiency. </p>
<p>Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºöTransformer ‰æùËµñ‰∫éËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•ÊèêÂèñÊàêÂØπÂÖÉÁ¥†‰πãÈó¥ÁöÑËØ≠‰πâ‰æùËµñÂÖ≥Á≥ª„ÄÇ‰∏∫‰∫ÜÈôç‰ΩéÂéüÂßã Transformer ÁöÑ <span class="arithmatex">\(O(L^2)\)</span> Êó∂Èó¥ÂíåÂÜÖÂ≠òÂ§çÊùÇÂ∫¶ÔºåÊúÄËøëÁöÑÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏§ÁßçÊèêÈ´òÊïàÁéáÁöÑÁ≠ñÁï•„ÄÇ</p>
<p>On the one hand, <mark>LogTrans</mark> and <mark>Pyraformer</mark> explicitly introduce <strong>a sparsity bias</strong> into the self-attention scheme. Specifically, LogTrans uses a Logsparse mask to reduce the computational complexity to <span class="arithmatex">\(O(L \log L)\)</span> while Pyraformer adopts pyramidal attention that captures hierarchically multi-scale temporal dependencies with an <span class="arithmatex">\(O(L)\)</span> time and memory complexity. </p>
<blockquote>
<p>‰ªÄ‰πàÂè´Âú®Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÂºïÂÖ• Á®ÄÁñèÊÄßÂÅèÂ∑Æ</p>
</blockquote>
<p>‰∏ÄÊñπÈù¢ÔºåLogTrans Âíå Pyraformer ÊòéÁ°ÆÂú∞Âú®Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÂºïÂÖ•‰∫ÜÁ®ÄÁñèÊÄßÂÅèÂ∑Æ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåLogTrans ‰ΩøÁî® Logsparse Êé©Á†ÅÂ∞ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶Èôç‰ΩéÂà∞ <span class="arithmatex">\(O(L \log L)\)</span>ÔºåËÄå Pyraformer ÈááÁî®ÈáëÂ≠óÂ°îÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ª• <span class="arithmatex">\(O(L)\)</span> ÁöÑÊó∂Èó¥ÂíåÂÜÖÂ≠òÂ§çÊùÇÂ∫¶ÊçïËé∑Â±ÇÊ¨°ÂåñÁöÑÂ§öÂ∞∫Â∫¶Êó∂Èó¥‰æùËµñÂÖ≥Á≥ª„ÄÇ</p>
<p>On the other hand, Informer and FEDformer use the low-rank property in the self-attention matrix. <mark>Informer</mark> proposes a <strong>ProbSparse self-attention mechanism</strong> and a <strong>self-attention distilling operation</strong> to decrease the complexity to <span class="arithmatex">\(O(L \log L)\)</span>, and <mark>FEDformer</mark> designs a <strong>Fourier enhanced block</strong> and <strong>a wavelet enhanced block</strong> with random selection to obtain <span class="arithmatex">\(O(L)\)</span> complexity. Lastly, <mark>Autoformer</mark> designs a series-wise auto-correlation mechanism to replace the original self-attention layer.</p>
<p>Âè¶‰∏ÄÊñπÈù¢ÔºåInformer Âíå FEDformer Âú®Ëá™Ê≥®ÊÑèÂäõÁü©Èòµ‰∏≠‰ΩøÁî®‰∫Ü‰ΩéÁß©Â±ûÊÄß„ÄÇInformer ÊèêÂá∫‰∫Ü‰∏ÄÁßç ProbSparse Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåËá™Ê≥®ÊÑèÂäõËí∏È¶èÊìç‰ΩúÊù•Èôç‰ΩéÂ§çÊùÇÂ∫¶Ëá≥ <span class="arithmatex">\(O(L \log L)\)</span>ÔºåËÄå FEDformer ËÆæËÆ°‰∫ÜÂÇÖÈáåÂè∂Â¢ûÂº∫ÂùóÂíåÂ∞èÊ≥¢Â¢ûÂº∫ÂùóÔºåÈÄöËøáÈöèÊú∫ÈÄâÊã©Ëé∑Âæó <span class="arithmatex">\(O(L)\)</span> Â§çÊùÇÂ∫¶„ÄÇÊúÄÂêéÔºåAutoformer ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂ∫èÂàóËá™Áõ∏ÂÖ≥Êú∫Âà∂Êù•Êõø‰ª£ÂéüÂßãÁöÑËá™Ê≥®ÊÑèÂäõÂ±Ç„ÄÇ</p>
<blockquote>
<p>ÂÖ∂ÂÆûÊó∂Èó¥Â∫èÂàóÂçï‰∏™ÁÇπÁöÑ‰ø°ÊÅØÊòØÈùûÂ∏∏Á®ÄÁñèÁöÑÔºåÊâÄ‰ª•ÂêéÈù¢ÊèêÂá∫ PatchTST ÊàñËÄÖËΩ¨‰∫åÁª¥Êää‰ø°ÊÅØÂâ•Á¶ªÂá∫Êù•‰πüÊòØÂêàÁêÜÁöÑ</p>
</blockquote>
<p><strong>Decoders:</strong> The vanilla Transformer decoder outputs sequences in an autoregressive manner, resulting in a slow inference speed and error accumulation effects, especially for long-term predictions. </p>
<p><strong>Ëß£Á†ÅÂô®Ôºö</strong> ‰º†ÁªüÁöÑTransformerËß£Á†ÅÂô®‰ª•Ëá™ÂõûÂΩíÊñπÂºèËæìÂá∫Â∫èÂàóÔºåÂØºËá¥Êé®ÁêÜÈÄüÂ∫¶ÊÖ¢ÂíåËØØÂ∑ÆÁ¥ØÁßØÊïàÂ∫îÔºåÁâπÂà´ÊòØÂØπ‰∫éÈïøÊúüÈ¢ÑÊµã„ÄÇ</p>
<blockquote>
<p>ÈïøÊúüÈ¢ÑÊµãÂà´Áî®Ëá™ÂõûÂΩí</p>
</blockquote>
<p><mark>Informer</mark> designs a generative-style decoder for  <strong>DMS forecasting</strong>. Other Transformer variants employ similar DMS strategies. For instance, <mark>Pyraformer</mark> uses a fully-connected layer concatenating Spatio-temporal axes as the decoder. <mark>Autoformer</mark>  sums up two refined decomposed features from trend-cyclical components and the stacked auto-correlation mechanism for seasonal components to get the final prediction. <mark>FEDformer</mark> also uses a decomposition scheme with the proposed frequency attention block to decode the final results.</p>
<p>Informer‰∏∫Áõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÈ¢ÑÊµãËÆæËÆ°‰∫ÜÁîüÊàêÂºèËß£Á†ÅÂô®„ÄÇÂÖ∂‰ªñTransformerÂèò‰ΩìÈááÁî®Á±ª‰ººÁöÑDMSÁ≠ñÁï•„ÄÇ‰æãÂ¶ÇÔºåPyraformer‰ΩøÁî®ÂÖ®ËøûÊé•Â±ÇÂ∞ÜÊó∂Á©∫ËΩ¥ËøûÊé•Ëµ∑Êù•‰Ωú‰∏∫Ëß£Á†ÅÂô®„ÄÇAutoformerÂ∞ÜË∂ãÂäøÂë®ÊúüÊàêÂàÜÁöÑ‰∏§‰∏™ÁªÜÂåñÂàÜËß£ÁâπÂæÅÂíåÂ≠£ËäÇÊàêÂàÜÁöÑÂ†ÜÂè†Ëá™Áõ∏ÂÖ≥Êú∫Âà∂Áõ∏Âä†Ôºå‰ª•Ëé∑ÂæóÊúÄÁªàÈ¢ÑÊµã„ÄÇFEDformer‰πü‰ΩøÁî®ÂàÜËß£ÊñπÊ°àÂíåÊèêÂá∫ÁöÑÈ¢ëÁéáÊ≥®ÊÑèÂäõÂùóÊù•Ëß£Á†ÅÊúÄÁªàÁªìÊûú„ÄÇ</p>
<blockquote>
<p>ÂâçÈù¢ÂàÜÂà´‰ªé‰∏çÂêåÁöÑÊñπÊ≥ïËØ¥ÊòéTransformer-based Ê®°Âûã</p>
<ol>
<li>Time series decomposition</li>
<li>Input embedding strategies</li>
<li>Self-attention schemes</li>
<li>Decoders</li>
</ol>
</blockquote>
<p>The premise of Transformer models is the semantic correlations between paired elements, while the self-attention mechanism itself is permutation-invariant, and its capability of modeling temporal relations largely depends on positional encodings associated with input tokens. </p>
<p>Considering the raw numerical data in time series (e.g., stock prices or electricity values), there are hardly any point-wise semantic correlations between them. In time series modeling, we are mainly interested in the temporal relations among a continuous set of points, and the order of these elements instead of the paired relationship plays the most crucial role. While employing positional encoding and using tokens to embed sub-series facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. Due to the above observations, we are interested in revisiting the effectiveness of Transformer-based LTSF solutions.</p>
<p>TransformerÊ®°ÂûãÁöÑÂü∫Á°ÄÊòØÊàêÂØπÂÖÉÁ¥†‰πãÈó¥ÁöÑËØ≠‰πâÁõ∏ÂÖ≥ÊÄßÔºåËÄåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Êú¨Ë∫´ÊòØÊéíÂàó‰∏çÂèòÁöÑÔºåÂÖ∂ÂØπÊó∂Èó¥ÂÖ≥Á≥ªÁöÑÂª∫Ê®°ËÉΩÂäõÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùËµñ‰∫é‰∏éËæìÂÖ•Ê†áËÆ∞Áõ∏ÂÖ≥ËÅîÁöÑ‰ΩçÁΩÆÁºñÁ†Å„ÄÇËÄÉËôëÂà∞Êó∂Èó¥Â∫èÂàó‰∏≠ÁöÑÂéüÂßãÊï∞ÂÄºÊï∞ÊçÆÔºà‰æãÂ¶ÇÔºåËÇ°Á•®‰ª∑Ê†ºÊàñÁîµÂäõÂÄºÔºâÔºåÂÆÉ‰ª¨‰πãÈó¥Âá†‰πéÊ≤°ÊúâÁÇπÂØπÁÇπÁöÑËØ≠‰πâÁõ∏ÂÖ≥ÊÄß„ÄÇÂú®Êó∂Èó¥Â∫èÂàóÂª∫Ê®°‰∏≠ÔºåÊàë‰ª¨‰∏ªË¶ÅÂÖ≥Ê≥®ÁöÑÊòØËøûÁª≠ÁÇπÈõÜ‰πãÈó¥ÁöÑÊó∂Èó¥ÂÖ≥Á≥ªÔºåËøô‰∫õÂÖÉÁ¥†ÁöÑÈ°∫Â∫èËÄåÈùûÊàêÂØπÂÖ≥Á≥ªËµ∑ÁùÄÊúÄÂÖ≥ÈîÆÁöÑ‰ΩúÁî®„ÄÇËôΩÁÑ∂‰ΩøÁî®‰ΩçÁΩÆÁºñÁ†ÅÂíå‰ΩøÁî®Ê†áËÆ∞Êù•ÂµåÂÖ•Â≠êÂ∫èÂàóÊúâÂä©‰∫é‰øùÁïô‰∏Ä‰∫õÈ°∫Â∫è‰ø°ÊÅØÔºå‰ΩÜÊéíÂàó‰∏çÂèòÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊú¨Ë¥®‰∏çÂèØÈÅøÂÖçÂú∞ÂØºËá¥Êó∂Èó¥‰ø°ÊÅØÁöÑ‰∏¢Â§±„ÄÇÂü∫‰∫é‰∏äËø∞ËßÇÂØüÔºåÊàë‰ª¨ÊúâÂÖ¥Ë∂£ÈáçÊñ∞ÂÆ°ËßÜÂü∫‰∫éTransformerÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâËß£ÂÜ≥ÊñπÊ°àÁöÑÊúâÊïàÊÄß„ÄÇ</p>
<blockquote>
<p>ËøôÊÆµÊ∑±Êúâ‰Ωì‰ºöÔºå‰ΩøÁî®‰∫Ü‰ΩçÁΩÆÁºñÁ†ÅÔºåÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÁ≤æÂ∫¶‰ºöÊûÅÂ§ßÁöÑÊèêÈ´ò„ÄÇÂÅöËøáÂÆûÈ™å„ÄÇ</p>
</blockquote>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151040222.png" data-desc-position="bottom"><img alt="image-20250415104055276" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151040222.png"></a></p>
<p>Âõæ 1 Â±ïÁ§∫‰∫ÜÁé∞ÊúâÁöÑÂü∫‰∫é Transformer ÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâËß£ÂÜ≥ÊñπÊ°àÁöÑÊµÅÁ®ã„ÄÇÊï¥‰∏™ÊµÅÁ®ãÂàÜ‰∏∫Âõõ‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¢ÑÂ§ÑÁêÜÔºàPreprocessingÔºâ„ÄÅÂµåÂÖ•ÔºàEmbeddingÔºâ„ÄÅÁºñÁ†ÅÂô®ÔºàEncoderÔºâÂíåËß£Á†ÅÂô®ÔºàDecoderÔºâ„ÄÇ</p>
<p>(a) <strong>È¢ÑÂ§ÑÁêÜÔºàPreprocessingÔºâ</strong>Ôºö
- ÂåÖÊã¨ÂΩí‰∏ÄÂåñÔºàNormalizationÔºâÂíåÊó∂Èó¥Êà≥ÂáÜÂ§áÔºàTimestamp preparationÔºâ„ÄÇ
- ÂèØÈÄâÊìç‰ΩúÊúâÂ≠£ËäÇË∂ãÂäøÂàÜËß£ÔºàSeasonal-trend decompositionÔºâ„ÄÇ</p>
<p>(b) <strong>ÂµåÂÖ•ÔºàEmbeddingÔºâ</strong>Ôºö
- ÂåÖÊã¨ÈÄöÈÅìÊäïÂΩ±ÔºàChannel projectionÔºâÂíåÂõ∫ÂÆö‰ΩçÁΩÆÁºñÁ†ÅÔºàFixed positionÔºâ„ÄÇ
- ÂèØÈÄâÊìç‰ΩúÊúâÊú¨Âú∞Êó∂Èó¥Êà≥ÔºàLocal timestampÔºâÂíåÂÖ®Â±ÄÊó∂Èó¥Êà≥ÔºàGlobal timestampÔºâ„ÄÇ</p>
<p>(c) <strong>ÁºñÁ†ÅÂô®ÔºàEncoderÔºâ</strong>Ôºö
- ÂåÖÊã¨ LogSparse ÂíåÂç∑ÁßØËá™Ê≥®ÊÑèÂäõÔºàLogSparse and convolutional self-attention @LogTransÔºâ„ÄÇ
- ÂåÖÊã¨ ProbSparse ÂíåËí∏È¶èËá™Ê≥®ÊÑèÂäõÔºàProbSparse and distilling self-attention @InformerÔºâ„ÄÇ
- ÂåÖÊã¨Â∫èÂàóËá™Áõ∏ÂÖ≥‰∏éÂàÜËß£ÔºàSeries auto-correlation with decomposition @AutoformerÔºâ„ÄÇ
- ÂåÖÊã¨Â§öÂàÜËæ®ÁéáÈáëÂ≠óÂ°îÊ≥®ÊÑèÂäõÔºàMulti-resolution pyramidal attention @PyraformerÔºâ„ÄÇ
- ÂåÖÊã¨È¢ëÁéáÂ¢ûÂº∫Âùó‰∏éÂàÜËß£ÔºàFrequency enhanced block with decomposition @FEDformerÔºâ„ÄÇ</p>
<p>(d) <strong>Ëß£Á†ÅÂô®ÔºàDecoderÔºâ</strong>Ôºö
- ÂåÖÊã¨Ëø≠‰ª£Â§öÊ≠•ÔºàIMSÔºâÈ¢ÑÊµãÔºàIterated Multi-Step (IMS) @LogTransÔºâ„ÄÇ
- ÂåÖÊã¨Áõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÈ¢ÑÊµãÔºàDirect Multi-Step (DMS) @InformerÔºâ„ÄÇ
- ÂåÖÊã¨ DMS ‰∏éËá™Áõ∏ÂÖ≥ÂíåÂàÜËß£ÔºàDMS with auto-correlation and decomposition @AutoformerÔºâ„ÄÇ
- ÂåÖÊã¨ DMS Ê≤øÊó∂Á©∫Áª¥Â∫¶ÔºàDMS along spatio-temporal dimension @PyraformerÔºâ„ÄÇ
- ÂåÖÊã¨ DMS ‰∏éÈ¢ëÁéáÊ≥®ÊÑèÂäõÂíåÂàÜËß£ÔºàDMS with frequency attention and decomposition @FEDformerÔºâ„ÄÇ</p>
<p>Âõæ‰∏≠ÂÆûÁ∫øÊ°ÜË°®Á§∫Âü∫Êú¨Êìç‰ΩúÔºåËôöÁ∫øÊ°ÜË°®Á§∫ÂèØÈÄâÊìç‰Ωú„ÄÇ(c) Âíå (d) ÈÉ®ÂàÜÈíàÂØπ‰∏çÂêåÁöÑÊñπÊ≥ïÊúâ‰∏çÂêåÁöÑÂÆûÁé∞ÊñπÂºèÔºåÂàÜÂà´ÂØπÂ∫îÊñáÁåÆ [16, 18, 28, 30, 31] ‰∏≠ÁöÑÊñπÊ≥ï„ÄÇ</p>
<blockquote>
<p>Â∑≤ÁªèËØÅÊòé‰∫ÜÂπ∂Ë°åÂ§öÊ≠•È¢ÑÊµãÊïàÊûúÊõ¥Â•Ω</p>
</blockquote>
<h2 id="4-an-embarrassingly-simple-baseline">4. An Embarrassingly Simple Baseline<a class="headerlink" href="#4-an-embarrassingly-simple-baseline" title="Permanent link">¬∂</a></h2>
<p>In the experiments of existing Transformer-based LTSF solutions (<span class="arithmatex">\(T \gg 1\)</span>), all the compared (non-Transformer) baselines are IMS forecasting techniques, which are known to suffer from significant error accumulation effects. We hypothesize that the performance improvements in these works are largely due to the DMS strategy used in them.</p>
<p>Âú®Áé∞ÊúâÁöÑÂü∫‰∫éTransformerÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâËß£ÂÜ≥ÊñπÊ°àÔºà<span class="arithmatex">\(T \gg 1\)</span>ÔºâÁöÑÂÆûÈ™å‰∏≠ÔºåÊâÄÊúâÊØîËæÉÁöÑÔºàÈùûTransformerÔºâÂü∫Á∫øÈÉΩÊòØËø≠‰ª£Â§öÊ≠•ÔºàIMSÔºâÈ¢ÑÊµãÊäÄÊúØÔºåËøô‰∫õÊäÄÊúØÂ∑≤Áü•‰ºöÈÅ≠ÂèóÊòæËëóÁöÑËØØÂ∑ÆÁ¥ØÁßØÊïàÂ∫î„ÄÇÊàë‰ª¨ÂÅáËÆæËøô‰∫õÂ∑•‰Ωú‰∏≠ÁöÑÊÄßËÉΩÊèêÂçá‰∏ªË¶ÅÊòØÁî±‰∫éÂÖ∂‰∏≠‰ΩøÁî®ÁöÑÁõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÁ≠ñÁï•„ÄÇ</p>
<p>To validate this hypothesis, we present the simplest DMS model via a temporal linear layer, named LTSF-Linear, as a baseline for comparison. </p>
<p>The basic formulation of LTSF-Linear directly regresses historical time series for future prediction via a weighted sum operation (as illustrated in Figure 2). </p>
<p>The mathematical expression is <span class="arithmatex">\(\hat{X}_i = W X_i\)</span>, where <span class="arithmatex">\(W \in \mathbb{R}^{T \times L}\)</span> is a linear layer along the temporal axis. <span class="arithmatex">\(\hat{X}_i\)</span> and <span class="arithmatex">\(X_i\)</span> are the prediction and input for each <span class="arithmatex">\(i^{th}\)</span> variate. Note that LTSF-Linear shares weights across different variates and does not model any spatial correlations.</p>
<p>‰∏∫‰∫ÜÈ™åËØÅËøô‰∏ÄÂÅáËÆæÔºåÊàë‰ª¨ÈÄöËøá‰∏Ä‰∏™Êó∂Èó¥Á∫øÊÄßÂ±ÇÊèêÂá∫‰∫ÜÊúÄÁÆÄÂçïÁöÑÁõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÊ®°ÂûãÔºåÂëΩÂêç‰∏∫LTSF-LinearÔºå‰Ωú‰∏∫ÊØîËæÉÁöÑÂü∫Á∫ø„ÄÇLTSF-LinearÁöÑÂü∫Êú¨ÂÖ¨ÂºèÁõ¥Êé•ÈÄöËøáÂä†ÊùÉÊ±ÇÂíåÊìç‰ΩúÂõûÂΩíÂéÜÂè≤Êó∂Èó¥Â∫èÂàó‰ª•ËøõË°åÊú™Êù•È¢ÑÊµãÔºàÂ¶ÇÂõæ2ÊâÄÁ§∫Ôºâ„ÄÇÊï∞Â≠¶Ë°®ËææÂºè‰∏∫ <span class="arithmatex">\(\hat{X}_i = W X_i\)</span>ÔºåÂÖ∂‰∏≠ <span class="arithmatex">\(W \in \mathbb{R}^{T \times L}\)</span> ÊòØÊ≤øÊó∂Èó¥ËΩ¥ÁöÑÁ∫øÊÄßÂ±Ç„ÄÇ<span class="arithmatex">\(\hat{X}_i\)</span> Âíå <span class="arithmatex">\(X_i\)</span> ÂàÜÂà´ÊòØÊØè‰∏™Á¨¨ <span class="arithmatex">\(i\)</span> ‰∏™ÂèòÈáèÁöÑÈ¢ÑÊµãÂíåËæìÂÖ•„ÄÇËØ∑Ê≥®ÊÑèÔºåLTSF-LinearÂú®‰∏çÂêåÂèòÈáè‰πãÈó¥ÂÖ±‰∫´ÊùÉÈáçÔºåÂπ∂‰∏î‰∏çÂª∫Ê®°‰ªª‰ΩïÁ©∫Èó¥Áõ∏ÂÖ≥ÊÄß„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151046179.png" data-desc-position="bottom"><img alt="image-20250415104600444" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151046179.png"></a></p>
<p>LTSF-Linear is a set of linear models. Vanilla Linear is a one-layer linear model. To handle time series across different domains (e.g., finance, traffic, and energy domains), we further introduce two variants with two preprocessing methods, named DLinear and NLinear.</p>
<p>LTSF-LinearÊòØ‰∏ÄÁªÑÁ∫øÊÄßÊ®°Âûã„ÄÇÂÖ∂‰∏≠ÔºåVanilla LinearÊòØ‰∏Ä‰∏™ÂçïÂ±ÇÁ∫øÊÄßÊ®°Âûã„ÄÇ‰∏∫‰∫ÜÂ§ÑÁêÜ‰∏çÂêåÈ¢ÜÂüüÔºà‰æãÂ¶ÇÈáëËûç„ÄÅ‰∫§ÈÄöÂíåËÉΩÊ∫êÈ¢ÜÂüüÔºâÁöÑÊó∂Èó¥Â∫èÂàóÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÂºïÂÖ•‰∫Ü‰∏§ÁßçÈ¢ÑÂ§ÑÁêÜÊñπÊ≥ïÁöÑ‰∏§‰∏™Âèò‰ΩìÔºåÂàÜÂà´ÂëΩÂêç‰∏∫DLinearÂíåNLinear„ÄÇ</p>
<p>Specifically, DLinear is a combination of a Decomposition scheme used in Autoformer and FEDformer with linear layers. It first decomposes a raw data input into a trend component by a moving average kernel and a remainder (seasonal) component. Then, two one-layer linear layers are applied to each component, and we sum up the two features to get the final prediction. By explicitly handling trend, DLinear enhances the performance of a vanilla linear when there is a clear trend in the data.</p>
<p>ÂÖ∑‰ΩìÊù•ËØ¥ÔºåDLinearÊòØÁªìÂêà‰∫ÜAutoformerÂíåFEDformer‰∏≠‰ΩøÁî®ÁöÑÂàÜËß£ÊñπÊ°à‰∏éÁ∫øÊÄßÂ±ÇÁöÑÁªÑÂêà„ÄÇÂÆÉÈ¶ñÂÖàÈÄöËøáÁßªÂä®Âπ≥ÂùáÊ†∏Â∞ÜÂéüÂßãÊï∞ÊçÆËæìÂÖ•ÂàÜËß£‰∏∫Ë∂ãÂäøÊàêÂàÜÂíåÂâ©‰ΩôÔºàÂ≠£ËäÇÊÄßÔºâÊàêÂàÜ„ÄÇÁÑ∂ÂêéÔºåÂØπÊØè‰∏™ÊàêÂàÜÂ∫îÁî®‰∏§‰∏™ÂçïÂ±ÇÁ∫øÊÄßÂ±ÇÔºåÂπ∂Â∞Ü‰∏§‰∏™ÁâπÂæÅÁõ∏Âä†‰ª•Ëé∑ÂæóÊúÄÁªàÈ¢ÑÊµã„ÄÇÈÄöËøáÊòéÁ°ÆÂ§ÑÁêÜË∂ãÂäøÔºåÂΩìÊï∞ÊçÆ‰∏≠Â≠òÂú®ÊòéÊòæË∂ãÂäøÊó∂ÔºåDLinearÂ¢ûÂº∫‰∫ÜÊôÆÈÄöÁ∫øÊÄßÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ</p>
<blockquote>
<p>Â∫èÂàóÂàÜËß£ÊúâÁî®</p>
</blockquote>
<ul>
<li>Meanwhile, to boost the performance of LTSF-Linear when there is a distribution shift in the dataset, NLinear first subtracts the input by the last value of the sequence. Then, the input goes through a linear layer, and the subtracted part is added back before making the final prediction. The subtraction and addition in NLinear are a simple normalization for the input sequence.</li>
</ul>
<p>ÂêåÊó∂Ôºå‰∏∫‰∫ÜÂú®Êï∞ÊçÆÈõÜ‰∏≠Âá∫Áé∞ÂàÜÂ∏ÉÂÅèÁßªÊó∂ÊèêÂçáLTSF-LinearÁöÑÊÄßËÉΩÔºåNLinearÈ¶ñÂÖàÂ∞ÜËæìÂÖ•Â∫èÂàóÁöÑÊúÄÂêé‰∏Ä‰∏™ÂÄº‰ªéËæìÂÖ•‰∏≠ÂáèÂéª„ÄÇÁÑ∂ÂêéÔºåËæìÂÖ•ÁªèËøá‰∏Ä‰∏™Á∫øÊÄßÂ±ÇÔºåÊúÄÂêéÂú®ÂÅöÂá∫ÊúÄÁªàÈ¢ÑÊµãÂâçÂ∞ÜÂáèÂéªÁöÑÈÉ®ÂàÜÂä†Âõû„ÄÇNLinear‰∏≠ÁöÑÂáèÊ≥ïÂíåÂä†Ê≥ïÊòØÂØπËæìÂÖ•Â∫èÂàóËøõË°åÁöÑ‰∏ÄÁßçÁÆÄÂçïÂΩí‰∏ÄÂåñÂ§ÑÁêÜ„ÄÇ</p>
<blockquote>
<p>ÂáèÂéªÊúÄÂêé‰∏Ä‰∏™ÂÄºÔºåÂáèÂ∞ëÂàÜÂ∏ÉÂÅèÁßª</p>
</blockquote>
<h2 id="5-experiments">5. Experiments<a class="headerlink" href="#5-experiments" title="Permanent link">¬∂</a></h2>
<blockquote>
<p>ÊâÄ‰ª•ËØ¥ËøôÁØáËÆ∫Êñá‰∏ÄÂÆöË¶ÅËØªËØªÔºåÂõ†‰∏∫ÂÆÉÈÉΩÊ≤°Êúâmethod ÈÉ®ÂàÜÔºåÂÅö‰∫ÜÂæàÂ§öÂÆûÈ™å</p>
</blockquote>
<h3 id="51-experimental-settings">5.1. Experimental Settings<a class="headerlink" href="#51-experimental-settings" title="Permanent link">¬∂</a></h3>
<p><strong>Dataset.</strong> We conduct extensive experiments on nine widely-used real-world datasets, including ETT (Electricity Transformer Temperature) [30] (ETTh1, ETTh2, ETTm1, ETTm2), Traffic, Electricity, Weather, ILI, ExchangeRate [15]. All of them are multivariate time series. We leave data descriptions in the Appendix.</p>
<blockquote>
<p>ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™å</p>
</blockquote>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151057651.png" data-desc-position="bottom"><img alt="image-20250415105748104" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151057651.png"></a></p>
<p><strong>Evaluation metric.</strong> Following previous works [28, 30, 31], we use Mean Squared Error (MSE) and Mean Absolute Error (MAE) as the core metrics to compare performance. </p>
<p><strong>Compared methods.</strong> We include five recent Transformer-based methods: <strong>FEDformer [31], Autoformer [28], Informer [30], Pyraformer [18], and LogTrans [16].</strong> Besides, we include a naive DMS method: Closest Repeat (Repeat), which repeats the last value in the look-back window, as another simple baseline. Since there are two variants of FEDformer, we compare the one with better accuracy (FEDformer-f via Fourier transform).</p>
<p><strong>ÊØîËæÉÊñπÊ≥ï„ÄÇ</strong> Êàë‰ª¨Á∫≥ÂÖ•‰∫Ü‰∫îÁßçËøëÊúüÁöÑÂü∫‰∫éTransformerÁöÑÊñπÊ≥ïÔºöFEDformer[31]„ÄÅAutoformer[28]„ÄÅInformer[30]„ÄÅPyraformer[18]ÂíåLogTrans[16]„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂåÖÂê´‰∫Ü‰∏ÄÁßçÁÆÄÂçïÁöÑÁõ¥Êé•Â§öÊ≠•ÔºàDMSÔºâÊñπÊ≥ïÔºöClosest RepeatÔºàRepeatÔºâÔºåËØ•ÊñπÊ≥ïÈÄöËøáÈáçÂ§çÂõûÊ∫ØÁ™óÂè£‰∏≠ÁöÑÊúÄÂêé‰∏Ä‰∏™ÂÄºÊù•‰Ωú‰∏∫Âè¶‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂü∫Á∫ø„ÄÇÁî±‰∫éFEDformerÊúâ‰∏§ÁßçÂèò‰ΩìÔºåÊàë‰ª¨ÊØîËæÉÁöÑÊòØÂáÜÁ°ÆÂ∫¶Êõ¥È´òÁöÑÈÇ£‰∏ÄÁßçÔºàÈÄöËøáÂÇÖÈáåÂè∂ÂèòÊç¢ÁöÑFEDformer-fÔºâ„ÄÇ</p>
<h3 id="52-comparison-with-transformers">5.2. Comparison with Transformers<a class="headerlink" href="#52-comparison-with-transformers" title="Permanent link">¬∂</a></h3>
<p><strong>Quantitative results.</strong> In Table 2, we extensively evaluate all mentioned Transformers on nine benchmarks, following the experimental setting of previous work [28, 30, 31]. Surprisingly, the performance of LTSF-Linear surpasses the SOTA FEDformer in most cases by 20% ‚àº 50% improvements on the multivariate forecasting, where LTSFLinear even does not model correlations among variates. For different time series benchmarks, NLinear and DLinear show the superiority to handle the distribution shift and trend-seasonality features. We also provide results for univariate forecasting of ETT datasets in the Appendix, where LTSF-Linear still consistently outperforms Transformerbased LTSF solutions by a large margin.</p>
<p><strong>ÂÆöÈáèÁªìÊûú„ÄÇ</strong> Âú®Ë°®2‰∏≠ÔºåÊàë‰ª¨Ê†πÊçÆ‰πãÂâçÂ∑•‰Ωú[28, 30, 31]ÁöÑÂÆûÈ™åËÆæÁΩÆÔºåÂØπÊâÄÊúâÊèêÂà∞ÁöÑTransformerÊ®°ÂûãÂú®‰πù‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑËØÑ‰º∞„ÄÇ‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊòØÔºåLTSF-LinearÂú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÁöÑÊÄßËÉΩË∂ÖËøá‰∫ÜÊúÄÂÖàËøõÁöÑFEDformerÔºåÂÖ∂Âú®Â§öÂèòÈáèÈ¢ÑÊµã‰∏äÁöÑÊîπËøõÂπÖÂ∫¶ËææÂà∞‰∫Ü20%Ëá≥50%ÔºåÂ∞ΩÁÆ°LTSF-LinearÁîöËá≥Ê≤°ÊúâÂØπÂèòÈáè‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßËøõË°åÂª∫Ê®°„ÄÇÂØπ‰∫é‰∏çÂêåÁöÑÊó∂Èó¥Â∫èÂàóÂü∫ÂáÜÊµãËØïÔºåNLinearÂíåDLinearÊòæÁ§∫Âá∫Âú®Â§ÑÁêÜÂàÜÂ∏ÉÂÅèÁßªÂíåË∂ãÂäøÂ≠£ËäÇÊÄßÁâπÂæÅÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇÊàë‰ª¨ËøòÂú®ÈôÑÂΩï‰∏≠Êèê‰æõ‰∫ÜETTÊï∞ÊçÆÈõÜÁöÑÂçïÂèòÈáèÈ¢ÑÊµãÁªìÊûúÔºåÂÖ∂‰∏≠LTSF-Linear‰ªçÁÑ∂‰ª•ËæÉÂ§ß‰ºòÂäøÊåÅÁª≠Ë∂ÖË∂äÂü∫‰∫éTransformerÁöÑLTSFËß£ÂÜ≥ÊñπÊ°à„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151100018.png" data-desc-position="bottom"><img alt="image-20250415110002526" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151100018.png"></a></p>
<p>Ë°®2. Â§öÂèòÈáèÈïøÊúüÈ¢ÑÊµãËØØÂ∑Æ‰ª•ÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÂíåÂπ≥ÂùáÁªùÂØπËØØÂ∑ÆÔºàMAEÔºâË°®Á§∫ÔºåÊï∞ÂÄºË∂ä‰ΩéË∂äÂ•Ω„ÄÇÂÖ∂‰∏≠ÔºåILIÊï∞ÊçÆÈõÜÁöÑÈ¢ÑÊµãËåÉÂõ¥ <span class="arithmatex">\(T\)</span> Â±û‰∫é <span class="arithmatex">\(\{24, 36, 48, 60\}\)</span>„ÄÇÂØπ‰∫éÂÖ∂‰ªñÊï∞ÊçÆÈõÜÔºå<span class="arithmatex">\(T\)</span> Â±û‰∫é <span class="arithmatex">\(\{96, 192, 336, 720\}\)</span>„ÄÇRepeatÊñπÊ≥ïÈÄöËøáÈáçÂ§çÂõûÊ∫ØÁ™óÂè£‰∏≠ÁöÑÊúÄÂêé‰∏Ä‰∏™ÂÄºÊù•ËøõË°åÈ¢ÑÊµã„ÄÇÊúÄ‰Ω≥ÁªìÊûú‰ª•Á≤ó‰ΩìÊòæÁ§∫ÔºåËÄåÂü∫‰∫éTransformerÁöÑÊúÄ‰Ω≥ÁªìÊûúÂàô‰ª•‰∏ãÂàíÁ∫øÊ†áÂá∫„ÄÇÁõ∏Â∫îÂú∞ÔºåIMP.Ë°®Á§∫‰∏éÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÁõ∏ÊØîÔºåÁ∫øÊÄßÊ®°ÂûãÁöÑÊúÄ‰Ω≥ÁªìÊûú„ÄÇ</p>
<p>FEDformer achieves competitive forecasting accuracy on ETTh1. This because FEDformer employs classical time series analysis techniques such as frequency processing, which brings in time series inductive bias and benefits the ability of temporal feature extraction. In summary, these results reveal that existing complex Transformer-based LTSF solutions are not seemingly effective on the existing nine benchmarks while LTSF-Linear can be a powerful baseline.</p>
<p>FEDformerÂú®ETTh1Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊúâÁ´û‰∫âÂäõÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄß„ÄÇËøôÊòØÂõ†‰∏∫FEDformerÈááÁî®‰∫ÜËØ∏Â¶ÇÈ¢ëÁéáÂ§ÑÁêÜÁ≠âÁªèÂÖ∏ÁöÑÊó∂Èó¥Â∫èÂàóÂàÜÊûêÊäÄÊúØÔºåËøô‰∫õÊäÄÊúØÂºïÂÖ•‰∫ÜÊó∂Èó¥Â∫èÂàóÁöÑÂΩíÁ∫≥ÂÅèÂ•ΩÔºåÂπ∂ÊúâÂä©‰∫éÊó∂Èó¥ÁâπÂæÅÊèêÂèñÁöÑËÉΩÂäõ„ÄÇÊÄªÁªìÊù•ËØ¥ÔºåËøô‰∫õÁªìÊûúÊè≠Á§∫‰∫ÜÁé∞ÊúâÁöÑÂ§çÊùÇÂü∫‰∫éTransformerÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâËß£ÂÜ≥ÊñπÊ°àÂú®Áé∞ÊúâÁöÑ‰πù‰∏™Âü∫ÂáÜÊµãËØï‰∏≠‰ºº‰πéÂπ∂‰∏çÂçÅÂàÜÊúâÊïàÔºåËÄåLTSF-LinearÂèØ‰ª•Êàê‰∏∫‰∏Ä‰∏™Âº∫Â§ßÁöÑÂü∫Á∫øÊ®°Âûã„ÄÇ</p>
<p>Another interesting observation is that even though the naive Repeat method shows worse results when predicting long-term seasonal data (e.g., Electricity and Traffic), it surprisingly outperforms all Transformer-based methods on Exchange-Rate (around 45%). This is mainly caused by the wrong prediction of trends in Transformer-based solutions, which may overfit toward sudden change noises in the training data, resulting in significant accuracy degradation (see Figure 3(b)). Instead, Repeat does not have the bias.</p>
<p>Âè¶‰∏Ä‰∏™ÊúâË∂£ÁöÑËßÇÂØüÊòØÔºåÂ∞ΩÁÆ°Êú¥Á¥†ÁöÑRepeatÊñπÊ≥ïÂú®È¢ÑÊµãÈïøÊúüÂ≠£ËäÇÊÄßÊï∞ÊçÆÔºà‰æãÂ¶ÇÔºåÁîµÂäõÂíå‰∫§ÈÄöÔºâÊó∂Ë°®Áé∞Êõ¥Â∑ÆÔºå‰ΩÜÂÆÉÂú®Ê±áÁéáÔºàExchange-RateÔºâ‰∏äÊÑèÂ§ñÂú∞‰ºò‰∫éÊâÄÊúâÂü∫‰∫éTransformerÁöÑÊñπÊ≥ïÔºàÂ§ßÁ∫¶45%Ôºâ„ÄÇËøô‰∏ªË¶ÅÊòØÁî±‰∫éÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÂØπË∂ãÂäøÁöÑÈîôËØØÈ¢ÑÊµãÔºåËøô‰∫õËß£ÂÜ≥ÊñπÊ°àÂèØËÉΩËøáÂ∫¶ÊãüÂêàËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Á™ÅÁÑ∂ÂèòÂåñÁöÑÂô™Â£∞ÔºåÂØºËá¥ÊòæËëóÁöÑÂáÜÁ°ÆÊÄß‰∏ãÈôçÔºàËßÅÂõæ3(b)Ôºâ„ÄÇÁõ∏ÂèçÔºåRepeatÊñπÊ≥ïÊ≤°ÊúâËøôÁßçÂÅèÂ∑Æ„ÄÇ</p>
<p><strong>Qualitative results.</strong> As shown in Figure 3, we plot the prediction results on three selected time series datasets with Transformer-based solutions and LTSF-Linear: Electricity (Sequence 1951, Variate 36), Exchange-Rate (Sequence 676, Variate 3), and ETTh2 ( Sequence 1241, Variate 2), where these datasets have different temporal patterns. </p>
<p>When the input length is 96 steps, and the output horizon is 336 steps, Transformers [28, 30, 31] fail to capture the scale and bias of the future data on Electricity and ETTh2. </p>
<p>Moreover, they can hardly predict a proper trend on aperiodic data such as Exchange-Rate. </p>
<p>These phenomena further indicate the inadequacy of existing Transformer-based solutions for the LTSF task.</p>
<p><strong>ÂÆöÊÄßÁªìÊûú„ÄÇ</strong> Â¶ÇÂõæ3ÊâÄÁ§∫ÔºåÊàë‰ª¨ÁªòÂà∂‰∫Ü‰∏â‰∏™ÈÄâÂÆöÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÈõÜÁöÑÈ¢ÑÊµãÁªìÊûúÔºåËøô‰∫õÊï∞ÊçÆÈõÜÂÖ∑Êúâ‰∏çÂêåÁöÑÊó∂Èó¥Ê®°ÂºèÔºöÁîµÂäõÔºàSequence 1951, Variate 36Ôºâ„ÄÅÊ±áÁéáÔºàSequence 676, Variate 3ÔºâÂíåETTh2ÔºàSequence 1241, Variate 2ÔºâÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÂíåLTSF-LinearËøõË°åÊØîËæÉ„ÄÇÂΩìËæìÂÖ•ÈïøÂ∫¶‰∏∫96Ê≠•ÔºåËæìÂá∫ËåÉÂõ¥‰∏∫336Ê≠•Êó∂ÔºåTransformers[28, 30, 31]Êú™ËÉΩÊçïÊçâÂà∞ÁîµÂäõÂíåETTh2Êï∞ÊçÆÁöÑËßÑÊ®°ÂíåÂÅèÂ∑Æ„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰ª¨Âá†‰πéÊó†Ê≥ïÈ¢ÑÊµãËØ∏Â¶ÇÊ±áÁéáËøôÊ†∑ÁöÑÈùûÂë®ÊúüÊÄßÊï∞ÊçÆÁöÑÈÄÇÂΩìË∂ãÂäø„ÄÇËøô‰∫õÁé∞Ë±°Ëøõ‰∏ÄÊ≠•Ë°®ÊòéÁé∞ÊúâÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÂØπ‰∫éÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâ‰ªªÂä°ÁöÑ‰∏çË∂≥„ÄÇ</p>
<h3 id="53-more-analyses-on-ltsf-transformers">5.3. More Analyses on LTSF-Transformers<a class="headerlink" href="#53-more-analyses-on-ltsf-transformers" title="Permanent link">¬∂</a></h3>
<p>ËøôÈÉ®ÂàÜÁöÑËÑâÁªúÔºö</p>
<ol>
<li>Can existing LTSF-Transformers extract temporal relations well from longer input sequences? ÂõûÊ∫ØÁ™óÂè£ÁöÑÈïøÂ∫¶</li>
<li>What can be learned for long-term forecasting?Ôºàclose input&amp;far inputÔºâ</li>
<li>Are the self-attention scheme effective for LTSF?Â§çÊùÇÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊòØÂê¶ÊúâÁî®</li>
<li>Can existing LTSF-Transformers preserve temporal order well?Áé∞ÊúâÁöÑ Transformer Ê®°ÂûãÊòØÂê¶‰øùÂ≠ò‰∫ÜÊó∂Èó¥È°∫Â∫è</li>
<li>How effective are different embedding strategies?ÂµåÂÖ•Á≠ñÁï•ÁöÑËÆ®ËÆ∫</li>
<li>Is training data size a limiting factor for existing LTSFTransformers?ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑËßÑÊ®°Â§ßÂ∞èÊòØÂê¶ÂØπÊ®°ÂûãÊúâÊòæËëóÂΩ±Âìç</li>
<li>Is efficiency really a top-level priority?</li>
</ol>
<p>‰ΩúËÄÖÊèêÂá∫‰∫Ü 6 ‰∏™ÈóÆÈ¢òÔºåÂπ∂‰∏îÂàÜÂà´ËøõË°åÂÆûÈ™å</p>
<h4 id="1"><mark>(1)ÂõûÊ∫ØÁ™óÂè£ÁöÑÈïøÂ∫¶</mark><a class="headerlink" href="#1" title="Permanent link">¬∂</a></h4>
<p><strong>Can existing LTSF-Transformers extract temporal relations well from longer input sequences?</strong> </p>
<p>The size of the look-back window greatly impacts forecasting accuracy as it determines how much we can learn from historical data. Generally speaking, a powerful TSF model with a strong temporal relation extraction capability should be able to achieve better results with larger look-back window sizes.</p>
<p>Áé∞ÊúâÁöÑÂü∫‰∫éTransformerÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâÊ®°ÂûãËÉΩÂê¶ÂæàÂ•ΩÂú∞‰ªéËæÉÈïøÁöÑËæìÂÖ•Â∫èÂàó‰∏≠ÊèêÂèñÊó∂Èó¥ÂÖ≥Á≥ªÔºü</p>
<p>ÂõûÊ∫ØÁ™óÂè£ÁöÑÂ§ßÂ∞èÂØπÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÊúâÂæàÂ§ßÂΩ±ÂìçÔºåÂõ†‰∏∫ÂÆÉÂÜ≥ÂÆö‰∫ÜÊàë‰ª¨ÂèØ‰ª•‰ªéÂéÜÂè≤Êï∞ÊçÆ‰∏≠Â≠¶Âà∞Â§öÂ∞ë„ÄÇ‰∏ÄËà¨Êù•ËØ¥Ôºå‰∏Ä‰∏™Âº∫Â§ßÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâÊ®°ÂûãÂ¶ÇÊûúÂÖ∑ÊúâÂº∫Â§ßÁöÑÊó∂Èó¥ÂÖ≥Á≥ªÊèêÂèñËÉΩÂäõÔºåÂ∫îËØ•ËÉΩÂ§üÂú®Êõ¥Â§ßÁöÑÂõûÊ∫ØÁ™óÂè£Â∞∫ÂØ∏‰∏ãÂèñÂæóÊõ¥Â•ΩÁöÑÁªìÊûú„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂõûÊ∫ØÁ™óÂè£Â∞∫ÂØ∏Â¢ûÂ§ßÊó∂ÔºåÂü∫‰∫éTransformerÁöÑÊ®°ÂûãÁöÑÊÄßËÉΩ‰ºöÊÅ∂ÂåñÊàñ‰øùÊåÅÁ®≥ÂÆö„ÄÇËøôËøõ‰∏ÄÊ≠•Ë°®ÊòéÁé∞ÊúâÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÂØπ‰∫éÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâ‰ªªÂä°ÁöÑ‰∏çË∂≥„ÄÇ</p>
<p>To study the impact of input look-back window sizes, we conduct experiments with <span class="arithmatex">\(L \in \{24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720\}\)</span> for long-term forecasting (<span class="arithmatex">\(T=720\)</span>). </p>
<p>Figure 4 demonstrates the MSE results on two datasets. </p>
<p>Similar to the observations from previous studies [27, 30], existing Transformer-based models‚Äô performance deteriorates or stays stable when the look-back window size increases. </p>
<p>In contrast, the performances of all LTSF-Linear are significantly boosted with the increase of look-back window size. Thus, existing solutions tend to overfit temporal noises instead of extracting temporal information if given a longer sequence, and the input size 96 is exactly suitable for most Transformers.</p>
<p>‰∏∫‰∫ÜÁ†îÁ©∂ËæìÂÖ•ÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÁöÑÂΩ±ÂìçÔºåÊàë‰ª¨ÈíàÂØπÈïøÊúüÈ¢ÑÊµã (<span class="arithmatex">\(T=720\)</span>) ËøõË°å‰∫Ü <span class="arithmatex">\(L \in \{24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720\}\)</span> ÁöÑÂÆûÈ™å„ÄÇÂõæ4Â±ïÁ§∫‰∫Ü‰∏§‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑMSEÁªìÊûú„ÄÇ‰∏é‰πãÂâçÁ†îÁ©∂[27, 30]ÁöÑËßÇÂØüÁªìÊûúÁõ∏‰ººÔºåÁé∞ÊúâÁöÑÂü∫‰∫éTransformerÁöÑÊ®°ÂûãÂú®ÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÂ¢ûÂä†Êó∂ÊÄßËÉΩ‰ºöÊÅ∂ÂåñÊàñ‰øùÊåÅÁ®≥ÂÆö„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÊâÄÊúâLTSF-LinearÁöÑÊÄßËÉΩÈöèÁùÄÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÁöÑÂ¢ûÂä†ËÄåÊòæËëóÊèêÂçá„ÄÇ</p>
<p>Âõ†Ê≠§ÔºåÁé∞ÊúâÁöÑËß£ÂÜ≥ÊñπÊ°àÂú®ÁªôÂÆöÊõ¥ÈïøÂ∫èÂàóÊó∂ÂæÄÂæÄ‰ºöËøáÂ∫¶ÊãüÂêàÊó∂Èó¥Âô™Â£∞ÔºåËÄå‰∏çÊòØÊèêÂèñÊó∂Èó¥‰ø°ÊÅØÔºåËæìÂÖ•Â§ßÂ∞è96ÂØπ‰∫éÂ§ßÂ§öÊï∞TransformerÊù•ËØ¥ÊÅ∞Â•ΩÂêàÈÄÇ„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151139501.png" data-desc-position="bottom"><img alt="image-20250415113942382" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151139501.png"></a> </p>
<p>Âõæ3Â±ïÁ§∫‰∫Ü‰∫î‰∏™Ê®°ÂûãÂú®‰∏â‰∏™‰∏çÂêåÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÈõÜ‰∏äÁöÑÈïøÊúüÈ¢ÑÊµãËæìÂá∫ÔºàYËΩ¥Ôºâ‰∏éÁúüÂÆûÂÄºÔºàGrowthTruthÔºâÁöÑÂØπÊØî„ÄÇËøô‰∫õÊï∞ÊçÆÈõÜÂàÜÂà´ÊòØÁîµÂäõÔºàElectricityÔºâ„ÄÅÊ±áÁéáÔºàExchange-RateÔºâÂíåETTh2„ÄÇXËΩ¥‰ª£Ë°®Êó∂Èó¥Â∫èÂàóÁöÑÁ¥¢Âºï„ÄÇ</p>
<ul>
<li>
<p><strong>ÁîµÂäõÔºàElectricityÔºâ</strong>ÔºöÂú®ÁîµÂäõÊï∞ÊçÆÈõÜ‰∏äÔºåDLinearÊ®°ÂûãÔºàÈªÑËâ≤Á∫øÔºâÂíåFEDformerÔºàÈªÑËâ≤Á∫øÔºâÁöÑÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÂÄºÔºàÁ∫¢Ëâ≤Á∫øÔºâËæÉ‰∏∫Êé•ËøëÔºåÂ∞§ÂÖ∂ÊòØÂú®È¢ÑÊµãÁöÑÂâçÂçäÈÉ®ÂàÜ„ÄÇInformerÔºàËìùËâ≤Á∫øÔºâÂíåAutoformerÔºàÊµÖËìùËâ≤Á∫øÔºâÁöÑÈ¢ÑÊµãÁªìÊûúÂú®Êüê‰∫õÂå∫ÂüüÂÅèÁ¶ªÁúüÂÆûÂÄºËæÉÂ§ß„ÄÇ</p>
</li>
<li>
<p><strong>Ê±áÁéáÔºàExchange-RateÔºâ</strong>ÔºöÂú®Ê±áÁéáÊï∞ÊçÆÈõÜ‰∏äÔºåDLinearÊ®°ÂûãÔºàÈªÑËâ≤Á∫øÔºâÁöÑÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÂÄºÔºàÁ∫¢Ëâ≤Á∫øÔºâÊúÄ‰∏∫Êé•ËøëÔºåÂ∞§ÂÖ∂ÊòØÂú®È¢ÑÊµãÁöÑÂêéÂçäÈÉ®ÂàÜ„ÄÇFEDformerÔºàÈªÑËâ≤Á∫øÔºâÂíåInformerÔºàËìùËâ≤Á∫øÔºâÁöÑÈ¢ÑÊµãÁªìÊûúÂú®Êüê‰∫õÂå∫ÂüüÂÅèÁ¶ªÁúüÂÆûÂÄºËæÉÂ§ß„ÄÇ</p>
</li>
<li>
<p><strong>ETTh2</strong>ÔºöÂú®ETTh2Êï∞ÊçÆÈõÜ‰∏äÔºåDLinearÊ®°ÂûãÔºàÈªÑËâ≤Á∫øÔºâÁöÑÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÂÄºÔºàÁ∫¢Ëâ≤Á∫øÔºâËæÉ‰∏∫Êé•ËøëÔºåÂ∞§ÂÖ∂ÊòØÂú®È¢ÑÊµãÁöÑ‰∏≠Èó¥ÈÉ®ÂàÜ„ÄÇInformerÔºàËìùËâ≤Á∫øÔºâÂíåAutoformerÔºàÊµÖËìùËâ≤Á∫øÔºâÁöÑÈ¢ÑÊµãÁªìÊûúÂú®Êüê‰∫õÂå∫ÂüüÂÅèÁ¶ªÁúüÂÆûÂÄºËæÉÂ§ß„ÄÇ</p>
</li>
</ul>
<p>ÊÄª‰ΩìÊù•ÁúãÔºåDLinearÊ®°ÂûãÂú®Ëøô‰∏â‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÂÄºÊúÄ‰∏∫Êé•ËøëÔºåË°®Áé∞Âá∫ËæÉÂ•ΩÁöÑÈ¢ÑÊµãÊÄßËÉΩ„ÄÇËÄåÂÖ∂‰ªñÂü∫‰∫éTransformerÁöÑÊ®°ÂûãÔºàÂ¶ÇInformer„ÄÅAutoformerÂíåFEDformerÔºâÂú®Êüê‰∫õÂå∫ÂüüÁöÑÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÂÄºÂ≠òÂú®ËæÉÂ§ßÂÅèÂ∑Æ„ÄÇËøôË°®ÊòéÂú®Ëøô‰∫õÁâπÂÆöÁöÑÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÈõÜ‰∏äÔºåDLinearÊ®°ÂûãÂèØËÉΩÊõ¥ÈÄÇÂêàÊçïÊçâÊï∞ÊçÆÁöÑÊó∂Èó¥‰æùËµñÂÖ≥Á≥ª„ÄÇ</p>
<p>Additionally, we provide more quantitative results in the Appendix, and our conclusion holds in almost all cases.</p>
<p>Ê≠§Â§ñÔºåÊàë‰ª¨Âú®ÈôÑÂΩï‰∏≠Êèê‰æõ‰∫ÜÊõ¥ÂÆöÈáèÁöÑÁªìÊûúÔºåÂπ∂‰∏îÂú®Âá†‰πéÊâÄÊúâÊÉÖÂÜµ‰∏ãÊàë‰ª¨ÁöÑÁªìËÆ∫ÈÉΩ‰ºöÂæóÂá∫„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151143557.png" data-desc-position="bottom"><img alt="image-20250415114334054" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151143557.png"></a></p>
<p>Âõæ4Â±ïÁ§∫‰∫Ü‰∏çÂêåÊ®°ÂûãÂú®‰∫§ÈÄöÔºàTrafficÔºâÂíåÁîµÂäõÔºàElectricityÔºâÊï∞ÊçÆÈõÜ‰∏äËøõË°åÈïøÊúüÈ¢ÑÊµãÔºà<span class="arithmatex">\(T=720\)</span>Ê≠•ÔºâÊó∂ÁöÑÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÁªìÊûú„ÄÇXËΩ¥Ë°®Á§∫‰∏çÂêåÁöÑÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÔºåYËΩ¥Ë°®Á§∫MSEÂÄºÔºåÊï∞ÂÄºË∂ä‰ΩéË°®Á§∫È¢ÑÊµãÊÄßËÉΩË∂äÂ•Ω„ÄÇ</p>
<p>(a) 720Ê≠•-‰∫§ÈÄöÔºö
- ËØ•ÂõæÊòæÁ§∫‰∫ÜÂú®‰∫§ÈÄöÊï∞ÊçÆÈõÜ‰∏äÔºåÈöèÁùÄÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÁöÑÂ¢ûÂä†Ôºå‰∏çÂêåÊ®°ÂûãÁöÑMSEÂèòÂåñÊÉÖÂÜµ„ÄÇ
- ‰ªéÂõæ‰∏≠ÂèØ‰ª•ÁúãÂá∫ÔºåÂ§ßÂ§öÊï∞Âü∫‰∫éTransformerÁöÑÊ®°ÂûãÔºàÂ¶ÇInformer„ÄÅAutoformer„ÄÅPyraformerÂíåFEDformerÔºâÁöÑMSEÈöèÁùÄÁ™óÂè£Â§ßÂ∞èÁöÑÂ¢ûÂä†ËÄåÂ¢ûÂä†Êàñ‰øùÊåÅÁ®≥ÂÆöÔºåËøôË°®ÊòéËøô‰∫õÊ®°ÂûãÂú®Â§ÑÁêÜÊõ¥ÈïøÁöÑÂ∫èÂàóÊó∂ÂèØËÉΩ‰ºöËøáÊãüÂêàÊó∂Èó¥Âô™Â£∞ÔºåËÄå‰∏çÊòØÊèêÂèñÊó∂Èó¥‰ø°ÊÅØ„ÄÇ
- Áõ∏ÊØî‰πã‰∏ãÔºåLTSF-LinearÊ®°ÂûãÔºàÂåÖÊã¨NLinearÂíåDLinearÔºâÁöÑMSEÈöèÁùÄÁ™óÂè£Â§ßÂ∞èÁöÑÂ¢ûÂä†ËÄåÊòæËëóÈôç‰ΩéÔºåÊòæÁ§∫Âá∫Êõ¥Â•ΩÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ</p>
<p>(b) 720Ê≠•-ÁîµÂäõÔºö
- ËØ•ÂõæÊòæÁ§∫‰∫ÜÂú®ÁîµÂäõÊï∞ÊçÆÈõÜ‰∏äÔºå‰∏çÂêåÊ®°ÂûãÁöÑMSEÈöèÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÁöÑÂèòÂåñÊÉÖÂÜµ„ÄÇ
- ÂêåÊ†∑ÔºåÂü∫‰∫éTransformerÁöÑÊ®°ÂûãÔºàÂ¶ÇInformer„ÄÅAutoformer„ÄÅPyraformerÂíåFEDformerÔºâÁöÑMSEÂú®Á™óÂè£Â§ßÂ∞èÂ¢ûÂä†Êó∂Ë°®Áé∞‰∏çÁ®≥ÂÆöÊàñÂ¢ûÂä†„ÄÇ
- LTSF-LinearÊ®°ÂûãÔºàÂåÖÊã¨NLinearÂíåDLinearÔºâÂú®‰∏çÂêåÁ™óÂè£Â§ßÂ∞è‰∏ãÁöÑË°®Áé∞Áõ∏ÂØπÁ®≥ÂÆöÔºåÂπ∂‰∏îÂú®Êüê‰∫õÁ™óÂè£Â§ßÂ∞è‰∏ãË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ</p>
<p>ÊÄª‰ΩìËÄåË®ÄÔºåËøô‰∫õÁªìÊûúË°®ÊòéÔºåÁé∞ÊúâÁöÑÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÂú®Â§ÑÁêÜÈïøÊúüÈ¢ÑÊµã‰ªªÂä°Êó∂ÂèØËÉΩ‰∏çÂ¶ÇLTSF-LinearÊ®°ÂûãÊúâÊïàÔºåÁâπÂà´ÊòØÂú®ÂõûÊ∫ØÁ™óÂè£ËæÉÂ§ßÁöÑÊÉÖÂÜµ‰∏ã„ÄÇLTSF-LinearÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®Êõ¥ÈïøÁöÑÂéÜÂè≤Êï∞ÊçÆÊù•ÊèêÈ´òÈ¢ÑÊµãÂáÜÁ°ÆÊÄß„ÄÇ</p>
<h4 id="2close-input-far-input"><mark>(2)close input &amp; far input</mark><a class="headerlink" href="#2close-input-far-input" title="Permanent link">¬∂</a></h4>
<p><strong>What can be learned for long-term forecasting?</strong> While the temporal dynamics in the look-back window significantly impact the forecasting accuracy of short-term time series forecasting, we hypothesize that long-term forecasting depends on whether models can capture the trend and periodicity well only. That is, the farther the forecasting horizon, the less impact the look-back window itself has.</p>
<p><strong>ÈïøÊúüÈ¢ÑÊµãÁöÑÂêØÁ§∫Ôºö</strong> Â∞ΩÁÆ°ÂõûÊ∫ØÁ™óÂè£‰∏≠ÁöÑÊó∂Èó¥Âä®ÊÄÅÂØπÁü≠ÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÊúâÊòæËëóÂΩ±ÂìçÔºå‰ΩÜÊàë‰ª¨ÂÅáËÆæÈïøÊúüÈ¢ÑÊµã‰ªÖ‰æùËµñ‰∫éÊ®°ÂûãÊòØÂê¶ËÉΩÂ§üÂæàÂ•ΩÂú∞ÊçïÊçâË∂ãÂäøÂíåÂë®ÊúüÊÄß„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÈ¢ÑÊµãËåÉÂõ¥Ë∂äËøúÔºåÂõûÊ∫ØÁ™óÂè£Êú¨Ë∫´ÁöÑÂΩ±ÂìçÂ∞±Ë∂äÂ∞è„ÄÇ</p>
<blockquote>
<p>emÊàë‰πü‰ª•‰∏∫ÂõûÊ∫ØÁ™óÂè£Ë∂äÈïøÔºåÈ¢ÑÊµãÂáÜÁ°ÆÊÄßË∂äÈ´òÔºåÂèØÂÆûÈ™åÁªìÊûúÂç¥Â•ΩÂ•ΩÂÉèÊòØÔºåÂ¶ÇÊûúÂõûÊ∫ØÁ™óÂè£Â§™ÈïøÔºåÊó†ÂÖ≥‰ø°ÊÅØÈÄ†ÊàêÁöÑÂÜó‰Ωô‰ø°ÊÅØË∂äÂ§öÔºåÂèçËÄåË∂äÊù•Ë∂äÂ≠¶‰∏çÂà∞ÊúâÁî®ÁöÑ‰ø°ÊÅØ„ÄÇÊó∂Â∫èÊï∞ÊçÆÂ≥∞ÂÄºÊï∞ÊçÆÂèØËÉΩÂç†‰∫ÜÊõ¥Â§öÁöÑÊùÉÈáç</p>
</blockquote>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151146695.png" data-desc-position="bottom"><img alt="image-20250415114654222" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151146695.png"></a></p>
<p>Âõæ3Â±ïÁ§∫‰∫ÜFEDformerÂíåAutoformer‰∏§ÁßçÊ®°ÂûãÂú®‰∏çÂêåËæìÂÖ•Â∫èÂàóÔºàCloseÂíåFarÔºâ‰∏ãÁöÑÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÊØîËæÉÔºåÁî®‰∫éÊé¢Á¥¢ÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâÊ®°Âûã‰æùËµñ‰∫éÂì™‰∫õËæìÂÖ•Â∫èÂàó„ÄÇË°®‰∏≠ÂàóÂá∫‰∫Ü‰∏§ÁßçÊ®°ÂûãÂú®ÁîµÂäõÔºàElectricityÔºâÂíå‰∫§ÈÄöÔºàTrafficÔºâÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞„ÄÇ</p>
<ul>
<li><strong>CloseËæìÂÖ•</strong>ÔºöÊåáÁöÑÊòØ‰ΩøÁî®Êó∂Èó¥Â∫èÂàó‰∏≠ËæÉËøëÁöÑÊó∂Èó¥Ê≠•Ôºà‰æãÂ¶ÇÔºåÂØπ‰∫éÁîµÂäõÊï∞ÊçÆÈõÜÊòØÁ¨¨ <span class="arithmatex">\(96_{th}\)</span> Âà∞ <span class="arithmatex">\(191_{th}\)</span> Êó∂Èó¥Ê≠•Ôºâ‰Ωú‰∏∫ËæìÂÖ•Â∫èÂàó„ÄÇ</li>
<li><strong>FarËæìÂÖ•</strong>ÔºöÊåáÁöÑÊòØ‰ΩøÁî®Êó∂Èó¥Â∫èÂàó‰∏≠ËæÉËøúÁöÑÊó∂Èó¥Ê≠•Ôºà‰æãÂ¶ÇÔºåÂØπ‰∫éÁîµÂäõÊï∞ÊçÆÈõÜÊòØÁ¨¨ <span class="arithmatex">\(0_{th}\)</span> Âà∞ <span class="arithmatex">\(95_{th}\)</span> Êó∂Èó¥Ê≠•Ôºâ‰Ωú‰∏∫ËæìÂÖ•Â∫èÂàó„ÄÇ</li>
<li>‰∏§ÁßçËæìÂÖ•Â∫èÂàóÈÉΩÈ¢ÑÊµã‰ªéÁ¨¨ <span class="arithmatex">\(192_{th}\)</span> Êó∂Èó¥Ê≠•ÂºÄÂßãÁöÑÊú™Êù• <span class="arithmatex">\(720\)</span> ‰∏™Êó∂Èó¥Ê≠•„ÄÇ</li>
</ul>
<p>‰ªéË°®‰∏≠ÂèØ‰ª•ÁúãÂá∫Ôºö
- Âú®ÁîµÂäõÊï∞ÊçÆÈõÜ‰∏äÔºåÊó†ËÆ∫ÊòØCloseËøòÊòØFarËæìÂÖ•ÔºåFEDformerÁöÑMSEÈÉΩÁï•‰Ωé‰∫éAutoformerÔºåË°®ÊòéFEDformerÂú®Â§ÑÁêÜÁîµÂäõÊï∞ÊçÆÊó∂Ë°®Áé∞Á®çÂ•Ω„ÄÇ
- Âú®‰∫§ÈÄöÊï∞ÊçÆÈõÜ‰∏äÔºåÂΩìËæìÂÖ•‰∏∫CloseÊó∂ÔºåFEDformerÂíåAutoformerÁöÑMSEÁõ∏ËøëÔºõ‰ΩÜÂΩìËæìÂÖ•‰∏∫FarÊó∂ÔºåAutoformerÁöÑMSEÊòéÊòæÈ´ò‰∫éFEDformerÔºåË°®ÊòéFEDformerÂú®Â§ÑÁêÜËøúÁ´ØËæìÂÖ•Êó∂ÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ</p>
<p>ÊÄª‰ΩìËÄåË®ÄÔºåËøô‰∫õÁªìÊûúË°®ÊòéÔºåÂØπ‰∫éÈïøÊúüÈ¢ÑÊµã‰ªªÂä°ÔºåËæìÂÖ•Â∫èÂàóÁöÑÈÄâÊã©ÂØπÊ®°ÂûãÊÄßËÉΩÊúâÊòæËëóÂΩ±Âìç„ÄÇÁâπÂà´ÊòØÔºåFEDformerÂú®Â§ÑÁêÜËøúÁ´ØËæìÂÖ•Êó∂Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ</p>
<blockquote>
<p>ËØ¥ÊòéÈ¢ëÂüü‰ø°ÊÅØÊúâÁî®</p>
</blockquote>
<p>To validate the above hypothesis, in Table 3, we compare the forecasting accuracy for the same future 720 time steps with data from two different look-back windows: (i). the original input L=96 setting (called Close) and (ii). the far input L=96 setting (called Far) that is before the original 96 time steps. From the experimental results, the performance of the SOTA Transformers drops slightly, indicating these <strong>models only capture similar temporal information from the adjacent time series sequence</strong>. Since capturing the intrinsic characteristics of the dataset generally does not require a large number of parameters, i,e. one parameter can represent the periodicity. Using too many parameters will even cause overfitting, which partially explains why LTSFLinear performs better than Transformer-based methods.</p>
<p>‰∏∫‰∫ÜÈ™åËØÅ‰∏äËø∞ÂÅáËÆæÔºåÂú®Ë°®3‰∏≠ÔºåÊàë‰ª¨ÊØîËæÉ‰∫Ü‰ΩøÁî®‰∏§Áßç‰∏çÂêåÂõûÊ∫ØÁ™óÂè£Êï∞ÊçÆÈ¢ÑÊµãÁõ∏ÂêåÊú™Êù•720‰∏™Êó∂Èó¥Ê≠•ÁöÑÂáÜÁ°ÆÊÄßÔºö</p>
<p>(i) ÂéüÂßãËæìÂÖ•ËÆæÁΩÆ <span class="arithmatex">\(L=96\)</span>ÔºàÁß∞‰∏∫CloseÔºâÂíå</p>
<p>(ii) ËøúÁ´ØËæìÂÖ•ËÆæÁΩÆ <span class="arithmatex">\(L=96\)</span>ÔºàÁß∞‰∏∫FarÔºâÔºåÂç≥Âú®ÂéüÂßã96‰∏™Êó∂Èó¥Ê≠•‰πãÂâçÁöÑËÆæÁΩÆ„ÄÇ</p>
<p>‰ªéÂÆûÈ™åÁªìÊûúÊù•ÁúãÔºåÊúÄÂÖàËøõÁöÑTransformerÊ®°ÂûãÁöÑÊÄßËÉΩÁï•Êúâ‰∏ãÈôçÔºåËøôË°®ÊòéËøô‰∫õÊ®°Âûã‰ªÖ‰ªéÁõ∏ÈÇªÁöÑÊó∂Èó¥Â∫èÂàó‰∏≠ÊçïËé∑‰∫ÜÁ±ª‰ººÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÇÁî±‰∫éÊçïËé∑Êï∞ÊçÆÈõÜÁöÑÂÜÖÂú®ÁâπÂæÅÈÄöÂ∏∏‰∏çÈúÄË¶ÅÂ§ßÈáèÁöÑÂèÇÊï∞ÔºåÂç≥‰∏Ä‰∏™ÂèÇÊï∞Â∞±ÂèØ‰ª•‰ª£Ë°®Âë®ÊúüÊÄß„ÄÇ‰ΩøÁî®ËøáÂ§öÁöÑÂèÇÊï∞ÁîöËá≥‰ºöÂØºËá¥ËøáÊãüÂêàÔºåËøôÂú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äËß£Èáä‰∫Ü‰∏∫‰ªÄ‰πàLTSF-LinearÁöÑÊÄßËÉΩ‰ºò‰∫éÂü∫‰∫éTransformerÁöÑÊñπÊ≥ï„ÄÇ</p>
<blockquote>
<p>ÂóØÔºÅ</p>
<p>models only capture similar temporal information from the adjacent time series sequence</p>
<p>Áõ∏ÈÇªÊó∂Èó¥Â∫èÂàóÊçïËé∑Áõ∏ÂêåÁöÑË∂ãÂäø„ÄÇÊâÄ‰ª•Áõ¥Ëßâ‰∏ä‰ª•‰∏∫ÔºåÊ≤πÊ∏©ÂíåÊ∏©Â∫¶„ÄÅÊπøÂ∫¶ÊúâÂÖ≥ÁöÑÂΩ±ÂìçÂèçËÄå‰∏çÂ§ßÔºåÂèçËÄåÂ∏¶Êù•Êõ¥Â§öÁöÑÂô™Â£∞</p>
</blockquote>
<h4 id="3"><mark>(3)Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊòØÂê¶ÊúâÁî®</mark><a class="headerlink" href="#3" title="Permanent link">¬∂</a></h4>
<p><strong>Are the self-attention scheme effective for LTSF?</strong>  </p>
<p>We verify whether these complex designs in the existing Transformer (e.g., Informer) are essential. In Table 4, we gradually transform Informer to Linear. First, we replace each self-attention layer by a linear layer, called Att.-Linear, since a self-attention layer can be regarded as a fullyconnected layer where weights are dynamically changed. Furthermore, we discard other auxiliary designs (e.g., FFN) in Informer to leave embedding layers and linear layers, named Embed + Linear. Finally, we simplify the model to one linear layer. Surprisingly, the performance of Informer grows with the gradual simplification, indicating the unnecessary of the self-attention scheme and other complex modules at least for existing LTSF benchmarks.</p>
<p><strong>Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊòØÂê¶ÂØπÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâÊúâÊïàÔºü</strong> </p>
<p>Êàë‰ª¨ÈÄöËøáÂÆûÈ™åÈ™åËØÅÁé∞ÊúâTransformerÊ®°ÂûãÔºà‰æãÂ¶ÇInformerÔºâ‰∏≠Ëøô‰∫õÂ§çÊùÇËÆæËÆ°ÁöÑÂøÖË¶ÅÊÄß„ÄÇÂú®Ë°®4‰∏≠ÔºåÊàë‰ª¨ÈÄêÊ≠•Â∞ÜInformerËΩ¨Âèò‰∏∫Á∫øÊÄßÊ®°Âûã„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨Áî®Á∫øÊÄßÂ±ÇÊõøÊç¢ÊØè‰∏™Ëá™Ê≥®ÊÑèÂäõÂ±ÇÔºåÁß∞‰∏∫Att.-LinearÔºåÂõ†‰∏∫Ëá™Ê≥®ÊÑèÂäõÂ±ÇÂèØ‰ª•Ë¢´ËßÜ‰∏∫ÊùÉÈáçÂä®ÊÄÅÂèòÂåñÁöÑÂÖ®ËøûÊé•Â±Ç„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨‰∏¢ÂºÉInformer‰∏≠ÁöÑÂÖ∂‰ªñËæÖÂä©ËÆæËÆ°Ôºà‰æãÂ¶ÇÔºåFFNÔºâÔºå‰ªÖ‰øùÁïôÂµåÂÖ•Â±ÇÂíåÁ∫øÊÄßÂ±ÇÔºåÂëΩÂêç‰∏∫Embed + Linear„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Â∞ÜÊ®°ÂûãÁÆÄÂåñ‰∏∫‰∏Ä‰∏™Á∫øÊÄßÂ±Ç„ÄÇ‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊòØÔºåÈöèÁùÄÊ®°ÂûãÁöÑÈÄêÊ≠•ÁÆÄÂåñÔºåInformerÁöÑÊÄßËÉΩÂèçËÄåÊèêÈ´òÔºåËøôË°®ÊòéËá≥Â∞ëÂØπ‰∫éÁé∞ÊúâÁöÑLTSFÂü∫ÂáÜÊµãËØïÔºåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåÂÖ∂‰ªñÂ§çÊùÇÊ®°ÂùóÊòØ‰∏çÂøÖË¶ÅÁöÑ„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151156808.png" data-desc-position="bottom"><img alt="image-20250415115647556" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151156808.png"></a></p>
<p>Âõæ4‰∏≠ÁöÑË°®Ê†ºÂ±ïÁ§∫‰∫ÜÂ∞ÜInformerÊ®°ÂûãÈÄêÊ≠•ÁÆÄÂåñ‰∏∫Á∫øÊÄßÊ®°ÂûãÁöÑËøáÁ®ã‰∏≠Ôºå‰∏çÂêåÊ®°ÂûãÂú®‰∏§‰∏™Êï∞ÊçÆÈõÜÔºàExchangeÂíåETTh1Ôºâ‰∏äÁöÑÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÊØîËæÉ„ÄÇË°®Ê†º‰∏≠ÁöÑÂàóÂàÜÂà´‰ª£Ë°®‰∏çÂêåÁöÑÊ®°ÂûãÂèò‰ΩìÔºåË°å‰ª£Ë°®‰∏çÂêåÁöÑÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞è„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºö</p>
<ul>
<li><strong>Informer</strong>ÔºöÂéüÂßãÁöÑÂü∫‰∫éTransformerÁöÑÊ®°Âûã„ÄÇ</li>
<li><strong>Att.-Linear</strong>ÔºöÁî®Á∫øÊÄßÂ±ÇÊõøÊç¢ÊØè‰∏™Ëá™Ê≥®ÊÑèÂäõÂ±ÇÁöÑÊ®°ÂûãÔºåÂõ†‰∏∫Ëá™Ê≥®ÊÑèÂäõÂ±ÇÂèØ‰ª•Ë¢´ËßÜ‰∏∫ÊùÉÈáçÂä®ÊÄÅÂèòÂåñÁöÑÂÖ®ËøûÊé•Â±Ç„ÄÇ</li>
<li><strong>Embed + Linear</strong>ÔºöÂéªÊéâInformer‰∏≠ÁöÑÂÖ∂‰ªñËæÖÂä©ËÆæËÆ°Ôºà‰æãÂ¶ÇÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÔºåFFNÔºâÔºå‰ªÖ‰øùÁïôÂµåÂÖ•Â±ÇÂíåÁ∫øÊÄßÂ±Ç„ÄÇ</li>
<li><strong>Linear</strong>ÔºöËøõ‰∏ÄÊ≠•ÁÆÄÂåñ‰∏∫Âçï‰∏ÄÁ∫øÊÄßÂ±ÇÁöÑÊ®°Âûã„ÄÇ</li>
</ul>
<p>Ë°®Ê†º‰∏≠ÁöÑÊï∞ÊçÆÂ±ïÁ§∫‰∫ÜÈöèÁùÄÊ®°Âûã‰ªéÂ∑¶Âà∞Âè≥ÈÄêÊ≠•ÁÆÄÂåñÔºåMSEÁöÑÂèòÂåñÊÉÖÂÜµ„ÄÇÂèØ‰ª•ËßÇÂØüÂà∞‰ª•‰∏ãÂá†ÁÇπÔºö</p>
<ol>
<li>
<p><strong>ÊÄßËÉΩÊèêÂçá</strong>ÔºöÈöèÁùÄÊ®°ÂûãÁöÑÁÆÄÂåñÔºåInformerÁöÑÊÄßËÉΩÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÊúâÊâÄÊèêÂçáÔºåÁâπÂà´ÊòØÂú®ETTh1Êï∞ÊçÆÈõÜ‰∏äÔºåÁÆÄÂåñÂêéÁöÑÊ®°ÂûãÔºàEmbed + LinearÂíåLinearÔºâË°®Áé∞Âá∫Êõ¥‰ΩéÁöÑMSE„ÄÇ</p>
</li>
<li>
<p><strong>ÁÆÄÂåñÁöÑÊúâÊïàÊÄß</strong>ÔºöÂú®ExchangeÊï∞ÊçÆÈõÜ‰∏äÔºåÁÆÄÂåñÂêéÁöÑÊ®°ÂûãÔºàEmbed + LinearÂíåLinearÔºâÂú®ËæÉÂ§ßÁöÑÂõûÊ∫ØÁ™óÂè£Ôºà336Âíå720Ôºâ‰∏ãË°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊÄßËÉΩÔºåËÄåÂú®ËæÉÂ∞èÁöÑÂõûÊ∫ØÁ™óÂè£Ôºà96Âíå192Ôºâ‰∏ãÔºåÂéüÂßãInformerÊ®°ÂûãÁöÑÊÄßËÉΩÁï•Â•Ω„ÄÇ</p>
</li>
<li>
<p><strong>Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂøÖË¶ÅÊÄß</strong>ÔºöËøô‰∫õÁªìÊûúË°®ÊòéÔºåÂØπ‰∫éÁé∞ÊúâÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâÂü∫ÂáÜÊµãËØïÔºåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåÂÖ∂‰ªñÂ§çÊùÇÊ®°ÂùóÂèØËÉΩÂπ∂‰∏çÊòØÂøÖÈúÄÁöÑ„ÄÇÁÆÄÂåñÂêéÁöÑÊ®°ÂûãÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãËÉΩÂ§üÊèê‰æõÊõ¥Â•ΩÁöÑÈ¢ÑÊµãÊÄßËÉΩ„ÄÇ</p>
</li>
</ol>
<p>ÊÄªÁªìÊù•ËØ¥ÔºåËøô‰∫õÁªìÊûúÊåëÊàò‰∫ÜÁé∞ÊúâTransformerÊ®°ÂûãÂú®ÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰ªªÂä°‰∏≠ÁöÑÂøÖË¶ÅÊÄßÂíåÊúâÊïàÊÄßÔºåË°®ÊòéÁÆÄÂçïÁöÑÁ∫øÊÄßÊ®°ÂûãÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂèØËÉΩÊõ¥‰∏∫ÊúâÊïà„ÄÇ</p>
<blockquote>
<p>ÂïäÔºü‰πüËÆ∏Ê®°ÂûãÂπ∂‰∏çÊòØË∂äÂ§çÊùÇË∂äÂ•Ω</p>
</blockquote>
<h4 id="4"><mark>(4)‰ΩçÁΩÆ‰ø°ÊÅØÁöÑ‰øùÁïô</mark><a class="headerlink" href="#4" title="Permanent link">¬∂</a></h4>
<p><strong>Can existing LTSF-Transformers preserve temporal order well?</strong></p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151214419.png" data-desc-position="bottom"><img alt="image-20250415121412468" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151214419.png"></a></p>
<p>ËØ•ÂõæÁâáÂ±ïÁ§∫‰∫Ü‰∏ÄÂº†Ë°®Ê†ºÔºåÊ†áÈ¢ò‰∏∫‚ÄúTable 5. The MSE comparisons of models when shuffling the raw input sequence.‚Äù Ë°®Ê†ºÊØîËæÉ‰∫Ü‰∏çÂêåÊ®°ÂûãÂú®ËæìÂÖ•Â∫èÂàóË¢´Êâì‰π±Êó∂ÁöÑÂπ≥ÂùáÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåË°®Ê†º‰∏≠ÂàóÂá∫‰∫ÜÂõõÁßç‰∏çÂêåÁöÑÊâì‰π±Á≠ñÁï•ÔºöÂéüÂßãÂ∫èÂàóÔºàOri.Ôºâ„ÄÅÈöèÊú∫Êâì‰π±ÔºàShuf.Ôºâ„ÄÅÂçä‰∫§Êç¢ÔºàHalf-Ex.ÔºâÔºå‰ª•ÂèäÂú®Ëøô‰∫õÊâì‰π±Á≠ñÁï•‰∏ãÊ®°ÂûãÁöÑÊÄßËÉΩË°®Áé∞„ÄÇ</p>
<p>Ë°®Ê†º‰∏≠ÂåÖÂê´ÁöÑÊ®°ÂûãÊúâÔºö
- LinearÔºàÁ∫øÊÄßÊ®°ÂûãÔºâ
- FEDformer
- Autoformer
- Informer</p>
<p>È¢ÑÊµãÈïøÂ∫¶ÔºàPredict LengthÔºâÂàÜ‰∏∫Âõõ‰∏™‰∏çÂêåÁöÑÂÄºÔºö96„ÄÅ192„ÄÅ336„ÄÅ720„ÄÇ</p>
<p>ÂØπ‰∫éÊØè‰∏™Ê®°ÂûãÂíåÈ¢ÑÊµãÈïøÂ∫¶ÔºåË°®Ê†ºÂ±ïÁ§∫‰∫ÜÂú®‰∏çÂêåÊâì‰π±Á≠ñÁï•‰∏ãÁöÑÂπ≥ÂùáMSEÂÄº„ÄÇÊ≠§Â§ñÔºåËøòËÆ°ÁÆó‰∫ÜÊØèÁßçÊâì‰π±Á≠ñÁï•Áõ∏ÂØπ‰∫éÂéüÂßãÂ∫èÂàóÔºàOri.ÔºâÁöÑÂπ≥ÂùáÊÄßËÉΩ‰∏ãÈôçÁôæÂàÜÊØîÔºàAverage DropÔºâ„ÄÇ</p>
<p>‰ªéË°®Ê†º‰∏≠ÂèØ‰ª•ËßÇÂØüÂà∞Ôºö
- Á∫øÊÄßÊ®°ÂûãÔºàLinearÔºâÂú®ÊâÄÊúâÈ¢ÑÊµãÈïøÂ∫¶‰∏ãÔºåÈöèÊú∫Êâì‰π±ÔºàShuf.ÔºâÂíåÂçä‰∫§Êç¢ÔºàHalf-Ex.ÔºâÁ≠ñÁï•‰∏ãÁöÑÊÄßËÉΩ‰∏ãÈôçÁôæÂàÜÊØîÊúÄÂ§ßÔºåÂàÜÂà´‰∏∫27.26%Âíå46.81%ÔºàÂØπ‰∫éExchange RateÊï∞ÊçÆÈõÜÔºâ‰ª•Âèä81.06%Âíå4.78%ÔºàÂØπ‰∫éETTh1Êï∞ÊçÆÈõÜÔºâ„ÄÇ
- FEDformerÂíåAutoformerÂú®ÈöèÊú∫Êâì‰π±ÔºàShuf.ÔºâÁ≠ñÁï•‰∏ãÁöÑÊÄßËÉΩ‰∏ãÈôçÁôæÂàÜÊØîÁõ∏ÂØπËæÉÂ∞èÔºåÂàÜÂà´‰∏∫-0.09%Âíå0.09%ÔºàÂØπ‰∫éExchange RateÊï∞ÊçÆÈõÜÔºâ‰ª•Âèä73.28%Âíå56.91%ÔºàÂØπ‰∫éETTh1Êï∞ÊçÆÈõÜÔºâ„ÄÇ
- InformerÊ®°ÂûãÂú®ÊâÄÊúâÊâì‰π±Á≠ñÁï•‰∏ãÁöÑÊÄßËÉΩ‰∏ãÈôçÁôæÂàÜÊØîÊúÄÂ∞èÔºåÂàÜÂà´‰∏∫-0.12%Âíå-0.18%ÔºàÂØπ‰∫éExchange RateÊï∞ÊçÆÈõÜÔºâ‰ª•Âèä1.98%Âíå0.18%ÔºàÂØπ‰∫éETTh1Êï∞ÊçÆÈõÜÔºâ„ÄÇ</p>
<p>Ë°®Ê†ºÂ∫ïÈÉ®ÁöÑÊ≥®ÈáäËØ¥Êòé‰∫ÜËøô‰∫õÁªìÊûúÊòØÂü∫‰∫é‰∫îÊ¨°ËøêË°åÁöÑÂπ≥ÂùáÊµãËØïMSEÂæóÂá∫ÁöÑ„ÄÇÊÄª‰ΩìÊù•ÁúãÔºåInformerÊ®°ÂûãÂú®Â§ÑÁêÜÊâì‰π±ÁöÑËæìÂÖ•Â∫èÂàóÊó∂Ë°®Áé∞Êõ¥‰∏∫Á®≥ÂÅ•ÔºåËÄåÁ∫øÊÄßÊ®°ÂûãÁöÑÊÄßËÉΩ‰∏ãÈôçÊúÄ‰∏∫ÊòæËëó„ÄÇ</p>
<hr>
<p>Self-attention is inherently permutation invariant, i.e., regardless of the order. </p>
<p>However, in timeseries forecasting, the sequence order often plays a crucial role. We argue that even with positional and temporal embeddings, existing Transformer-based methods still suffer from temporal information loss. </p>
<p>ÔºàÊâì‰π±Á≠ñÁï•ÔºâIn Table 5, we shuffle the raw input before the embedding strategies. </p>
<p>Two shuffling strategies are presented: ‚ë† Shuf. randomly shuffles the whole input sequences and ‚ë° Half-Ex. exchanges the first half of the input sequence with the second half.</p>
<p>ÔºàÊâì‰π±‰ª•ÂêéÂæóÂÆûÈ™åÁªìÊûúÔºâInterestingly, compared with the original setting (Ori.) on the Exchange Rate, the performance of all Transformer-based methods does not fluctuate even when the input sequence is randomly shuffled. </p>
<p>By contrary, the performance of LTSF-Linear is damaged significantly. </p>
<p>ÔºàËÆ®ËÆ∫ÔºâThese indicate that LTSF-Transformers with different positional and temporal embeddings preserve quite limited temporal relations and are prone to overfit on noisy financial data, while the LTSF-Linear can model the order naturally and avoid overfitting with fewer parameters.</p>
<p>Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂Êú¨Ë¥®‰∏äÊòØÊéíÂàó‰∏çÂèòÁöÑÔºåÂç≥‰∏éÈ°∫Â∫èÊó†ÂÖ≥„ÄÇÁÑ∂ËÄåÔºåÂú®Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰∏≠ÔºåÂ∫èÂàóÈ°∫Â∫èÂæÄÂæÄËµ∑ÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇÊàë‰ª¨‰∏ªÂº†Âç≥‰Ωø‰ΩøÁî®‰ΩçÁΩÆÂíåÊó∂Èó¥ÂµåÂÖ•ÔºåÁé∞ÊúâÁöÑÂü∫‰∫éTransformerÁöÑÊñπÊ≥ï‰ªçÁÑ∂Â≠òÂú®Êó∂Èó¥‰ø°ÊÅØ‰∏¢Â§±ÁöÑÈóÆÈ¢ò„ÄÇ</p>
<p>Âú®Ë°®5‰∏≠ÔºåÊàë‰ª¨Âú®ÂµåÂÖ•Á≠ñÁï•‰πãÂâçÂØπÂéüÂßãËæìÂÖ•ËøõË°å‰∫ÜÈöèÊú∫Êâì‰π±„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊâì‰π±Á≠ñÁï•Ôºö‚ÄúShuf.‚ÄùÈöèÊú∫Êâì‰π±Êï¥‰∏™ËæìÂÖ•Â∫èÂàóÔºå‚ÄúHalf-Ex.‚ÄùÂ∞ÜËæìÂÖ•Â∫èÂàóÁöÑÂâçÂçäÈÉ®ÂàÜ‰∏éÂêéÂçäÈÉ®ÂàÜËøõË°å‰∫§Êç¢„ÄÇÊúâË∂£ÁöÑÊòØÔºå‰∏éÊ±áÁéáÊï∞ÊçÆÈõÜÁöÑÂéüÂßãËÆæÁΩÆÔºà‚ÄúOri.‚ÄùÔºâÁõ∏ÊØîÔºåÂç≥‰ΩøËæìÂÖ•Â∫èÂàóË¢´ÈöèÊú∫Êâì‰π±ÔºåÊâÄÊúâÂü∫‰∫éTransformerÁöÑÊñπÊ≥ïÁöÑÊÄßËÉΩ‰πüÊ≤°ÊúâÊ≥¢Âä®„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåLTSF-LinearÁöÑÊÄßËÉΩÂç¥ÂèóÂà∞‰∫ÜÊòæËëóÁöÑÊçüÂÆ≥„ÄÇ</p>
<p>Ëøô‰∫õÁªìÊûúË°®ÊòéÔºåÂÖ∑Êúâ‰∏çÂêå‰ΩçÁΩÆÂíåÊó∂Èó¥ÂµåÂÖ•ÁöÑLTSF-Transformer‰øùÁïôÁöÑÊó∂Èó¥ÂÖ≥Á≥ªÁõ∏ÂΩìÊúâÈôêÔºå‰∏îÂÆπÊòìÂú®ÂòàÊùÇÁöÑÈáëËûçÊï∞ÊçÆ‰∏äËøáÊãüÂêàÔºåËÄåÁÆÄÂçïÁöÑLTSF-LinearËÉΩÂ§üËá™ÁÑ∂Âú∞Ê®°ÊãüÈ°∫Â∫èÔºåÂπ∂‰∏îÁî±‰∫éÂèÇÊï∞ËæÉÂ∞ëËÄåÈÅøÂÖç‰∫ÜËøáÊãüÂêà„ÄÇ</p>
<p>For the ETTh1 dataset, FEDformer and Autoformer introduce time series inductive bias into their models, making them can extract certain temporal information when the dataset has more clear temporal patterns (e.g., periodicity) than the Exchange Rate.</p>
<p>Therefore, the average drops of the two Transformers are 73.28% and 56.91% under the Shuf. setting, where it loses the whole order information. </p>
<p>Moreover, Informer still suffers less from both Shuf. and Half-Ex. settings due to its no such temporal inductive bias. Overall, the average drops of LTSF-Linear are larger than Transformer-based methods for all cases, indicating the existing Transformers do not preserve temporal order well.</p>
<p>ÂØπ‰∫éETTh1Êï∞ÊçÆÈõÜÔºåFEDformerÂíåAutoformerÈÄöËøáÂºïÂÖ•Êó∂Èó¥Â∫èÂàóÂΩíÁ∫≥ÂÅèÁΩÆÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®Êï∞ÊçÆÈõÜÂÖ∑ÊúâÊõ¥Ê∏ÖÊô∞ÁöÑÊó∂Èó¥Ê®°ÂºèÔºà‰æãÂ¶ÇÂë®ÊúüÊÄßÔºâÊó∂ÊèêÂèñÁâπÂÆöÁöÑÊó∂Èó¥‰ø°ÊÅØÔºåËøôÊØîÊ±áÁéáÊï∞ÊçÆÈõÜÊõ¥ÂÖ∑‰ºòÂäø„ÄÇ</p>
<p>Âõ†Ê≠§ÔºåÂú®ÈöèÊú∫Êâì‰π±ÔºàShuf.ÔºâËÆæÁΩÆ‰∏ãÔºåËøô‰∏§‰∏™TransformerÁöÑÂπ≥ÂùáÊÄßËÉΩ‰∏ãÈôçÂπÖÂ∫¶ÂàÜÂà´‰∏∫73.28%Âíå56.91%ÔºåÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÊï¥‰∏™È°∫Â∫è‰ø°ÊÅØ‰∏¢Â§±„ÄÇ</p>
<p>Ê≠§Â§ñÔºåÁî±‰∫éInformerÊ≤°ÊúâËøôÁßçÊó∂Èó¥ÂΩíÁ∫≥ÂÅèÁΩÆÔºåÂõ†Ê≠§Âú®ÈöèÊú∫Êâì‰π±ÔºàShuf.ÔºâÂíåÂçä‰∫§Êç¢ÔºàHalf-Ex.ÔºâËÆæÁΩÆ‰∏ãÂèóÂà∞ÁöÑÂΩ±ÂìçËæÉÂ∞è„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåLTSF-LinearÂú®ÊâÄÊúâÊÉÖÂÜµ‰∏ãÂπ≥ÂùáÊÄßËÉΩ‰∏ãÈôçÂπÖÂ∫¶ÈÉΩÂ§ß‰∫éÂü∫‰∫éTransformerÁöÑÊñπÊ≥ïÔºåËøôË°®ÊòéÁé∞ÊúâÁöÑTransformerÂú®‰øùÁïôÊó∂Èó¥È°∫Â∫èÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ</p>
<h4 id="5"><mark>(5)ËÆ®ËÆ∫‰∏çÂêåÁöÑÂµåÂÖ•Á≠ñÁï•</mark><a class="headerlink" href="#5" title="Permanent link">¬∂</a></h4>
<p>How effective are different embedding strategies? </p>
<p>We study the benefits of position and timestamp embeddings used in Transformer-based methods. In Table 6, the forecasting errors of Informer largely increase without positional embeddings (wo/Pos.). Without timestamp embeddings (wo/Temp.) will gradually damage the performance of Informer as the forecasting lengths increase. Since Informer uses a single time step for each token, it is necessary to introduce temporal information in tokens.</p>
<p>‰∏çÂêåÁöÑÂµåÂÖ•Á≠ñÁï•Âú®TransformerÊ®°Âûã‰∏≠ÁöÑÊïàÊûúÂ¶Ç‰ΩïÔºü</p>
<p>Êàë‰ª¨Á†îÁ©∂‰∫Ü‰ΩçÁΩÆÂµåÂÖ•Ôºàpositional embeddingsÔºâÂíåÊó∂Èó¥Êà≥ÂµåÂÖ•Ôºàtimestamp embeddingsÔºâÂú®Âü∫‰∫éTransformerÁöÑÊñπÊ≥ï‰∏≠ÁöÑÊïàÊûú„ÄÇÂ¶ÇË°® 6 ÊâÄÁ§∫ÔºåÂØπ‰∫éInformerÊ®°ÂûãËÄåË®ÄÔºåÊ≤°Êúâ‰ΩçÁΩÆÂµåÂÖ•Ôºàwo/Pos.ÔºâÁöÑÊÉÖÂÜµ‰∏ãÔºåÈ¢ÑÊµãËØØÂ∑ÆÊòæËëóÂ¢ûÂä†„ÄÇ Ê≤°ÊúâÊó∂Èó¥Êà≥ÂµåÂÖ•Ôºàwo/Temp.Ôºâ‰ºöÈöèÁùÄÈ¢ÑÊµãÈïøÂ∫¶ÁöÑÂ¢ûÂä†ËÄåÈÄêÊ∏êÊçüÂÆ≥InformerÁöÑÊÄßËÉΩ„ÄÇËøôÊòØÂõ†‰∏∫InformerÊØè‰∏™token‰ªÖ‰ΩøÁî®‰∏Ä‰∏™Êó∂Èó¥Ê≠•ÈïøÔºåÂõ†Ê≠§Âú®token‰∏≠ÂºïÂÖ•Êó∂Èó¥‰ø°ÊÅØÊòØÂøÖË¶ÅÁöÑ„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151219356.png" data-desc-position="bottom"><img alt="image-20250415121920071" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151219356.png"></a></p>
<p>‚ÄúTable 6. The MSE comparisons of different embedding strategies on Transformer-based methods with look-back window size 96 and forecasting lengths {96, 192, 336, 720}.‚Äù </p>
<p>Ë°®Ê†ºÊØîËæÉ‰∫Ü‰∏çÂêåÂµåÂÖ•Á≠ñÁï•Âú®Âü∫‰∫éTransformerÁöÑÊñπÊ≥ï‰∏äÁöÑÂπ≥ÂùáÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÔºåËøô‰∫õÊñπÊ≥ï‰ΩøÁî®96Â§ßÂ∞èÁöÑÂõûÊ∫ØÁ™óÂè£ÔºåÂπ∂Âú®‰∏çÂêåÁöÑÈ¢ÑÊµãÈïøÂ∫¶Ôºà96„ÄÅ192„ÄÅ336„ÄÅ720Ôºâ‰∏ãËøõË°åÊµãËØï„ÄÇ</p>
<p>Ë°®Ê†º‰∏≠ÂåÖÂê´ÁöÑÊ®°ÂûãÊúâÔºö
- FEDformer
- Autoformer
- Informer</p>
<p>ÂµåÂÖ•Á≠ñÁï•ÂåÖÊã¨Ôºö
- ‰ΩøÁî®ÊâÄÊúâÂµåÂÖ•ÔºàAllÔºâ
- ‰∏ç‰ΩøÁî®‰ΩçÁΩÆÂµåÂÖ•Ôºàwo/Pos.Ôºâ
- ‰∏ç‰ΩøÁî®Êó∂Èó¥Êà≥ÂµåÂÖ•Ôºàwo/Temp.Ôºâ
- ÂêåÊó∂‰∏ç‰ΩøÁî®‰ΩçÁΩÆÂíåÊó∂Èó¥Êà≥ÂµåÂÖ•Ôºàwo/Pos.-Temp.Ôºâ</p>
<p>ÂØπ‰∫éÊØè‰∏™Ê®°ÂûãÂíåÈ¢ÑÊµãÈïøÂ∫¶ÔºåË°®Ê†ºÂ±ïÁ§∫‰∫ÜÂú®‰∏çÂêåÂµåÂÖ•Á≠ñÁï•‰∏ãÁöÑÂπ≥ÂùáMSEÂÄº„ÄÇËøô‰∫õÁªìÊûúÂèØ‰ª•Â∏ÆÂä©Êàë‰ª¨ÁêÜËß£‰ΩçÁΩÆÂµåÂÖ•ÂíåÊó∂Èó¥Êà≥ÂµåÂÖ•Âú®‰∏çÂêåÊ®°Âûã‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ</p>
<p>‰ªéË°®Ê†º‰∏≠ÂèØ‰ª•ËßÇÂØüÂà∞Ôºö
- ÂØπ‰∫éFEDformerÊ®°ÂûãÔºå‰∏ç‰ΩøÁî®‰ΩçÁΩÆÂµåÂÖ•Ôºàwo/Pos.ÔºâÊó∂ÔºåMSEÂÄºÁï•Êúâ‰∏ãÈôçÔºåËÄå‰∏ç‰ΩøÁî®Êó∂Èó¥Êà≥ÂµåÂÖ•Ôºàwo/Temp.ÔºâÊàñÂêåÊó∂‰∏ç‰ΩøÁî®‰∏§ËÄÖÔºàwo/Pos.-Temp.ÔºâÊó∂ÔºåMSEÂÄºÊúâÊâÄÂ¢ûÂä†„ÄÇ
- ÂØπ‰∫éAutoformerÊ®°ÂûãÔºå‰∏ç‰ΩøÁî®‰ΩçÁΩÆÂµåÂÖ•Ôºàwo/Pos.ÔºâÊó∂ÔºåMSEÂÄºÁï•Êúâ‰∏ãÈôçÔºåËÄå‰∏ç‰ΩøÁî®Êó∂Èó¥Êà≥ÂµåÂÖ•Ôºàwo/Temp.ÔºâÊàñÂêåÊó∂‰∏ç‰ΩøÁî®‰∏§ËÄÖÔºàwo/Pos.-Temp.ÔºâÊó∂ÔºåMSEÂÄºÊòæËëóÂ¢ûÂä†„ÄÇ
- ÂØπ‰∫éInformerÊ®°ÂûãÔºå‰∏ç‰ΩøÁî®‰ΩçÁΩÆÂµåÂÖ•Ôºàwo/Pos.ÔºâÊó∂ÔºåMSEÂÄºÊòæËëóÂ¢ûÂä†ÔºåËÄå‰∏ç‰ΩøÁî®Êó∂Èó¥Êà≥ÂµåÂÖ•Ôºàwo/Temp.ÔºâÊàñÂêåÊó∂‰∏ç‰ΩøÁî®‰∏§ËÄÖÔºàwo/Pos.-Temp.ÔºâÊó∂ÔºåMSEÂÄº‰πüÊòæËëóÂ¢ûÂä†ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËæÉÈïøÁöÑÈ¢ÑÊµãÈïøÂ∫¶‰∏ã„ÄÇ</p>
<p>‰ΩçÁΩÆÂµåÂÖ•ÂíåÊó∂Èó¥Êà≥ÂµåÂÖ•ÂØπ‰∫éInformerÊ®°ÂûãÁöÑÊÄßËÉΩËá≥ÂÖ≥ÈáçË¶ÅÔºåËÄåÂØπ‰∫éFEDformerÂíåAutoformerÊ®°ÂûãÔºå‰ΩçÁΩÆÂµåÂÖ•ÁöÑÂΩ±ÂìçËæÉÂ∞èÔºåÊó∂Èó¥Êà≥ÂµåÂÖ•ÁöÑÂΩ±ÂìçËæÉ‰∏∫ÊòæËëó„ÄÇ</p>
<p>Rather than using a single time step in each token, FEDformer and Autoformer input a sequence of timestamps to embed the temporal information. Hence, they can achieve comparable or even better performance without fixed positional embeddings. However, without timestamp embeddings, the performance of Autoformer declines rapidly because of the loss of global temporal information. Instead, thanks to the frequency-enhanced module proposed in FEDformer to introduce temporal inductive bias, it suffers less from removing any position/timestamp embeddings.</p>
<p>‰∏éÊØè‰∏™token‰ªÖ‰ΩøÁî®Âçï‰∏ÄÊó∂Èó¥Ê≠•Èïø‰∏çÂêåÔºåFEDformerÂíåAutoformerËæìÂÖ•‰∏ÄÁ≥ªÂàóÊó∂Èó¥Êà≥Êù•ÂµåÂÖ•Êó∂Èó¥‰ø°ÊÅØ„ÄÇÂõ†Ê≠§ÔºåÂÆÉ‰ª¨ËÉΩÂ§üÂú®Ê≤°ÊúâÂõ∫ÂÆö‰ΩçÁΩÆÂµåÂÖ•ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ÂèØÊØîÁîöËá≥Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂ¶ÇÊûúÊ≤°ÊúâÊó∂Èó¥Êà≥ÂµåÂÖ•ÔºåAutoformerÁöÑÊÄßËÉΩ‰ºöÂõ†‰∏∫Â§±ÂéªÂÖ®Â±ÄÊó∂Èó¥‰ø°ÊÅØËÄåËøÖÈÄü‰∏ãÈôç„ÄÇÁõ∏ÂèçÔºåÁî±‰∫éFEDformer‰∏≠ÊèêÂá∫ÁöÑÈ¢ëÁéáÂ¢ûÂº∫Ê®°ÂùóÂºïÂÖ•‰∫ÜÊó∂Èó¥ÂΩíÁ∫≥ÂÅèÁΩÆÔºåÂÆÉÂú®ÁßªÈô§‰ªª‰Ωï‰ΩçÁΩÆ/Êó∂Èó¥Êà≥ÂµåÂÖ•Êó∂ÂèóÂà∞ÁöÑÂΩ±ÂìçËæÉÂ∞è„ÄÇ</p>
<blockquote>
<p>‰ΩúËÄÖÂ•ΩÊáÇËøô‰∫õÊ®°Âûã</p>
</blockquote>
<h4 id="6"><mark>(6)Êï∞ÊçÆÈõÜÁöÑËßÑÊ®°</mark><a class="headerlink" href="#6" title="Permanent link">¬∂</a></h4>
<p><strong>Is training data size a limiting factor for existing LTSFTransformers?</strong> Some may argue that the poor performance of Transformer-based solutions is due to the small sizes of the benchmark datasets. Unlike computer vision or natural language processing tasks, TSF is performed on collected time series, and it is difficult to scale up the training data size. In fact, the size of the training data would indeed have a significant impact on the model performance. Accordingly, we conduct experiments on Traffic, comparing the performance of the model trained on a full dataset (17,544*0.7 hours), named Ori., with that trained on a shortened dataset (8,760 hours, i.e., 1 year), called Short.Unexpectedly, Table 7 presents that the prediction errors with reduced training data are lower in most cases. This might because the whole-year data maintains more clear temporal features than a longer but incomplete data size. While we cannot conclude that we should use less data for training, it demonstrates that the training data scale is not the limiting reason for the performances of Autoformer and FEDformer.</p>
<p>Êúâ‰∫∫ÂèØËÉΩ‰ºöËÆ§‰∏∫Âü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÊÄßËÉΩ‰∏ç‰Ω≥ÊòØÁî±‰∫éÂü∫ÂáÜÊï∞ÊçÆÈõÜÁöÑËßÑÊ®°ËæÉÂ∞è„ÄÇ‰∏éËÆ°ÁÆóÊú∫ËßÜËßâÊàñËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏çÂêåÔºåÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâÊòØÂú®Êî∂ÈõÜÂà∞ÁöÑÊó∂Èó¥Â∫èÂàó‰∏äÊâßË°åÁöÑÔºåÂπ∂‰∏îÂæàÈöæÊâ©Â§ßËÆ≠ÁªÉÊï∞ÊçÆÁöÑËßÑÊ®°„ÄÇÂÆûÈôÖ‰∏äÔºåËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§ßÂ∞èÁ°ÆÂÆû‰ºöÂØπÊ®°ÂûãÊÄßËÉΩ‰∫ßÁîüÈáçÂ§ßÂΩ±Âìç„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨Âú®TrafficÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÊØîËæÉ‰∫ÜÂú®ÂÆåÊï¥Êï∞ÊçÆÈõÜÔºà17,544*0.7Â∞èÊó∂Ôºâ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºàÂëΩÂêç‰∏∫Ori.Ôºâ‰∏éÂú®Áº©Áü≠Êï∞ÊçÆÈõÜÔºà8,760Â∞èÊó∂ÔºåÂç≥1Âπ¥Ôºâ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºàÁß∞‰∏∫ShortÔºâÁöÑÊÄßËÉΩ„ÄÇÂá∫‰πéÊÑèÊñôÁöÑÊòØÔºåË°®7ÊòæÁ§∫ÔºåÂú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÔºåÂáèÂ∞ëËÆ≠ÁªÉÊï∞ÊçÆÂêéÁöÑÈ¢ÑÊµãËØØÂ∑ÆÊõ¥‰Ωé„ÄÇËøôÂèØËÉΩÊòØÂõ†‰∏∫ÂÖ®Âπ¥Êï∞ÊçÆÊØîÊõ¥Èïø‰ΩÜ‰∏çÂÆåÊï¥ÁöÑÊï∞ÊçÆÈõÜ‰øùÊåÅ‰∫ÜÊõ¥Ê∏ÖÊô∞ÁöÑÊó∂Èó¥ÁâπÂæÅ„ÄÇËôΩÁÑ∂Êàë‰ª¨‰∏çËÉΩÂæóÂá∫Êàë‰ª¨Â∫îËØ•‰ΩøÁî®Êõ¥Â∞ëÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÁöÑÁªìËÆ∫Ôºå‰ΩÜÂÆÉË°®ÊòéËÆ≠ÁªÉÊï∞ÊçÆËßÑÊ®°Âπ∂‰∏çÊòØAutoformerÂíåFEDformerÊÄßËÉΩÁöÑÈôêÂà∂ÂéüÂõ†„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151225895.png" data-desc-position="bottom"><img alt="image-20250415122504931" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151225895.png"></a></p>
<p>ËÆ≠ÁªÉÊï∞ÊçÆÈõÜËßÑÊ®°ÂèòÂ∞è‰ΩÜÊòØÈ¢ÑÊµãÊïàÊûúÊõ¥Â•Ω‰∫ÜÔºåÂõ†‰∏∫ËôΩÁÑ∂ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂèòÂ∞è‰∫ÜÔºå‰ΩÜ‰øùÂ≠ò‰∫ÜÂÆåÊï¥ÁöÑÊï∞ÊçÆÂèòÂåñÊ®°Âºè„ÄÇ  </p>
<h4 id="7"><mark>(7)ÊïàÁéáÁúüÁöÑÂæàÈáçË¶ÅÂêóÔºü</mark><a class="headerlink" href="#7" title="Permanent link">¬∂</a></h4>
<p><strong>Is efficiency really a top-level priority?</strong> </p>
<p>Existing LTSF-Transformers claim that the <span class="arithmatex">\(O(L^2)\)</span> complexity of the vanilla Transformer is unaffordable for the LTSF problem.ÂéüÂßã Transformer ÊòØÂπ≥ÊñπÁ∫ßÂÜÖÂ≠òÂ§çÊùÇÂ∫¶ÂíåÊó∂Èó¥Â§çÊùÇÂ∫¶</p>
<p>Although they prove to be able to improve the theoretical time and memory complexity from <span class="arithmatex">\(O(L^2)\)</span> to <span class="arithmatex">\(O(L)\)</span>, it is unclear whether 1) the actual inference time and memory cost on devices are improved, and 2) the memory issue is unacceptable and urgent for today's GPU (e.g., an NVIDIA Titan XP here). </p>
<p>ÊïàÁéáÁúüÁöÑÊòØÊúÄ‰ºòÂÖàËÄÉËôëÁöÑÂõ†Á¥†ÂêóÔºüÁé∞ÊúâÁöÑÈïøÂ∫èÂàóÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãTransformerÔºàLTSF-TransformersÔºâÂ£∞Áß∞Ôºå‰º†ÁªüTransformerÁöÑ<span class="arithmatex">\(O(L^2)\)</span>Â§çÊùÇÂ∫¶ÂØπ‰∫éÈïøÂ∫èÂàóÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâÈóÆÈ¢òÊù•ËØ¥ÊòØÈöæ‰ª•ÊâøÂèóÁöÑ„ÄÇÂ∞ΩÁÆ°ÂÆÉ‰ª¨ËØÅÊòé‰∫ÜËÉΩÂ§üÂ∞ÜÁêÜËÆ∫Êó∂Èó¥ÂíåÂÜÖÂ≠òÂ§çÊùÇÂ∫¶‰ªé<span class="arithmatex">\(O(L^2)\)</span>ÊîπËøõÂà∞<span class="arithmatex">\(O(L)\)</span>Ôºå‰ΩÜÁõÆÂâçËøò‰∏çÊ∏ÖÊ•ö1ÔºâËÆæÂ§á‰∏äÁöÑÂÆûÈôÖÊé®ÁêÜÊó∂Èó¥ÂíåÂÜÖÂ≠òÊàêÊú¨ÊòØÂê¶ÂæóÂà∞ÊîπÂñÑÔºå‰ª•Âèä2ÔºâÂÜÖÂ≠òÈóÆÈ¢òÊòØÂê¶ÊòØÂΩìÂâçGPUÔºà‰æãÂ¶ÇÔºåËøôÈáåÁöÑNVIDIA Titan XPÔºâÊâÄ‰∏çËÉΩÊé•Âèó‰∏îÁ¥ßËø´ÁöÑ„ÄÇ</p>
<p>In Table 8, we compare the average practical efficiencies with 5 runs. Interestingly, compared with the vanilla Transformer (with the same DMS decoder), most Transformer variants incur similar or even worse inference time and parameters in practice. These follow-ups introduce more additional design elements to make practical costs high. Moreover, the memory cost of the vanilla Transformer is practically acceptable, even for output length <span class="arithmatex">\(L = 720\)</span>, which weakens the importance of developing a memory-efficient Transformers, at least for existing benchmarks.</p>
<p>Âú®Ë°®8‰∏≠ÔºåÊàë‰ª¨ÊØîËæÉ‰∫Ü5Ê¨°ËøêË°åÁöÑÂπ≥ÂùáÂÆûÈôÖÊïàÁéá„ÄÇÊúâË∂£ÁöÑÊòØÔºå‰∏é‰º†ÁªüTransformerÔºà‰ΩøÁî®Áõ∏ÂêåÁöÑDMSËß£Á†ÅÂô®ÔºâÁõ∏ÊØîÔºåÂ§ßÂ§öÊï∞TransformerÂèò‰ΩìÂú®ÂÆûË∑µ‰∏≠‰∫ßÁîü‰∫ÜÁõ∏‰ººÁîöËá≥Êõ¥Â∑ÆÁöÑÊé®ÁêÜÊó∂Èó¥ÂíåÂèÇÊï∞„ÄÇËøô‰∫õÂêéÁª≠Â∑•‰ΩúÂºïÂÖ•‰∫ÜÊõ¥Â§öÁöÑËÆæËÆ°ÂÖÉÁ¥†Ôºå‰ΩøÂæóÂÆûÈôÖÊàêÊú¨ÂèòÈ´ò„ÄÇÊ≠§Â§ñÔºåÂç≥‰ΩøÂØπ‰∫éËæìÂá∫ÈïøÂ∫¶<span class="arithmatex">\(L = 720\)</span>Ôºå‰º†ÁªüTransformerÁöÑÂÜÖÂ≠òÊàêÊú¨Âú®ÂÆûË∑µ‰∏≠‰πüÊòØÂèØ‰ª•Êé•ÂèóÁöÑÔºåËøôÂâäÂº±‰∫ÜËá≥Â∞ëÂØπ‰∫éÁé∞ÊúâÂü∫ÂáÜÊµãËØïËÄåË®ÄÂºÄÂèëÂÜÖÂ≠òÈ´òÊïàTransformerÁöÑÈáçË¶ÅÊÄß„ÄÇ</p>
<blockquote>
<p>ËøôÁØáËÆ∫ÊñáÂá∫Êù•ÔºåÂ§ßÂÆ∂‰ª•ÂêéÊÄé‰πàÁ†îÁ©∂Êó∂Â∫èÂëÄÔºåÂÖ®Èù¢Âê¶ÂÆöÁöÑÁ®ãÂ∫¶</p>
</blockquote>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151232240.png" data-desc-position="bottom"><img alt="image-20250415123206921" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151232240.png"></a></p>
<p>‚ÄúTable 8. Comparison of practical efficiency of LTSF-Transformers under L=96 and T=720 on the Electricity.‚Äù Ë°®Ê†ºÊØîËæÉ‰∫ÜÂú®ÁîµÂäõÊï∞ÊçÆÈõÜ‰∏äÔºåÈ¢ÑÊµãÈïøÂ∫¶L=96ÂíåÊó∂Èó¥Ê≠•ÈïøT=720Êó∂Ôºå‰∏çÂêåÈïøÂ∫èÂàóÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâTransformerÊ®°ÂûãÁöÑÂÆûÈôÖÊïàÁéá„ÄÇÊïàÁéáÈÄöËøáÂõõ‰∏™ÊåáÊ†áÊù•Ë°°ÈáèÔºöMACsÔºà‰πòÁ¥ØÂä†Êìç‰ΩúÊï∞Ôºâ„ÄÅÂèÇÊï∞Êï∞Èáè„ÄÅÊé®ÁêÜÊó∂Èó¥‰ª•ÂèäÂÜÖÂ≠ò‰ΩøÁî®Èáè„ÄÇ</p>
<p>Ë°®Ê†º‰∏≠ÂåÖÂê´ÁöÑÊ®°ÂûãÊúâÔºö
- DLinear
- Transformer√óÔºà‰øÆÊîπËá™AutoformerÁöÑ‰∏ÄÊ≠•Ëß£Á†ÅÂô®Ôºâ
- Informer
- Autoformer
- Pyraformer
- FEDformer</p>
<p>Ë°®Ê†º‰∏≠ÁöÑÊï∞ÊçÆÂ¶Ç‰∏ãÔºö
- DLinearÊ®°ÂûãÁöÑMACs‰∏∫0.04GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫139.K7ÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫0.4msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫687MiB„ÄÇ
- Transformer√óÊ®°ÂûãÁöÑMACs‰∏∫4.03GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫13.61MÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫26.8msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫6091MiB„ÄÇ
- InformerÊ®°ÂûãÁöÑMACs‰∏∫3.93GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫14.39MÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫49.3msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫3869MiB„ÄÇ
- AutoformerÊ®°ÂûãÁöÑMACs‰∏∫4.41GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫14.91MÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫164.1msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫7607MiB„ÄÇ
- PyraformerÊ®°ÂûãÁöÑMACs‰∏∫0.80GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫241.4M*ÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫3.4msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫7017MiB„ÄÇÔºàÊ≥®ÔºöPyraformerÁöÑÂèÇÊï∞Êï∞Èáè‰∏≠236.7MÊù•Ëá™ÂÖ∂Á∫øÊÄßËß£Á†ÅÂô®Ôºâ
- FEDformerÊ®°ÂûãÁöÑMACs‰∏∫4.41GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫20.68MÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫40.5msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫4143MiB„ÄÇ</p>
<p>Ë°®Ê†ºÂ∫ïÈÉ®ÁöÑÊ≥®ÈáäËØ¥ÊòéÔºö
- Transformer√óÊ®°ÂûãË¢´‰øÆÊîπ‰∏∫‰∏éAutoformerÁõ∏ÂêåÁöÑ‰∏ÄÊ≠•Ëß£Á†ÅÂô®„ÄÇ
- PyraformerÊ®°ÂûãÁöÑÂèÇÊï∞Êï∞Èáè‰∏≠Ôºå236.7MÊù•Ëá™ÂÖ∂Á∫øÊÄßËß£Á†ÅÂô®„ÄÇ
- ‰ΩøÁî®DLinear‰Ωú‰∏∫ÊØîËæÉÂü∫ÂáÜÔºåÂõ†‰∏∫ÂÆÉÂú®LTSF-Linear‰∏≠ÁöÑÊàêÊú¨ÊòØDLinearÁöÑ‰∏§ÂÄç„ÄÇ
- Êé®ÁêÜÊó∂Èó¥ÊòØ5Ê¨°ËøêË°åÁöÑÂπ≥ÂùáÂÄº„ÄÇ</p>
<p>ÊÄª‰ΩìÊù•ÁúãÔºåDLinearÊ®°ÂûãÂú®ÊâÄÊúâÊåáÊ†á‰∏äÈÉΩË°®Áé∞Âá∫ËæÉÈ´òÁöÑÊïàÁéáÔºåËÄåAutoformerÂíåFEDformerÂú®ÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏äËæÉÈ´òÔºåInformerÂú®Êé®ÁêÜÊó∂Èó¥‰∏äÊúÄÈ´ò„ÄÇ</p>
<p>ËØ•ÂõæÁâáÂ±ïÁ§∫‰∫Ü‰∏ÄÂº†Ë°®Ê†ºÔºåÊ†áÈ¢ò‰∏∫‚ÄúTable 8. Comparison of practical efficiency of LTSF-Transformers under L=96 and T=720 on the Electricity.‚Äù Ë°®Ê†ºÊØîËæÉ‰∫ÜÂú®ÁîµÂäõÊï∞ÊçÆÈõÜ‰∏äÔºåÈ¢ÑÊµãÈïøÂ∫¶L=96ÂíåÊó∂Èó¥Ê≠•ÈïøT=720Êó∂Ôºå‰∏çÂêåÈïøÂ∫èÂàóÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâTransformerÊ®°ÂûãÁöÑÂÆûÈôÖÊïàÁéá„ÄÇÊïàÁéáÈÄöËøáÂõõ‰∏™ÊåáÊ†áÊù•Ë°°ÈáèÔºöMACsÔºà‰πòÁ¥ØÂä†Êìç‰ΩúÊï∞Ôºâ„ÄÅÂèÇÊï∞Êï∞Èáè„ÄÅÊé®ÁêÜÊó∂Èó¥‰ª•ÂèäÂÜÖÂ≠ò‰ΩøÁî®Èáè„ÄÇ</p>
<p>Ë°®Ê†º‰∏≠ÂåÖÂê´ÁöÑÊ®°ÂûãÊúâÔºö
- DLinear
- Transformer√óÔºà‰øÆÊîπËá™AutoformerÁöÑ‰∏ÄÊ≠•Ëß£Á†ÅÂô®Ôºâ
- Informer
- Autoformer
- Pyraformer
- FEDformer</p>
<p>Ë°®Ê†º‰∏≠ÁöÑÊï∞ÊçÆÂ¶Ç‰∏ãÔºö
- DLinearÊ®°ÂûãÁöÑMACs‰∏∫0.04GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫139.K7ÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫0.4msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫687MiB„ÄÇ
- Transformer√óÊ®°ÂûãÁöÑMACs‰∏∫4.03GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫13.61MÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫26.8msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫6091MiB„ÄÇ
- InformerÊ®°ÂûãÁöÑMACs‰∏∫3.93GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫14.39MÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫49.3msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫3869MiB„ÄÇ
- AutoformerÊ®°ÂûãÁöÑMACs‰∏∫4.41GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫14.91MÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫164.1msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫7607MiB„ÄÇ
- PyraformerÊ®°ÂûãÁöÑMACs‰∏∫0.80GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫241.4M*ÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫3.4msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫7017MiB„ÄÇÔºàÊ≥®ÔºöPyraformerÁöÑÂèÇÊï∞Êï∞Èáè‰∏≠236.7MÊù•Ëá™ÂÖ∂Á∫øÊÄßËß£Á†ÅÂô®Ôºâ
- FEDformerÊ®°ÂûãÁöÑMACs‰∏∫4.41GÔºåÂèÇÊï∞Êï∞Èáè‰∏∫20.68MÔºåÊé®ÁêÜÊó∂Èó¥‰∏∫40.5msÔºåÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏∫4143MiB„ÄÇ</p>
<p>Ë°®Ê†ºÂ∫ïÈÉ®ÁöÑÊ≥®ÈáäËØ¥ÊòéÔºö
- Transformer√óÊ®°ÂûãË¢´‰øÆÊîπ‰∏∫‰∏éAutoformerÁõ∏ÂêåÁöÑ‰∏ÄÊ≠•Ëß£Á†ÅÂô®„ÄÇ
- PyraformerÊ®°ÂûãÁöÑÂèÇÊï∞Êï∞Èáè‰∏≠Ôºå236.7MÊù•Ëá™ÂÖ∂Á∫øÊÄßËß£Á†ÅÂô®„ÄÇ
- ‰ΩøÁî®DLinear‰Ωú‰∏∫ÊØîËæÉÂü∫ÂáÜÔºåÂõ†‰∏∫ÂÆÉÂú®LTSF-Linear‰∏≠ÁöÑÊàêÊú¨ÊòØDLinearÁöÑ‰∏§ÂÄç„ÄÇ
- Êé®ÁêÜÊó∂Èó¥ÊòØ5Ê¨°ËøêË°åÁöÑÂπ≥ÂùáÂÄº„ÄÇ</p>
<p>ÊÄª‰ΩìÊù•ÁúãÔºåDLinearÊ®°ÂûãÂú®ÊâÄÊúâÊåáÊ†á‰∏äÈÉΩË°®Áé∞Âá∫ËæÉÈ´òÁöÑÊïàÁéáÔºåËÄåAutoformerÂíåFEDformerÂú®ÂÜÖÂ≠ò‰ΩøÁî®Èáè‰∏äËæÉÈ´òÔºåInformerÂú®Êé®ÁêÜÊó∂Èó¥‰∏äÊúÄÈ´ò„ÄÇ</p>
<blockquote>
<p>emm</p>
</blockquote>
<h2 id="6-conclusion-and-future-work">6. Conclusion and Future Work<a class="headerlink" href="#6-conclusion-and-future-work" title="Permanent link">¬∂</a></h2>
<p><strong>Conclusion.</strong> This work questions the effectiveness of emerging favored Transformer-based solutions for the longterm time series forecasting problem. We use an embarrassingly simple linear model LTSF-Linear as a DMS forecasting baseline to verify our claims. Note that our contributions do not come from proposing a linear model but rather from throwing out an important question, showing surprising comparisons, and demonstrating why LTSFTransformers are not as effective as claimed in these works through various perspectives. We sincerely hope our comprehensive studies can benefit future work in this area.</p>
<p>ÁªìËÆ∫„ÄÇÊú¨Â∑•‰ΩúÂØπÊñ∞ÂÖ¥ÁöÑÂü∫‰∫éTransformerÁöÑËß£ÂÜ≥ÊñπÊ°àÂú®ÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÈóÆÈ¢ò‰∏äÁöÑÊúâÊïàÊÄßÊèêÂá∫‰∫ÜË¥®Áñë„ÄÇÊàë‰ª¨‰ΩøÁî®‰∏Ä‰∏™ÊûÅÂÖ∂ÁÆÄÂçïÁöÑÁ∫øÊÄßÊ®°ÂûãLTSF-Linear‰Ωú‰∏∫DMSÈ¢ÑÊµãÁöÑÂü∫Á∫øÊù•È™åËØÅÊàë‰ª¨ÁöÑËÆ∫Êñ≠„ÄÇÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÊàë‰ª¨ÁöÑË¥°ÁåÆÂπ∂ÈùûÊù•Ëá™‰∫éÊèêÂá∫‰∏Ä‰∏™Á∫øÊÄßÊ®°ÂûãÔºåËÄåÊòØÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈáçË¶ÅÁöÑÈóÆÈ¢òÔºåÂ±ïÁ§∫‰∫Ü‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊØîËæÉÁªìÊûúÔºåÂπ∂‰ªéÂ§ö‰∏™ËßíÂ∫¶ËØÅÊòé‰∫Ü‰∏∫‰ªÄ‰πàLTSF-TransformersÂπ∂‰∏çÂÉèËøô‰∫õÂ∑•‰Ωú‰∏≠ÊâÄÂ£∞Áß∞ÁöÑÈÇ£Ê†∑ÊúâÊïà„ÄÇÊàë‰ª¨ÁúüËØöÂú∞Â∏åÊúõÊàë‰ª¨ÂÖ®Èù¢ÁöÑÁ†îÁ©∂ÊâÄËÉΩ‰∏∫Ëøô‰∏ÄÈ¢ÜÂüüÁöÑÊú™Êù•Â∑•‰ΩúÂ∏¶Êù•ÁõäÂ§Ñ„ÄÇ</p>
<blockquote>
<p>Áé∞Âú® DLinear ÂèàË¢´ÂæàÂ§öÊ®°ÂûãÂØπÊØîÔºå‰ΩÜ‰πüÂõ†Ê≠§Âá∫Áé∞ÂæàÂ§ö Linear Á≥ªÁöÑÊñáÁ´†ÔºåÊØîÂ¶Ç UNetTSF</p>
</blockquote>
<p><strong>Future work.</strong>  LTSF-Linear has a limited model capacity, and it merely serves a simple yet competitive baseline with strong interpretability for future research. For example, the one-layer linear network is hard to capture the temporal dynamics caused by change points [25]. Consequently, we believe there is a great potential for new model designs, data processing, and benchmarks to tackle the challenging LTSF problem.</p>
<p>Êú™Êù•Â∑•‰Ωú„ÄÇLTSF-LinearÊ®°ÂûãÁöÑÂÆπÈáèÊúâÈôêÔºåÂÆÉ‰ªÖ‰Ωú‰∏∫‰∏Ä‰∏™ÁÆÄÂçï‰ΩÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÂü∫Á∫øÔºå‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõÂº∫ÊúâÂäõÁöÑÂèØËß£ÈáäÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂçïÂ±ÇÁ∫øÊÄßÁΩëÁªúÂæàÈöæÊçïÊçâÁî±ÂèòÂåñÁÇπÂºïËµ∑ÁöÑÊó∂Èó¥Âä®ÊÄÅ„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨Áõ∏‰ø°Âú®Êñ∞Ê®°ÂûãËÆæËÆ°„ÄÅÊï∞ÊçÆÂ§ÑÁêÜÂíåÂü∫ÂáÜÊµãËØïÊñπÈù¢ÔºåËß£ÂÜ≥ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈïøÊúüÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàLTSFÔºâÈóÆÈ¢òÂÖ∑ÊúâÂ∑®Â§ßÊΩúÂäõ„ÄÇ</p>
<h2 id="appendix">Appendix<a class="headerlink" href="#appendix" title="Permanent link">¬∂</a></h2>
<p>In this Appendix, we provide descriptions of non Transformer-based TSF solutions, detailed experimental settings, more comparisons under different look-back window sizes, and the visualization of LTSF-Linear on all datasets. We also append our code to reproduce the results shown in the paper.</p>
<p>Âú®Êú¨ÈôÑÂΩï‰∏≠ÔºåÊàë‰ª¨Êèê‰æõ‰∫ÜÈùûTransformerÂü∫Á°ÄÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâËß£ÂÜ≥ÊñπÊ°àÁöÑÊèèËø∞„ÄÅËØ¶ÁªÜÁöÑÂÆûÈ™åËÆæÁΩÆ„ÄÅÂú®‰∏çÂêåÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞è‰∏ãÁöÑÊõ¥Â§öÊØîËæÉÔºå‰ª•ÂèäLTSF-LinearÂú®ÊâÄÊúâÊï∞ÊçÆÈõÜ‰∏äÁöÑÂèØËßÜÂåñÁªìÊûú„ÄÇÊàë‰ª¨ËøòÈôÑ‰∏ä‰∫ÜÈáçÁé∞ËÆ∫Êñá‰∏≠Â±ïÁ§∫ÁªìÊûúÁöÑ‰ª£Á†Å„ÄÇ</p>
<h3 id="a-related-work-non-transformer-based-tsf-solutions">A. Related Work: Non-Transformer-Based TSF Solutions<a class="headerlink" href="#a-related-work-non-transformer-based-tsf-solutions" title="Permanent link">¬∂</a></h3>
<p>As a long-standing problem with a wide range of applications, statistical approaches (e.g., autoregressive integrated moving average (ARIMA) [1], exponential smoothing [12], and structural models [14]) for time series forecasting have been used from the 1970s onward. Generally speaking, the parametric models used in statistical methods require significant domain expertise to build.</p>
<p>‰Ωú‰∏∫‰∏Ä‰∏™ÂÖ∑ÊúâÂπøÊ≥õÂ∫îÁî®ÁöÑÈïøÊúüÈóÆÈ¢òÔºåËá™20‰∏ñÁ∫™70Âπ¥‰ª£‰ª•Êù•ÔºåÁªüËÆ°ÊñπÊ≥ïÔºà‰æãÂ¶ÇÔºåËá™ÂõûÂΩíÁßØÂàÜÊªëÂä®Âπ≥ÂùáÊ®°ÂûãÔºàARIMAÔºâ[1]„ÄÅÊåáÊï∞Âπ≥Êªë[12]ÂíåÁªìÊûÑÊ®°Âûã[14]ÔºâÂ∑≤Ë¢´Áî®‰∫éÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÁªüËÆ°ÊñπÊ≥ï‰∏≠‰ΩøÁî®ÁöÑÂèÇÊï∞Ê®°ÂûãÈúÄË¶ÅÊòæËëóÁöÑÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜÊù•ÊûÑÂª∫„ÄÇ</p>
<p>To relieve this burden, many machine learning techniques such as gradient boosting regression tree (GBRT) [10, 11] gain popularity, which learns the temporal dynamics of time series in a data-driven manner. However, these methods still require manual feature engineering and model designs. With the powerful representation learning capability of deep neural networks (DNNs) from abundant data, various deep learning-based TSF solutions are proposed in the literature, achieving better forecasting accuracy than traditional techniques in many cases.</p>
<p>‰∏∫‰∫ÜÂáèËΩªËøô‰∏ÄË¥üÊãÖÔºåËÆ∏Â§öÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÔºåÂ¶ÇÊ¢ØÂ∫¶ÊèêÂçáÂõûÂΩíÊ†ëÔºàGBRTÔºâ[10, 11]ÔºåÂõ†ÂÖ∂ËÉΩÂ§ü‰ª•Êï∞ÊçÆÈ©±Âä®ÁöÑÊñπÂºèÂ≠¶‰π†Êó∂Èó¥Â∫èÂàóÁöÑÊó∂Èó¥Âä®ÊÄÅËÄåÂèòÂæóÊµÅË°å„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ï‰ªçÁÑ∂ÈúÄË¶Å‰∫∫Â∑•ÁâπÂæÅÂ∑•Á®ãÂíåÊ®°ÂûãËÆæËÆ°„ÄÇÂæóÁõä‰∫éÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÔºàDNNsÔºâÁöÑÂº∫Â§ßË°®Á§∫Â≠¶‰π†ËÉΩÂäõÔºå‰ªéÂ§ßÈáèÊï∞ÊçÆ‰∏≠ÔºåÊñáÁåÆ‰∏≠ÊèêÂá∫‰∫ÜÂêÑÁßçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑTSFËß£ÂÜ≥ÊñπÊ°àÔºåÂú®ËÆ∏Â§öÊÉÖÂÜµ‰∏ãÊØî‰º†ÁªüÊäÄÊúØÂÆûÁé∞‰∫ÜÊõ¥Â•ΩÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄß„ÄÇ</p>
<p>Besides Transformers, the other two popular DNN architectures are also applied for time series forecasting:</p>
<p>Èô§‰∫ÜTransformersÔºåÂè¶Â§ñ‰∏§ÁßçÊµÅË°åÁöÑDNNÊû∂ÊûÑ‰πüË¢´Â∫îÁî®‰∫éÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºö</p>
<blockquote>
<p>ÂÖ∂ÂÆûÊòØËÉΩÊÑüËßâÂà∞ÁöÑÔºåÊ∑±Â∫¶Â≠¶‰π†ËÆæËÆ°ÁöÑÁΩëÁªúÂæàÂ∞ëÊ∂âÂèä‰∏ì‰∏öÁü•ËØÜÔºåÂá†‰πé‰∏çÈúÄË¶ÅÊ≥®ÂÖ•Â§™Â§öÁöÑ‰∏ì‰∏öÁü•ËØÜÔºåÁé∞Âú®Êó∂Â∫èÈ¢ÑÊµãÊ®°ÂûãÔºåËÆæËÆ°Ê∑±Â∫¶Â≠¶‰π†ÁöÑÂºÄÂßãÂºïÂÖ•‰º†ÁªüÁªüËÆ°Áü•ËØÜÔºåÊØîÂ¶ÇÂ∫èÂàóÂàÜËß£ÔºåÈ¢ëÂüüÁü•ËØÜÔºåÂàöÂºÄÂßãÁöÑËÆæËÆ°‰πü‰∏çÂÉèÊòØ‰∏ìÈó®ÈíàÂØπÊó∂Â∫èÊï∞ÊçÆÔºå‰ΩÜÂÖ∂ÂÆûÊääÂÖ∑ÊúâÈïøÊúüË∂ãÂäøÁöÑÊàêÂàÜÂâ•Á¶ªÂá∫Êù•Ôºå‰∏çÂ∞±ÊòØÁî® Transformer</p>
</blockquote>
<ul>
<li>Recurrent neural networks (RNNs) based methods (e.g., [21]) summarize the past information compactly in internal memory states and recursively update themselves for forecasting.</li>
<li>Âü∫‰∫éÈÄíÂΩíÁ•ûÁªèÁΩëÁªúÔºàRNNsÔºâÁöÑÊñπÊ≥ïÔºà‰æãÂ¶ÇÔºå[21]ÔºâÂú®ÂÜÖÈÉ®ËÆ∞ÂøÜÁä∂ÊÄÅ‰∏≠Á¥ßÂáëÂú∞ÊÄªÁªìËøáÂéª‰ø°ÊÅØÔºåÂπ∂ÈÄíÂΩíÂú∞Ëá™ÊàëÊõ¥Êñ∞‰ª•ËøõË°åÈ¢ÑÊµã„ÄÇ</li>
<li>Convolutional neural networks (CNNs) based methods (e.g., [3]), wherein convolutional filters are used to capture local temporal features.</li>
<li>Âü∫‰∫éÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNsÔºâÁöÑÊñπÊ≥ïÔºà‰æãÂ¶ÇÔºå[3]ÔºâÔºåÂÖ∂‰∏≠‰ΩøÁî®Âç∑ÁßØÊª§Ê≥¢Âô®Êù•ÊçïÊçâÂ±ÄÈÉ®Êó∂Èó¥ÁâπÂæÅ„ÄÇ</li>
</ul>
<p>RNN-based TSF methods belong to IMS forecasting techniques. Depending on whether the decoder is implemented in an autoregressive manner, there are either IMS or DMS forecasting techniques for CNN-based TSF methods [3, 17].</p>
<ul>
<li>Âü∫‰∫éRNNÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâÊñπÊ≥ïÂ±û‰∫éÂçïÊ≠•È¢ÑÊµãÔºàIMSÔºâÊäÄÊúØ„ÄÇÊ†πÊçÆËß£Á†ÅÂô®ÊòØÂê¶‰ª•Ëá™ÂõûÂΩíÊñπÂºèÂÆûÁé∞ÔºåÂØπ‰∫éÂü∫‰∫éCNNÁöÑTSFÊñπÊ≥ïÔºåÂèØ‰ª•ÊòØÂçïÊ≠•È¢ÑÊµãÔºàIMSÔºâÊàñÂ§öÊ≠•È¢ÑÊµãÔºàDMSÔºâÊäÄÊúØ[3, 17]„ÄÇ</li>
</ul>
<h3 id="b-experimental-details">B. Experimental Details<a class="headerlink" href="#b-experimental-details" title="Permanent link">¬∂</a></h3>
<h4 id="b1-data-descriptions">B.1. Data Descriptions<a class="headerlink" href="#b1-data-descriptions" title="Permanent link">¬∂</a></h4>
<p>We use nine wildly-used datasets in the main paper. The details are listed in the following.</p>
<ul>
<li>ETT (Electricity Transformer Temperature) [30]2 consists of two hourly-level datasets (ETTh) and two 15minute-level datasets (ETTm). Each of them contains seven oil and load features of electricity transformers from July 2016 to July 2018.  </li>
<li>ETTÔºàÁîµÂäõÂèòÂéãÂô®Ê∏©Â∫¶Ôºâ[30] ÂåÖÂê´‰∏§‰∏™Â∞èÊó∂Á∫ßÂà´ÁöÑÊï∞ÊçÆÈõÜÔºàETThÔºâÂíå‰∏§‰∏™15ÂàÜÈíüÁ∫ßÂà´ÁöÑÊï∞ÊçÆÈõÜÔºàETTmÔºâ„ÄÇÊØè‰∏™Êï∞ÊçÆÈõÜÈÉΩÂåÖÂê´‰∫Ü‰ªé2016Âπ¥7ÊúàËá≥2018Âπ¥7ÊúàÁöÑÁîµÂäõÂèòÂéãÂô®ÁöÑ‰∏É‰∏™Ê≤πÂíåË¥üËΩΩÁâπÂæÅ„ÄÇ</li>
<li>Traffic3 describes the road occupancy rates. It contains the hourly data recorded by the sensors of San Francisco freeways from 2015 to 2016.  </li>
<li>‰∫§ÈÄö[3] ÊèèËø∞‰∫ÜÈÅìË∑ØÂç†Áî®Áéá„ÄÇÂÆÉÂåÖÂê´‰∫Ü2015Âπ¥Ëá≥2016Âπ¥ÊóßÈáëÂ±±È´òÈÄüÂÖ¨Ë∑Ø‰º†ÊÑüÂô®ËÆ∞ÂΩïÁöÑÊØèÂ∞èÊó∂Êï∞ÊçÆ„ÄÇ</li>
<li>Electricity4 collects the hourly electricity consumption of 321 clients from 2012 to 2014.  </li>
<li>ÁîµÂäõ[4] Êî∂ÈõÜ‰∫Ü2012Âπ¥Ëá≥2014Âπ¥321‰∏™ÂÆ¢Êà∑ÁöÑÊØèÂ∞èÊó∂ÁîµÂäõÊ∂àËÄóÊï∞ÊçÆ„ÄÇ</li>
<li>Exchange-Rate [15]5 collects the daily exchange rates of 8 countries from 1990 to 2016.  </li>
<li>Ê±áÁéá[15] Êî∂ÈõÜ‰∫Ü1990Âπ¥Ëá≥2016Âπ¥8‰∏™ÂõΩÂÆ∂ÁöÑÊØèÊó•Ê±áÁéá„ÄÇ</li>
<li>Weather6 includes 21 indicators of weather, such as air temperature, and humidity. Its data is recorded every 10 min for 2020 in Germany.  </li>
<li>Â§©Ê∞î[6] ÂåÖÊã¨21‰∏™Ê∞îË±°ÊåáÊ†áÔºåÂ¶ÇÁ©∫Ê∞îÊ∏©Â∫¶ÂíåÊπøÂ∫¶„ÄÇÂÖ∂Êï∞ÊçÆÊòØ2020Âπ¥Âú®Âæ∑ÂõΩÊØè10ÂàÜÈíüËÆ∞ÂΩï‰∏ÄÊ¨°ÁöÑ„ÄÇ</li>
<li>ILI7 describes the ratio of patients seen with influenzalike illness and the number of patients. It includes weekly data from the Centers for Disease Control and Prevention of the United States from 2002 to 2021.</li>
<li>ILI[7] ÊèèËø∞‰∫ÜÊÇ£ÊúâÊµÅÊÑüÊ†∑ÁñæÁóÖÁöÑÊÇ£ËÄÖÊØî‰æãÂíåÊÇ£ËÄÖÊï∞Èáè„ÄÇÂÆÉÂåÖÊã¨‰∫Ü2002Âπ¥Ëá≥2021Âπ¥ÁæéÂõΩÁñæÁóÖÊéßÂà∂‰∏éÈ¢ÑÈò≤‰∏≠ÂøÉÁöÑÊØèÂë®Êï∞ÊçÆ„ÄÇ</li>
</ul>
<h4 id="b2-implementation-details">B.2. Implementation Details<a class="headerlink" href="#b2-implementation-details" title="Permanent link">¬∂</a></h4>
<p>For existing Transformer-based TSF solutions: the implementation of Autoformer [28], Informer [30], and the vanilla Transformer [26] are all taken from the Autoformer work [28]; the implementation of FEDformer [31] and Pyraformer [18] are from their respective code repository. We also adopt their default hyper-parameters to train the models. For DLinear, the moving average kernel size for decomposition is 25, which is the same as Autoformer. The total parameters of a vanilla linear model and a NLinear are TL. The total parameters of the DLinear are 2TL. Since LTSF-Linear will be underfitting when the input length is short, and LTSF-Transformers tend to overfit on a long lookback window size. To compare the best performance of existing LTSF-Transformers with LTSF-Linear, we report L=336 for LTSF-Linear and L=96 for Transformers by default. For more hyper-parameters of LTSF-Linear, please refer to our code.</p>
<p>ÂØπ‰∫éÁé∞ÊúâÁöÑÂü∫‰∫éTransformerÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâËß£ÂÜ≥ÊñπÊ°àÔºöAutoformer[28]„ÄÅInformer[30]Âíå‰º†ÁªüTransformer[26]ÁöÑÂÆûÁé∞ÈÉΩÂèñËá™AutoformerÁöÑÂ∑•‰Ωú[28]ÔºõFEDformer[31]ÂíåPyraformer[18]ÁöÑÂÆûÁé∞ÂàÜÂà´Êù•Ëá™ÂÆÉ‰ª¨ÂêÑËá™ÁöÑ‰ª£Á†ÅÂ∫ì„ÄÇ</p>
<p>Êàë‰ª¨ËøòÈááÁî®‰∫ÜÂÆÉ‰ª¨ÁöÑÈªòËÆ§Ë∂ÖÂèÇÊï∞Êù•ËÆ≠ÁªÉÊ®°Âûã„ÄÇÂØπ‰∫éDLinearÔºåÂàÜËß£Êó∂ÁßªÂä®Âπ≥ÂùáÊ†∏ÁöÑÂ§ßÂ∞è‰∏∫25ÔºåËøô‰∏éAutoformerÁõ∏Âêå„ÄÇ</p>
<p>‰∏Ä‰∏™‰º†ÁªüÁ∫øÊÄßÊ®°ÂûãÂíå‰∏Ä‰∏™NLinearÁöÑÊÄªÂèÇÊï∞Èáè‰∏∫TL„ÄÇDLinearÁöÑÊÄªÂèÇÊï∞Èáè‰∏∫2TL„ÄÇ</p>
<p>Áî±‰∫éÂΩìËæìÂÖ•ÈïøÂ∫¶ËæÉÁü≠Êó∂ÔºåLTSF-LinearÂèØËÉΩ‰ºöÂá∫Áé∞Ê¨†ÊãüÂêàÔºåËÄåLTSF-TransformersÂú®ËæÉÈïøÁöÑÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÊó∂ÂàôÂÄæÂêë‰∫éËøáÊãüÂêà„ÄÇ‰∏∫‰∫ÜÊØîËæÉÁé∞ÊúâLTSF-Transformers‰∏éLTSF-LinearÁöÑÊúÄ‰Ω≥ÊÄßËÉΩÔºåÊàë‰ª¨ÈªòËÆ§Êä•ÂëäLTSF-LinearÁöÑL=336ÂíåTransformersÁöÑL=96„ÄÇÊúâÂÖ≥LTSF-LinearÁöÑÊõ¥Â§öË∂ÖÂèÇÊï∞ÔºåËØ∑ÂèÇÈòÖÊàë‰ª¨ÁöÑ‰ª£Á†Å„ÄÇ</p>
<h3 id="c-additional-comparison-with-transformers">C. Additional Comparison with Transformers<a class="headerlink" href="#c-additional-comparison-with-transformers" title="Permanent link">¬∂</a></h3>
<p>We further compare LTSF-Linear with LTSFTransformer for Univariate Forecasting on four ETT datasets. Moreover, in Figure 4 of the main paper, we demonstrate that existing Transformers fail to exploit large look-back window sizes with two examples. Here, we give comprehensive comparisons between LTSF-Linear and Transformer-based TSF solutions under various look-back window sizes on all benchmarks.</p>
<p>Êàë‰ª¨Ëøõ‰∏ÄÊ≠•Â∞ÜLTSF-Linear‰∏éLTSF-TransformerÂú®Âõõ‰∏™ETTÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂçïÂèòÈáèÈ¢ÑÊµãÁöÑÊØîËæÉ„ÄÇÊ≠§Â§ñÔºåÂú®‰∏ªË¶ÅËÆ∫ÊñáÁöÑÂõæ4‰∏≠ÔºåÊàë‰ª¨ÈÄöËøá‰∏§‰∏™‰æãÂ≠êÂ±ïÁ§∫‰∫ÜÁé∞ÊúâÁöÑTransformerÊó†Ê≥ïÂà©Áî®Â§ßÁöÑÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞è„ÄÇÂú®Ê≠§ÔºåÊàë‰ª¨Âú®ÊâÄÊúâÂü∫ÂáÜÊµãËØï‰∏äÔºåÂú®ÂêÑÁßçÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞è‰∏ãÔºåÂØπLTSF-LinearÂíåÂü∫‰∫éTransformerÁöÑTSFËß£ÂÜ≥ÊñπÊ°àËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÊØîËæÉ„ÄÇ</p>
<h4 id="c1-comparison-of-univariate-forecasting">C.1. Comparison of Univariate Forecasting<a class="headerlink" href="#c1-comparison-of-univariate-forecasting" title="Permanent link">¬∂</a></h4>
<p>We present the univariate forecasting results on the four ETT datasets in table 9. Similarly, LTSF-Linear, especially for NLinear can consistently outperform all transformerbased methods by a large margin in most time. We find that there are serious distribution shifts between training and test sets (as shown in Fig. 5 (a), (b)) on ETTh1 and ETTh2 datasets. Simply normalization via the last value from the lookback window can greatly relieve the distribution shift problem.</p>
<p>Êàë‰ª¨Âú®Ë°®9‰∏≠Â±ïÁ§∫‰∫ÜÂõõ‰∏™ETTÊï∞ÊçÆÈõÜ‰∏äÁöÑÂçïÂèòÈáèÈ¢ÑÊµãÁªìÊûú„ÄÇÂêåÊ†∑Âú∞ÔºåLTSF-LinearÔºåÁâπÂà´ÊòØNLinearÔºåÂú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÈÉΩËÉΩÊòæËëó‰ºò‰∫éÊâÄÊúâÂü∫‰∫éTransformerÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂú®ETTh1ÂíåETTh2Êï∞ÊçÆÈõÜ‰∏äÔºåËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ‰πãÈó¥Â≠òÂú®‰∏•ÈáçÁöÑÂàÜÂ∏ÉÂÅèÁßªÔºàÂ¶ÇÂõæ5(a)„ÄÅ(b)ÊâÄÁ§∫Ôºâ„ÄÇÁÆÄÂçïÂú∞ÈÄöËøáÂõûÊ∫ØÁ™óÂè£ÁöÑÊúÄÂêé‰∏Ä‰∏™ÂÄºËøõË°åÂΩí‰∏ÄÂåñÂèØ‰ª•Â§ßÂ§ßÁºìËß£ÂàÜÂ∏ÉÂÅèÁßªÈóÆÈ¢ò„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151351106.png" data-desc-position="bottom"><img alt="image-20250415135121350" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151351106.png"></a></p>
<p>‚ÄúTable 9. Univariate long sequence time-series forecasting results on ETT full benchmark.‚Äù Ë°®Ê†ºÂàóÂá∫‰∫ÜÂú®ETTÂÆåÊï¥Âü∫ÂáÜÊµãËØï‰∏äÔºå‰∏çÂêåÊ®°ÂûãÂú®ÂçïÂèòÈáèÈïøÂ∫èÂàóÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇË°®Ê†º‰∏≠ÂåÖÂê´ÁöÑÊ®°ÂûãÊúâÔºö</p>
<ul>
<li>LinearÔºàÁ∫øÊÄßÊ®°ÂûãÔºâ</li>
<li>NLinear</li>
<li>DLinear</li>
<li>FEDformer-f</li>
<li>FEDformer-w</li>
<li>Autoformer</li>
<li>Informer</li>
<li>LogTrans</li>
</ul>
<p>È¢ÑÊµãÈïøÂ∫¶ÔºàPredict LengthÔºâÂàÜ‰∏∫Âõõ‰∏™‰∏çÂêåÁöÑÂÄºÔºö96„ÄÅ192„ÄÅ336„ÄÅ720„ÄÇ</p>
<p>ÂØπ‰∫éÊØè‰∏™Ê®°ÂûãÂíåÈ¢ÑÊµãÈïøÂ∫¶ÔºåË°®Ê†ºÂ±ïÁ§∫‰∫Ü‰∏§ÁßçËØÑ‰ª∑ÊåáÊ†áÔºöÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÂíåÂπ≥ÂùáÁªùÂØπËØØÂ∑ÆÔºàMAEÔºâ„ÄÇË°®Ê†º‰∏≠Áî®Á≤ó‰ΩìÂ≠óÁ™ÅÂá∫ÊòæÁ§∫‰∫ÜÊúÄ‰Ω≥ÁªìÊûúÔºåËÄåÁî®‰∏ãÂàíÁ∫øÊ†áÂá∫‰∫ÜTransformerÊ®°ÂûãÁöÑÊúÄ‰Ω≥ÁªìÊûú„ÄÇ</p>
<p>‰ªéË°®Ê†º‰∏≠ÂèØ‰ª•ËßÇÂØüÂà∞Ôºö
- Âú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÔºåDLinearÊ®°ÂûãÂú®MSEÂíåMAEÊåáÊ†á‰∏äÈÉΩÂèñÂæó‰∫ÜÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ
- Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÔºåNLinearÊ®°Âûã‰πüÂèñÂæó‰∫ÜÁõ∏ÂØπËæÉÂ•ΩÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®ETTh1Êï∞ÊçÆÈõÜÁöÑ720È¢ÑÊµãÈïøÂ∫¶‰∏ã„ÄÇ
- Âü∫‰∫éTransformerÁöÑÊ®°ÂûãÔºàFEDformer„ÄÅAutoformer„ÄÅInformer„ÄÅLogTransÔºâÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ã‰πüË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÈÄöÂ∏∏‰∏çÂ¶ÇDLinearÂíåNLinearÊ®°Âûã„ÄÇ
- LinearÊ®°ÂûãÂú®ÊâÄÊúâÊÉÖÂÜµ‰∏ãÁöÑÊÄßËÉΩÈÉΩÁõ∏ÂØπËæÉÂ∑Æ„ÄÇ</p>
<p>Ëøô‰∫õÁªìÊûúË°®ÊòéÔºåÂú®ETTÊï∞ÊçÆÈõÜ‰∏äÁöÑÂçïÂèòÈáèÈïøÂ∫èÂàóÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰ªªÂä°‰∏≠ÔºåDLinearÂíåNLinearÊ®°ÂûãÈÄöÂ∏∏ÊØîÂÖ∂‰ªñÊ®°ÂûãÔºàÂåÖÊã¨Âü∫‰∫éTransformerÁöÑÊ®°ÂûãÔºâË°®Áé∞Êõ¥Â•Ω„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151352827.png" data-desc-position="bottom"><img alt="image-20250415135252330" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151352827.png"></a></p>
<p>ETT Êï∞ÊçÆÈõÜ„ÄÅElectricity Êï∞ÊçÆÈõÜ„ÄÅILI ËÆ≠ÁªÉÈõÜ&amp;ÊµãËØïÈõÜ‰∏äÁöÑÂàÜÂ∏ÉÂÅèÁßªÈóÆÈ¢ò</p>
<h4 id="c2-comparison-under-different-look-back-windows">C.2. Comparison under Different Look-back Windows<a class="headerlink" href="#c2-comparison-under-different-look-back-windows" title="Permanent link">¬∂</a></h4>
<p>In Figure 6, we provide the MSE comparisons of five LTSF-Transformers with LTSF-Linear under different lookback window sizes to explore whether existing Transformers can extract temporal well from longer input sequences. For hourly granularity datasets (ETTh1, ETTh2, Traffic, and Electricity), the increasing look-back window sizes are {24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720}, which represent {1, 2, 3, 4, 5, 6, 7, 8, 14, 21, 28, 30} days. The forecasting steps are {24, 720}, which mean {1, 30} days. For 5-minute granularity datasets (ETTm1 and ETTm2), we set the look-back window size as {24, 36, 48, 60, 72, 144, 288}, which represent {2, 3, 4, 5, 6, 12, 24} hours. For 10-minute granularity datasets (Weather), we set the look-back window size as {24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720}, which mean {4, 8, 12, 16, 20, 24, 28, 32, 56, 84, 112, 120} hours. The forecasting steps are {24, 720} that are {4, 120} hours. For weekly granularity dataset (ILI), we set the look-back window size as {26, 52, 78, 104, 130, 156, 208}, which represent {0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4} years. The corresponding forecasting steps are {26, 208}, meaning {0.5, 4} years.</p>
<p>Âú®Âõæ6‰∏≠ÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∫îÁßçLTSF-Transformer‰∏éLTSF-LinearÂú®‰∏çÂêåÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞è‰∏ãÁöÑÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÊØîËæÉÔºå‰ª•Êé¢Á¥¢Áé∞ÊúâÁöÑTransformerÊòØÂê¶ËÉΩÂ§ü‰ªéÊõ¥ÈïøÁöÑËæìÂÖ•Â∫èÂàó‰∏≠ÂæàÂ•ΩÂú∞ÊèêÂèñÊó∂Èó¥ÁâπÂæÅ„ÄÇÂØπ‰∫éÂ∞èÊó∂Á≤íÂ∫¶Êï∞ÊçÆÈõÜÔºàETTh1„ÄÅETTh2„ÄÅ‰∫§ÈÄöÂíåÁîµÂäõÔºâÔºåÂ¢ûÂä†ÁöÑÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞è‰∏∫{24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720}ÔºåËøô‰ª£Ë°®{1, 2, 3, 4, 5, 6, 7, 8, 14, 21, 28, 30}Â§©„ÄÇÈ¢ÑÊµãÊ≠•Èïø‰∏∫{24, 720}ÔºåËøôÊÑèÂë≥ÁùÄ{1, 30}Â§©„ÄÇ</p>
<p>ÂØπ‰∫é5ÂàÜÈíüÁ≤íÂ∫¶Êï∞ÊçÆÈõÜÔºàETTm1ÂíåETTm2ÔºâÔºåÊàë‰ª¨Â∞ÜÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èËÆæÁΩÆ‰∏∫{24, 36, 48, 60, 72, 144, 288}ÔºåËøô‰ª£Ë°®{2, 3, 4, 5, 6, 12, 24}Â∞èÊó∂„ÄÇ</p>
<p>ÂØπ‰∫é10ÂàÜÈíüÁ≤íÂ∫¶Êï∞ÊçÆÈõÜÔºàÂ§©Ê∞îÔºâÔºåÊàë‰ª¨Â∞ÜÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èËÆæÁΩÆ‰∏∫{24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720}ÔºåËøôÊÑèÂë≥ÁùÄ{4, 8, 12, 16, 20, 24, 28, 32, 56, 84, 112, 120}Â∞èÊó∂„ÄÇÈ¢ÑÊµãÊ≠•Èïø‰∏∫{24, 720}ÔºåÂç≥{4, 120}Â∞èÊó∂„ÄÇ</p>
<p>ÂØπ‰∫éÂë®Á≤íÂ∫¶Êï∞ÊçÆÈõÜÔºàILIÔºâÔºåÊàë‰ª¨Â∞ÜÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èËÆæÁΩÆ‰∏∫{26, 52, 78, 104, 130, 156, 208}ÔºåËøô‰ª£Ë°®{0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4}Âπ¥„ÄÇÁõ∏Â∫îÁöÑÈ¢ÑÊµãÊ≠•Èïø‰∏∫{26, 208}ÔºåÊÑèÂë≥ÁùÄ{0.5, 4}Âπ¥„ÄÇ</p>
<p>As shown in Figure 6, with increased look-back window sizes, the performance of LTSF-Linear is significantly boosted for most datasets (e.g., ETTm1 and Traffic), while this is not the case for Transformer-based TSF solutions. Most of their performance fluctuates or gets worse as the input lengths increase. To be specific, the results of Exchange-Rate do not show improved results with a long look-back window (from Figure 6(m) and (n)), and we attribute it to the low information-to-noise ratio in such financial data.</p>
<p>Â¶ÇÂõæ6ÊâÄÁ§∫ÔºåÈöèÁùÄÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÁöÑÂ¢ûÂä†ÔºåLTSF-LinearÂú®Â§ßÂ§öÊï∞Êï∞ÊçÆÈõÜÔºà‰æãÂ¶ÇÔºåETTm1Âíå‰∫§ÈÄöÔºâ‰∏äÁöÑÊÄßËÉΩÊòæËëóÊèêÂçáÔºåËÄåÂü∫‰∫éTransformerÁöÑÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÔºàTSFÔºâËß£ÂÜ≥ÊñπÊ°àÂàôÂπ∂ÈùûÂ¶ÇÊ≠§„ÄÇÈöèÁùÄËæìÂÖ•ÈïøÂ∫¶ÁöÑÂ¢ûÂä†ÔºåÂÆÉ‰ª¨ÁöÑÂ§ßÂ§öÊï∞ÊÄßËÉΩÊ≥¢Âä®ÊàñÂèòÂæóÊõ¥Â∑Æ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊ±áÁéáÁöÑÁªìÊûúÂπ∂Êú™ÊòæÁ§∫Âá∫‰ΩøÁî®ÈïøÂõûÊ∫ØÁ™óÂè£ÂêéÁöÑÊîπÂñÑÔºàËßÅÂõæ6(m)Âíå(n)ÔºâÔºåÊàë‰ª¨Â∞ÜÊ≠§ÂΩíÂõ†‰∫éËøôÁ±ªÈáëËûçÊï∞ÊçÆ‰∏≠‰Ωé‰ø°ÊÅØÂô™Â£∞ÊØî„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151357872.png" data-desc-position="bottom"><img alt="image-20250415135721653" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151357872.png"></a></p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151357387.png" data-desc-position="bottom"><img alt="image-20250415135746909" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151357387.png"></a></p>
<p>Âõæ6. ‰∏çÂêåÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞èÔºàXËΩ¥Ôºâ‰∏ãÊ®°ÂûãÁöÑÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÁªìÊûúÔºàYËΩ¥ÔºâÔºåËøô‰∫õÁªìÊûúÊ∂µÁõñ‰∫ÜÈïøÊúüÈ¢ÑÊµãÔºà‰æãÂ¶ÇÔºå720‰∏™Êó∂Èó¥Ê≠•ÈïøÔºâÂíåÁü≠ÊúüÈ¢ÑÊµãÔºà‰æãÂ¶ÇÔºå24‰∏™Êó∂Èó¥Ê≠•ÈïøÔºâÂú®‰∏çÂêåÂü∫ÂáÜÊµãËØï‰∏äÁöÑË°®Áé∞„ÄÇ</p>
<h3 id="d-ablation-study-on-the-ltsf-linear">D. Ablation study on the LTSF-Linear<a class="headerlink" href="#d-ablation-study-on-the-ltsf-linear" title="Permanent link">¬∂</a></h3>
<h4 id="d1-motivation-of-nlinear">D.1. Motivation of NLinear<a class="headerlink" href="#d1-motivation-of-nlinear" title="Permanent link">¬∂</a></h4>
<p>If we normalize the test data by the mean and variance of train data, there could be a distribution shift in testing data, i.e, the mean value of testing data is not 0. If the model made a prediction that is out of the distribution of true value, a large error would occur. For example, there is a large error between the true value and the true value minus/add one. </p>
<p>Â¶ÇÊûúÊàë‰ª¨Ê†πÊçÆËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÂØπÊµãËØïÊï∞ÊçÆËøõË°åÂΩí‰∏ÄÂåñÔºåÊµãËØïÊï∞ÊçÆÂèØËÉΩ‰ºöÂá∫Áé∞ÂàÜÂ∏ÉÂÅèÁßªÔºåÂç≥ÊµãËØïÊï∞ÊçÆÁöÑÂùáÂÄº‰∏ç‰∏∫0„ÄÇÂ¶ÇÊûúÊ®°ÂûãÂÅöÂá∫ÁöÑÈ¢ÑÊµãË∂ÖÂá∫‰∫ÜÁúüÂÆûÂÄºÁöÑÂàÜÂ∏ÉËåÉÂõ¥ÔºåÂ∞±‰ºöÂá∫Áé∞ËæÉÂ§ßÁöÑËØØÂ∑Æ„ÄÇ‰æãÂ¶ÇÔºåÁúüÂÆûÂÄº‰∏éÁúüÂÆûÂÄºÂáè‰∏ÄÊàñÂä†‰∏Ä‰πãÈó¥Â≠òÂú®ËæÉÂ§ßÁöÑËØØÂ∑Æ„ÄÇ</p>
<p>Therefore, in NLinear, we use the subtraction and addition to shift the model prediction toward the distribution of true value. Then, large errors are avoided, and the model performances can be improved. Figure 5 illustrates histograms of the trainset-test set distributions, where each bar represents the number of data points. Clear distribution shifts between training and testing data can be observed in ETTh1, ETTh2, and ILI. Accordingly, from Table 9 and Table 2 in the main paper, we can observe that there are great improvements in the three datasets comparing the NLinear to the Linear, showing the effectiveness of the NLinear in relieving distribution shifts. Moreover, for the datasets without obvious distribution shifts, like Electricity in Figure 5(c), using the vanilla Linear can be enough, demonstrating the similar performance with NLinear and DLinear.</p>
<p>Âõ†Ê≠§ÔºåÂú®NLinear‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî®ÂáèÊ≥ïÂíåÂä†Ê≥ïÂ∞ÜÊ®°ÂûãÈ¢ÑÊµãÂêëÁúüÂÆûÂÄºÁöÑÂàÜÂ∏ÉÊñπÂêëË∞ÉÊï¥„ÄÇËøôÊ†∑ÔºåÂ∞±ÂèØ‰ª•ÈÅøÂÖçÂ§ßÁöÑËØØÂ∑ÆÔºåÂπ∂‰∏îÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇÂõæ5Â±ïÁ§∫‰∫ÜËÆ≠ÁªÉÈõÜ-ÊµãËØïÈõÜÂàÜÂ∏ÉÁöÑÁõ¥ÊñπÂõæÔºåÂÖ∂‰∏≠ÊØè‰∏™Êù°ÂΩ¢‰ª£Ë°®Êï∞ÊçÆÁÇπÁöÑÊï∞Èáè„ÄÇÂú®ETTh1„ÄÅETTh2ÂíåILI‰∏≠ÂèØ‰ª•ËßÇÂØüÂà∞ËÆ≠ÁªÉÊï∞ÊçÆÂíåÊµãËØïÊï∞ÊçÆ‰πãÈó¥ÊòéÊòæÁöÑÂàÜÂ∏ÉÂÅèÁßª„ÄÇÁõ∏Â∫îÂú∞Ôºå‰ªé‰∏ªËÆ∫Êñá‰∏≠ÁöÑË°®9ÂíåË°®2ÔºåÊàë‰ª¨ÂèØ‰ª•ËßÇÂØüÂà∞Âú®Ëøô‰∏â‰∏™Êï∞ÊçÆÈõÜ‰∏äÔºåÂ∞ÜNLinear‰∏éLinearËøõË°åÊØîËæÉÊó∂ÔºåÊÄßËÉΩÊúâÊòæËëóÊèêÂçáÔºåËøôÊòæÁ§∫‰∫ÜNLinearÂú®ÁºìËß£ÂàÜÂ∏ÉÂÅèÁßªÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåÂØπ‰∫éÊ≤°ÊúâÊòéÊòæÂàÜÂ∏ÉÂÅèÁßªÁöÑÊï∞ÊçÆÈõÜÔºåÂ¶ÇÂõæ5(c)‰∏≠ÁöÑÁîµÂäõÊï∞ÊçÆÈõÜÔºå‰ΩøÁî®‰º†ÁªüÁöÑLinearÂèØËÉΩÂ∞±Ë∂≥Â§ü‰∫ÜÔºåËøôË°®ÊòéLinear‰∏éNLinearÂíåDLinearÁöÑÊÄßËÉΩÁõ∏‰ºº„ÄÇ</p>
<h4 id="d2-the-features-of-ltsf-linear">D.2. The Features of LTSF-Linear<a class="headerlink" href="#d2-the-features-of-ltsf-linear" title="Permanent link">¬∂</a></h4>
<p>Although LTSF-Linear is simple, it has some compelling characteristics:</p>
<p>Â∞ΩÁÆ°LTSF-LinearÊ®°ÂûãÁÆÄÂçïÔºå‰ΩÜÂÆÉÂÖ∑Êúâ‰∏Ä‰∫õÂºï‰∫∫Ê≥®ÁõÆÁöÑÁâπÊÄßÔºö</p>
<ul>
<li>An O(1) maximum signal traversing path length: The shorter the path, the better the dependencies are captured [18], making LTSF-Linear capable of capturing both short-range and long-range temporal relations.</li>
<li><span class="arithmatex">\(O(1)\)</span>ÁöÑÊúÄÂ§ß‰ø°Âè∑Á©øË∂äË∑ØÂæÑÈïøÂ∫¶ÔºöË∑ØÂæÑË∂äÁü≠ÔºåÊçïËé∑ÁöÑ‰æùËµñÂÖ≥Á≥ªË∂äÂ•Ω[18]Ôºå‰ΩøÂæóLTSF-LinearËÉΩÂ§üÊçïÊçâÂà∞Áü≠ÊúüÂíåÈïøÊúüÁöÑÊó∂Â∫èÂÖ≥Á≥ª„ÄÇ</li>
<li>High-efficiency: As LTSF-Linear is a linear model with two linear layers at most, it costs much lower memory and fewer parameters and has a faster inference speed than existing Transformers (see Table 8 in main paper).</li>
<li>È´òÊïàÁéáÔºöÁî±‰∫éLTSF-LinearÊúÄÂ§öÂåÖÂê´‰∏§‰∏™Á∫øÊÄßÂ±ÇÁöÑÁ∫øÊÄßÊ®°ÂûãÔºåÂÆÉÁöÑÂÜÖÂ≠òÊ∂àËÄóÊõ¥‰ΩéÔºåÂèÇÊï∞Êõ¥Â∞ëÔºåÂπ∂‰∏îÊØîÁé∞ÊúâÁöÑTransformerÂÖ∑ÊúâÊõ¥Âø´ÁöÑÊé®ÁêÜÈÄüÂ∫¶ÔºàËßÅ‰∏ªËÆ∫Êñá‰∏≠ÁöÑË°®8Ôºâ„ÄÇ</li>
<li>Interpretability: After training, we can visualize weights from the seasonality and trend branches to have some insights on the predicted values [9].</li>
<li>ÂèØËß£ÈáäÊÄßÔºöËÆ≠ÁªÉÂêéÔºåÊàë‰ª¨ÂèØ‰ª•‰ªéÂ≠£ËäÇÊÄßÂíåË∂ãÂäøÂàÜÊîØ‰∏≠ÂèØËßÜÂåñÊùÉÈáçÔºå‰ª•ÂØπÈ¢ÑÊµãÂÄºÊúâÊâÄ‰∫ÜËß£[9]„ÄÇ</li>
<li>Easy-to-use: LTSF-Linear can be obtained easily without tuning model hyper-parameters.</li>
<li>ÊòìÁî®ÊÄßÔºöLTSF-LinearÂèØ‰ª•ËΩªÊùæËé∑ÂæóÔºåÊó†ÈúÄË∞ÉÊï¥Ê®°ÂûãË∂ÖÂèÇÊï∞„ÄÇ</li>
</ul>
<h4 id="d3-interpretability-of-ltsf-linear">D.3. Interpretability of LTSF-Linear<a class="headerlink" href="#d3-interpretability-of-ltsf-linear" title="Permanent link">¬∂</a></h4>
<p>Because LTSF-Linear is a set of linear models, the weights of linear layers can directly reveal how LTSFLinear works. The weight visualization of LTSF-Linear can also reveal certain characteristics in the data used for forecasting.</p>
<p>Áî±‰∫éLTSF-LinearÊòØ‰∏ÄÁªÑÁ∫øÊÄßÊ®°ÂûãÔºåÁ∫øÊÄßÂ±ÇÁöÑÊùÉÈáçÂèØ‰ª•Áõ¥Êé•Êè≠Á§∫LTSF-LinearÁöÑÂ∑•‰ΩúÂéüÁêÜ„ÄÇLTSF-LinearÁöÑÊùÉÈáçÂèØËßÜÂåñËøòÂèØ‰ª•Êè≠Á§∫Áî®‰∫éÈ¢ÑÊµãÁöÑÊï∞ÊçÆ‰∏≠ÁöÑÊüê‰∫õÁâπÂæÅ„ÄÇ</p>
<p>Here we take DLinear as an example. Accordingly, we visualize the trend and remainder weights of all datasets with a fixed input length of 96 and four different forecasting horizons. To obtain a smooth weight with a clear pattern in visualization, we initialize the weights of the linear layers in DLinear as 1/L rather than random initialization. That is, we use the same weight for every forecasting time step in the look-back window at the start of training.</p>
<p>ËøôÈáåÊàë‰ª¨‰ª•DLinear‰∏∫‰æã„ÄÇÁõ∏Â∫îÂú∞ÔºåÊàë‰ª¨ÂèØËßÜÂåñ‰∫ÜÊâÄÊúâÊï∞ÊçÆÈõÜÁöÑË∂ãÂäøÂíåÊÆãÂ∑ÆÊùÉÈáçÔºåËøô‰∫õÊï∞ÊçÆÈõÜÂÖ∑ÊúâÂõ∫ÂÆöÁöÑËæìÂÖ•ÈïøÂ∫¶96ÂíåÂõõ‰∏™‰∏çÂêåÁöÑÈ¢ÑÊµãËåÉÂõ¥„ÄÇ‰∏∫‰∫ÜÂú®ÂèØËßÜÂåñ‰∏≠Ëé∑ÂæóÂπ≥Êªë‰∏îÂÖ∑ÊúâÊ∏ÖÊô∞Ê®°ÂºèÁöÑÊùÉÈáçÔºåÊàë‰ª¨ÂàùÂßãÂåñDLinear‰∏≠Á∫øÊÄßÂ±ÇÁöÑÊùÉÈáç‰∏∫<span class="arithmatex">\(1/L\)</span>ÔºåËÄå‰∏çÊòØÈöèÊú∫ÂàùÂßãÂåñ„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂú®ËÆ≠ÁªÉÂºÄÂßãÊó∂ÔºåÊàë‰ª¨Âú®ÂõûÊ∫ØÁ™óÂè£‰∏≠ÁöÑÊØè‰∏™È¢ÑÊµãÊó∂Èó¥Ê≠•‰ΩøÁî®Áõ∏ÂêåÁöÑÊùÉÈáç„ÄÇ</p>
<p>**How the model works:**Figure 7(c) visualize the weights of the trend and the remaining layers on the Exchange-Rate dataset. Due to the lack of periodicity and seasonality in financial data, it is hard to observe clear patterns, but the trend layer reveals greater weights of information closer to the outputs, representing their larger contributions to the predicted values.</p>
<p>**Ê®°ÂûãÂ∑•‰ΩúÂéüÁêÜÔºö**Âõæ7(c)Â±ïÁ§∫‰∫ÜÊ±áÁéáÊï∞ÊçÆÈõÜ‰∏äË∂ãÂäøÂ±ÇÂíåÂâ©‰ΩôÂ±ÇÁöÑÊùÉÈáç„ÄÇÁî±‰∫éÈáëËûçÊï∞ÊçÆÁº∫‰πèÂë®ÊúüÊÄßÂíåÂ≠£ËäÇÊÄßÔºåÂæàÈöæËßÇÂØüÂà∞Ê∏ÖÊô∞ÁöÑÊ®°ÂºèÔºå‰ΩÜË∂ãÂäøÂ±ÇÊè≠Á§∫‰∫ÜÊõ¥Êé•ËøëËæìÂá∫ÁöÑ‰ø°ÊÅØÂÖ∑ÊúâÊõ¥Â§ßÁöÑÊùÉÈáçÔºåËøô‰ª£Ë°®‰∫ÜÂÆÉ‰ª¨ÂØπÈ¢ÑÊµãÂÄºÁöÑÊõ¥Â§ßË¥°ÁåÆ„ÄÇ</p>
<p>Periodicity of data: For Traffic data, as shown in Figure 7(d), the model gives high weights to the latest time step of the look-back window for the 0,23,47...719 forecast ing steps. Among these forecasting time steps, the 0, 167, 335, 503, 671 time steps have higher weights. Note that 24 time steps are a day, and 168 time steps are a week. This indicates that Traffic has a daily periodicity and a weekly periodicity.</p>
<p>Êï∞ÊçÆÁöÑÂë®ÊúüÊÄßÔºöÂØπ‰∫é‰∫§ÈÄöÊï∞ÊçÆÔºåÂ¶ÇÂõæ7(d)ÊâÄÁ§∫ÔºåÊ®°ÂûãÂØπ‰∫éÈ¢ÑÊµãÊ≠•Èïø0„ÄÅ23„ÄÅ47...719ÁöÑÊúÄÊñ∞Êó∂Èó¥Ê≠•Ëµã‰∫à‰∫ÜËæÉÈ´òÁöÑÊùÉÈáç„ÄÇÂú®Ëøô‰∫õÈ¢ÑÊµãÊó∂Èó¥Ê≠•‰∏≠Ôºå0„ÄÅ167„ÄÅ335„ÄÅ503„ÄÅ671Êó∂Èó¥Ê≠•ÁöÑÊùÉÈáçÊõ¥È´ò„ÄÇËØ∑Ê≥®ÊÑèÔºå24‰∏™Êó∂Èó¥Ê≠•‰ª£Ë°®‰∏ÄÂ§©Ôºå168‰∏™Êó∂Èó¥Ê≠•‰ª£Ë°®‰∏ÄÂë®„ÄÇËøôË°®Êòé‰∫§ÈÄöÊï∞ÊçÆÂÖ∑ÊúâÊó•Âë®ÊúüÊÄßÂíåÂë®Âë®ÊúüÊÄß„ÄÇ</p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151513500.png" data-desc-position="bottom"><img alt="image-20250415151309257" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151513500.png"></a> </p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151513710.png" data-desc-position="bottom"><img alt="image-20250415151345205" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151513710.png"></a></p>
<p><a class="glightbox" data-type="image" data-width="80%" data-height="auto" href="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151514516.png" data-desc-position="bottom"><img alt="image-20250415151439841" src="https://cdn.jsdelivr.net/gh/dearRongerr/PicGo@main/202504151514516.png"></a> </p>
<p>ËøôÂπÖÂõæÊòØLTSF-LinearÊ®°ÂûãÂú®‰∏çÂêåÂü∫ÂáÜÊµãËØï‰∏äÁöÑÊùÉÈáçÔºàT*LÔºâÂèØËßÜÂåñ„ÄÇÂõæ‰∏≠Â±ïÁ§∫‰∫ÜÊ®°ÂûãÂú®ÂõûÊ∫ØÁ™óÂè£Â§ßÂ∞è‰∏∫LÔºàXËΩ¥ÔºâÂíå‰∏çÂêåÁöÑÈ¢ÑÊµãÊó∂Èó¥Ê≠•ÈïøTÔºàYËΩ¥Ôºâ‰∏ãËÆ≠ÁªÉÂæóÂà∞ÁöÑÊùÉÈáç„ÄÇÂõæ‰∏≠ÂåÖÊã¨‰∫ÜË∂ãÂäøÂ±ÇÔºàTrendÔºâÂíåÊÆãÂ∑ÆÂ±ÇÔºàRemainderÔºâÁöÑÊùÉÈáç„ÄÇ</p>
<p>ÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂõæ‰∏≠Â±ïÁ§∫‰∫Ü‰ª•‰∏ãÂõõÁßçÊÉÖÂÜµÁöÑÊùÉÈáçÂàÜÂ∏ÉÔºö
1. (f1) Âíå (f3)ÔºöÊÆãÂ∑ÆÂ±ÇÁöÑÊùÉÈáçÔºåÂàÜÂà´ÂØπÂ∫îÈ¢ÑÊµãÊ≠•Èïø‰∏∫24Âíå36„ÄÇ
2. (f2) Âíå (f4)ÔºöË∂ãÂäøÂ±ÇÁöÑÊùÉÈáçÔºåÂàÜÂà´ÂØπÂ∫îÈ¢ÑÊµãÊ≠•Èïø‰∏∫24Âíå36„ÄÇ
3. (f5) Âíå (f7)ÔºöÊÆãÂ∑ÆÂ±ÇÁöÑÊùÉÈáçÔºåÂàÜÂà´ÂØπÂ∫îÈ¢ÑÊµãÊ≠•Èïø‰∏∫48Âíå60„ÄÇ
4. (f6) Âíå (f8)ÔºöË∂ãÂäøÂ±ÇÁöÑÊùÉÈáçÔºåÂàÜÂà´ÂØπÂ∫îÈ¢ÑÊµãÊ≠•Èïø‰∏∫48Âíå60„ÄÇ</p>
<p>ÊØè‰∏™Â≠êÂõæÁöÑXËΩ¥Ë°®Á§∫ÂõûÊ∫ØÁ™óÂè£‰∏≠ÁöÑÊó∂Èó¥Ê≠•ÈïøÔºåYËΩ¥Ë°®Á§∫È¢ÑÊµãÁöÑÊó∂Èó¥Ê≠•Èïø„ÄÇÈ¢úËâ≤Êù°Ë°®Á§∫ÊùÉÈáçÁöÑÂ§ßÂ∞èÔºåÈ¢úËâ≤‰ªéËìùËâ≤ÔºàË¥üÊùÉÈáçÔºâÂà∞ÈªÑËâ≤ÔºàÊ≠£ÊùÉÈáçÔºâÂèòÂåñÔºåÊùÉÈáçÁöÑÁªùÂØπÂÄºË∂äÂ§ßÔºåÈ¢úËâ≤Ë∂äÊé•ËøëÈªÑËâ≤„ÄÇ</p>
<p>ÈÄöËøáËøô‰∫õÂèØËßÜÂåñÔºåÂèØ‰ª•ËßÇÂØüÂà∞Ê®°ÂûãÂú®‰∏çÂêåÊó∂Èó¥Ê≠•Èïø‰∏äÁöÑÊùÉÈáçÂàÜÂ∏ÉÊÉÖÂÜµÔºå‰ªéËÄå‰∫ÜËß£Ê®°ÂûãÊòØÂ¶Ç‰Ωï‰ªéËæìÂÖ•Êï∞ÊçÆ‰∏≠Â≠¶‰π†Êó∂Èó¥‰æùËµñÂÖ≥Á≥ªÁöÑ„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•ËßÇÂØüÂà∞Êüê‰∫õÊó∂Èó¥Ê≠•ÈïøÁöÑÊùÉÈáçÂú®ÁâπÂÆöÈ¢ÑÊµãÊ≠•Èïø‰∏äÊõ¥‰∏∫ÊòæËëóÔºåËøôÂèØËÉΩË°®ÊòéËøô‰∫õÊó∂Èó¥Ê≠•ÈïøÂØπÈ¢ÑÊµãÁªìÊûúÊúâÊõ¥Â§ßÁöÑÂΩ±Âìç„ÄÇ</p>

                

  <div id="__comments"></div>


<!-- <h3>È¢úËâ≤‰∏ªÈ¢òË∞ÉÊï¥</h3>
    <div class="tx-switch">
    <button class="button1" data-md-color-primary="red" style="background-color:red">red</button>
    <button class="button1" data-md-color-primary="pink" style="background-color:pink;color:black">pink</button>
    <button class="button1" data-md-color-primary="purple" style="background-color:purple">purple</button>
    <button class="button1" data-md-color-primary="indigo" style="background-color:indigo">indigo</button>
    <button class="button1" data-md-color-primary="blue" style="background-color:blue">blue</button>
    <button class="button1" data-md-color-primary="cyan" style="background-color:cyan;color:black">cyan</button>
    <button class="button1" data-md-color-primary="teal" style="background-color:teal">teal</button>
    <button class="button1" data-md-color-primary="green" style="background-color:green">green</button>
    <button class="button1" data-md-color-primary="lime" style="background-color:lime;color:black">lime</button>
    <button class="button1" data-md-color-primary="orange" style="background-color:orange;color:black">orange</button>
    <button class="button1" data-md-color-primary="brown" style="background-color:brown;border-radius=3px">brown</button>
    <button class="button1" data-md-color-primary="grey" style="background-color:grey">grey</button>
    <button class="button1" data-md-color-primary="black" style="background-color:black">black</button>
    <button class="button1" data-md-color-primary="white" style="background-color:white;color:black">white</button>
    </div> -->

<!-- Giscus -->
<script src="https://giscus.app/client.js" data-repo="Auzers/notes" data-repo-id="R_kgDONhXY1w" data-category="Announcements" data-category-id="DIC_kwDONhXY184ClgKT" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async>
</script>

<script>
    var buttons = document.querySelectorAll("button[data-md-color-primary]")
    buttons.forEach(function (button) {
        button.addEventListener("click", function () {
            var attr = this.getAttribute("data-md-color-primary")
            document.body.setAttribute("data-md-color-primary", attr)
            localStorage.setItem("data-md-color-primary", attr);
        })
    })
</script>

<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")
</script>

<script>
    var giscus = document.querySelector("script[src*=giscus]")

    /* Set palette on initial load */
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate" ? "dark" : "light"
        giscus.setAttribute("data-theme", theme)
    }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function () {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function () {
            var palette = __md_get("__palette")
            if (palette && typeof palette.color === "object") {
                var theme = palette.color.scheme === "slate" ? "dark" : "light"

                /* Instruct Giscus to change theme */
                var frame = document.querySelector(".giscus-frame")
                frame.contentWindow.postMessage(
                    { giscus: { setConfig: { theme } } },
                    "https://giscus.app"
                )
            }
        })
    })
</script>


              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.action.edit", "content.action.view", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.code.select", "content.tooltips", "content.footnote.tooltips", "content.tabs.link", "header.autohide", "navigation.tabs", "navigation.tracking", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.top", "navigation.path"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../../assets/javascripts/katex.js"></script>
      
        <script src="../../../assets/javascripts/mathjax.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml-full.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg-full.js"></script>
      
        <script src="../../../assets/javascripts/tablesort.js"></script>
      
        <script src="../../../assets/javascripts/toc.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="../../../mkdocs/javascripts/katex.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
      
        <script src="../../../assets/javascripts/web-time.js"></script>
      
        <script src="https://cdn.jsdelivr.net/gh/Wcowin/Wcowin.github.io@main/docs/javascripts/extra.js"></script>
      
        <script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      
        <script src="../../../assets/document_dates/tippy/popper.min.js"></script>
      
        <script src="../../../assets/document_dates/tippy/tippy.umd.min.js"></script>
      
        <script src="../../../assets/document_dates/core/default.config.js"></script>
      
        <script src="../../../assets/document_dates/user.config.js"></script>
      
        <script src="../../../assets/document_dates/core/utils.js"></script>
      
        <script src="../../../assets/document_dates/core/core.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script><script>document.addEventListener('DOMContentLoaded', () => {
    let attempts = 0;
    const maxAttempts = 50;

    const waitForNav = setInterval(() => {
        attempts++;
        const nav = document.querySelector('.md-sidebar--secondary .md-nav__list');
        if (!nav) return;

        const navItems = Array.from(nav.querySelectorAll('.md-nav__item'));
        if (navItems.length === 0) {
            if (attempts >= maxAttempts) {
                clearInterval(waitForNav);
                console.error('ÁõÆÂΩïÂä†ËΩΩË∂ÖÊó∂');
            }
            return;
        }

        clearInterval(waitForNav);
        console.log('ÁõÆÂΩïÈ°πÊï∞Èáè:', navItems.length);

        // Ëé∑Âèñ‰∏ªÂÜÖÂÆπÂå∫Âüü
        const mainContent = document.querySelector('.md-content__inner');
        if (!mainContent) return;

        // Ëé∑ÂèñÊâÄÊúâÊ†áÈ¢ò
        const headings = Array.from(mainContent.querySelectorAll('h1, h2, h3, h4, h5, h6'));
        console.log('Ê†áÈ¢òÊï∞Èáè:', headings.length);

        // Êî∂ÈõÜÊâÄÊúâÂàÜÈöîÁ∫ø‰ΩçÁΩÆ
        const walker = document.createTreeWalker(mainContent, NodeFilter.SHOW_TEXT, {
            acceptNode: (node) => {
                if (!node || !node.textContent) return NodeFilter.FILTER_REJECT;
                if (node.parentElement?.closest('pre, code, .md-nav, script, style')) {
                    return NodeFilter.FILTER_REJECT;
                }
                return NodeFilter.FILTER_ACCEPT;
            }
        }, false);

        const hrPositions = [];
        let node;
        while (node = walker.nextNode()) {
            const text = node.textContent.trim();
            const match = text.match(/^---(.+?)---$/);
            if (match) {
                hrPositions.push({
                    node,
                    content: match[1].trim()
                });
            }
        }

        // Â§ÑÁêÜÊØè‰∏™ÂàÜÈöîÁ∫ø
        hrPositions.forEach((hrData, index) => {
            // ÂàõÂª∫ÂàÜÈöîÁ∫ø
            const hr = document.createElement('hr');
            hr.setAttribute('data-content', hrData.content);
            hr.classList.add('custom-hr');
            hr.id = `hr-${index + 1}`;
            hrData.node.parentNode.replaceChild(hr, hrData.node);

            // ÂàõÂª∫ÁõÆÂΩïÈ°π
            const li = document.createElement('li');
            li.classList.add('md-nav__item', 'hr-nav-item');
            const a = document.createElement('a');
            a.classList.add('md-nav__link');
            a.href = `#hr-${index + 1}`;
            a.textContent = `${hrData.content}`; // ‰øÆÊîπËøôÈáåÔºåÊ∑ªÂä†ÂâçÂêéÁöÑÁ†¥ÊäòÂè∑
            li.appendChild(a);



            // ÊâæÂà∞ÂàÜÈöîÁ∫øÂêéÁöÑÁ¨¨‰∏Ä‰∏™Ê†áÈ¢ò
            let nextHeadingIndex = -1;
            for (let i = 0; i < headings.length; i++) {
                const heading = headings[i];
                if (hr.compareDocumentPosition(heading) & Node.DOCUMENT_POSITION_FOLLOWING) {
                    nextHeadingIndex = i;
                    break;
                }
            }

            console.log('ÂàÜÈöîÁ∫ø:', hrData.content, '‰∏ã‰∏Ä‰∏™Ê†áÈ¢òÁ¥¢Âºï:', nextHeadingIndex);

            if (nextHeadingIndex !== -1) {
                // ÊâæÂà∞ÂØπÂ∫îÁöÑÁõÆÂΩïÈ°π
                const nextHeading = headings[nextHeadingIndex];
                const nextNavItem = navItems.find(item => {
                    const link = item.querySelector('a');
                    const text = link?.textContent?.replace('¬∂', '').trim();
                    return text === nextHeading.textContent.replace('¬∂', '').trim();
                });

                if (nextNavItem) {
                    nav.insertBefore(li, nextNavItem);
                    console.log('ÊèíÂÖ•Âà∞Ê†áÈ¢òÂâç:', nextHeading.textContent);
                    return;
                }
            }

            nav.appendChild(li);
            console.log('ËøΩÂä†Âà∞Êú´Â∞æ:', hrData.content);
        });
    }, 100);
});</script></body></html>