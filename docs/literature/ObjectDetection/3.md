# ï¼ˆDETRï¼‰End-to-End Object Detection with Transformer

[è®ºæ–‡](https://arxiv.org/pdf/2005.12872)

[æºç ](https://github.com/facebookresearch/detr)

![image-20241127115450547](images/image-20241127115450547.png)

é¢˜ç›®ï¼šç«¯åˆ°ç«¯çš„ã€åŸºäºTransformerçš„ç›®æ ‡æ£€æµ‹

ä½œè€…ï¼šFacebook

æ—¶é—´ï¼š2020å¹´5æœˆ28æ—¥

æœŸåˆŠï¼š

## â­ï¸ æ‘˜è¦

> We present a new method that views object detection as a direct set prediction problem. 

å°†ç›®æ ‡æ£€æµ‹çœ‹åšæ˜¯ä¸€ä¸ªé¢„æµ‹é—®é¢˜

> Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. 

ç®€åŒ–äº†æ£€æµ‹è¿‡ç¨‹ã€å»æ‰äº†å¾ˆå¤šäººå·¥æ­¥éª¤ï¼šNMSã€ç”Ÿæˆé”šæ¡†

> The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. 

æˆ‘ä»¬çš„æ¨¡å‹åå­—ï¼šDETRã€å…¨å±€æŸå¤±ã€åŒˆç‰™åˆ©äºŒåˆ†åŒ¹é…ç®—æ³•ã€åŸºäºTransformer Encoder decoderçš„æ£€æµ‹ç»“æ„

> Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel.

ç»™å®šlearned object queriesï¼Ÿï¼ŒDETRæ¨ç† å¯¹è±¡å’Œå…¨å±€å›¾åƒçš„ä¸Šä¸‹æ–‡å…³ç³»ï¼Œå¹¶è¡Œçš„è¾“å‡ºæœ€ç»ˆçš„é¢„æµ‹é›†åˆ

> The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. 

DETRæ¦‚å¿µä¸Šç®€å•ã€ä¸éœ€è¦ä¸“é—¨çš„åº“ã€è·Ÿå…¶ä»–æ£€æµ‹å™¨ä¸å¤ªä¸€æ ·

> ï¼ˆç»“æœï¼‰DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. 

DETRçš„å‡†ç¡®æ€§å’Œè¿è¡Œæ—¶é—´æ€§èƒ½ éƒ½å¯ä»¥åª²ç¾ å¾ˆæˆç†Ÿçš„ã€ä¼˜åŒ–å¾ˆå¥½çš„ faster RCNN

æ£€æµ‹æ•°æ®é›†çš„baselineï¼šCOCO

> Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines.

ä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ï¼ŒDETRå¯ä»¥æ¨å¹¿åˆ°å…¨æ™¯åˆ†å‰²ä»»åŠ¡

## â­ï¸ Introduction

### ç¬¬ä¸€æ®µ Modern detectors

> The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest.

ç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„å®šä¹‰

>  Modern detectors address this set prediction task in an indirect way, by defining surrogate regression and classification problems on a large set of proposals [37,5], anchors [23], or window centers [53,46]. 

ç°åœ¨çš„æ£€æµ‹æ–¹æ³•ï¼šé—´æ¥çš„æ–¹æ³•è¿›è¡Œæ£€æµ‹

> - [ ] Their performances are significantly influenced by postprocessing steps to collapse near-duplicate predictions, by the design of the anchor sets and by the heuristics that assign target boxes to anchors [52]. 

ç°åœ¨çš„æ£€æµ‹æ–¹æ³•ï¼šè¡¨ç°å—åˆ°åå¤„ç†æ­¥éª¤çš„æ˜¾è‘—å½±å“ï¼Œè¿™äº›æ­¥éª¤åŒ…æ‹¬åˆå¹¶è¿‘é‡å¤çš„é¢„æµ‹ã€é”šç‚¹é›†çš„è®¾è®¡ï¼Œä»¥åŠå°†ç›®æ ‡æ¡†åˆ†é…ç»™é”šç‚¹çš„å¯å‘å¼æ–¹æ³•[52]ã€‚

> To simplify these pipelines, we propose a direct set prediction approach to bypass the surrogate tasks. 

ä¸ºäº†ç®€åŒ–è¿™äº›æµç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´æ¥çš„é›†åˆé¢„æµ‹æ–¹æ³•

> This end-to-end philosophy has led to significant advances in complex structured prediction tasks such as machine translation or speech recognition, but not yet in object detection: previous attempts [43,16,4,39] either add other forms of prior knowledge, or have not proven to be competitive with strong baselines on challenging benchmarks.

è¿™ç§ç«¯åˆ°ç«¯çš„ç†å¿µåœ¨å¤æ‚çš„ç»“æ„åŒ–é¢„æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¯”å¦‚æœºå™¨ç¿»è¯‘æˆ–è¯­éŸ³è¯†åˆ«ï¼Œä½†åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸå°šæœªå®ç°ï¼šä¹‹å‰çš„å°è¯•[43,16,4,39]è¦ä¹ˆå¢åŠ äº†å…¶ä»–å½¢å¼çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¦ä¹ˆåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­æœªèƒ½è¯æ˜ä¸å¼ºå¤§çš„åŸºçº¿ç›¸ç«äº‰ã€‚

> This paper aims to bridge this gap.

### å›¾1

![image-20241127132150037](images/image-20241127132150037.png)

- DETRç›´æ¥ã€å¹¶è¡Œçš„é¢„æµ‹æœ€ç»ˆçš„æ£€æµ‹é›†åˆ
- DETRï¼šCNNå’ŒTransformeræ¶æ„ç»„åˆ
- è®­ç»ƒé˜¶æ®µï¼šäºŒéƒ¨å›¾åŒ¹é…å°†é¢„æµ‹å’ŒçœŸå®æ¡†å…³è”èµ·æ¥ã€æ²¡æœ‰åŒ¹é…çš„é¢„æµ‹èƒ½å¤Ÿäº§ç”Ÿä¸€ä¸ªæ²¡æœ‰å¯¹è±¡çš„ç±»åˆ«é¢„æµ‹

### ç¬¬äºŒæ®µ based on transformers & self-attention mechanisms

> We streamline the training pipeline by viewing object detection as a direct set prediction problem. 

å°†ç›®æ ‡æ£€æµ‹è§†ä¸ºä¸€ä¸ªç›´æ¥çš„é›†åˆé¢„æµ‹é—®é¢˜æ¥ç®€åŒ–è®­ç»ƒæµç¨‹

> We adopt an encoder-decoder architecture based on transformers [47], a popular architecture for sequence prediction. 

é‡‡ç”¨åŸºäºTransformerçš„ç»“æ„

> The self-attention mechanisms of transformers, which explicitly model all pairwise interactions between elements in a sequence, make these architectures particularly suitable for specific constraints of set prediction such as removing duplicate predictions.

Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼šæ˜ç¡®ç»™å‡ºåºåˆ—ä¸­æ‰€æœ‰å…ƒç´ çš„æˆå¯¹äº¤äº’ï¼Œç‰¹åˆ«é€‚åˆäºé›†åˆé¢„æµ‹çš„ç‰¹å®šçº¦æŸï¼Œä¾‹å¦‚å»é™¤é‡å¤é¢„æµ‹

ï¼ˆå°±æ˜¯è¯´æ˜Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å¾ˆé€‚åˆç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼Œç»™å‡ºæˆå¯¹å…ƒç´ ä¹‹é—´çš„å…³ç³»ï¼‰

### ç¬¬ä¸‰æ®µ trained end-to-end 

> Our DEtection TRansformer (DETR, see Figure 1) predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects. 

- ä¸€æ¬¡æ€§ã€ç«¯åˆ°æ®µé¢„æµ‹
- é€šè¿‡ä¸€ä¸ªé›†åˆæŸå¤±å‡½æ•°è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œè¯¥å‡½æ•°åœ¨é¢„æµ‹å¯¹è±¡å’ŒçœŸå®å¯¹è±¡ä¹‹é—´æ‰§è¡ŒäºŒåˆ†å›¾åŒ¹é…

> DETR simplifies the detection pipeline by dropping multiple hand-designed components that encode prior knowledge, like spatial anchors or non-maximal suppression. 

DETRé€šè¿‡èˆå¼ƒå¤šä¸ªç¼–ç å…ˆéªŒçŸ¥è¯†çš„æ‰‹å·¥è®¾è®¡ç»„ä»¶ï¼Œå¦‚ç©ºé—´é”šç‚¹æˆ–éæå¤§å€¼æŠ‘åˆ¶ï¼Œç®€åŒ–äº†æ£€æµ‹æµç¨‹ã€‚

> Unlike most existing detection methods, DETR doesnâ€™t require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes.1

ä¸å¤§å¤šæ•°ç°æœ‰çš„æ£€æµ‹æ–¹æ³•ä¸åŒï¼ŒDETRä¸éœ€è¦ä»»ä½•å®šåˆ¶å±‚ï¼Œå› æ­¤å¯ä»¥åœ¨åŒ…å«æ ‡å‡†CNNå’Œå˜æ¢å™¨ç±»çš„ä»»ä½•æ¡†æ¶ä¸­è½»æ¾å¤ç°ã€‚

### ç¬¬å››æ®µ bipartite matching loss

> Compared to most previous work on direct set prediction, the main features of DETR are the conjunction of the bipartite matching loss and transformers with (non-autoregressive) parallel decoding [29,12,10,8]. 

DETRçš„ç‰¹ç‚¹ï¼šç»“åˆäºŒåˆ†å›¾åŒ¹é…æŸå¤±å’ŒTransformerçš„å¹¶è¡Œè§£ç æŸå¤±

ğŸ“¢ transformers with (non-autoregressive) éè‡ªå›å½’

> In contrast, previous work focused on autoregressive decoding with RNNs [43,41,30,36,42]. 

å…ˆå‰çš„æŸå¤±æ˜¯ï¼šRNNè§£ç å™¨çš„è‡ªå›å½’æŸå¤±

> Our matching loss function uniquely assigns a prediction to a ground truth object, and is invariant to a permutation of predicted objects, so we can emit them in parallel.

åŒ¹é…æŸå¤±å‡½æ•°å”¯ä¸€åœ°å°†é¢„æµ‹åˆ†é…ç»™çœŸå®å¯¹è±¡ï¼Œå¹¶ä¸”å¯¹é¢„æµ‹å¯¹è±¡çš„æ’åˆ—æ˜¯ä¸å˜çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å¹¶è¡Œåœ°è¾“å‡ºå®ƒä»¬ã€‚

### ç¬¬äº”æ®µ baseline

> We evaluate DETR on one of the most popular object detection datasets, COCO [24], against a very competitive Faster R-CNN baseline [37]. 

æ•°æ®é›†ï¼šobject detection datasets, COCO

æ¨¡å‹ï¼šFaster R-CNN baseline

> More precisely, DETR demonstrates significantly better performance on large objects, a result likely enabled by the non-local computations of the transformer. 

DETRåœ¨å¤§ç›®æ ‡çš„æ£€æµ‹æ€§èƒ½æ¯”è¾ƒå¥½

å¯èƒ½çš„åŸå› ï¼šTransformerçš„éå±€éƒ¨è®¡ç®—ï¼ˆæ˜¯çš„ï¼ŒTransformeræ˜¯å¯¹æ‰€æœ‰è¯ï¼Œä¸¤ä¸¤ä¹‹é—´ä»»æ„å¯èƒ½å¾—å…³ç³»éƒ½è¿›è¡Œå»ºæ¨¡ï¼Œæ˜¯ä¸€ç§å…¨å±€å»ºæ¨¡æ–¹æ³•ï¼Œé—®é¢˜å°±æ˜¯ ä¼šå¯¹ä¸é‚£ä¹ˆé‡è¦çš„è¯ ä¹Ÿè¿›è¡Œäº†å…³æ³¨ï¼‰

> It obtains, however, lower performances on small objects. We expect that future work will improve this aspect in the same way the development of FPN [22] did for Faster R-CNN.

ä½†æ˜¯ï¼Œå°ç‰©ä½“æ£€æµ‹æ€§èƒ½å°±ä¸é‚£ä¹ˆå¥½

### ç¬¬å…­æ®µ differ from standard object detectors

> The new model requires extra-long training schedule and benefits from auxiliary decoding losses in the transformer. 

æ–°æ¨¡å‹ï¼ˆDETRï¼‰éœ€è¦ä¸€ä¸ªè¶…é•¿çš„è®­ç»ƒæ—¶é—´è¡¨ï¼Œå¹¶ä¸”ä»transformerä¸­çš„è¾…åŠ©è§£ç æŸå¤±ä¸­å—ç›Š

### ç¬¬ä¸ƒæ®µ extend to more complex tasks

> The design ethos of DETR easily extend to more complex tasks. 

å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ä»»åŠ¡

> In our experiments, we show that a simple segmentation head trained on top of a pretrained DETR outperfoms competitive baselines on **Panoptic Segmentation** [19], a challenging pixel-level recognition task that has recently gained popularity.

ï¼ˆå…¨æ™¯åˆ†å‰²ï¼‰åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„åˆ†å‰²å¤´ï¼Œå®ƒåœ¨é¢„è®­ç»ƒçš„DETRä¹‹ä¸Šè®­ç»ƒï¼Œå…¶æ€§èƒ½åœ¨Panopticåˆ†å‰²[19]ä¸Šè¶…è¶Šäº†ç«äº‰åŸºçº¿ï¼ŒPanopticåˆ†å‰²æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åƒç´ çº§è¯†åˆ«ä»»åŠ¡ï¼Œæœ€è¿‘å˜å¾—æµè¡Œèµ·æ¥ã€‚

## â­ï¸ Related work

> Our work build on prior work in several domains: bipartite matching losses for set prediction, encoder-decoder architectures based on the transformer, parallel decoding, and object detection methods.

**DETRæ¶‰åŠåˆ°çš„ç›¸å…³é¢†åŸŸï¼š**

- bipartite matching losses for set prediction å¯¹äºé›†åˆé¢„æµ‹çš„äºŒåˆ†å›¾åŒ¹é…æŸå¤±
- encoder-decoder architectures based on the transformer 
- parallel decoding å¹¶è¡Œè§£ç 
- object detection methods.

### ç›¸å…³é¢†åŸŸå·¥ä½œ1ï¼šSet Prediction $\rightarrow  $ åŒˆç‰™åˆ©æŸå¤±

> There is no canonicalï¼ˆæ ‡å‡†çš„ï¼‰ deep learning model to directly predict sets. 

å¯¹äºé›†åˆé¢„æµ‹ï¼Œæ²¡æœ‰æ ‡å‡†çš„æ·±åº¦å­¦ä¹ æ¨¡å‹

> The basic set prediction task is multilabel classification for which the baseline approach, one-vs-rest, does not apply to problems such as detection where there is an underlying structure between elements (i.e., near-identical boxes). 

é›†åˆé¢„æµ‹ä»»åŠ¡æ˜¯ä¸€ç§å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œä¸€å¯¹å¤šï¼Œä¸é€‚åˆæ£€æµ‹ä»»åŠ¡

æ£€æµ‹ä»»åŠ¡çš„ç‰¹ç‚¹ï¼šå…ƒç´ ä¹‹é—´æœ‰ç»“æ„å…³ç³»ï¼Œä¾‹å¦‚ï¼Œæ£€æµ‹ä¸­çš„è¿‘ç›¸åŒæ¡†

> The first difficulty in these tasks is to avoid near-duplicates.ï¼ˆé¿å…è¿‘ä¼¼é‡å¤ï¼‰ 

æ£€æµ‹ä»»åŠ¡çš„ç¬¬ä¸€ä¸ªéš¾ç‚¹ï¼šå»é‡

> Most current detectors use postprocessings such as non-maximal suppression to address this issue, but direct set prediction are postprocessing-free. 

æ£€æµ‹ä»»åŠ¡çš„åå¤„ç†æ–¹æ³•ï¼šNMS

ä½†æ˜¯ï¼Œé›†åˆé¢„æµ‹æ˜¯ ä¸éœ€è¦è¿›è¡Œåå¤„ç†çš„

> They need global inference schemes that model interactions between all predicted elements to avoid redundancy. 

é›†åˆé¢„æµ‹éœ€è¦ å…¨å±€æ¨ç†

ï¼ˆè¯´çš„æ˜¯IOUã€NMSï¼‰éœ€è¦å…¨å±€æ¨ç†æ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ¡ˆæ¨¡æ‹Ÿæ‰€æœ‰é¢„æµ‹å…ƒç´ ä¹‹é—´çš„äº¤äº’ä»¥é¿å…å†—ä½™

> For constant-size set prediction, dense fully connected networks [9] are sufficient but costly. A general approach is to use auto-regressive sequence models such as recurrent neural networks [48].

å¯¹äºå›ºå®šå¤§å°çš„é›†åˆé¢„æµ‹é—®é¢˜ï¼šå…¨è¿æ¥ç½‘ï¼Œç¼ºç‚¹ï¼šæˆæœ¬é«˜

è§£å†³ï¼šè‡ªå›å½’æ¨¡å‹ RNN

>  In all cases, the loss function should be invariant by a permutation of the predictions. 

é›†åˆé¢„æµ‹çš„æŸå¤±å‡½æ•° åº”è¯¥ä¸é¢„æµ‹çš„é¡ºåº æ— å…³

> The usual solution is to design a loss based on the Hungarian algorithm [20], to find a bipartite matching between ground-truth and prediction. 

å¦‚ä½•å®ç° æŸå¤±å‡½æ•°ä¸é¡ºåºæ— å…³ï¼Ÿ åŒˆç‰™åˆ©ç®—æ³•

å¯»æ‰¾gtå’Œé¢„æµ‹çš„äºŒåˆ†åŒ¹é…

> This enforces permutation-invariance, and guarantees that each target element has a unique match. 

ç‰¹ç‚¹ï¼š

- é¡ºåºæ’åˆ—ä¸å˜å½¢
- å”¯ä¸€åŒ¹é…

> We follow the bipartite matching loss approach. In contrast to most prior work however, we step away from autoregressive models and use transformers with parallel decoding.

- DETR ä½¿ç”¨ äºŒåˆ†åŒ¹é…æŸå¤±æ–¹æ³•
- ä½¿ç”¨Transformerçš„å¹¶è¡Œè§£ç æ¨¡å‹
- æ²¡æœ‰ç”¨è‡ªå›å½’æ¨¡å‹

### ç›¸å…³å·¥ä½œ2ï¼šTransformers and Parallel Decoding

#### ç¬¬ä¸€æ®µ ä»‹ç»Transformeræ˜¯ä»€ä¹ˆï¼Œä»¥åŠä¼˜ç‚¹

**Transformers**  were introduced by Vaswani et al . [47] as a new attention-based building block for machine translation. 

**Attention mechanisms**  [2] are neural network layers that aggregate information from the entire input sequence. 

Transformers introduced **self-attention layers**, which, similarly to Non-Local Neural Networks [49], scan through each element of a sequence and update it by aggregating information from the whole sequence. 

> è‡ªæ³¨æ„åŠ›å±‚ï¼Œç±»ä¼¼éå±€éƒ¨ç¥ç»ç½‘ç»œ
>
> æ‰«æåºåˆ—ä¸­æ¯ä¸ªå…ƒç´ ï¼Œèšåˆæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯å¹¶è¿›è¡Œæ›´æ–°

One of the main **advantages**  of attention-based models is their global computations and perfect memory, which makes them more suitable than RNNs on long sequences. Transformers are now replacing RNNs in many problems in natural language processing, speech processing and computer vision [8,27,45,34,31].

#### ç¬¬äºŒæ®µ  trçš„ç¼ºç‚¹ ä»¥åŠ æˆ‘ä»¬

> Transformers were first used in auto-regressive models, following early sequence-to-sequence models [44], generating output tokens one by one. 

Transformeçš„åº”ç”¨é¢†åŸŸï¼šè‡ªå›å½’æ¨¡å‹ï¼Œseq2seqæ¨¡å‹ï¼Œone by oneè¾“å‡º

> However, the prohibitive inference cost (proportional to output length, and hard to batch) lead to the development of parallel sequence generation, in the domains of audio [29], machine translation [12,10], word representation learning [8], and more recently speech recognition [6]. 

Transformerçš„ç¼ºç‚¹ï¼šå¹³æ–¹å¤æ‚åº¦ï¼Œé«˜æ˜‚çš„è®¡ç®—æˆæœ¬

> We also combine transformers and parallel decoding for their suitable trade-off between computational cost and the ability to perform the global computations required for set prediction.

æˆ‘ä»¬çš„åšæ³•ï¼šTransformer+å¹¶è¡Œè§£ç 

æƒè¡¡ è®¡ç®—æˆæœ¬ å’Œ å…¨å±€è®¡ç®—èƒ½åŠ›

### ç›¸å…³å·¥ä½œ3 Object detection

#### ç¬¬ä¸€æ®µ æ˜¯ä»€ä¹ˆ

> Most  **modern object detection**  methods make predictions relative to some initial guesses. 

ç°åœ¨çš„ç›®æ ‡æ£€æµ‹æ–¹æ³• éœ€è¦ç»™å‡º åˆå§‹çŒœæµ‹

> **Two-stage detectors**  [37,5] predict boxes w.r.t. proposals, whereas  **single-stage methods**  make predictions w.r.t. anchors [23] or a grid of possible object centers [53,46]. 

ä¸¤é˜¶æ®µæ£€æµ‹æ–¹æ³• & å•é˜¶æ®µæ£€æµ‹æ–¹æ³•

ä¸¤é˜¶æ®µé¢„æµ‹ é¢„æµ‹æ¡†

å•é˜¶æ®µé¢„æµ‹ é¢„æµ‹é”šç‚¹ or ç›®æ ‡ä¸­å¿ƒçš„ç½‘æ ¼

> Recent work [52] demonstrate that the final performance of these systems heavily depends on the exact way these initial guesses are set.

ç¼ºç‚¹ï¼šä¾èµ–åˆå€¼

> In our model we are able to remove this hand-crafted process and streamline the detection process by directly predicting the set of detections with absolute box prediction w.r.t. the input image rather than an anchor.

æˆ‘ä»¬ï¼š

howï¼Ÿ

- ç§»é™¤æ‰‹å·¥è¿‡ç¨‹
- ç®€åŒ–æ£€æµ‹è¿‡ç¨‹

whatï¼Ÿ

- ï¼ˆç›´æ¥è¯´æ˜¯ä»€ä¹ˆï¼‰ç›´æ¥é¢„æµ‹æ£€æµ‹é›†ï¼šç›´æ¥é¢„æµ‹æ¡†
- ï¼ˆç”¨ç›¸å¯¹å…³ç³»è¯´æ˜¯ä»€ä¹ˆï¼‰æ˜¯å›¾åƒè€Œä¸æ˜¯é”šç‚¹

---

#### ç¬¬ä¸€éƒ¨åˆ† Set-based loss.åŸºäºé›†åˆçš„æŸå¤±

##### ç¬¬äºŒæ®µ

Several object detectors [9,25,35] used the bipartite matching loss. However, in these early deep learning models, the relation between different prediction was modeled with convolutional or fully-connected layers only and a hand-designed NMS post-processing can improve their performance. More recent detectors [37,23,53] use non-unique assignment rules between ground truth and predictions together with an NMS.

##### ç¬¬ä¸‰æ®µ

Learnable NMS methods [16,4] and relation networks [17] explicitly model relations between different predictions with attention. Using direct set losses, they do not require any post-processing steps. However, these methods employ additional hand-crafted context features like proposal box coordinates to model relations between detections efficiently, while we look for solutions that reduce the prior knowledge encoded in the model.

#### ç¬¬äºŒéƒ¨åˆ† Recurrent detectors.  æ£€æµ‹æ–¹æ³•

##### ç¬¬å››æ®µ

Closest to our approach are end-to-end set predictions for object detection [43] and instance segmentation [41,30,36,42]. Similarly to us, they use bipartite-matching losses with encoder-decoder architectures based on CNN activations to directly produce a set of bounding boxes. These approaches, however, were only evaluated on small datasets and not against modern baselines. In particular, they are based on autoregressive models (more precisely RNNs), so they do not leverage the recent transformers with parallel decoding.

## â­ï¸ ç»“è®º

### ç¬¬ä¸€æ®µ

We presented DETR, a new design for object detection systems based on transformers and bipartite matching loss for direct set prediction. The approach achieves comparable results to an optimized Faster R-CNN baseline on the challenging COCO dataset. DETR is straightforward to implement and has a flexible architecture that is easily extensible to panoptic segmentation, with competitive results. In addition, it achieves significantly better performance on large objects than Faster R-CNN, likely thanks to the processing of global information performed by the self-attention. 

### ç¬¬äºŒæ®µ

This new design for detectors also comes with new challenges, in particular regarding training, optimization and performances on small objects. Current detectors required several years of improvements to cope with similar issues, and we expect future work to successfully address them for DETR.



## â­ï¸ The DETR model

Two ingredients are essential for direct set predictions in detection: (1) a set prediction loss that forces unique matching between predicted and ground truth boxes; (2) an architecture that predicts (in a single pass) a set of objects and models their relation. We describe our architecture in detail in Figure 2.





Object detection set prediction loss