# 2024ã€UnetTSF

[https://arxiv.org/pdf/2401.03001](https://arxiv.org/pdf/2401.03001)

![image-20250408085427055](images/image-20250408085427055.png) 

### III. PROPOSED METHOD

æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜æ˜¯åœ¨ç»™å®šé•¿åº¦ä¸º $L$ çš„å†å²æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹é•¿åº¦ä¸º $T$ çš„æœªæ¥æ•°æ®ã€‚è¾“å…¥çš„å†å²æ•°æ® $X = \{x_1, x_2, x_3, \ldots, x_L\}$  å…·æœ‰å›ºå®šé•¿åº¦çš„å›çœ‹çª—å£ Lï¼Œæ¨¡å‹è¾“å‡ºé¢„æµ‹æ•°æ®  $x = \{x_{L+1}, x_{L+2}, \ldots, x_{L+T}\}$ ï¼Œå…¶ä¸­  $x_i$ è¡¨ç¤ºåœ¨æ—¶é—´  $t = i$  æ—¶ç»´åº¦ä¸º $C$ çš„å‘é‡ï¼Œ$C$ è¡¨ç¤ºè¾“å…¥æ•°æ®é›†ä¸­çš„é€šé“æ•°ã€‚æˆ‘ä»¬è®¾è®¡äº† UnetTSFï¼Œä½¿ç”¨ U-Net [27] æ¶æ„ï¼Œå¹¶ä¸“é—¨è®¾è®¡äº†é€‚åˆæ—¶é—´åºåˆ—æ•°æ®çš„ FPN [12] å’Œå¤šæ­¥èåˆæ¨¡å—ï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚

**UNetTSF model**ï¼š UnetTSFç”±å…¨è¿æ¥å±‚å’Œæ± åŒ–å±‚ç»„æˆã€‚æ¨¡å‹çš„å·¦ä¾§ä¸»è¦ç”±æ—¶é—´åºåˆ—ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰æ„æˆï¼Œæ± åŒ–å‡½æ•°ç”¨äºå½¢æˆè¾“å…¥æ•°æ®çš„æè¿°æ€§ç‰¹å¾$ X = \{ X_1, X_2, X_3, ..., X_{\text{stage} } \} $  ã€‚è¿™é‡Œçš„â€œstageâ€è¡¨ç¤ºUnetç½‘ç»œçš„å±‚æ•°ï¼Œæ¨¡å‹çš„å³ä¾§æ˜¯èåˆæ¨¡å—ã€‚å…¨è¿æ¥å±‚ç”¨äºå°†ä¸Šå±‚ç‰¹å¾ä¸å±€éƒ¨å±‚ç‰¹å¾èåˆï¼Œä»¥è¾“å‡ºå½“å‰å±‚çš„æœ€ç»ˆç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒç‰¹å¾é•¿åº¦ä¸å˜ã€‚

**Times series FPN** ï¼ˆæ—¶é—´åºåˆ—ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼‰ï¼šæ•°æ®åˆ†è§£é€šå¸¸è¢«æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ç”¨æ¥ä»æ—¶é—´åºåˆ—æ•°æ®ä¸­æå–ç‰¹å¾ã€‚é€šå¸¸ï¼Œæ•°æ®è¢«åˆ†ä¸ºå­£èŠ‚æ€§ã€å‘¨æœŸæ€§ã€è¶‹åŠ¿å’Œæ³¢åŠ¨é¡¹ã€‚Autoformer å’Œ DLiner éƒ½ä½¿ç”¨å¤§è§„æ¨¡è‡ªé€‚åº”å¹³æ»‘æ ¸ä»åŸå§‹æ•°æ®ä¸­æå–è¶‹åŠ¿é¡¹ã€‚ä»åŸå§‹æ•°æ®ä¸­å‡å»è¶‹åŠ¿é¡¹ä¼šå¾—åˆ°å­£èŠ‚æ€§é¡¹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æŸäº›ç‰¹å¾ä¸¢å¤±ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šå±‚æ¬¡æå–æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå°†æ•°æ®è®¾ç½®ä¸ºåˆ†ä¸º4å±‚ï¼ˆ$\text{stage = 4}$ï¼‰ï¼Œä½¿ç”¨å…·æœ‰ $\text{kernel\_size} = 3$ã€$\text{stride} = 2$ å’Œ $padding = 0$ é…ç½®çš„å¹³å‡æ± åŒ–ï¼ˆ$\text{avgpool}$ï¼‰æå–è¶‹åŠ¿ç‰¹å¾ï¼Œå°†åŸå§‹è¾“å…¥æ•°æ®è®¾ä¸º $x$ ï¼Œå¹¶é€šè¿‡ $\text{FPN}$ æ¨¡å—åï¼Œå½¢æˆå››ä¸ªå±‚æ¬¡çš„è¾“å…¥æ•°æ®ï¼š $X = [x_1, x_2, x_3, x_4]$ ã€‚
$$
x_1 = x
$$

$$
x_2 = \text{AvgPool}(x_1)
$$

$$
x_3 = \text{AvgPool}(x_2)
$$

$$
x_4 = \text{AvgPool}(x_3)
$$

$$
\text{len}(x_i) = \left\lfloor \frac{x_{i-1} + 2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1 \right\rfloor
$$

å¦‚å›¾2(b)æ‰€ç¤ºï¼Œæ—¶é—´åºåˆ—æ•°æ®çš„FPNç»“æ„å¯ä»¥æœ‰æ•ˆæå–è¶‹åŠ¿ç‰¹å¾ã€‚é‡‘å­—å¡”é¡¶å±‚çš„è¶‹åŠ¿ä¿¡æ¯æ¯”åº•å±‚æ›´é›†ä¸­ï¼Œè€Œåº•å±‚çš„å­£èŠ‚æ€§ç‰¹å¾æ›´ä¸ºä¸°å¯Œã€‚

<details>
<summary>è¯´æ˜ï¼šé‡‘å­—å¡”é¡¶å±‚çš„è¶‹åŠ¿ä¿¡æ¯æ¯”åº•å±‚æ›´é›†ä¸­ï¼Œè€Œåº•å±‚çš„å­£èŠ‚æ€§ç‰¹å¾æ›´ä¸ºä¸°å¯Œ</summary>
<p>
é¦–å…ˆï¼Œå…³äºé¡¶å±‚å’Œåº•å±‚ï¼šåº•å±‚é è¿‘è¾“å…¥ç«¯ï¼Œåºåˆ—é•¿åº¦é•¿ï¼Œæ‰€è°“çš„åˆ†è¾¨ç‡é«˜
é¡¶å±‚é è¿‘è¾“å‡ºç«¯ï¼Œåºåˆ—é•¿åº¦çŸ­ï¼Œæ‰€è°“çš„åˆ†è¾¨ç‡ä½
å…¶æ¬¡ï¼Œå…³äºæ± åŒ–æ“ä½œåœ¨æ—¶é—´åºåˆ—ä¸­çš„ä½¿ç”¨ï¼Œç›¸å½“äºæ¯æ¬¡ä¸¢æ‰äº†è¶‹åŠ¿ä¿¡æ¯ï¼Œæœ€é¡¶å±‚çš„ç‰¹å¾æœ€æŠ½è±¡ï¼Œä¿ç•™äº†å…¨å±€ä¿¡æ¯å’Œæ³¢åŠ¨ä¿¡æ¯
éœ€è¦æ³¨æ„ï¼Œæ± åŒ–æ˜¯æ± åŒ–æ‰äº†é•¿æœŸè¶‹åŠ¿ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯å¤§æ ¸æ± åŒ–æå– trendï¼Œæ¯”å¦‚ Autoformer
- å¹³å‡æ± åŒ–ï¼šä¼šå¹³æ»‘çŸ­æœŸæ³¢åŠ¨ï¼Œéƒ¨åˆ†ä¿ç•™å­£èŠ‚æ€§æ¨¡å¼çš„å¤§è‡´å½¢çŠ¶ï¼Œå°¤å…¶æ˜¯å½“å­£èŠ‚å‘¨æœŸè¿œå¤§äºæ± åŒ–çª—å£å¤§å°æ—¶
- æœ€å¤§æ± åŒ–ä¼šä¿ç•™æ˜¾è‘—å³°å€¼ï¼Œè¿™äº›å³°å€¼é€šå¸¸æ˜¯å­£èŠ‚æ€§æ¨¡å¼çš„å…³é”®ç‰¹å¾
</p>
</details>
**Multi stage fusion module**ï¼ˆå¤šé˜¶æ®µèåˆæ¨¡å—ï¼‰ï¼šé€šè¿‡æ—¶é—´åºåˆ—æ•°æ®çš„FPNæ¨¡å—ï¼Œå½¢æˆäº†ä¸€ä¸ªå¤šå°ºåº¦çš„æ—¶é—´ç‰¹å¾ $X$ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›ç‰¹å¾ï¼Œä½¿ç”¨å¤šä¸ªå…¨è¿æ¥é¢„æµ‹æ¥è·å¾— $Y = [y_1, y_2, y_3, y_4]$ã€‚$y_i$ çš„é•¿åº¦è®¡ç®—æ–¹æ³•ä¸ $x$ ç›¸åŒï¼Œå¹¶ä¸”ä½¿ç”¨ç›¸åŒçš„æ± åŒ–æ“ä½œæ¥è®¡ç®—æ¯ä¸ªå±‚çº§ä¸ $X$ ç›¸åŒçš„é•¿åº¦ã€‚èåˆæ¨¡å—é‡‡ç”¨ $y_i$  å’Œ $y_{i-1}$ è¿›è¡Œæ‹¼æ¥ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ªå…¨è¿æ¥å±‚æ¥è¾“å‡º $y_i^{'}$ï¼Œ$y_i^{'}$ å’Œ  $y_i$ çš„é•¿åº¦æ˜¯ç›¸åŒçš„ã€‚

$$
\text{len}(y1) = \text{len}(y) = T
$$

$$
y_{i-1}^{'} = \text{Linear}(\text{cat}(y_i^{'}, y_{i-1}))
$$

$$
\text{len}(y_i) = \left\lfloor \frac{y_{i-1} + 2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1 \right\rfloor
$$

æºç ï¼š[https://github.com/lichuustc/UnetTSF/tree/main/models](https://github.com/lichuustc/UnetTSF/tree/main/models)

æ€ä¹ˆé…ç½®ï¼Ÿ

- å®˜ç½‘ä¸‹è½½ä¸‹æ¥æ¨¡å‹æ–‡ä»¶
- é…ç½®æ–‡ä»¶æ¬è¿‡æ¥ï¼Œå®Œäº‹ã€‚

<details>
<summary>æ¨¡å‹æ–‡ä»¶ï¼Œå·²é…ç½®å¥½</summary>
<p>

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from layers.RevIN import RevIN
import custom_repr
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class block_model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self, input_channels, input_len, out_len, individual):
        super(block_model, self).__init__()
        self.channels = input_channels
        self.input_len = input_len
        self.out_len = out_len
        self.individual = individual

        if self.individual:
            self.Linear_channel = nn.ModuleList()
            
            for i in range(self.channels):
                self.Linear_channel.append(nn.Linear(self.input_len, self.out_len))
        else:
            self.Linear_channel = nn.Linear(self.input_len, self.out_len)
        self.ln = nn.LayerNorm(out_len)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        if self.individual:
            output = torch.zeros([x.size(0),x.size(1),self.out_len],dtype=x.dtype).to(x.device)
            for i in range(self.channels):
                output[:,i,:] = self.Linear_channel[i](x[:,i,:])
        else:
            output = self.Linear_channel(x)
        #output = self.ln(output)
        #output = self.relu(output)
        return output # [Batch, Channel, Output length]


class Model(nn.Module):
    def __init__(self, configs):
        super(Model, self).__init__()

        self.input_channels = configs.enc_in
        self.input_len = configs.seq_len
        self.out_len = configs.pred_len
        self.individual = configs.individual
        # ä¸‹é‡‡æ ·è®¾å®š
        self.stage_num = configs.stage_num
        self.stage_pool_kernel = configs.stage_pool_kernel
        self.stage_pool_stride = configs.stage_pool_stride
        self.stage_pool_padding = configs.stage_pool_padding

        self.revin_layer = RevIN(self.input_channels, affine=True, subtract_last=False)
        
        len_in = self.input_len
        len_out = self.out_len
        down_in = [len_in]
        down_out = [len_out]
        i = 0
        while i < self.stage_num - 1:
            linear_in = int((len_in + 2 * self.stage_pool_padding - self.stage_pool_kernel)/self.stage_pool_stride + 1 )
            linear_out = int((len_out + 2 * self.stage_pool_padding - self.stage_pool_kernel)/self.stage_pool_stride + 1 )
            down_in.append(linear_in)
            down_out.append(linear_out)
            len_in = linear_in
            len_out = linear_out
            i = i + 1

        # æœ€å¤§æ± åŒ–å±‚
        self.Maxpools = nn.ModuleList() 
        # å·¦è¾¹ç‰¹å¾æå–å±‚
        self.down_blocks = nn.ModuleList()
        for in_len,out_len in zip(down_in,down_out):
            self.down_blocks.append(block_model(self.input_channels, in_len, out_len, self.individual))
            self.Maxpools.append(nn.AvgPool1d(kernel_size=self.stage_pool_kernel, stride=self.stage_pool_stride, padding=self.stage_pool_padding))
        
        # å³è¾¹ç‰¹å¾èåˆå±‚
        self.up_blocks = nn.ModuleList()
        len_down_out = len(down_out)
        for i in range(len_down_out -1):
            print(len_down_out, len_down_out - i -1, len_down_out - i - 2)
            in_len = down_out[len_down_out - i - 1] + down_out[len_down_out - i - 2]
            out_len = down_out[len_down_out - i - 2]
            self.up_blocks.append(block_model(self.input_channels, in_len, out_len, self.individual))
        
        #self.linear_out = nn.Linear(self.out_len * 2, self.out_len)

    def forward(self, x):
        x = self.revin_layer(x, 'norm')
        x1 = x.permute(0,2,1)
        e_out = []
        i = 0
        for down_block in self.down_blocks:
            e_out.append(down_block(x1))
            x1 = self.Maxpools[i](x1)
            i = i+1

        e_last = e_out[self.stage_num - 1]
        for i in range(self.stage_num - 1):
            e_last = torch.cat((e_out[self.stage_num - i -2], e_last), dim=2)
            e_last = self.up_blocks[i](e_last)
        e_last = e_last.permute(0,2,1)
        e_last = self.revin_layer(e_last, 'denorm')
        return e_last

import argparse
import os
import torch
import random
import numpy as np

parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')

# random seed
parser.add_argument('--random_seed', type=int, default=2021, help='random seed')

# basic config
parser.add_argument('--is_training', type=int, required=False, default=1, help='status')
parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')
parser.add_argument('--model', type=str, required=False, default='Transformer',
                    help='model name, options: [Autoformer, Informer, Transformer]')

# data loader
parser.add_argument('--data', type=str, required=False, default='ETTh1', help='dataset type')
parser.add_argument('--root_path', type=str, default='./dataset/ETT/', help='root path of the data file')
parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')
parser.add_argument('--features', type=str, default='M',
                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')
parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')
parser.add_argument('--freq', type=str, default='h',
                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')
parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')

# forecasting task
parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')
parser.add_argument('--label_len', type=int, default=48, help='start token length')
parser.add_argument('--pred_len', type=int, default=720, help='prediction sequence length')

#u_linear
parser.add_argument('--stage_num', type=int, default=4, help='stage num')
parser.add_argument('--stage_pool_kernel', type=int, default=3, help='AvgPool1d kernel_size')
parser.add_argument('--stage_pool_stride', type=int, default=2, help='AvgPool1d stride')
parser.add_argument('--stage_pool_padding', type=int, default=0, help='AvgPool1d padding')

# DLinear
#parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')

# PatchTST
parser.add_argument('--fc_dropout', type=float, default=0.05, help='fully connected dropout')
parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')
parser.add_argument('--patch_len', type=int, default=16, help='patch length')
parser.add_argument('--stride', type=int, default=8, help='stride')
parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')
parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')
parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')
parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')
parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')
parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')
parser.add_argument('--individual', type=int, default=1, help='individual head; True 1 False 0')

# Formers 
parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')
parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels
parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')
parser.add_argument('--c_out', type=int, default=7, help='output size')
parser.add_argument('--d_model', type=int, default=512, help='dimension of model')
parser.add_argument('--n_heads', type=int, default=8, help='num of heads')
parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')
parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')
parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')
parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')
parser.add_argument('--factor', type=int, default=1, help='attn factor')
parser.add_argument('--distil', action='store_false',
                    help='whether to use distilling in encoder, using this argument means not using distilling',
                    default=True)
parser.add_argument('--dropout', type=float, default=0.05, help='dropout')
parser.add_argument('--embed', type=str, default='timeF',
                    help='time features encoding, options:[timeF, fixed, learned]')
parser.add_argument('--activation', type=str, default='gelu', help='activation')
parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')
parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')

#corr
parser.add_argument('--corr_lower_limit', type=float, default=0.6, help='find corr ')

# optimization
parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')
parser.add_argument('--itr', type=int, default=1, help='experiments times')
parser.add_argument('--train_epochs', type=int, default=80, help='train epochs')
parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')
parser.add_argument('--patience', type=int, default=20, help='early stopping patience')
parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')
parser.add_argument('--des', type=str, default='test', help='exp description')
parser.add_argument('--loss', type=str, default='mse', help='loss function')
parser.add_argument('--lradj', type=str, default='type3', help='adjust learning rate')
parser.add_argument('--pct_start', type=float, default=0.3, help='pct_start')
parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)

# GPU
parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')
parser.add_argument('--gpu', type=int, default=0, help='gpu')
parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)
parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')
parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')

args = parser.parse_args()

# random seed
fix_seed = args.random_seed
random.seed(fix_seed)
torch.manual_seed(fix_seed)
np.random.seed(fix_seed)


args.use_gpu =  True

if args.use_gpu and args.use_multi_gpu:
    args.dvices = args.devices.replace(' ', '')
    device_ids = args.devices.split(',')
    args.device_ids = [int(id_) for id_ in device_ids]
    args.gpu = args.device_ids[0]

# print('Args in experiment:')
# print(args.seq_len)


batch_x = torch.randn(args.batch_size, args.seq_len, args.enc_in)
# ETTh1 => 17420 , channels = 7,Frequence = 7

model = Model(args).float()

outputs = model(batch_x)
```
</p>
</details>

```python
def forward(self, x):
    x = self.revin_layer(x, 'norm')
    x1 = x.permute(0,2,1)
    e_out = []
    i = 0
    for down_block in self.down_blocks:
        e_out.append(down_block(x1))
        x1 = self.Maxpools[i](x1)
        i = i+1

    e_last = e_out[self.stage_num - 1]
    for i in range(self.stage_num - 1):
        e_last = torch.cat((e_out[self.stage_num - i -2], e_last), dim=2)
        e_last = self.up_blocks[i](e_last)
    e_last = e_last.permute(0,2,1)
    e_last = self.revin_layer(e_last, 'denorm')
    return e_last
```

å¤è¿°å½¢çŠ¶å˜åŒ–ï¼š

> - `pred_len  =720` 

ï¼ˆ1ï¼‰ä¼ è¿›æ¥çš„x å½¢çŠ¶ `args.batch_size, args.seq_len, args.enc_in =  [32,96,7]` 

ï¼ˆ2ï¼‰å¯é€†å®ä¾‹å½’ä¸€åŒ–ï¼Œè¿˜æ˜¯xï¼Œå½¢çŠ¶ [32,96,7]

ï¼ˆ3ï¼‰permute è½¬æ¢ç»´åº¦ï¼Œå¾—åˆ° x1ï¼Œå½¢çŠ¶[32,7,96]ï¼Œä»è¿™é‡Œå¼€å§‹æ‰€æœ‰çš„æ—¶é—´åºåˆ—æ•°æ®å°è£…æ¨¡å¼ éƒ½æ˜¯ ç‰¹å¾ä¼˜å…ˆã€‚

ï¼ˆ4ï¼‰è¾“å…¥ x1.shape = [32,7,96]ï¼Œå¤„ç† down_block(x1)ï¼Œè¾“å‡ºå½¢çŠ¶ [32, 7, 720]ï¼Œè¿™ä¸ªè¾“å‡ºå½¢çŠ¶å¹¶æ²¡æœ‰å‚ä¸åç»­çš„æ± åŒ–å¤„ç†ã€‚è€Œæ˜¯å­˜åˆ°äº† `e_out`    å†é‡å¤ä¸€éå°±æ˜¯å®šä¹‰äº† 7 ä¸ªç‹¬ç«‹çš„çº¿æ€§å±‚ï¼Œåˆ†åˆ«å°† 96 ä¸ªè¾“å…¥åºåˆ— æ˜ å°„åˆ° 720 çš„é¢„æµ‹æ­¥é•¿ã€‚

<details>
<summary>è¡¥å…… å…³äº  down_block(x1)</summary>
<p>
ç»“æ„æ˜¯ï¼šå¤šä¸ªå¹¶è¡Œçº¿æ€§å±‚
block_model(
  (Linear_channel): ModuleList(
    (0-6): 7 x Linear(in_features=96, out_features=720, bias=True)
  )
  (ln): LayerNorm((720,), eps=1e-05, elementwise_affine=True)
  (relu): ReLU(inplace=True)
)
â‘  (0-6): 7 x Linear(in_features=96, out_features=720, bias=True)ï¼Œè¡¨ç¤ºæ¨¡å‹åŒ…å« 7ä¸ªç‹¬ç«‹çš„çº¿æ€§å±‚ï¼Œæ¯ä¸ªçº¿æ€§å±‚ä¸“é—¨å¤„ç†ä¸€ä¸ªè¾“å…¥é€šé“çš„æ•°æ®
â‘¡ (0-6): è¡¨ç¤ºç´¢å¼•èŒƒå›´ï¼Œä»0åˆ°6ï¼Œå…±7ä¸ªå…ƒç´ 
â‘¢ 7 x: è¡¨ç¤ºæœ‰7ä¸ªç›¸åŒç±»å‹çš„æ¨¡å—
â‘£ Linear(in_features=96, out_features=720, bias=True): æ¯ä¸ªæ¨¡å—éƒ½æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œè¾“å…¥ç‰¹å¾96ï¼Œè¾“å‡ºç‰¹å¾720
â‘¤ å½“ individual=True æ—¶ï¼ˆåœ¨ä»£ç ä¸­ args.individual=1ï¼‰ï¼Œæ¨¡å‹ä¸ºæ¯ä¸ªè¾“å…¥é€šé“åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„çº¿æ€§å±‚ã€‚==ã€‹"é€šé“ç‹¬ç«‹å¤„ç†"ï¼Œå¸¸ç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹
è§£é‡Šï¼šç›®å‰éƒ½åœ¨ç”¨çš„é€šé“ç‹¬ç«‹
â‘  ç‰¹åŒ–å¤„ç†: æ¯ä¸ªé€šé“(å˜é‡)å¯èƒ½æœ‰ä¸åŒçš„æ¨¡å¼å’Œç‰¹æ€§ï¼Œç‹¬ç«‹å¤„ç†å¯ä»¥æ›´å¥½åœ°æ•æ‰è¿™äº›å·®å¼‚
â‘¡ é¿å…ç‰¹å¾å¹²æ‰°: ä¸åŒå˜é‡å¯èƒ½æœ‰ä¸åŒçš„è§„æ¨¡å’Œåˆ†å¸ƒï¼Œç‹¬ç«‹å¤„ç†å¯ä»¥é¿å…å˜é‡é—´çš„å¹²æ‰°
</p>
</details>

ï¼ˆ5ï¼‰`x1 = self.Maxpools[i](x1)`  è¾“å…¥  x1 å½¢çŠ¶  [32,7,96]ï¼Œæœ€å¤§æ± åŒ–ä»¥åï¼Œè¾“å‡ºå½¢çŠ¶  `torch.Size([32, 7, 47])`

æˆ‘åˆç†æ¸…æ¥šäº†ï¼ï¼èµ¶ç´§è®°ä¸‹æ¥ï¼

å¼€å§‹ï¼š

```python
x = self.revin_layer(x, 'norm')   # torch.Size([32, 96, 7])
x1 = x.permute(0,2,1)  #  [32,7,96]
e_out = []
i = 0
â‘  for down_block in self.down_blocks:
    e_out.append(down_block(x1))
        x1 torch.Size([32, 7, 96])
        down_blockçš„ä½œç”¨å°±æ˜¯ 
        è¾“å…¥ï¼š[Batch, Channel, Input_length]
        è¾“å‡ºï¼š[Batch, Channel, Output_length]
        down_block(x1) torch.Size([32, 7, 720])
        e_out.append(torch.Size([32, 7, 720]))
    x1 = self.Maxpools[i](x1)
    	self.Maxpools[0]( x1 torch.Size([32, 7, 96]))
        è¾“å‡º x1.shape = torch.Size([32, 7, 47])
        i=1
        â‘¡ for down_block in self.down_blocks:
            e_out.append(down_block(x1))
            x1.shape = torch.Size([32, 7, 47])
            down_block(x1).shape = torch.Size([32, 7, 359])
            x1 = self.Maxpools[i](x1)
            x1.shape = torch.Size([32, 7, 47])
            x1 = self.Maxpools[i](x1).shape = torch.Size([32, 7, 23])
            i = i+1
            i = 2
            â‘¢ for down_block in self.down_blocks:
                e_out.append(down_block(x1))
                x1.shape = torch.Size([32, 7, 23])
                down_block(x1).shape = torch.Size([32, 7, 179])
                x1 = self.Maxpools[i](x1)
                x1.shape = torch.Size([32, 7, 23])
                x1 = self.Maxpools[i](x1) = torch.Size([32, 7, 11])
                i = 3
                â‘£ for down_block in self.down_blocks:
                    e_out.append(down_block(x1))
                    x1 = torch.Size([32, 7, 11])
                    down_block(x1) = torch.Size([32, 7, 89])
                    x1 = self.Maxpools[i](x1)
                    x1.shape = torch.Size([32, 7, 11])
                    x1 = self.Maxpools[i](x1) = torch.Size([32, 7, 5])
```

å¯¹äºè¿™é‡Œçš„ down_block(x1) æ¥è¯´ï¼Œ

==åˆ†åˆ«æ¥æ”¶== 

- x1 = torch.Size([32, 7, 96]) â†’ down_block(x1) torch.Size([32, 7, 720])
- x1 = torch.Size([32, 7, 47]) â†’ down_block(x1) = torch.Size([32, 7, 359])
- x1 = torch.Size([32, 7, 23]) â†’ down_block(x1) = torch.Size([32, 7, 179])
- x1 = torch.Size([32, 7, 11]) â†’ down_block(x1) = torch.Size([32, 7, 89])

å±‚é—´æ˜¯ `x1 = *self*.Maxpools[i](x1)`   è¾“å…¥åºåˆ—é•¿åº¦å‡åŠ

å±‚å†…æ˜¯ `down_block`ï¼Œå°†è¾“å…¥åºåˆ—æ˜ å°„åˆ°è¾“å‡ºåºåˆ—å¯¹åº”çš„é•¿åº¦

å…³äº `e_out` 

- `e_out[0].shape = [32,7,720]`
- `e_out[1].shape = [32,7,359]`
- `e_out[2].shape = [32,7,179]`
- `e_out[3].shape = [32,7,89]`

```python
ğŸŒˆe_last = e_out[self.stage_num - 1]
ğŸ”µe_out[self.stage_num - 1] = e_out[3] = torch.Size([32, 7, 89])
ğŸªfor i in range(self.stage_num - 1):
    ğŸŒˆe_last = torch.cat((e_out[self.stage_num - i -2], e_last), 
                       dim=2)
    ğŸ”´e_out[self.stage_num - i -2] = e_out[2] = torch.Size([32, 7, 179])
    ğŸ”´e_last = e_out[3] = torch.Size([32, 7, 89])
    ğŸ”´e_last = torch.Size([32, 7, 268])
    ğŸŒˆe_last = self.up_blocks[i](e_last)
    ğŸ”µe_last = torch.Size([32, 7, 268])
    ğŸ”µe_last = self.up_blocks[i](e_last) = torch.Size([32, 7, 179])
    ğŸªfor i in range(self.stage_num - 1):
        ğŸŒˆe_last = torch.cat((e_out[self.stage_num - i -2], e_last), 
                           dim=2)
        ğŸ”µe_out[self.stage_num - i -2] = e_out[1] = torch.Size([32, 7, 359])
        ğŸ”µe_last.shape = e_out[2] = torch.Size([32, 7, 179])
        ğŸ”µe_last = cat = torch.Size([32, 7, 538])
        ğŸŒˆe_last = self.up_blocks[i](e_last)
        ğŸ”´e_last.shape = torch.Size([32, 7, 538])
        ğŸ”´e_last = self.up_blocks[i](e_last).shape = torch.Size([32, 7, 359])
        ğŸªfor i in range(self.stage_num - 1):
             ğŸŒˆe_last = torch.cat((e_out[self.stage_num - i -2],
                                 e_last),
                                dim=2)
            
             ğŸ”´e_out[self.stage_num - i -2] = e_out[0] =  torch.Size([32, 7, 720])
             ğŸ”´e_last = torch.Size([32, 7, 359])
             ğŸ”´e_last = cat = torch.Size([32, 7, 1079])
             ğŸŒˆe_last = self.up_blocks[i](e_last)
            ğŸ”µe_last = torch.Size([32, 7, 1079])
            ğŸ”µe_last = self.up_blocks[i](e_last) = torch.Size([32, 7, 720])  
```

