# Autoformer

## github æºç ä¸»é¡µ

Autoformer (NeurIPS 2021) è‡ªåŠ¨æˆå‹æœº (NeurIPS 2021)

Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting
Autoformerï¼šç”¨äºé•¿æœŸåºåˆ—é¢„æµ‹çš„å…·æœ‰è‡ªç›¸å…³çš„åˆ†è§£å˜å‹å™¨

Time series forecasting is a critical demand for real applications. Enlighted by the classic time series analysis and stochastic process theory, we propose the Autoformer as a general series forecasting model [[paper](https://arxiv.org/abs/2106.13008)]. **Autoformer goes beyond the Transformer family and achieves the series-wise connection for the first time.**
æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯å®é™…åº”ç”¨çš„å…³é”®éœ€æ±‚ã€‚å—ç»å…¸æ—¶é—´åºåˆ—åˆ†æå’Œéšæœºè¿‡ç¨‹ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† Autoformer ä½œä¸ºé€šç”¨åºåˆ—é¢„æµ‹æ¨¡å‹ [[è®ºæ–‡](https://arxiv.org/abs/2106.13008)]ã€‚Autoformer**è¶…è¶Šäº† Transformer å®¶æ—ï¼Œé¦–æ¬¡å®ç°äº†åºåˆ—è¿æ¥ã€‚**

In long-term forecasting, Autoformer achieves SOTA, with a **38% relative improvement** on six benchmarks, covering five practical applications: **energy, traffic, economics, weather and disease**.
åœ¨é•¿æœŸé¢„æµ‹ä¸­ï¼ŒAutoformer å®ç°äº† SOTAï¼Œåœ¨å…­ä¸ªåŸºå‡†ä¸Š**ç›¸å¯¹æå‡äº† 38%** ï¼Œæ¶µç›–äº†**èƒ½æºã€äº¤é€šã€ç»æµã€å¤©æ°”å’Œç–¾ç—…**äº”ä¸ªå®é™…åº”ç”¨ã€‚

**News** (2023.08) Autoformer has been included in [Hugging Face](https://huggingface.co/models?search=autoformer). See [blog](https://huggingface.co/blog/autoformer).
ğŸš©**æ–°é—»**(2023.08) Autoformer å·²åŒ…å«åœ¨[Hugging Face](https://huggingface.co/models?search=autoformer)ä¸­ã€‚æŸ¥çœ‹[åšå®¢](https://huggingface.co/blog/autoformer)ã€‚

ğŸš©**News** (2023.06) The extension version of Autoformer ([Interpretable weather forecasting for worldwide stations with a unified deep model](https://www.nature.com/articles/s42256-023-00667-9)) has been published in Nature Machine Intelligence as the [Cover Article](https://www.nature.com/natmachintell/volumes/5/issues/6).
ğŸš©**æ–°é—»**(2023.06) Autoformer çš„æ‰©å±•ç‰ˆæœ¬ ([ä½¿ç”¨ç»Ÿä¸€æ·±åº¦æ¨¡å‹ä¸ºå…¨çƒç«™ç‚¹æä¾›å¯è§£é‡Šçš„å¤©æ°”é¢„æŠ¥](https://www.nature.com/articles/s42256-023-00667-9)) åœ¨ã€Šè‡ªç„¶æœºå™¨æ™ºèƒ½ã€‹æ‚å¿—ä¸Šä½œä¸º[å°é¢æ–‡ç« ](https://www.nature.com/natmachintell/volumes/5/issues/6)å‘è¡¨ã€‚

ğŸš©**News** (2023.02) Autoformer has been included in our [[Time-Series-Library\]](https://github.com/thuml/Time-Series-Library), which covers long- and short-term forecasting, imputation, anomaly detection, and classification.
ğŸš©**æ–°é—»**(2023.02) Autoformer å·²åŒ…å«åœ¨æˆ‘ä»¬çš„[[æ—¶é—´åºåˆ—åº“\]](https://github.com/thuml/Time-Series-Library)ä¸­ï¼Œå®ƒæ¶µç›–é•¿æœŸå’ŒçŸ­æœŸé¢„æµ‹ã€å½’çº³ã€å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»ã€‚

ğŸš©**News** (2022.02-2022.03) Autoformer has been deployed in [2022 Winter Olympics](https://en.wikipedia.org/wiki/2022_Winter_Olympics) to provide weather forecasting for competition venues, including wind speed and temperature.
ğŸš©**æ–°é—»**ï¼ˆ2022.02-2022.03ï¼‰Autoformer å·²éƒ¨ç½²åœ¨[2022 å¹´å†¬å¥¥ä¼šï¼Œ](https://en.wikipedia.org/wiki/2022_Winter_Olympics)ä¸ºæ¯”èµ›åœºé¦†æä¾›å¤©æ°”é¢„æŠ¥ï¼ŒåŒ…æ‹¬é£é€Ÿã€æ¸©åº¦ç­‰ã€‚

## å‡†å¤‡

### git clone

![image-20250317144505215](images/image-20250317144505215.png)

å…‹éš†è¿œç¨‹ä»“åº“çš„æ–¹æ³•ï¼š

ï¼ˆ1ï¼‰HTTPSï¼Œåœ¨æŠŠæœ¬åœ°ä»“åº“çš„ä»£ç  push åˆ°è¿œç¨‹ä»“åº“çš„æ—¶å€™ï¼Œéœ€è¦éªŒè¯ç”¨æˆ·åå’Œå¯†ç 

ï¼ˆ2ï¼‰SSHï¼Œgit å¼€å¤´çš„æ˜¯ SSH åè®®ï¼Œè¿™ç§æ–¹å¼åœ¨æ¨é€çš„æ—¶å€™ï¼Œä¸éœ€è¦éªŒè¯ç”¨æˆ·åå’Œå¯†ç ï¼Œä½†æ˜¯éœ€è¦åœ¨ github ä¸Šæ·»åŠ SSHå…¬é’¥çš„é…ç½®ï¼ˆæ¨èï¼‰

ï¼ˆ3ï¼‰zip download

æˆ‘è¿™é‡Œä½¿ç”¨äº† SSH é…ç½®ï¼š

![image-20250317144903028](images/image-20250317144903028.png)

æœåŠ¡å™¨ç›´æ¥ git clone æ˜¯å¾ˆæ…¢ã€‚æ‰€ä»¥æœ¬åœ° git cloneï¼Œç„¶åå†ä¸Šä¼ æœåŠ¡å™¨ã€‚

![image-20250317145242243](images/image-20250317145242243.png)

æœ¬åœ°ä¸‹è½½å¥½ä»¥åï¼Œä½¿ç”¨ FileZillaä¸Šä¼ åˆ°è¿œç¨‹æœåŠ¡å™¨

![image-20250317145427044](images/image-20250317145427044.png)

downåˆ°æœ¬åœ°ä»¥åï¼Œåˆ é™¤ .gitæ–‡ä»¶ï¼Œå–æ¶ˆè¿æ¥ç€è¿œç¨‹ä»“åº“

![image-20250317145705700](images/image-20250317145705700.png)

![image-20250317145752968](images/image-20250317145752968.png)

### readme

ä¸‹è½½æ•°æ®é›†

è®¾ç½®æ•°æ®é›†è·¯å¾„

![image-20250317150739551](images/image-20250317150739551.png)

### è°ƒè¯•é…ç½®

æ–°å»ºé…ç½®æ–‡ä»¶

![image-20250317150852423](images/image-20250317150852423.png)

ä¿®æ”¹é…ç½®æ–‡ä»¶

![image-20250317151048416](images/image-20250317151048416.png) 

ä¿®æ”¹é…ç½®æ–‡ä»¶

```
        {
            "name": "Autoformer",
            "type": "python",
            "request": "attach",
            "connect": {
                "host": "localhost",
                "port": 5997
            }
        },
```

ä¿®æ”¹ sh æ–‡ä»¶

```
python -m debugpy --listen 5997 --wait-for-client run.py \
```

### æ–°å»º python è™šæ‹Ÿç¯å¢ƒ

æœ¬å®éªŒæ‰€éœ€è¦çš„å®éªŒç¯å¢ƒ

> Install Python 3.6, PyTorch 1.9.0.

å‚è€ƒå‘½ä»¤

```
conda create -n dave python==3.8
conda activate dave
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia
conda install numpy
conda install scikit-image
conda install scikit-learn
conda install tqdm
conda install pycocotools
```

æ¿€æ´»ã€é€€å‡ºï¼š

```
# To activate this environment, use               
#     $ conda activate Autoformer
#
# To deactivate an active environment, use
#
#     $ conda deactivate
```

ç”¨ requirements.txt å®‰è£…éœ€è¦çš„åº“

```
conda create -n SegRNN python=3.8
conda activate SegRNN
pip install -r requirements.txt
```

å¯åŠ¨ sh æ–‡ä»¶ï¼š

```
sh run_main.sh
```

**é€‚ç”¨äºæœ¬å®éªŒçš„æ‰€æœ‰å‘½ä»¤ :**

```
conda create -n Autoformer python=3.6
conda activate Autoformer
```

[pytorch å®˜ç½‘](https://pytorch.org/)æŸ¥çœ‹æ‰€éœ€å‘½ä»¤

![image-20250317154037815](images/image-20250317154037815.png) 

![image-20250317153952283](images/image-20250317153952283.png)

```
conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=10.2 -c pytorch
```

### requirements

```
pip install -r requirements.txt
```

æˆ–è€…ï¼š

```
conda create -n Autoformer python=3.6
conda activate Autoformer
conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=10.2 -c pytorch
conda install pandas
conda install scikit-learn
conda install debugpy
conda install matplotlib
conda install reformer_pytorch
```

é…ç½®å¥½ä»¥åï¼ŒæˆåŠŸè¿›å…¥è°ƒè¯•ï¼š

![image-20250317155629524](images/image-20250317155629524.png)  

## å¼€å§‹è°ƒè¯•

ä»£ç ç›¸ä¼¼åº¦æé«˜ã€‚

**Autoformer initï¼š36ï¼ˆ18ï¼‰-ã€‹24**

![image-20250317160059801](images/image-20250317160059801.png)

setting:

```
ili_36_24_Autoformer_custom_ftM_sl36_ll18_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0
```

model_id  36 é¢„æµ‹ 24 æ­¥é•¿ï¼ˆlabel=18ï¼‰ã€AutoFormer æ¨¡å‹ï¼Œè‡ªå®šä¹‰æ•°æ®é›†ï¼Œé¢„æµ‹å¤šå˜é‡ï¼Œè¾“å…¥åºåˆ— 36ï¼Œæ ‡ç­¾åºåˆ— 18ï¼Œé¢„æµ‹åºåˆ— 24ï¼ŒåµŒå…¥ç»´åº¦ 512ï¼Œæ³¨æ„åŠ›å¤´æ•° 8ï¼Œ2å±‚ç¼–ç å±‚ï¼Œ1 å±‚è§£ç å±‚ï¼Œ

```
df2048_fc3_ebtimeF_dtTrue_Exp_0
         		args.d_ff,
                args.factor,
                args.embed,
                args.distil,
                args.des, ii)
```

**Autoformer model**

```
Model(
  (decomp): series_decomp(
    (moving_avg): moving_avg(
      (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
    )
  )
  (enc_embedding): DataEmbedding_wo_pos(
    (value_embedding): TokenEmbedding(
      (tokenConv): Conv1d(7, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    )
    (position_embedding): PositionalEmbedding()
    (temporal_embedding): TimeFeatureEmbedding(
      (embed): Linear(in_features=4, out_features=512, bias=False)
    )
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (dec_embedding): DataEmbedding_wo_pos(
    (value_embedding): TokenEmbedding(
      (tokenConv): Conv1d(7, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    )
    (position_embedding): PositionalEmbedding()
    (temporal_embedding): TimeFeatureEmbedding(
      (embed): Linear(in_features=4, out_features=512, bias=False)
    )
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0): EncoderLayer(
        (attention): AutoCorrelationLayer(
          (inner_correlation): AutoCorrelation(
            (dropout): Dropout(p=0.05, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)
        (decomp1): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (decomp2): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (1): EncoderLayer(
        (attention): AutoCorrelationLayer(
          (inner_correlation): AutoCorrelation(
            (dropout): Dropout(p=0.05, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)
        (decomp1): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (decomp2): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (dropout): Dropout(p=0.05, inplace=False)
      )
    )
    (norm): my_Layernorm(
      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attention): AutoCorrelationLayer(
          (inner_correlation): AutoCorrelation(
            (dropout): Dropout(p=0.05, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (cross_attention): AutoCorrelationLayer(
          (inner_correlation): AutoCorrelation(
            (dropout): Dropout(p=0.05, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)
        (decomp1): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (decomp2): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (decomp3): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (dropout): Dropout(p=0.05, inplace=False)
        (projection): Conv1d(512, 7, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
      )
    )
    (norm): my_Layernorm(
      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (projection): Linear(in_features=512, out_features=7, bias=True)
  )
)
```



æ•°æ®é›†çš„åŠ è½½æ˜¯å®Œå…¨ä¸€æ ·çš„ã€‚