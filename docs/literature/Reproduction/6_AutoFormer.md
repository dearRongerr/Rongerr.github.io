# Autoformer

## github æºç ä¸»é¡µ

Autoformer (NeurIPS 2021) è‡ªåŠ¨æˆåž‹æœº (NeurIPS 2021)

Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting
Autoformerï¼šç”¨äºŽé•¿æœŸåºåˆ—é¢„æµ‹çš„å…·æœ‰è‡ªç›¸å…³çš„åˆ†è§£å˜åŽ‹å™¨

Time series forecasting is a critical demand for real applications. Enlighted by the classic time series analysis and stochastic process theory, we propose the Autoformer as a general series forecasting model [[paper](https://arxiv.org/abs/2106.13008)]. **Autoformer goes beyond the Transformer family and achieves the series-wise connection for the first time.**
æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯å®žé™…åº”ç”¨çš„å…³é”®éœ€æ±‚ã€‚å—ç»å…¸æ—¶é—´åºåˆ—åˆ†æžå’Œéšæœºè¿‡ç¨‹ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† Autoformer ä½œä¸ºé€šç”¨åºåˆ—é¢„æµ‹æ¨¡åž‹ [[è®ºæ–‡](https://arxiv.org/abs/2106.13008)]ã€‚Autoformer**è¶…è¶Šäº† Transformer å®¶æ—ï¼Œé¦–æ¬¡å®žçŽ°äº†åºåˆ—è¿žæŽ¥ã€‚**

In long-term forecasting, Autoformer achieves SOTA, with a **38% relative improvement** on six benchmarks, covering five practical applications: **energy, traffic, economics, weather and disease**.
åœ¨é•¿æœŸé¢„æµ‹ä¸­ï¼ŒAutoformer å®žçŽ°äº† SOTAï¼Œåœ¨å…­ä¸ªåŸºå‡†ä¸Š**ç›¸å¯¹æå‡äº† 38%** ï¼Œæ¶µç›–äº†**èƒ½æºã€äº¤é€šã€ç»æµŽã€å¤©æ°”å’Œç–¾ç—…**äº”ä¸ªå®žé™…åº”ç”¨ã€‚

**News** (2023.08) Autoformer has been included in [Hugging Face](https://huggingface.co/models?search=autoformer). See [blog](https://huggingface.co/blog/autoformer).
ðŸš©**æ–°é—»**(2023.08) Autoformer å·²åŒ…å«åœ¨[Hugging Face](https://huggingface.co/models?search=autoformer)ä¸­ã€‚æŸ¥çœ‹[åšå®¢](https://huggingface.co/blog/autoformer)ã€‚

ðŸš©**News** (2023.06) The extension version of Autoformer ([Interpretable weather forecasting for worldwide stations with a unified deep model](https://www.nature.com/articles/s42256-023-00667-9)) has been published in Nature Machine Intelligence as the [Cover Article](https://www.nature.com/natmachintell/volumes/5/issues/6).
ðŸš©**æ–°é—»**(2023.06) Autoformer çš„æ‰©å±•ç‰ˆæœ¬ ([ä½¿ç”¨ç»Ÿä¸€æ·±åº¦æ¨¡åž‹ä¸ºå…¨çƒç«™ç‚¹æä¾›å¯è§£é‡Šçš„å¤©æ°”é¢„æŠ¥](https://www.nature.com/articles/s42256-023-00667-9)) åœ¨ã€Šè‡ªç„¶æœºå™¨æ™ºèƒ½ã€‹æ‚å¿—ä¸Šä½œä¸º[å°é¢æ–‡ç« ](https://www.nature.com/natmachintell/volumes/5/issues/6)å‘è¡¨ã€‚

ðŸš©**News** (2023.02) Autoformer has been included in our [[Time-Series-Library\]](https://github.com/thuml/Time-Series-Library), which covers long- and short-term forecasting, imputation, anomaly detection, and classification.
ðŸš©**æ–°é—»**(2023.02) Autoformer å·²åŒ…å«åœ¨æˆ‘ä»¬çš„[[æ—¶é—´åºåˆ—åº“\]](https://github.com/thuml/Time-Series-Library)ä¸­ï¼Œå®ƒæ¶µç›–é•¿æœŸå’ŒçŸ­æœŸé¢„æµ‹ã€å½’çº³ã€å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»ã€‚

ðŸš©**News** (2022.02-2022.03) Autoformer has been deployed in [2022 Winter Olympics](https://en.wikipedia.org/wiki/2022_Winter_Olympics) to provide weather forecasting for competition venues, including wind speed and temperature.
ðŸš©**æ–°é—»**ï¼ˆ2022.02-2022.03ï¼‰Autoformer å·²éƒ¨ç½²åœ¨[2022 å¹´å†¬å¥¥ä¼šï¼Œ](https://en.wikipedia.org/wiki/2022_Winter_Olympics)ä¸ºæ¯”èµ›åœºé¦†æä¾›å¤©æ°”é¢„æŠ¥ï¼ŒåŒ…æ‹¬é£Žé€Ÿã€æ¸©åº¦ç­‰ã€‚

## å‡†å¤‡

### git clone

![image-20250317144505215](images/image-20250317144505215.png)

å…‹éš†è¿œç¨‹ä»“åº“çš„æ–¹æ³•ï¼š

ï¼ˆ1ï¼‰HTTPSï¼Œåœ¨æŠŠæœ¬åœ°ä»“åº“çš„ä»£ç  push åˆ°è¿œç¨‹ä»“åº“çš„æ—¶å€™ï¼Œéœ€è¦éªŒè¯ç”¨æˆ·åå’Œå¯†ç 

ï¼ˆ2ï¼‰SSHï¼Œgit å¼€å¤´çš„æ˜¯ SSH åè®®ï¼Œè¿™ç§æ–¹å¼åœ¨æŽ¨é€çš„æ—¶å€™ï¼Œä¸éœ€è¦éªŒè¯ç”¨æˆ·åå’Œå¯†ç ï¼Œä½†æ˜¯éœ€è¦åœ¨ github ä¸Šæ·»åŠ SSHå…¬é’¥çš„é…ç½®ï¼ˆæŽ¨èï¼‰

ï¼ˆ3ï¼‰zip download

æˆ‘è¿™é‡Œä½¿ç”¨äº† SSH é…ç½®ï¼š

![image-20250317144903028](images/image-20250317144903028.png)

æœåŠ¡å™¨ç›´æŽ¥ git clone æ˜¯å¾ˆæ…¢ã€‚æ‰€ä»¥æœ¬åœ° git cloneï¼Œç„¶åŽå†ä¸Šä¼ æœåŠ¡å™¨ã€‚

![image-20250317145242243](images/image-20250317145242243.png)

æœ¬åœ°ä¸‹è½½å¥½ä»¥åŽï¼Œä½¿ç”¨ FileZillaä¸Šä¼ åˆ°è¿œç¨‹æœåŠ¡å™¨

![image-20250317145427044](images/image-20250317145427044.png)

downåˆ°æœ¬åœ°ä»¥åŽï¼Œåˆ é™¤ .gitæ–‡ä»¶ï¼Œå–æ¶ˆè¿žæŽ¥ç€è¿œç¨‹ä»“åº“

![image-20250317145705700](images/image-20250317145705700.png)

![image-20250317145752968](images/image-20250317145752968.png)

### readme

ä¸‹è½½æ•°æ®é›†

è®¾ç½®æ•°æ®é›†è·¯å¾„

![image-20250317150739551](images/image-20250317150739551.png)

### è°ƒè¯•é…ç½®

æ–°å»ºé…ç½®æ–‡ä»¶

![image-20250317150852423](images/image-20250317150852423.png)

ä¿®æ”¹é…ç½®æ–‡ä»¶

![image-20250317151048416](images/image-20250317151048416.png) 

ä¿®æ”¹é…ç½®æ–‡ä»¶

```
        {
            "name": "Autoformer",
            "type": "python",
            "request": "attach",
            "connect": {
                "host": "localhost",
                "port": 5997
            }
        },
```

ä¿®æ”¹ sh æ–‡ä»¶

```
python -m debugpy --listen 5997 --wait-for-client run.py \
```

### æ–°å»º python è™šæ‹ŸçŽ¯å¢ƒ

æœ¬å®žéªŒæ‰€éœ€è¦çš„å®žéªŒçŽ¯å¢ƒ

> Install Python 3.6, PyTorch 1.9.0.

å‚è€ƒå‘½ä»¤

```
conda create -n dave python==3.8
conda activate dave
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia
conda install numpy
conda install scikit-image
conda install scikit-learn
conda install tqdm
conda install pycocotools
```

æ¿€æ´»ã€é€€å‡ºï¼š

```
# To activate this environment, use               
#     $ conda activate Autoformer
#
# To deactivate an active environment, use
#
#     $ conda deactivate
```

ç”¨ requirements.txt å®‰è£…éœ€è¦çš„åº“

```
conda create -n SegRNN python=3.8
conda activate SegRNN
pip install -r requirements.txt
```

å¯åŠ¨ sh æ–‡ä»¶ï¼š

```
sh run_main.sh
```

**é€‚ç”¨äºŽæœ¬å®žéªŒçš„æ‰€æœ‰å‘½ä»¤ :**

```
conda create -n Autoformer python=3.6
conda activate Autoformer
```

[pytorch å®˜ç½‘](https://pytorch.org/)æŸ¥çœ‹æ‰€éœ€å‘½ä»¤

![image-20250317154037815](images/image-20250317154037815.png) 

![image-20250317153952283](images/image-20250317153952283.png)

```
conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=10.2 -c pytorch
```

### requirements

```
pip install -r requirements.txt
```

æˆ–è€…ï¼š

```
conda create -n Autoformer python=3.6
conda activate Autoformer
conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=10.2 -c pytorch
conda install pandas
conda install scikit-learn
conda install debugpy
conda install matplotlib
conda install reformer_pytorch
```

é…ç½®å¥½ä»¥åŽï¼ŒæˆåŠŸè¿›å…¥è°ƒè¯•ï¼š

![image-20250317155629524](images/image-20250317155629524.png)  

## å¼€å§‹è°ƒè¯•

ä»£ç ç›¸ä¼¼åº¦æžé«˜ã€‚

**Autoformer initï¼š36ï¼ˆ18ï¼‰-ã€‹24**

![image-20250317160059801](images/image-20250317160059801.png)

setting:

```
ili_36_24_Autoformer_custom_ftM_sl36_ll18_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0
```

model_id  36 é¢„æµ‹ 24 æ­¥é•¿ï¼ˆlabel=18ï¼‰ã€AutoFormer æ¨¡åž‹ï¼Œè‡ªå®šä¹‰æ•°æ®é›†ï¼Œé¢„æµ‹å¤šå˜é‡ï¼Œè¾“å…¥åºåˆ— 36ï¼Œæ ‡ç­¾åºåˆ— 18ï¼Œé¢„æµ‹åºåˆ— 24ï¼ŒåµŒå…¥ç»´åº¦ 512ï¼Œæ³¨æ„åŠ›å¤´æ•° 8ï¼Œ2å±‚ç¼–ç å±‚ï¼Œ1 å±‚è§£ç å±‚ï¼Œ

```
df2048_fc3_ebtimeF_dtTrue_Exp_0
         		args.d_ff,
                args.factor,
                args.embed,
                args.distil,
                args.des, ii)
```

**Autoformer model**

```python
Model(
  (decomp): series_decomp(
    (moving_avg): moving_avg(
      (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
    )
  )
  (enc_embedding): DataEmbedding_wo_pos(
    (value_embedding): TokenEmbedding(
      (tokenConv): Conv1d(7, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    )
    (position_embedding): PositionalEmbedding()
    (temporal_embedding): TimeFeatureEmbedding(
      (embed): Linear(in_features=4, out_features=512, bias=False)
    )
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (dec_embedding): DataEmbedding_wo_pos(
    (value_embedding): TokenEmbedding(
      (tokenConv): Conv1d(7, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    )
    (position_embedding): PositionalEmbedding()
    (temporal_embedding): TimeFeatureEmbedding(
      (embed): Linear(in_features=4, out_features=512, bias=False)
    )
    (dropout): Dropout(p=0.05, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0): EncoderLayer(
        (attention): AutoCorrelationLayer(
          (inner_correlation): AutoCorrelation(
            (dropout): Dropout(p=0.05, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)
        (decomp1): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (decomp2): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (1): EncoderLayer(
        (attention): AutoCorrelationLayer(
          (inner_correlation): AutoCorrelation(
            (dropout): Dropout(p=0.05, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)
        (decomp1): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (decomp2): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (dropout): Dropout(p=0.05, inplace=False)
      )
    )
    (norm): my_Layernorm(
      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attention): AutoCorrelationLayer(
          (inner_correlation): AutoCorrelation(
            (dropout): Dropout(p=0.05, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (cross_attention): AutoCorrelationLayer(
          (inner_correlation): AutoCorrelation(
            (dropout): Dropout(p=0.05, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)
        (decomp1): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (decomp2): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (decomp3): series_decomp(
          (moving_avg): moving_avg(
            (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))
          )
        )
        (dropout): Dropout(p=0.05, inplace=False)
        (projection): Conv1d(512, 7, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
      )
    )
    (norm): my_Layernorm(
      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (projection): Linear(in_features=512, out_features=7, bias=True)
  )
)
```



æ•°æ®é›†çš„åŠ è½½æ˜¯å®Œå…¨ä¸€æ ·çš„ã€‚

## forward è§£è¯»

1. **è¾“å…¥å¤„ç†**ï¼š
   - åŽ†å²æ•°æ® x_enc [B, L, D]
   - é¢„æµ‹å’Œæ ‡ç­¾æ•°æ® x_dec [B, L+P, D]
2. **æ—¶é—´åºåˆ—åˆ†è§£**ï¼š
   - å°†åŽ†å²åºåˆ—åˆ†è§£ä¸ºå­£èŠ‚æ€§å’Œè¶‹åŠ¿ä¸¤ä¸ªæˆåˆ†
3. **åˆå§‹å€¼å‡†å¤‡**ï¼š
   - è¶‹åŠ¿åˆå§‹å€¼ï¼šåŽ†å²åºåˆ—å‡å€¼
   - å­£èŠ‚æ€§åˆå§‹å€¼ï¼šå…¨é›¶å¼ é‡
4. **è§£ç å™¨è¾“å…¥æž„å»º**ï¼š
   - è¶‹åŠ¿è¾“å…¥ï¼šåŽ†å²è¶‹åŠ¿æœ«å°¾ + è¶‹åŠ¿åˆå§‹å€¼
   - å­£èŠ‚æ€§è¾“å…¥ï¼šåŽ†å²å­£èŠ‚æ€§æœ«å°¾ + å­£èŠ‚æ€§åˆå§‹å€¼(é›¶)
5. **ç¼–è§£ç å™¨å¤„ç†**ï¼š
   - ç¼–ç å™¨å¤„ç†åŽ†å²æ•°æ®
   - è§£ç å™¨åˆ©ç”¨ç¼–ç å™¨è¾“å‡ºå’Œç»„è£…çš„åˆå§‹è¾“å…¥ç”Ÿæˆé¢„æµ‹
6. **æœ€ç»ˆè¾“å‡º**ï¼š
   - è¶‹åŠ¿å’Œå­£èŠ‚æ€§é¢„æµ‹ç›¸åŠ 
   - æå–æœ«å°¾ pred_len é•¿åº¦ä½œä¸ºæœ€ç»ˆé¢„æµ‹ç»“æžœ

è¿™ç§è®¾è®¡ä½“çŽ°äº† Autoformer çš„æ ¸å¿ƒæ€æƒ³ï¼šå°†æ—¶é—´åºåˆ—åˆ†è§£ä¸ºä¸åŒé¢‘çŽ‡æˆåˆ†å¹¶åˆ†åˆ«å»ºæ¨¡ï¼Œå†ç»„åˆç”Ÿæˆæœ€ç»ˆé¢„æµ‹ã€‚

### è¶‹åŠ¿é¡¹ å’Œ å­£èŠ‚é¡¹

```python
seasonal_init, trend_init = self.decomp(x_enc)
```

â–¶ï¸

```python
self.decomp = series_decomp(kernel_size)
```

â–¶ï¸

```python
class series_decomp(nn.Module):
```

ðŸŸ¢ ç±»çš„å®šä¹‰

```python
class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):

        # è®¡ç®—ç§»åŠ¨å¹³å‡ï¼Œæå–åºåˆ—è¶‹åŠ¿åˆ†é‡
        # x å½¢çŠ¶[B, L, D] -> moving_meanå½¢çŠ¶[B, L, D]
        #  moving_avgå†…éƒ¨ä¼šè¿›è¡Œå¡«å……ï¼Œä¿è¯è¾“å‡ºå½¢çŠ¶ä¸Žè¾“å…¥ç›¸åŒ
        moving_mean = self.moving_avg(x)

        # é€šè¿‡åŽŸå§‹åºåˆ—å‡åŽ»è¶‹åŠ¿åˆ†é‡ï¼Œå¾—åˆ°æ®‹å·®(å­£èŠ‚æ€§åˆ†é‡)ï¼Œé€å…ƒç´ å‡æ³•æ“ä½œ
        # xå½¢çŠ¶[B, L, D] - moving_meanå½¢çŠ¶[B, L, D] -> reså½¢çŠ¶[B, L, D]
        res = x - moving_mean

        # è¿”å›žå­£èŠ‚æ€§åˆ†é‡å’Œè¶‹åŠ¿åˆ†é‡ï¼Œå‡ä¿æŒåŽŸå§‹å½¢çŠ¶[B, L, D]
        # ç¬¬ä¸€ä¸ªè¿”å›žå€¼resæ˜¯å­£èŠ‚æ€§åˆ†é‡ï¼Œç¬¬äºŒä¸ªè¿”å›žå€¼moving_meanæ˜¯è¶‹åŠ¿åˆ†é‡
        return res, moving_mean
```

ç±»å†… è°ƒç”¨ `moving_avg`

![image-20250317202431440](images/image-20250317202431440.png)

â–¶ï¸

```python
class moving_avg(nn.Module):
```

ðŸŸ¢ `moving_avg` å®šä¹‰

```python
class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series

        # æå–ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥å¹¶é‡å¤ï¼Œç”¨äºŽå‰ç«¯å¡«å……
        #  [B, L, D] -> [B, 1, D] -> [B, (kernel_size-1)//2, D]
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1) 

        # æå–æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥å¹¶é‡å¤ï¼Œç”¨äºŽåŽç«¯å¡«å……
        # [B, L, D] -> [B, 1, D] -> [B, (kernel_size-1)//2, D]
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)

        # è¿žæŽ¥å¡«å……éƒ¨åˆ†ä¸ŽåŽŸåºåˆ—
        # [B, (k-1)//2, D] + [B, L, D] + [B, (k-1)//2, D] -> [B, L+(k-1), D]
        x = torch.cat([front, x, end], dim=1)

        # è½¬ç½®å¹¶åº”ç”¨ä¸€ç»´å¹³å‡æ± åŒ–
        # [B, L+(k-1), D] -> [B, D, L+(k-1)] -> [B, D, L]
        # æ± åŒ–çª—å£å¤§å°ä¸ºkernel_sizeï¼Œæ­¥é•¿ä¸º1ï¼Œè¾“å‡ºé•¿åº¦ä¸º(L+(k-1)-k+1)=L ï¼ˆlength + 2P - K + 1ï¼‰
        x = self.avg(x.permute(0, 2, 1))

        # è½¬ç½®å›žåŽŸå§‹ç»´åº¦é¡ºåº [B, D, L] -> [B, L, D]
        x = x.permute(0, 2, 1)
        return x
```

æ€»ç»“ï¼š3 æ¬¡è°ƒç”¨ï¼š

```python
seasonal_init, trend_init = self.decomp(x_enc)

self.decomp = series_decomp(kernel_size)

class series_decomp(nn.Module):
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)
        
    def forward(self, x):
        moving_mean = self.moving_avg(x)

class moving_avg(nn.Module):
     def forward(self, x):
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1) 
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x	
```



#### Autoformeråºåˆ—åˆ†è§£æµç¨‹å›¾

```
                    è¾“å…¥: x_enc [B, L, D]
                          |
                          v
            +---------------------------+
            | Model.forward()           |
            | è°ƒç”¨: self.decomp(x_enc)  |
            +---------------------------+
                          |
                          v
            +---------------------------+
            | series_decomp(kernel_size)|
            | self.decompå®žä¾‹           |
            +---------------------------+
                          |
                          v
            +---------------------------+
            | series_decomp.forward(x)  |
            | 1. è°ƒç”¨ç§»åŠ¨å¹³å‡è®¡ç®—è¶‹åŠ¿   |
            | 2. åŽŸåºåˆ—å‡åŽ»è¶‹åŠ¿å¾—åˆ°å­£èŠ‚æ€§|
            +---------------------------+
                          |
                  +-------+-------+
                  |               |
                  v               v
    +---------------------------+  +---------------------------+
    | moving_avg.forward(x)     |  | å­£èŠ‚æ€§è®¡ç®—                |
    | æ­¥éª¤:                     |  | res = x - moving_mean     |
    | 1.å‰åŽå¡«å……åºåˆ—           |  |                           |
    | 2.åº”ç”¨å¹³å‡æ± åŒ–           |  |                           |
    | 3.è¿”å›žè¶‹åŠ¿åˆ†é‡           |  |                           |
    +---------------------------+  +---------------------------+
                  |               |
                  v               v
             è¶‹åŠ¿åˆ†é‡        å­£èŠ‚æ€§åˆ†é‡
          trend_init [B,L,D]  seasonal_init [B,L,D]
                  |               |
                  +       +       +
                          |
                          v
                è¿”å›žåˆ°Model.forward()
                è¿›è¡ŒåŽç»­å¤„ç†
```

1. **Model.forward()** è°ƒç”¨ self.decomp(x_enc)è¿›è¡Œåºåˆ—åˆ†è§£

2. **series_decomp.forward(x)**

   åŒ…å«ä¸¤ä¸ªä¸»è¦æ­¥éª¤:

   - è°ƒç”¨ self.moving_avg(x)è®¡ç®—ç§»åŠ¨å¹³å‡ï¼Œå¾—åˆ°è¶‹åŠ¿åˆ†é‡
   - è®¡ç®—åŽŸåºåˆ—ä¸Žè¶‹åŠ¿åˆ†é‡çš„å·®å€¼ï¼Œå¾—åˆ°å­£èŠ‚æ€§åˆ†é‡

3. **moving_avg.forward(x)**

   æ‰§è¡Œç§»åŠ¨å¹³å‡è®¡ç®—:

   - é€šè¿‡é‡å¤é¦–å°¾å…ƒç´ è¿›è¡Œåºåˆ—å¡«å……
   - åº”ç”¨ä¸€ç»´å¹³å‡æ± åŒ–æ“ä½œ
   - è¿”å›žå¹³æ»‘åŽçš„è¶‹åŠ¿åˆ†é‡

è¿™ä¸ªåˆ†è§£è¿‡ç¨‹å°†åŽŸå§‹åºåˆ— x_enc åˆ†è§£ä¸ºä¸¤ä¸ªç›¸åŒå½¢çŠ¶ [B,L,D] çš„å¼ é‡ï¼šè¶‹åŠ¿æˆåˆ†å’Œå­£èŠ‚æ€§æˆåˆ†

### ç¼–ç å™¨

ç›®çš„ï¼šç»“åˆæ—¶é—´ç‰¹å¾ï¼Œå°† æ•°æ®ç‰¹å¾åµŒå…¥åˆ°æŒ‡å®šç»´åº¦

```python
enc_out = self.enc_embedding(x_enc, x_mark_enc)
```



```python
self.enc_embedding = DataEmbedding_wo_pos(configs.enc_in, configs.d_model, configs.embed, configs.freq,configs.dropout)
```



![image-20250317204752276](images/image-20250317204752276.png)



![image-20250317205258626](images/image-20250317205258626.png)



**æµç¨‹å›¾**

```python
è¾“å…¥:
x_enc [B, L, D]        x_mark_enc [B, L, time_features]
    |                        |
    v                        v
+-----------------------------------------------+
|           Model.forward()è°ƒç”¨                  |
|      self.enc_embedding(x_enc, x_mark_enc)    |
+-----------------------------------------------+
            |                |
            v                v
+------------------------+  +---------------------------+
| TokenEmbedding (å€¼åµŒå…¥) |  | TemporalEmbedding (æ—¶é—´åµŒå…¥)|
+------------------------+  +---------------------------+
| è¾“å…¥: x [B, L, D]      |  | è¾“å…¥: x_mark [B, L, time_f]|
|                        |  |                           |
| æ“ä½œ:                  |  | æ“ä½œ:                     |
| 1.è½¬ç½®: [B, D, L]      |  | 1.è½¬æ¢ä¸ºlongç±»åž‹          |
| 2.1Då·ç§¯: D -> d_model |  | 2.æå–æ—¶é—´ç‰¹å¾:           |
| 3.è½¬ç½®å›ž: [B, L, d_model]|  |   - month_x (x[:,:,0])   |
|                        |  |   - day_x (x[:,:,1])      |
| è¾“å‡º: [B, L, d_model]  |  |   - weekday_x (x[:,:,2])  |
|                        |  |   - hour_x (x[:,:,3])     |
+------------------------+  |   - minute_x (å¯é€‰)       |
            |               |                           |
            |               | 3.æŸ¥è¡¨èŽ·å–å„æ—¶é—´ç‰¹å¾çš„åµŒå…¥  |
            |               | 4.å°†æ‰€æœ‰æ—¶é—´åµŒå…¥ç›¸åŠ        |
            |               |                           |
            |               | è¾“å‡º: [B, L, d_model]     |
            |               +---------------------------+
            |                        |
            +------------+------------+
                         v
            +---------------------------+
            | ç›¸åŠ å¹¶åº”ç”¨Dropout         |
            | value_emb + temporal_emb |
            +---------------------------+
                         |
                         v
                  è¾“å‡º: enc_out
                 [B, L, d_model]
```



1. **å€¼åµŒå…¥ (TokenEmbedding)**:
   - é€šè¿‡å·ç§¯æ“ä½œå°†åŽŸå§‹ç‰¹å¾ [B, L, D] æ˜ å°„åˆ°æ›´é«˜ç»´åº¦è¡¨ç¤º [B, L, d_model]
   - ä½¿ç”¨å¾ªçŽ¯å¡«å……çš„1Då·ç§¯æ•èŽ·å±€éƒ¨ç‰¹å¾æ¨¡å¼
2. **æ—¶é—´åµŒå…¥ (TemporalEmbedding)**:
   - å°†æ—¶é—´æ ‡è®° [B, L, time_features] è½¬æ¢ä¸º [B, L, d_model] çš„åµŒå…¥å‘é‡
   - åˆ†åˆ«ä¸ºæœˆã€æ—¥ã€æ˜ŸæœŸã€å°æ—¶ç­‰æ—¶é—´ç‰¹å¾æŸ¥è¡¨èŽ·å–åµŒå…¥ï¼Œç„¶åŽç›¸åŠ 
   - æ—¶é—´åµŒå…¥å¸®åŠ©æ¨¡åž‹è¯†åˆ«æ—¶é—´æ¨¡å¼(å­£èŠ‚æ€§ã€æ¯æ—¥/æ¯å‘¨å‘¨æœŸç­‰)
3. **ç»„åˆåµŒå…¥**:
   - å°†å€¼åµŒå…¥å’Œæ—¶é—´åµŒå…¥ç›¸åŠ ï¼Œå½¢æˆæœ€ç»ˆç¼–ç å™¨è¾“å…¥ [B, L, d_model]
   - æ³¨æ„æ­¤ç‰ˆæœ¬ä¸åŒ…å«ä½ç½®åµŒå…¥(DataEmbedding_wo_pos)

è¿™ç§å¤šé‡åµŒå…¥æ–¹å¼ä½¿æ¨¡åž‹èƒ½åŒæ—¶åˆ©ç”¨æ—¶é—´åºåˆ—çš„å€¼ä¿¡æ¯å’Œæ—¶é—´ç‰¹å¾ä¿¡æ¯ï¼Œä¸ºåŽç»­çš„æ³¨æ„åŠ›æœºåˆ¶å’Œæ—¶é—´åºåˆ—å»ºæ¨¡æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€‚

## æ¨¡åž‹å®šä¹‰



### åµŒå…¥éƒ¨åˆ†

```mermaid
classDiagram
    class DataEmbedding_wo_pos {
        +TokenEmbedding value_embedding
        +PositionalEmbedding position_embedding
        +TemporalEmbedding temporal_embedding
        +Dropout dropout
        +forward(x, x_mark)
    }
    
    class TokenEmbedding {
        +Conv1d tokenConv
        +forward(x)
    }
    
    class PositionalEmbedding {
        +Tensor pe
        +forward(x)
    }
    
    class TemporalEmbedding {
        +Embedding minute_embed
        +Embedding hour_embed
        +Embedding weekday_embed
        +Embedding day_embed
        +Embedding month_embed
        +forward(x)
    }
    
    class TimeFeatureEmbedding {
        +Linear embed
        +forward(x)
    }
    
    DataEmbedding_wo_pos --> TokenEmbedding
    DataEmbedding_wo_pos --> PositionalEmbedding
    DataEmbedding_wo_pos --> TemporalEmbedding
    TemporalEmbedding --> TimeFeatureEmbedding
```



### ç¼–ç å™¨ è§£ç å™¨éƒ¨åˆ†



```mermaid
classDiagram
    class Model {
        +DataEmbedding_wo_pos enc_embedding
        +DataEmbedding_wo_pos dec_embedding
        +Encoder encoder
        +Decoder decoder
        +series_decomp decomp
        +forward(x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)
    }
    
    class Encoder {
        +List~EncoderLayer~ layers
        +my_Layernorm norm_layer
        +forward(x, attn_mask)
    }
    
    class EncoderLayer {
        +AutoCorrelationLayer attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +Dropout dropout
        +activation
        +forward(x, attn_mask)
    }
    
    class AutoCorrelationLayer {
        +AutoCorrelation attention
        +Linear query_projection
        +Linear key_projection
        +Linear value_projection
        +Linear out_projection
        +forward(queries, keys, values, attn_mask)
    }
    
    class AutoCorrelation {
        +bool mask_flag
        +int factor
        +float scale
        +Dropout dropout
        +bool output_attention
        +time_delay_agg_training(values, corr)
        +time_delay_agg_inference(values, corr)
        +forward(queries, keys, values, attn_mask)
    }
    
    class Decoder {
        +List~DecoderLayer~ layers
        +my_Layernorm norm_layer
        +Linear projection
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }
    
    class DecoderLayer {
        +AutoCorrelationLayer self_attention
        +AutoCorrelationLayer cross_attention
        +Conv1d conv1
        +Conv1d conv2
        +series_decomp decomp1
        +series_decomp decomp2
        +Dropout dropout
        +activation
        +forward(x, enc_out, x_mask, cross_mask, trend)
    }
    
    Model --> Encoder
    Model --> Decoder
    Encoder --> EncoderLayer
    EncoderLayer --> AutoCorrelationLayer
    EncoderLayer --> Conv1d
    EncoderLayer --> series_decomp
    AutoCorrelationLayer --> AutoCorrelation
    Decoder --> DecoderLayer
    DecoderLayer --> AutoCorrelationLayer
    DecoderLayer --> Conv1d
    DecoderLayer --> series_decomp
```

