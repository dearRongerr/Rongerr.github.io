![image-20241118111619085](images/image-20241118111619085.png)

[åŸæ–‡é“¾æ¥](https://arxiv.org/pdf/2311.16122v1)

[æºç é“¾æ¥](https://github.com/perladoubinsky/SemAug)

![image-20241118111832412](images/image-20241118111832412.png)

æ ‡é¢˜ï¼šSemantic Generative Augmentations for Few-Shot Counting  å°æ ·æœ¬è®¡æ•° è¯­ä¹‰ç”Ÿæˆå¢å¼º

æœ¬æ–‡â¤ï¸ï¼š

- ä¸ºäº†ä½¿ç”Ÿæˆçš„å›¾åƒ çš„ç›®æ ‡æ•°é‡å’ŒåŸå›¾åƒä¿æŒä¸å˜ï¼Œä½¿ç”¨ stable diffusionåˆæˆå›¾åƒæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨åŒæ¡ä»¶ï¼šprompt & å¯†åº¦å›¾
- ä¸ºäº†è§£å†³ç”Ÿæˆå›¾åƒæ€»æ˜¯è·Ÿè®­ç»ƒå›¾åƒç›¸åŒçš„é—®é¢˜ï¼Œæå‡ºå¢å¼ºå›¾åƒå¤šæ ·æ€§çš„ç­–ç•¥ï¼šéšæœºæ‰“ä¹±å­—å¹•æè¿°ï¼Œç›®çš„æ˜¯ä¸ºäº†åˆ›å»ºæ²¡è§è¿‡ä½†æ˜¯åˆç†çš„ å¯¹è±¡ç±»å‹å’Œç©ºé—´åˆ†å¸ƒ

åˆæˆå›¾åƒ & å¤šæ ·åŒ–ç­–ç•¥

## Abstract

ï¼ˆæ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹ï¼‰

> With the availability of powerful text-to-image diffusion models, recent works have explored the use of synthetic data to improve image classification performances. 
>
> éšç€å¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯ç”¨æ€§ï¼Œæœ€è¿‘çš„å·¥ä½œæ¢ç´¢äº†ä½¿ç”¨åˆæˆæ•°æ®æ¥æé«˜å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚
>
> These works show that it can effectively augment or even replace real data.
>
> è¿™äº›å·¥ä½œè¡¨æ˜ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºç”šè‡³æ›¿ä»£çœŸå®æ•°æ®ã€‚
>

In this work, we investigate how synthetic data can benefit few-shot class-agnostic counting. 

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åˆæˆæ•°æ®å¦‚ä½•æœ‰åˆ©äºå°æ ·æœ¬ç±»åˆ«æ— å…³è®¡æ•°ã€‚

==åˆæˆå›¾åƒçš„ç¬¬ä¸€ä¸ªè¦æ±‚ï¼šåˆæˆçš„å›¾ç‰‡ ç›®æ ‡æ•°é‡æ˜¯ç›¸ç­‰çš„==

This requires to generate images that correspond to a given input number of objects. 

è¿™å°±éœ€è¦ç”Ÿæˆä¸ç»™å®šçš„è¾“å…¥ç‰©ä½“æ•°é‡ç›¸å¯¹åº”çš„å›¾åƒã€‚

However, text-to-image models struggle to grasp the notion of count.

ç„¶è€Œï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹å¾ˆéš¾æŠŠæ¡è®¡æ•°çš„æ¦‚å¿µã€‚

==åˆæˆå›¾åƒçš„ç›‘ç£ä¿¡å·ï¼špromptå’Œå¯†åº¦å›¾ï¼›å…·ä½“ç”¨çš„æ¨¡å‹ï¼šStable Diffusion== 

We propose to rely on a double conditioning of Stable Diffusion with both a prompt and a density map in order to augment a training dataset for few-shot counting. 

æˆ‘ä»¬æå‡ºä½¿ç”¨ç¨³å®šæ‰©æ•£( Stable Diffusion )çš„æç¤ºå›¾å’Œå¯†åº¦å›¾çš„åŒé‡æ¡ä»¶æ¥å¢åŠ å°‘æ ·æœ¬è®¡æ•°çš„è®­ç»ƒæ•°æ®é›†ã€‚

Due to the small dataset size, the fine-tuned model tends to generate images close to the training images. 

ç”±äºæ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼Œå¾®è°ƒåçš„æ¨¡å‹å€¾å‘äºç”Ÿæˆæ¥è¿‘è®­ç»ƒå›¾åƒçš„å›¾åƒã€‚

==ä¸ºäº†è§£å†³ å›¾åƒæ€»æ˜¯æ¥è¿‘è®­ç»ƒå›¾åƒçš„é—®é¢˜ï¼Œæå‡º éšæœºæ‰“ä¹±å›¾åƒä¹‹é—´çš„å­—å¹•==

We propose to enhance the diversity of synthesized images by exchanging captions between images thus creating unseen configurations of object types and spatial layout. 

æˆ‘ä»¬æå‡ºé€šè¿‡åœ¨å›¾åƒä¹‹é—´äº¤æ¢å­—å¹•æ¥å¢å¼ºåˆæˆå›¾åƒçš„å¤šæ ·æ€§ï¼Œä»è€Œåˆ›å»ºçœ‹ä¸è§çš„å¯¹è±¡ç±»å‹å’Œç©ºé—´å¸ƒå±€é…ç½®ã€‚

ï¼ˆç»“æœï¼‰Our experiments show that our diversified generation strategy significantly improves the counting accuracy of two recent and performing few-shot counting models on FSC147 and CARPK.

æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šæ ·åŒ–ç”Ÿæˆç­–ç•¥æ˜¾è‘—æé«˜äº†FSC147å’ŒCARPKä¸Šæœ€è¿‘æ‰§è¡Œçš„ä¸¤ä¸ªå°‘æ ·æœ¬è®¡æ•°æ¨¡å‹çš„è®¡æ•°å‡†ç¡®ç‡ã€‚

æ•°æ®é›†ï¼š

- FSC147
- CARPK

## å¼•å…¥-è´¡çŒ®

**To tackle few-shot counting, we propose to synthesize unseen data with Stable Diffusion conditioned by both a textual prompt and a density map.**

ä¸ºäº†è§£å†³å°æ ·æœ¬è®¡æ•°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç¨³å®šæ‰©æ•£æ¥åˆæˆçœ‹ä¸è§çš„æ•°æ®ï¼Œå…¶æ¡ä»¶æ˜¯æ–‡æœ¬æç¤ºå’Œå¯†åº¦å›¾ã€‚

??? question  "stable diffusionï¼Ÿ"

!!! note
	â‘  ä½¿ç”¨stable diffusionåˆæˆæ•°æ®   
	â‘¡ ç›‘ç£ä¿¡å·ï¼šæ–‡æœ¬æç¤ºå’Œå¯†åº¦å›¾       

We thus build an ==augmented FSC dataset== that is used to train a deep counting network. 

å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¢å¹¿çš„FSCæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ·±åº¦è®¡æ•°ç½‘ç»œã€‚

The double conditioning, implemented with ==ControlNet== [42], allows us to generate novel synthetic images with a precise control, preserving the ground truth for the counting task. 

ç”¨controlnetç½‘ç»œ[ 42 ]å®ç°çš„åŒé‡æ¡ä»¶åŒ–ï¼Œå¯ä»¥ä½¿æˆ‘ä»¬åœ¨ç²¾ç¡®æ§åˆ¶ä¸‹ç”Ÿæˆæ–°çš„åˆæˆå›¾åƒï¼Œä»è€Œä¸ºè®¡æ•°ä»»åŠ¡ä¿ç•™åŸºæœ¬çš„çœŸå€¼ã€‚

It deals well with large numbers of objects, while current methods fail in such cases [19, 27]. 

å®ƒå¯ä»¥å¾ˆå¥½åœ°å¤„ç†å¤§é‡çš„å¯¹è±¡ï¼Œè€Œç›®å‰çš„æ–¹æ³•åœ¨è¿™ç§æƒ…å†µä¸‹å¤±æ•ˆ[ 19ã€27]ã€‚

To increase the diversity of the augmented training set, we swap image descriptions between the n available training samples, leading to $\frac{n(nâˆ’1)}{2} $ novel couples, each being the source of several possible synthetic images.

ä¸ºäº†å¢åŠ æ‰©å……è®­ç»ƒé›†çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬åœ¨nä¸ªå¯ç”¨çš„è®­ç»ƒæ ·æœ¬ä¹‹é—´äº¤æ¢å›¾åƒæè¿°ï¼Œå¾—åˆ°n ( n-1 ) 2ä¸ªæ–°çš„å¯¹å­ï¼Œæ¯ä¸ªå¯¹å­éƒ½æ˜¯è‹¥å¹²å¯èƒ½çš„åˆæˆå›¾åƒçš„æ¥æºã€‚

However, we show that some combinations do not make sense and lead to poor quality samples. 

ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜ä¸€äº›ç»„åˆæ²¡æœ‰æ„ä¹‰ï¼Œå¹¶å¯¼è‡´è´¨é‡è¾ƒå·®çš„æ ·æœ¬ã€‚

Therefore, ==we only select plausible pairs, resulting in improved augmentation quality==. 

å› æ­¤ï¼Œæˆ‘ä»¬åªé€‰æ‹©äº†ä¼¼æ˜¯è€Œéçš„é…å¯¹ï¼Œä»è€Œæé«˜äº†å¢å¼ºè´¨é‡ã€‚

We evaluate our approach on two class-agnostic counting networks, namely SAFECount [41] and CounTR [6]. We show that it significantly improves the performances on the benchmark dataset FSC147 [28] and allow for a better generalization on the CARPK dataset [14].

æˆ‘ä»¬åœ¨SAFECount [ 41 ]å’ŒCoun TR [ 6 ]ä¸¤ä¸ªç±»ä¸å¯çŸ¥è®¡æ•°ç½‘ç»œä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è¯æ˜äº†å®ƒåœ¨åŸºå‡†æ•°æ®é›†FSC147 [ 28 ]ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨CARPKæ•°æ®é›†[ 14 ]ä¸Šå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

- å¯¹æ¯”æ¨¡å‹ï¼šSAFECount [41] and CounTR [6]

- benchmarkï¼šFSC147ã€CARPK
- æœ¬æ–‡æå‡ºçš„æ˜¯ å¯¹æ•°æ®è¾“å…¥çš„å¤šæ ·æ€§è¿›è¡Œæ‰©å……

## ç»“è®º

7 Conclusion

ç”±æ‰©æ•£æ¨¡å‹åˆæˆæ•°æ®æé«˜FSCè®¡æ•°æ€§èƒ½

> We show that synthetic data generated by diffusion models improve deep models for few-shot counting. 

ä»¥å¯†åº¦å›¾ä¸ºæ¡ä»¶ï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡ç”Ÿå›¾æ¨¡å‹

> We adapt a pretrained text-to-image model with a density map conditioning and 

æˆ‘ä»¬æå‡ºçš„å¤šæ ·åŒ–ç­–ç•¥ï¼šåˆ©ç”¨å­—å¹•ç›¸ä¼¼æ€§ï¼Œç”Ÿæˆåˆç†çš„ä½†æ˜¯ æ··åˆäº†ä¸åŒè®­ç»ƒå›¾åƒå’Œè¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯

> we propose a diversification strategy that exploits caption similarities to generate unseen but plausible data that mixes the semantics and the geometry of different training images. 

å±•ç¤ºäº†é€‰æ‹©  compatible images ï¼ˆç›¸å®¹çš„å›¾åƒï¼Ÿï¼‰åˆæˆå›¾åƒï¼Œå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½|| æˆ‘è®°å¾—æœ‰ä¸€ä¸ªæ¨¡å‹çš„æ‹¼æ¥å›¾åƒæ¥ç€ï¼Œå“ªç¯‡è®ºæ–‡æ¥ç€ï¼Ÿ

> We show that selecting compatible images improves synthetic image quality with beneficial effects on model performance. 

æˆ‘ä»¬æå‡ºçš„å¤šæ ·æ€§æ•°æ®åˆæˆç­–ç•¥æé«˜äº†è®¡æ•°æ€§èƒ½ï¼ŒFSC147 å’Œ CARPK

> We demonstrate that learning with our diverse synthetic data leads to improved counting accuracy on FSC147 and state of the art generalization on CARPK.

æˆ‘ä»¬æå‡ºçš„æ•°æ®åˆæˆç­–ç•¥ç»è¿‡å¾®è°ƒå¯ä»¥ç”¨äºå…¶ä»–é¢†åŸŸï¼šç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²

> This strategy could be adapted to other tasks requiring fine grained compositionality, such as object detection and semantic segmentation. 

æˆ‘ä»¬çš„å¤šæ ·åŒ–ç­–ç•¥ï¼šé€šè¿‡åœ¨å¯†åº¦å›¾å¼•å…¥åˆé€‚çš„ç›¸ä¼¼æ€§åº¦é‡ã€é€šè¿‡æ–‡æœ¬ä¹‹é—´ç›¸äº’äº¤æ¢å’Œå¯†åº¦å›¾çš„æ§åˆ¶ï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥æ‰©å±•

> Our diversification scheme could be further extended by swapping both the captions and the density controls, by introducing a suitable similarity metric that operates on the density maps.

## å¼•å…¥

 1 Introduction

P1 

ç›®æ ‡è®¡æ•°çš„åº”ç”¨é¢†åŸŸ

> Counting objects is a task with applications in many domains e.g. manufacturing, medicine, monitoring, that involve different types of objects.

ç‰¹å®šç›®æ ‡è®¡æ•° â†’FSC CACè®¡æ•°

ä¸¤ä¸ªçªå‡ºç‰¹ç‚¹ï¼šâ‘  bounding boxes (cf. Fig. 2),  â‘¡ an extract-**then**-match manner [21].

([åé¢æœ‰äººåˆ›æ–°](https://arxiv.org/pdf/2305.04440v2)ï¼Œå°±æŠŠè¿™ä¸ªthenæ”¹æˆ and)

> While earlier works focused on learning specialized networks [2, 7, 14, 16], Few-Shot object Counting (FSC) [31] was recently introduced to train models that can count any object, including from categories outside the training data. Methods tackling FSC rely on exemplar objects annotated with bounding boxes (cf. Fig. 2), in an extract-then-match manner [21]. 

å¦‚ä½•å¯¹å›¾åƒç‰¹å¾ å’Œ æ ·ä¾‹æ¡†ç‰¹å¾ è¿›è¡ŒåŒ¹é…ï¼šâ‘  correlation maps [31, 41]   â‘¡attention [6, 9]

> The features of the exemplars and query image are compared using e.g. correlation maps [31, 41] or attention [6, 9]. Matched features are then transformed into a density map indicating at each location in the image the density of the objects of interest. The density map is then summed to obtain the predicted count.

P2

<u>æ•°æ®é›†ç”Ÿæˆï¼šä»GAN â†’ æ‰©æ•£æ¨¡å‹</u>

**æå‡ºchallenge  FSC147æ•°æ®é›†æœ‰é™** The reference dataset for FSC, namely FSC147 [31], contains a **limited amount of data** (3659 train images) thus boundingé™åˆ¶ the performances of counting networks [30]. 

**æ‰©å……æ•°æ®é›†å¾ˆéº»çƒ¦** Expanding such a dataset is costly as the annotation process requires pinpointing the center of each object present in a query image, with a potentially high number of occurrences. 

**solutions** To overcome the small dataset size, Ranjan et al. [30] augment FSC147 using a GAN to diversify the image styles.solutions**â‘   Ranjan et al. [30]è¿™ä¸ªäººç”¨GAN å¤šæ ·åŒ–å›¾åƒçš„æ ¼å¼**

Diffusion models have now surpassed GANs owing to their training stability and lower sensitivity to mode collapse. **ç°çŠ¶ï¼šæ‰©æ•£æ¨¡å‹ğŸ”¥äº†**

These models produce more effective and diverse augmentations [12, 37, 39]. Recent works mostly aim at augmenting classification datasets e.g. ImageNet [8], where augmentations are generated by prompting the models with the image labels. **æ‰©æ•£æ¨¡å‹ğŸ”¥çš„è¯æ®ï¼Œä¸”ä¸»è¦ğŸ”¥åœ¨åˆ†ç±»æ•°æ®é›†**ï¼Œé€šè¿‡å›¾åƒæ ‡ç­¾æ¥æç¤ºæ¨¡å‹

**motivation åˆ°äº†æˆ‘ä»¬è¦è®¨è®ºçš„é—®é¢˜ï¼šæ²¡ğŸ”¥åˆ°è®¡æ•°æ•°æ®é›†**This fails to produce satisfying images for counting datasets as text-to-image models struggle to generate the correct number of objects [26]. å› ä¸ºæ–‡æœ¬åˆ°å›¾åƒå¾ˆéš¾äº§ç”Ÿ å¯¹è±¡æ•°é‡æ­£ç¡®çš„æ•°æ®é›†

**åšçš„ä¸€äº›åŠªåŠ›** Some works tackle improving compositionality in vision-language models [19, 25, 27] but are limited to small numbers of objects. ä¸€äº›å·¥ä½œè‡´åŠ›äºæé«˜è§†è§‰è¯­è¨€æ¨¡å‹[ 19ã€25ã€27]çš„ç»„åˆæ€§ï¼Œä½†ä»…é™äºå°‘é‡å¯¹è±¡ã€‚

Other works add more control to pre-trained text-to-image models [15, 23, 42].å…¶ä»–å·¥ä½œåœ¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹[ 15ã€23ã€42]ä¸­æ·»åŠ äº†æ›´å¤šçš„æ§åˆ¶ã€‚

P3 æœ¬æ–‡çš„ï¼šStable Diffusion & ControlNet [42]

**æˆ‘ä»¬çš„å·¥ä½œ â­ï¸**To tackle few-shot counting, **we propose** to synthesize unseen data with Stable Diffusion conditioned by both a textual prompt and a density map. 

åŸºäºæ–‡æœ¬æç¤ºå’Œå¯†åº¦å›¾ ä½¿ç”¨æ‰©æ•£æ¨¡å‹ ç”Ÿæˆæ•°æ®

We thus build an augmented FSC dataset that is used to train a deep counting network. 

å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¢å¹¿çš„FSCæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ·±åº¦è®¡æ•°ç½‘ç»œã€‚

The double conditioning, implemented with ControlNet [42], allows us to generate novel synthetic images with a precise control, preserving the ground truth for the counting task. 

ç”¨controlnetç½‘ç»œ[ 42 ]å®ç°çš„åŒé‡æ¡ä»¶åŒ–ï¼Œå¯ä»¥ä½¿æˆ‘ä»¬åœ¨ç²¾ç¡®æ§åˆ¶ä¸‹ç”Ÿæˆæ–°çš„åˆæˆå›¾åƒï¼Œä»è€Œä¸ºè®¡æ•°ä»»åŠ¡ä¿ç•™åŸºæœ¬çš„çœŸå€¼ã€‚

It deals well with large numbers of objects, while current methods fail in such cases [19, 27].

å®ƒå¯ä»¥å¾ˆå¥½åœ°å¤„ç†å¤§é‡çš„å¯¹è±¡ï¼Œè€Œç›®å‰çš„æ–¹æ³•åœ¨è¿™ç§æƒ…å†µä¸‹å¤±æ•ˆ[ 19ã€27]ã€‚

To increase the diversity of the augmented training set, we swap image descriptions between the $n$ available training samples, leading to $\frac{n(nâˆ’1)}{2} $ novel couples, each being the source of several possible synthetic images. 

ä¸ºäº†å¢åŠ æ‰©å……è®­ç»ƒé›†çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬åœ¨nä¸ªå¯ç”¨çš„è®­ç»ƒæ ·æœ¬ä¹‹é—´äº¤æ¢å›¾åƒæè¿°ï¼Œå¾—åˆ°n ( n-1 ) 2ä¸ªæ–°çš„å¯¹å­ï¼Œæ¯ä¸ªå¯¹å­éƒ½æ˜¯è‹¥å¹²å¯èƒ½çš„åˆæˆå›¾åƒçš„æ¥æºã€‚

However, we show that some combinations do not make sense and lead to poor quality samples. 

ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜ä¸€äº›ç»„åˆæ²¡æœ‰æ„ä¹‰ï¼Œå¹¶å¯¼è‡´è´¨é‡è¾ƒå·®çš„æ ·æœ¬ã€‚

Therefore, we only select plausible pairs, resulting in improved augmentation quality. 

å› æ­¤ï¼Œæˆ‘ä»¬åªé€‰æ‹©äº†ä¼¼æ˜¯è€Œéçš„é…å¯¹ï¼Œä»è€Œæé«˜äº†å¢å¼ºè´¨é‡ã€‚

We evaluate our approach on two class-agnostic counting networks, namely SAFECount [41] and CounTR [6].

æˆ‘ä»¬åœ¨SAFECount [ 41 ]å’ŒCoun TR [ 6 ]ä¸¤ä¸ªç±»ä¸å¯çŸ¥è®¡æ•°ç½‘ç»œä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚

 We show that it significantly improves the performances on the benchmark dataset FSC147 [28] and allow for a better generalization on the CARPK dataset [14].

æˆ‘ä»¬è¯æ˜äº†å®ƒåœ¨åŸºå‡†æ•°æ®é›†FSC147 [ 28 ]ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨CARPKæ•°æ®é›†[ 14 ]ä¸Šå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

### æ€»ç»“

æœ¬æ–‡çš„å¼•å…¥æ˜¯ä¸‰æ®µï¼š

ç¬¬ä¸€æ®µï¼šç›®æ ‡è®¡æ•°çš„åº”ç”¨é¢†åŸŸ ç»å†äº†ä»ç‰¹å®šç‰©ä½“ åˆ° é€šç”¨ç‰©ä½“

ç¬¬äºŒæ®µï¼šä»‹ç»æ•°æ®é›†åˆæˆæ–¹æ³•ï¼šä»GAN åˆ° Stable Diffusion

ç¬¬ä¸‰æ®µï¼šæŒ‡å‡ºæœ¬æ–‡è´¡çŒ®ï¼Œå†æ¬¡å¼ºè°ƒ

- ç”Ÿæˆå›¾åƒï¼šStable diffusion
  - åˆæˆå›¾åƒçš„ç›®æ ‡æ•°é‡  å’Œ å‚è€ƒå›¾åƒ æ˜¯ç›¸åŒçš„
  - prompt å’Œ density mapåŒæ—¶æŒ‡å¯¼å›¾åƒåˆæˆ
- åˆæˆå›¾åƒçš„å¤šæ ·æ€§ç­–ç•¥ï¼š
  - swap image descriptions ï¼›éšæœºäº¤æ¢å›¾åƒæè¿°

## ç›¸å…³å·¥ä½œ

æ€»ç»“ï¼šæœ¬æ–‡çš„ç›¸å…³å·¥ä½œä»ä¸¤æ–¹é¢å±•å¼€ï¼š

**Learning with Generated Data**

**Few-shot Object Counting**

**ç”Ÿæˆæ•°æ®çš„å­¦ä¹  å’Œ å°æ ·æœ¬è®¡æ•°**

**ç¬¬ä¸€æ®µï¼š**

ç¬¬ä¸€éƒ¨åˆ†ï¼šLearning with Generated Data

**ï¼ˆç°çŠ¶ï¼šï¼‰**

> Improvements in image synthesis using generative models have sparked great interest in generating fake images to train deep neural networks. 
>
> ä½¿ç”¨ç”Ÿæˆæ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆçš„æ”¹è¿›æ¿€å‘äº†äººä»¬å¯¹ç”Ÿæˆå‡å›¾åƒä»¥è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„æå¤§å…´è¶£ã€‚
>

**ï¼ˆä»GANå¼€å§‹ï¼‰**

> GANs were the first popular models to synthesize data for image classification [1, 5, 17], crowd counting [40] and image segmentation [43]. 
>
> GANsæ˜¯ç¬¬ä¸€ä¸ªæµè¡Œçš„ç”¨äºå›¾åƒåˆ†ç±»[ 1ã€5ã€17]ã€äººç¾¤è®¡æ•°[ 40 ]å’Œå›¾åƒåˆ†å‰²[ 43 ]çš„æ•°æ®åˆæˆæ¨¡å‹ã€‚
>

**ï¼ˆåˆ°ç°åœ¨çš„æ‰©æ•£æ¨¡å‹ï¼šDDPMã€ Latent Diffusion ï¼‰**

> Nowadays, diffusion models such as DDPM [13] or Latent Diffusion [32] seem to outperform GANs, demonstrating more stable training, better coverage of the training distribution and higher image quality. 
>
> å¦‚ä»Šï¼Œæ‰©æ•£æ¨¡å‹å¦‚DDPM [ 13 ]æˆ–Latent Diffusion [ 32 ]ä¼¼ä¹ä¼˜äºGANsï¼Œæ˜¾ç¤ºå‡ºæ›´ç¨³å®šçš„è®­ç»ƒï¼Œæ›´å¥½çš„è®­ç»ƒåˆ†å¸ƒè¦†ç›–ç‡å’Œæ›´é«˜çš„å›¾åƒè´¨é‡ã€‚
>

ï¼ˆæ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼Œä»¥æ–‡æœ¬ä¸ºæ¡ä»¶çš„æ‰©æ•£æ¨¡å‹ï¼‰

> The availability of powerful text-conditioned diffusion models **æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹** [24, 29, 32, 33] has led to many works exploring how to leverage synthetic data for computer vision åˆ©ç”¨ç”Ÿæˆæ•°æ®è¿›è¡Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡, **e.g. image classification in low-data regime [12], zero/few-shot learning [37, 39], ImageNet classification [3, 4, 34] and self-supervised learning [38].** 
>
> å¼ºå¤§çš„æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹[ 24ã€29ã€32ã€33]çš„å‡ºç°ï¼Œå¼•å‘äº†è®¸å¤šç ”ç©¶å¦‚ä½•åˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œè®¡ç®—æœºè§†è§‰çš„å·¥ä½œã€‚
>
> These works focus on how to **reduce domain gap å‡å°‘é¢†åŸŸé¸¿æ²Ÿ** [12], improve the prompts **æ”¹è¿›æç¤º** using e.g. text-to-sentence model [12] or WordNet [34] and **increase diversity å¢åŠ å¤šæ ·æ€§** by **optimizing the guidance scale [3, 34, 37].   ä¼˜åŒ–æŒ‡å¯¼å°ºåº¦**
>
> è¿™äº›å·¥ä½œä¸»è¦é›†ä¸­åœ¨å¦‚ä½•å‡å°‘é¢†åŸŸé¸¿æ²Ÿ[ 12 ]ï¼Œä½¿ç”¨æ–‡æœ¬åˆ°å¥å­æ¨¡å‹[ 12 ]æˆ–è¯ç½‘[ 34 ]æ¥æ”¹è¿›æç¤ºï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æŒ‡å¯¼å°ºåº¦[ 3,34,37]æ¥å¢åŠ å¤šæ ·æ€§ã€‚
>
> This body of literature consistently demonstrates how generated data allow deep networks to learn more robust representations and improve generalization for image classification. 
>
> è¿™ç»„æ–‡çŒ®ä¸€è‡´åœ°å±•ç¤ºäº†ç”Ÿæˆæ•°æ®å¦‚ä½•è®©æ·±åº¦ç½‘ç»œå­¦ä¹ æ›´é²æ£’çš„è¡¨ç¤ºï¼Œå¹¶æé«˜å›¾åƒåˆ†ç±»çš„æ³›åŒ–æ€§ã€‚
>

æˆ‘ä»¬åœ¨å›¾åƒåˆæˆé¢†åŸŸçš„å·¥ä½œï¼š

â¤ï¸ï¼šTo bring the power of synthetic data to counting,  **we propose** to condition diffusion models  not only on text prompts but also on counting density maps to generate images with the correct number of objects in the desired spatial configuration. 

- æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼šåŸºäºæ–‡æœ¬æç¤ºå’Œå¯†åº¦å›¾
- åœ¨æœŸæœ›çš„ç©ºé—´ä½ç½®ä¸Šï¼Œç”Ÿæˆæœ‰æ­£ç¡®æ•°é‡çš„å›¾ç‰‡

> We focus more specifically on few-shot class-agnostic object counting. Compared to image classification, this task involves small datasets and local spatial understanding, as objects can be small and follow complex layouts.
>
> æˆ‘ä»¬æ›´ä¸“æ³¨äºå°‘æ ·æœ¬ç±»æ— å…³ç‰©ä½“è®¡æ•°ã€‚ä¸å›¾åƒåˆ†ç±»ç›¸æ¯”ï¼Œè¿™é¡¹ä»»åŠ¡æ¶‰åŠå°å‹æ•°æ®é›†å’Œå±€éƒ¨ç©ºé—´ç†è§£ï¼Œå› ä¸ºå¯¹è±¡å¯ä»¥æ˜¯å°å‹çš„ï¼Œå¹¶ä¸”éµå¾ªå¤æ‚çš„å¸ƒå±€ã€‚
>
> The generated data needs **a level of compositionality** that current generative models, including **diffusion models æ‰©æ•£æ¨¡å‹**, struggle to achieve. 
>
> ç”Ÿæˆçš„æ•°æ®éœ€è¦ä¸€ç§ **ç»„åˆæ€§æ°´å¹³** ï¼Œè¿™æ˜¯å½“å‰çš„ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬**æ‰©æ•£æ¨¡å‹**ï¼Œåœ¨å®ç°ä¸Šå­˜åœ¨å›°éš¾çš„ã€‚
>
> ç”Ÿæˆæ•°æ®éœ€è¦ç»„åˆï¼Œè¿™æ˜¯ç°åœ¨çš„ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹ï¼Œéš¾ä»¥å®ç°çš„ã€‚
>
> To bring the power of synthetic data to counting,  **we propose** to condition diffusion models æ¡ä»¶æ‰©æ•£æ¨¡å‹ not only on text prompts but also on counting density maps æ–‡æœ¬æç¤º+å¯†åº¦å›¾ to generate images with the correct number of objects ç”Ÿæˆæœ‰æ­£ç¡®å¯¹è±¡æ•°é‡çš„å›¾ç‰‡ in the desired spatial configuration. åœ¨æœŸæœ›çš„ç©ºé—´ä½ç½®ä¸Š
>

> ä¸ºäº†å°†åˆæˆæ•°æ®çš„èƒ½åŠ›ç”¨äºè®¡æ•°ï¼Œæˆ‘ä»¬æå‡ºä¸ä»…åœ¨æ–‡æœ¬æç¤ºä¸Šï¼Œè€Œä¸”åœ¨è®¡æ•°å¯†åº¦å›¾ä¸Šå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»¥ç”Ÿæˆæ‰€éœ€ç©ºé—´é…ç½®ä¸­å…·æœ‰æ­£ç¡®æ•°é‡å¯¹è±¡çš„å›¾åƒã€‚
>
> We exploit this double control to generate diversified unseen data by prompting the model with novel combinations of the controls.
>
> æˆ‘ä»¬åˆ©ç”¨è¿™ç§åŒé‡æ§åˆ¶æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„æœªè§æ•°æ®ï¼Œé€šè¿‡ä½¿ç”¨æ–°çš„æ§åˆ¶ç»„åˆæ¥ä¿ƒä½¿æ¨¡å‹ã€‚

!!! tip
	æˆ‘å†™è¿™éƒ¨åˆ†æ–‡çŒ®ç»¼è¿°çš„æ—¶å€™ï¼Œä¹Ÿä»GANå¼€å§‹ï¼Œå†™åˆ°diffusionï¼Œæœ€ååˆ°å…³äºç›®æ ‡è®¡æ•°

ç¬¬äºŒæ®µ

**ç¬¬äºŒéƒ¨åˆ†ï¼šFew-shot Object Counting** å°æ ·æœ¬è®¡æ•°çš„å‘å±•

CACä»»åŠ¡çš„å®šä¹‰

> The goal of few-shot class agnostic object counting is to count how many instances of objects of any arbitrary category there are in a given image, by leveraging only a few exemplars of the category of interest. 

æ–‡çŒ®1ï¼šFamNet

> This was initially formulated as matching exemplars and image patch features [21]. FSC147 [31] was later put forward as the main dataset for this task, with an open set train and test split to evaluate generalization to unseen object categories. Its authors introduced FamNet, a deep net trained to infer density maps from feature similarities.

æ–‡çŒ®2ï¼šBMNet

>  In the same lineage, BMNet [36] refines the similarity map by learning the similarity metric jointly with the counting network. 

æ–‡çŒ®3ï¼šSAFECount

> In SAFECount [41], the similarities are used to fuse exemplars features into the query image features. The density map is then predicted from the enhanced features.

æ–‡çŒ®4ã€5ï¼šCounTR [6] and LOCA [9]

>  Other works e.g. CounTR [6] and LOCA [9] focus on improving the feature representations using a Transformer backbone as the visual encoder and injecting information about the exemplarsâ€™ shape in the network [9]. 

æ–‡çŒ®6ï¼šä¸æˆ‘ä»¬å·¥ä½œæœ€ç›¸å…³çš„æ–‡çŒ® Vicinal Couting Network from Rajan et al. [30]  â†’ è¯´æ˜ å›¾åƒå¢å¼ºå¯ä»¥æ˜¾è‘—æé«˜è®¡æ•°æ€§èƒ½

> The closest comparison to our work is the Vicinal Couting Network from Rajan et al. [30]. It augments FSC147 with generated data by training a conditional GAN jointly with the counting network, producing augmentations that preserve the image content while modifying its visual appearance. While outperformed by later models, it introduced the idea that well-chosen augmentations can significantly boost counting accuracy. 

æˆ‘ä»¬çš„å·¥ä½œï¼šå¤šæ ·æ€§åˆæˆç­–ç•¥ï¼šä¸ä»…åˆæˆå¤–è§‚ï¼Œè¿˜å¯ä»¥æ”¹å˜å†…å®¹ï¼›ä½¿ç”¨çš„æ–‡ç”Ÿå›¾çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹

> In this work, we leverage large pre-trained text-to-image diffusion models to produce diverse augmentations that not only alter the appearance, but are also able to change the content, to synthesize augmentations with a variety of object semantics.

24Â·11Â·18

todoï¼šmethod