
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mydomain.org/mysite/learning/convs/">
      
      
        <link rel="prev" href="../pe/">
      
      
        <link rel="next" href="../3/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>卷积 - 溶err</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/timeago.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
      <link rel="stylesheet" href="../../mkdocs/css/no-footer.css">
    
      <link rel="stylesheet" href="../../mkdocs/css/unordered-list-symbols.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="溶err" class="md-header__button md-logo" aria-label="溶err" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            溶err
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              卷积
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../sticks/mkdocs_learn/" class="md-tabs__link">
          
  
    
  
  便签

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../bagu/questions/1_questions/" class="md-tabs__link">
          
  
    
  
  面试

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Error/github/" class="md-tabs__link">
          
  
    
  
  捉个虫

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../3_ViT/" class="md-tabs__link">
          
  
    
  
  笔记

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../literature/" class="md-tabs__link">
          
  
    
  
  文献

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../logs/" class="md-tabs__link">
          
  
    
  
  杂

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="溶err" class="md-nav__button md-logo" aria-label="溶err" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    溶err
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    便签
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            便签
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/mkdocs_learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MkDocs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/markdwon_learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    markdown
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/latex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LaTex
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/GitHub/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GitHub
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/MacOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MacOS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/shell/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/screen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    screen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/docker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/writting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    写作
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/1_github_v1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    github v1.0
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/2_python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sticks/3_vscode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    面试
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            面试
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    题目
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            题目
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/questions/1_questions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    面试问题
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../bagu/leetcode/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    力扣
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            力扣
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/leetcode/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1 两数之和
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/leetcode/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2 两数相加
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../bagu/deeplearning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    深度学习
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            深度学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕Transformer代码
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/former1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    空
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/former2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    空
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/pytorch_shape_function/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pytorch的维度变换函数
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/deeplearning/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visionTransformer代码
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    机器学习
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/machinelearning/kmeans/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕kmeans
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../bagu/machinelearning/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕反向传播
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    捉个虫
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            捉个虫
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/github/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    github
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/latex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latex
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/macos/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Error/docker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    docker
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    笔记
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            笔记
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_ViT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ViT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2_MOCO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MOCO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    图解LayerNorm &amp; BatchNorm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5种归一化方法
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision Transformer的原理与难点源码实现
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../swintransformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SwinTransformer 学习笔记
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4种位置编码
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    卷积
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    卷积
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1 库函数实现卷积
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1 库函数实现卷积">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#api" class="md-nav__link">
    <span class="md-ellipsis">
      首先看一下 二维卷积的api
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conv2d" class="md-nav__link">
    <span class="md-ellipsis">
      CONV2D
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2 手撕普通卷积
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3 转置卷积
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3 转置卷积">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchunfold-api" class="md-nav__link">
    <span class="md-ellipsis">
      torch.unfold api
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torch.unfold api">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      实例讲解
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      什么是转置卷积？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-flatten-convolution" class="md-nav__link">
    <span class="md-ellipsis">
      从 kernel flatten convolution 开始
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      转置卷积
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4 膨胀卷积 &amp; 空洞卷积
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5 分组卷积 &amp; 群卷积
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 分组卷积 &amp; 群卷积">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      什么是分组卷积？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#depthwise-pointwise" class="md-nav__link">
    <span class="md-ellipsis">
      补充深度可分离卷积 depthwise &amp; pointwise：
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      为什么需要分组卷积？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      分组卷积中的变与不变
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dilationgroups" class="md-nav__link">
    <span class="md-ellipsis">
      代码实现 dilation&amp;groups 手撕 &amp; 库函数
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6 汇总代码
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-1d" class="md-nav__link">
    <span class="md-ellipsis">
      7 1D 卷积
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8 深度可分离卷积
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      卷积过后输出特征图的大小
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    李沐 目标检测部分
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_GAN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_Bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BERT从零详细解读
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_Diffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDPM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_Diffusion1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_Clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Clip
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_WeightNorm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    WeightNorm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9_cGAN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN 变体
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_ResNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    项目实战：ResNet果蔬分类
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_excelcsvtensor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基础：excel\csv文件→tensor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_KLdivergence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KL divergence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_RNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_LSTM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LSTM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_ContrastiveLearning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对比学习
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_YOLO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    YOLO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_DETR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DETR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_DINO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DINO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_GPT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_distill/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    知识蒸馏
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_FastRCNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    21 FastRCNN
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    文献
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            文献
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/TSP/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    时间序列预测
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            时间序列预测
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/0_note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NOTE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/1_SegRNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/2_DLinear/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DLinear
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/3_TimesNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TimesNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/4_Informer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021|Informer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/TSP/5_Autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021|Autoformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/ObejectCounting/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    目标计数
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            目标计数
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank1%20CountGD/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank1 CountGD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank2%20GeCo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank2 GeCo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank3%20DAVE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank3 DAVE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank4%20CACViT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank4 CACViT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank5%20SSD/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank5 SSD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank6%20LOCA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank6 LOCA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank7%20SemAug_CountTR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank7 SemAug CountTR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank8%20CounTR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank8 CounTR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank9%20SemAug_SAFECount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank9 SemAug SAFECount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank10%20SPDCN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank10 SPDCN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank11%20GCA_SUN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank11 GCA SUN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank12%20SAFECount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank12 SAFECount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank13%20BMNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank13 BMNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank14%20LaoNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank14 LaoNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank15%20CounTX/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank15 CounTX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank16%20Counting_DETR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank16 Counting DETR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank17%20RCC/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank17 RCC
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank18%20Omnicount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank18 Omnicount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObejectCounting/rank19%20FamNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank19 FamNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/Reproduction/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    复现&代码
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            复现&代码
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/DAVE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DAVE复现
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一些模块
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    特征融合方式
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一些感悟
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    预训练权重
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/5_SegRNN_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/5_SegRNN_v1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    复现SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/5_SegRNN_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    复现 SegRNN_v2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/Reproduction/6_AutoFormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autoformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/ObjectDetection/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    目标检测
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_5">
            <span class="md-nav__icon md-icon"></span>
            目标检测
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObjectDetection/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    目标检测基础知识
    
  </span>
  

      </a>
    </li>
  

              
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObjectDetection/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DETR论文系列
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObjectDetection/3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    （DETR）End-to-End Object Detection with Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/ObjectDetection/4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../literature/MultiModal/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    多模态
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_6" id="__nav_5_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6">
            <span class="md-nav__icon md-icon"></span>
            多模态
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../literature/MultiModal/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../logs/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    杂
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            杂
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../logs/diary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    乐观 &amp; 坚强
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1 库函数实现卷积
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1 库函数实现卷积">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#api" class="md-nav__link">
    <span class="md-ellipsis">
      首先看一下 二维卷积的api
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conv2d" class="md-nav__link">
    <span class="md-ellipsis">
      CONV2D
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2 手撕普通卷积
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3 转置卷积
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3 转置卷积">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchunfold-api" class="md-nav__link">
    <span class="md-ellipsis">
      torch.unfold api
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torch.unfold api">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      实例讲解
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      什么是转置卷积？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-flatten-convolution" class="md-nav__link">
    <span class="md-ellipsis">
      从 kernel flatten convolution 开始
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      转置卷积
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4 膨胀卷积 &amp; 空洞卷积
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5 分组卷积 &amp; 群卷积
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5 分组卷积 &amp; 群卷积">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      什么是分组卷积？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#depthwise-pointwise" class="md-nav__link">
    <span class="md-ellipsis">
      补充深度可分离卷积 depthwise &amp; pointwise：
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      为什么需要分组卷积？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      分组卷积中的变与不变
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dilationgroups" class="md-nav__link">
    <span class="md-ellipsis">
      代码实现 dilation&amp;groups 手撕 &amp; 库函数
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6 汇总代码
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-1d" class="md-nav__link">
    <span class="md-ellipsis">
      7 1D 卷积
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8 深度可分离卷积
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      卷积过后输出特征图的大小
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="_1">卷积<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 转置卷积、反卷积</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 分组卷积、深度可分离卷积</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 1×1卷积、逐点卷积</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 膨胀卷积、空洞卷积卷积</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> 可变形卷积</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> 大核卷积</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 1D 卷积</li>
</ul>
<p><img alt="image-20241125105147313" src="../images/image-20241125105147313.png" /></p>
<h2 id="1">1 库函数实现卷积<a class="headerlink" href="#1" title="Permanent link">&para;</a></h2>
<ul>
<li>类：<code>torch.nn.Conv2d</code></li>
<li>函数：<code>F.conv2d</code>  or <code>torch.nn.functional.conv2d</code></li>
</ul>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-0-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-0-3"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
</span><span id="__span-0-4">
</span><span id="__span-0-5"><span class="n">in_channels</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-0-6"><span class="n">out_channels</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-0-7"><span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">3</span>
</span><span id="__span-0-8"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-0-9"><span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="__span-0-10">
</span><span id="__span-0-11"><span class="n">input_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span><span class="n">in_channels</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
</span><span id="__span-0-12">
</span><span id="__span-0-13"><span class="c1"># 第一种实现</span>
</span><span id="__span-0-14"><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span><span class="n">out_channels</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
</span><span id="__span-0-15">
</span><span id="__span-0-16"><span class="n">input_feature_map</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
</span><span id="__span-0-17"><span class="n">out_feature_map</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">input_feature_map</span><span class="p">)</span>
</span><span id="__span-0-18"><span class="c1"># print(input_feature_map)</span>
</span><span id="__span-0-19"><span class="c1"># print(conv_layer.weight)  # 1*1*3*3=out_channels*in_channels*height*width</span>
</span><span id="__span-0-20">
</span><span id="__span-0-21"><span class="nb">print</span><span class="p">(</span><span class="n">out_feature_map</span><span class="p">)</span>
</span><span id="__span-0-22">
</span><span id="__span-0-23"><span class="n">out_feature_map1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">input_feature_map</span><span class="p">,</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span><span id="__span-0-24">
</span><span id="__span-0-25"><span class="nb">print</span><span class="p">(</span><span class="n">out_feature_map1</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<h3 id="api">首先看一下 二维卷积的api<a class="headerlink" href="#api" title="Permanent link">&para;</a></h3>
<blockquote>
<p><img alt="image-20241125105756485" src="../images/image-20241125105756485.png" /></p>
</blockquote>
<p>谷歌搜索 pytorch conv2d，出现两个api ：</p>
<ul>
<li>一个是大写的二维卷积、 class</li>
<li>一个是 torch.nn.functional.conv2d小写的二维卷积、函数</li>
</ul>
<p>区别：</p>
<blockquote>
<ul>
<li>
<p>（第一个区别）</p>
</li>
<li>
<p>第一个大写的是一个class，如果我们要用第一个的话，我们首先需要对这个class进行一个实例化，然后对实例化的对象，再对输入特征图进行一个卷积 操作；  </p>
</li>
<li>
<p>第二个是一个函数，不需要实例化，就直接接收一个输入特征图，直接进行一个卷积操作；以上是第一个区别； </p>
</li>
<li>
<p>（第二个区别）</p>
</li>
<li>class可以自己去创建操作，包括weight和bias，可以自动去创建，就不需要手动创建；</li>
<li>对于函数来说， 需要手动的传入weight和bias；</li>
</ul>
</blockquote>
<h3 id="conv2d">CONV2D<a class="headerlink" href="#conv2d" title="Permanent link">&para;</a></h3>
<p><img alt="image-20241125120445236" src="../images/image-20241125120445236.png" /></p>
<ul>
<li>调用：torch.nn.Conv2d</li>
<li>需要传入的参数：</li>
<li>输入通道</li>
<li>输出通道</li>
<li>kernel的大小</li>
<li>步长</li>
<li>padding填充</li>
<li>膨胀dilation</li>
<li>
<p>group</p>
</li>
<li>
<p>区分 卷积 &amp; 全连接：</p>
</li>
</ul>
<blockquote>
<p>神经网络最核心的一个操作：仿射变换：将一个矩阵 乘以 输入向量 得到 另外一个向量。这是全连接网络的一个做法， 所以我们一般会对一个向量 做全连接的网络 的输入；比方说：一个word embedding向量；比方说 要预测房价，城市的人口还有物价等，不同的浮点数 组成的向量，这些都可以送入 全连接网络。</p>
<p>所以全连接网络 是把 输入当成一个向量，然后统一的去乘 一个矩阵，进行操作。但是，还有很多其他东西，不能仅仅使用一个向量来进行刻画，比如图像有长度和宽度，是一个二维的，还有RGB三个通道，这些 我们不能仅仅只是把图片拉直处理，这样破坏了图片的空间结构；</p>
<p>类似的还有语音，语言有时间维还有频率维，我们每个时刻发出的声音， 是由不同的频率组合的，同样对于语音这种信号，我们也不能仅仅是 当成 一维信号处理，甚至更复杂的是 图像和语音信号的结合，比如视频。所以对于这些我们不能仅仅只是当成一个向量处理，这样的话，全连接网络也就无法刻画它，我们可以用卷积网络刻画，对于卷积网络 和 哪些操作 比较相关呢？就是互相关，如果学过信号与系统的话，互相关就是 对于两个一维向量，我们把一个一维信号 沿着 另外一个一维信号，不断地进行 滑动相乘的操作，然后计算 一个相关系数。卷积也是类似的，对于一张图片，如果我们有一个卷积核的话，叫做kernel，我们会把 kernel 沿着 图片的不同区域 进行一个滑动相乘，来得到一个特征的表示</p>
</blockquote>
<ul>
<li>数学例子：</li>
</ul>
<blockquote>
<p><img alt="image-20241125135101570" src="../images/image-20241125135101570.png" /></p>
<ul>
<li>假设我们的input feature map=4×4，kernel=3×3，卷积操作就是将kernel在图片上 不同位置元素相乘 element-wise，不同位置元素相乘再相加，得到输出；</li>
<li>k=3，p=0，s=1</li>
<li>kernel的移动轨迹是Z字型的，从左到右，从上到下</li>
<li>输入input future map的大小是4×4的，而且 channel=1，再用一个3×3的kernel，与输入特征图 进行卷积操作，得到output，并且output大小 2×2，channel=1，同时这里我们设置的bias=False，不加 bias；</li>
<li>如果我们加入 bias呢？</li>
<li>如果 channel=1，那么 bias就是一个标量，直接相加就好了，这就是一个 bias的操作</li>
<li>如果 输入的通道数不止是1呢？比如两个通道，这个时候 就会有两个kernel，第一个kernel得到y1 y2 y3 y4；第二个kernel又会得到一个y1，y2,y3,y4,然后我们再把两个kernel得到的输出 再进行一个点对点的输出，这样得到 最终的output，这是对输入特征图有多个通道的情况。（换一种说法：输入通道的channel有几个，kernel的channel就有几个）</li>
<li>那如果我们 输出 特征图 也有多个通道的情况 会怎么处理呢？ 刚刚 我们得到了第一个通道，对于第二个通道，我们同样 在另外创造 不同的kernel，对输入进行一个卷积操作，最后把 输入的通道 加起来，变成 输出 通道的第二个输出（还是理解为：有几个kernel就有几个输出；kernel的通道数由输入的通道数决定）</li>
</ul>
</blockquote>
<p>以上是所有 卷积的过程：</p>
<ul>
<li>有几个卷积核 就有几个 输出通道；</li>
<li>
<p>单个卷积核的通道数 取决于 输入特征图的通道数</p>
</li>
<li>
<p>我们将 3×3的kernel，在输入的特征图上 进行一个Z字型的滑动相乘的操作</p>
</li>
<li>==（拉直滑动输入区域）==其实这里的滑动相乘 可以理解为 如果把输入的特征图（被卷积核覆盖的区域）3×3的区域 拉成一个向量的话 然后我们把kernel也拉成一个向量，其实就是计算 两个向量的 一个内积。内积越大 两个向量 越相似。</li>
<li>所以卷积网络 学习的是什么呢？卷积网络 会 不断的更新 kernel和 bias。就是为了学到：</li>
<li>比方说 人脸识别，就希望kernel能够学到 能够反映人脸的 特征，然后把kernel对图片的不同区域，进行比对，如果刚好发现，图片的某一个区域刚好与人脸的kernel很相似的话，那就说明你给我们已经找到人脸了，总之卷积神经网络是 给定一个目标 不断的学习kernel，最终希望kernel，能够跟图片的某一个区域，相似度达到一个比较高的值，得到一个比较好的特征，然后再不断的往 深层去传</li>
</ul>
<p>使用api的时候，需要注意📢</p>
<ul>
<li>
<p>Conv2d默认输入是4维的，第一维是batch size维，我们设置batch size=1，并添加到input_size即可;</p>
</li>
<li>
<p>input feature map的形状：<strong>batch size × 通道数 × 高 × 宽</strong> 可以查看官网 找到需要的输入形状</p>
</li>
</ul>
<blockquote>
<p><img alt="image-20241125140243598" src="../images/image-20241125140243598.png" /></p>
</blockquote>
<ul>
<li>并且打印 卷积层的 weight，也就是kernel，还可以打印输入和输出</li>
</ul>
<blockquote>
<p><img alt="image-20241125140007045" src="../images/image-20241125140007045.png" /></p>
<blockquote>
<ul>
<li>
<p>输出三个张量 第一个是 输入特征图、第二个是卷积的weight、或者kernel，第三个是 卷积的输出</p>
</li>
<li>
<p>输出的大小是 1×1×4的；</p>
</li>
<li>
<p>kernel是1×1×3×3 权重就是out channel× input channel×height×width</p>
</li>
</ul>
<blockquote>
<p>也就是说 对于 二维卷积，weight是4维的，那么总的数目 等于 输出通道数×输入通道数×卷积核的高度×卷积核的宽度，如果我们认为 卷积核是一个二维的图片的话，那么一共有 输入通道数 × 输出通道数 这么多个  卷积核图片</p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>
<p>torch.nn.Conv2d(class 的api)</p>
</li>
<li>
<p>functional的api(函数的api)</p>
</li>
</ul>
<blockquote>
<p><img alt="image-20241125140339530" src="../images/image-20241125140339530.png" /></p>
</blockquote>
<p>对于这个api 我们需要手动的指定 weight 和 bias，为了验证，我们可以直接把刚刚的weight传入，可以看到 结果是一样的:</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-1-1"><span class="n">output_feature_map1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">input_feature_map</span><span class="p">,</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>kernel就是在训练中，不断更新的</li>
</ul>
<h2 id="2">2 手撕普通卷积<a class="headerlink" href="#2" title="Permanent link">&para;</a></h2>
<p>从两种角度看卷积：</p>
<ul>
<li>把卷积看成是 首先对输入特征图进行展开，然后再进行矩阵的相乘；</li>
<li>对kernel或者filter进行展开，然后再进行矩阵相乘；</li>
</ul>
<blockquote>
<ul>
<li>有了这种方法 可以顺其自然的引出 转置卷积；之后会讲 转置卷积 也称为反卷积，但是反卷积的说法不太准确，因为 转置卷积虽然说是上采样，但是不能从output去恢复input，转置卷积 恢复的只是 input的形状，不是input的元素值</li>
<li>更准确的定义 就是转置卷积；为什么叫转置卷积呢？再说完 对kernel 进行展开，再进行矩阵相乘 就明白了</li>
<li>当我们把常规的卷积 看成是对kernel的展开，然后再矩阵相乘的话，那么转置卷积可以看成 将kernel进行一个 转置操作，然后再进行矩阵相乘，就能得到转置卷积的输出</li>
</ul>
</blockquote>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-2-1"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 卷积 输入特征图</span>
</span><span id="__span-2-2"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># 卷积核</span>
</span><span id="__span-2-3"><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 卷积偏置，默认输出通道数目等于1</span>
</span><span id="__span-2-4">
</span><span id="__span-2-5"><span class="c1"># step1 用原始的矩阵运算来实现二维卷积，先不考虑 batch size维度 和 channel维度</span>
</span><span id="__span-2-6"><span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiplication_for_conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="__span-2-7">
</span><span id="__span-2-8">  <span class="k">if</span> <span class="n">padding</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
</span><span id="__span-2-9">    <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,(</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">))</span>
</span><span id="__span-2-10">
</span><span id="__span-2-11">
</span><span id="__span-2-12">  <span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-2-13">  <span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-2-14">
</span><span id="__span-2-15">  <span class="n">output_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的高度</span>
</span><span id="__span-2-16">  <span class="n">output_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的宽度 </span>
</span><span id="__span-2-17">  <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">)</span> <span class="c1"># 初始化 输出矩阵</span>
</span><span id="__span-2-18">
</span><span id="__span-2-19">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1"># 对高度进行遍历</span>
</span><span id="__span-2-20">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span>  <span class="c1"># 对宽度维进行遍历</span>
</span><span id="__span-2-21">      <span class="n">region</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">kernel_h</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">kernel_w</span><span class="p">]</span>  <span class="c1"># 取出被核滑动到的区域</span>
</span><span id="__span-2-22">      <span class="n">output</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">stride</span><span class="p">),</span><span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="o">/</span><span class="n">stride</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span> <span class="c1"># 点乘 并赋值给输出位置的元素 </span>
</span><span id="__span-2-23">
</span><span id="__span-2-24">  <span class="k">return</span> <span class="n">output</span>
</span><span id="__span-2-25">
</span><span id="__span-2-26">
</span><span id="__span-2-27"><span class="c1"># step2 用原始的矩阵运算来实现二维卷积，先不考虑 batch size维度 和 channel维度，flatten版本</span>
</span><span id="__span-2-28"><span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiplication_for_conv2d_flatten</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="__span-2-29">
</span><span id="__span-2-30">  <span class="k">if</span> <span class="n">padding</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
</span><span id="__span-2-31">    <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,(</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">))</span>
</span><span id="__span-2-32">
</span><span id="__span-2-33">
</span><span id="__span-2-34">  <span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-2-35">  <span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-2-36">
</span><span id="__span-2-37">  <span class="n">output_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的高度</span>
</span><span id="__span-2-38">  <span class="n">output_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的宽度 </span>
</span><span id="__span-2-39">  <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">)</span> <span class="c1"># 初始化 输出矩阵</span>
</span><span id="__span-2-40">
</span><span id="__span-2-41">  <span class="n">region_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span><span class="n">kernel</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span> <span class="c1">#存储着所有拉平后特征区域</span>
</span><span id="__span-2-42">  <span class="n">kernel_matrix</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">kernel</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 存储着kernel的 列向量（矩阵）形式</span>
</span><span id="__span-2-43">  <span class="n">row_index</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-2-44">
</span><span id="__span-2-45">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1"># 对高度进行遍历</span>
</span><span id="__span-2-46">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span>  <span class="c1"># 对宽度维进行遍历</span>
</span><span id="__span-2-47">      <span class="n">region</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">kernel_h</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">kernel_w</span><span class="p">]</span>  <span class="c1"># 取出被核滑动到的区域</span>
</span><span id="__span-2-48">      <span class="n">region_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">region</span><span class="p">)</span>
</span><span id="__span-2-49">      <span class="n">region_matrix</span><span class="p">[</span><span class="n">row_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">region_vector</span>
</span><span id="__span-2-50">      <span class="n">row_index</span> <span class="o">+=</span><span class="mi">1</span>
</span><span id="__span-2-51">
</span><span id="__span-2-52">  <span class="n">output_matrix</span> <span class="o">=</span> <span class="n">region_matrix</span> <span class="o">@</span> <span class="n">kernel_matrix</span>
</span><span id="__span-2-53">  <span class="n">output</span> <span class="o">=</span> <span class="n">output_matrix</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">))</span><span class="o">+</span><span class="n">bias</span>
</span><span id="__span-2-54">
</span><span id="__span-2-55">  <span class="k">return</span> <span class="n">output</span>
</span><span id="__span-2-56">
</span><span id="__span-2-57">
</span><span id="__span-2-58"><span class="c1"># 矩阵运算实现卷积的结果</span>
</span><span id="__span-2-59"><span class="n">mat_mul_conv_output</span> <span class="o">=</span> <span class="n">matrix_multiplication_for_conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-2-60"><span class="c1"># print(mat_mul_conv_output)</span>
</span><span id="__span-2-61">
</span><span id="__span-2-62"><span class="c1"># 调用pytorch api卷积的结果</span>
</span><span id="__span-2-63"><span class="n">pytorch_api_conv_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
</span><span id="__span-2-64">                                   <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
</span><span id="__span-2-65">                                   <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-2-66">
</span><span id="__span-2-67"><span class="c1"># 矩阵运算实现卷积的结果 flatten input版本</span>
</span><span id="__span-2-68"><span class="n">mat_mul_conv_output_flatten</span> <span class="o">=</span> <span class="n">matrix_multiplication_for_conv2d_flatten</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-2-69"><span class="c1"># 验证了 flatten版本卷积 与 pytorch 官方卷积的结果，正确</span>
</span><span id="__span-2-70"><span class="n">flag1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mat_mul_conv_output</span><span class="p">,</span><span class="n">pytorch_api_conv_output</span><span class="p">)</span>
</span><span id="__span-2-71"><span class="n">flag2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mat_mul_conv_output_flatten</span><span class="p">,</span><span class="n">pytorch_api_conv_output</span><span class="p">)</span>
</span><span id="__span-2-72"><span class="nb">print</span><span class="p">(</span><span class="n">flag1</span><span class="p">)</span>
</span><span id="__span-2-73"><span class="nb">print</span><span class="p">(</span><span class="n">flag2</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-3-1"><span class="c1"># step3 用原始的矩阵运算来实现二维卷积，考虑 batch size维度 和 channel维度</span>
</span><span id="__span-3-2"><span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiplication_for_conv2d_full</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="__span-3-3">
</span><span id="__span-3-4">  <span class="c1"># input kernel 都是4维张量</span>
</span><span id="__span-3-5">  <span class="k">if</span> <span class="n">padding</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
</span><span id="__span-3-6">    <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,(</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-3-7">
</span><span id="__span-3-8">  <span class="n">bs</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-3-9">  <span class="n">out_channel</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-3-10">
</span><span id="__span-3-11">  <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-3-12">    <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_channel</span><span class="p">)</span>
</span><span id="__span-3-13">
</span><span id="__span-3-14">
</span><span id="__span-3-15">  <span class="n">output_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的高度</span>
</span><span id="__span-3-16">  <span class="n">output_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的宽度 </span>
</span><span id="__span-3-17">  <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">out_channel</span><span class="p">,</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">)</span> <span class="c1"># 初始化 输出矩阵</span>
</span><span id="__span-3-18">
</span><span id="__span-3-19">
</span><span id="__span-3-20">  <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
</span><span id="__span-3-21">    <span class="k">for</span> <span class="n">oc</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_channel</span><span class="p">):</span>
</span><span id="__span-3-22">      <span class="k">for</span> <span class="n">ic</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_channel</span><span class="p">):</span>
</span><span id="__span-3-23">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1"># 对高度进行遍历</span>
</span><span id="__span-3-24">          <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span>  <span class="c1"># 对宽度维进行遍历</span>
</span><span id="__span-3-25">            <span class="n">region</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">ic</span><span class="p">,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">kernel_h</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">kernel_w</span><span class="p">]</span>  <span class="c1"># 取出被核滑动到的区域</span>
</span><span id="__span-3-26">            <span class="n">output</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">oc</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">stride</span><span class="p">),</span><span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="o">/</span><span class="n">stride</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">[</span><span class="n">oc</span><span class="p">,</span><span class="n">ic</span><span class="p">])</span> <span class="c1"># 点乘 并赋值给输出位置的元素 </span>
</span><span id="__span-3-27">      <span class="n">output</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">oc</span><span class="p">]</span> <span class="o">+=</span> <span class="n">bias</span><span class="p">[</span><span class="n">oc</span><span class="p">]</span>
</span><span id="__span-3-28">  <span class="k">return</span> <span class="n">output</span>
</span><span id="__span-3-29">
</span><span id="__span-3-30"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># bs*in_channel*in_h*in_w</span>
</span><span id="__span-3-31"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># out_channel*in_channel*kernel_h*kernel_w</span>
</span><span id="__span-3-32"><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</span><span id="__span-3-33">
</span><span id="__span-3-34"><span class="c1"># 验证matrxi_multiplication_for_conv2d_full与官方API结果是否一致</span>
</span><span id="__span-3-35"><span class="n">pytorch_api_conv_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-3-36"><span class="n">mm_conv2d_full_output</span> <span class="o">=</span> <span class="n">matrix_multiplication_for_conv2d_full</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-3-37"><span class="n">flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">pytorch_api_conv_output</span><span class="p">,</span><span class="n">mm_conv2d_full_output</span><span class="p">)</span>
</span><span id="__span-3-38"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;all close:&quot;</span><span class="p">,</span><span class="n">flag</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="3">3 转置卷积<a class="headerlink" href="#3" title="Permanent link">&para;</a></h2>
<p>代码实现：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-4-1"><span class="c1"># step4 通过对kernel进行展开来实现二维卷积，并推导出转置卷积，不考虑batch、channel大小，不考虑padding，假设stride=1</span>
</span><span id="__span-4-2"><span class="k">def</span><span class="w"> </span><span class="nf">get_kernel_matrix</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="n">input_size</span><span class="p">):</span>
</span><span id="__span-4-3">    <span class="c1"># 基于kernel和输入特征图的大小来得到填充拉直后的kernel堆叠后的矩阵</span>
</span><span id="__span-4-4">    <span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-4-5">    <span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-4-6">    <span class="n">num_out_fea_map</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积公式</span>
</span><span id="__span-4-7">    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_out_fea_map</span><span class="p">,</span><span class="n">input_h</span><span class="o">*</span><span class="n">input_w</span><span class="p">))</span> <span class="c1">#初始化结果矩阵，输出特征图元素个数*输入特征图元素个数</span>
</span><span id="__span-4-8">    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-4-9">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-4-10">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-4-11">            <span class="c1"># 填充成 跟 输入特征图一样大小</span>
</span><span id="__span-4-12">            <span class="c1"># padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j))</span>
</span><span id="__span-4-13">            <span class="n">padded_kernel</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">kernel</span><span class="p">,(</span><span class="n">j</span><span class="p">,</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">-</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="o">-</span><span class="n">i</span><span class="p">))</span>
</span><span id="__span-4-14">            <span class="n">result</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="n">padded_kernel</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span id="__span-4-15">            <span class="n">count</span> <span class="o">+=</span><span class="mi">1</span>
</span><span id="__span-4-16">    <span class="k">return</span> <span class="n">result</span>  
</span><span id="__span-4-17">
</span><span id="__span-4-18">
</span><span id="__span-4-19">
</span><span id="__span-4-20"><span class="c1"># 测试1：验证 二维卷积</span>
</span><span id="__span-4-21"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span><span id="__span-4-22"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</span><span id="__span-4-23"><span class="n">kernel_matrix</span> <span class="o">=</span> <span class="n">get_kernel_matrix</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 4*16</span>
</span><span id="__span-4-24">
</span><span id="__span-4-25"><span class="c1"># 通过矩阵相乘来计算卷积</span>
</span><span id="__span-4-26"><span class="n">mm_conv2d_output</span> <span class="o">=</span> <span class="n">kernel_matrix</span> <span class="o">@</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>  
</span><span id="__span-4-27">
</span><span id="__span-4-28"><span class="c1"># pytorch conv2d API</span>
</span><span id="__span-4-29"><span class="n">pytorch_conv2d_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="n">kernel</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-4-30"><span class="c1"># print(kernel)</span>
</span><span id="__span-4-31"><span class="c1"># print(kernel_matrix)</span>
</span><span id="__span-4-32"><span class="c1"># print(mm_conv2d_output)</span>
</span><span id="__span-4-33"><span class="c1"># print(pytorch_conv2d_output)</span>
</span><span id="__span-4-34">
</span><span id="__span-4-35"><span class="c1"># 测试2  通过矩阵乘积来计算转置卷积 || 验证二维转置卷积</span>
</span><span id="__span-4-36"><span class="n">mm_transposed_conv2d_output</span> <span class="o">=</span> <span class="n">kernel_matrix</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="n">mm_conv2d_output</span>
</span><span id="__span-4-37"><span class="n">pytorch_transposed_conv2d_conv2d</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">pytorch_conv2d_output</span><span class="p">,</span><span class="n">kernel</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1">#API</span>
</span><span id="__span-4-38"><span class="nb">print</span><span class="p">(</span><span class="n">mm_transposed_conv2d_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span><span id="__span-4-39"><span class="nb">print</span><span class="p">(</span><span class="n">pytorch_transposed_conv2d_conv2d</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-5-1"><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.9213</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1975</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0054</span><span class="p">,</span>  <span class="mf">1.9133</span><span class="p">],</span>
</span><span id="__span-5-2">        <span class="p">[</span> <span class="mf">1.1103</span><span class="p">,</span>  <span class="mf">6.4068</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9560</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6305</span><span class="p">],</span>
</span><span id="__span-5-3">        <span class="p">[</span><span class="o">-</span><span class="mf">3.2193</span><span class="p">,</span>  <span class="mf">3.4451</span><span class="p">,</span>  <span class="mf">0.5374</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8065</span><span class="p">],</span>
</span><span id="__span-5-4">        <span class="p">[</span> <span class="mf">0.5796</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2003</span><span class="p">,</span>  <span class="mf">3.8138</span><span class="p">,</span>  <span class="mf">0.9070</span><span class="p">]])</span>
</span><span id="__span-5-5"><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">0.9213</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1975</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0054</span><span class="p">,</span>  <span class="mf">1.9133</span><span class="p">],</span>
</span><span id="__span-5-6">          <span class="p">[</span> <span class="mf">1.1103</span><span class="p">,</span>  <span class="mf">6.4068</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9560</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6305</span><span class="p">],</span>
</span><span id="__span-5-7">          <span class="p">[</span><span class="o">-</span><span class="mf">3.2193</span><span class="p">,</span>  <span class="mf">3.4451</span><span class="p">,</span>  <span class="mf">0.5374</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8065</span><span class="p">],</span>
</span><span id="__span-5-8">          <span class="p">[</span> <span class="mf">0.5796</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2003</span><span class="p">,</span>  <span class="mf">3.8138</span><span class="p">,</span>  <span class="mf">0.9070</span><span class="p">]]]])</span>
</span></code></pre></div></td></tr></table></div>
<h3 id="torchunfold-api">torch.unfold api<a class="headerlink" href="#torchunfold-api" title="Permanent link">&para;</a></h3>
<p><img alt="image-20241125142430923" src="../images/image-20241125142430923.png" /></p>
<p>查官网，看具体用法：</p>
<p><img alt="image-20241125142457039" src="../images/image-20241125142457039.png" /></p>
<h4 id="_2">实例讲解<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h4>
<p><img alt="image-20241125142529716" src="../images/image-20241125142529716.png" /></p>
<p>逐行解释：</p>
<ul>
<li>第一行，实例化 Unfold操作，这里调用的是nn.Unfold，然后传入 kernel size，kernel size是2×3的</li>
<li>第二行，然后定义input，传入 2×5×3×4的张量</li>
<li>再把input作为unfold的输入，传进去得到output</li>
<li>得到output的形状：2×30×4</li>
</ul>
<p>解释output的形状：</p>
<ul>
<li>
<p>每个patch包含了30个数值，为什么是30个数值？就是因为这里input的形状2×5×3×4</p>
</li>
<li>
<p>2是batch size</p>
</li>
<li>
<p>5是 input channel</p>
</li>
<li>
<p>3和4分别是 input的高度和宽度</p>
</li>
<li>
<p>如果我们对input 把每一次 卷积的块 拿出来的的话，那么一共是 2×3×5 这么多个值</p>
<blockquote>
<p>为什么是这么多个值呢？首先2×3是kernel size的面积，然后由于 input有5个channel，其实这个是把channel一起考虑进来了，那每个patch就有30个值；</p>
</blockquote>
</li>
<li>
<p>然后我们这里 输入大小是 3×4，而kernel size是2×3的，那么这样的话，如果默认stride=1，padding=0的话，就一共有4个blocks，就是2×2的一个输出 <span class="arithmatex">\([3-2+1=2]\)</span>  ×  <span class="arithmatex">\([ 4-3 +1 =2]\)</span> </p>
</li>
</ul>
<p>一句话总结 torch.unfold api卷积核滑动input，得到对应的region，跟卷积核一样大，拉成行向量，形状是 </p>
<p>（对于单个卷积核）</p>
<p><code>batch size×input region的元素数（=kernel的元素数 通道数*h*w）×滑动了几个区域（=输出特征图的高 × 宽）</code></p>
<p>（对于 多个卷积核 torch.unfold输出的形状是什么？）</p>
<h3 id="_3">什么是转置卷积？<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>卷积的两种角度：</p>
<ul>
<li>flatten input feature map region</li>
</ul>
<blockquote>
<ol>
<li>
<p>我们将input进行展开，也就是说 我们是把，每一个input区域拉直，拉成一个向量 ，然后把所有的区域组合成一个矩阵，然后再跟 kernel，也把kernel拉成一个向量，然后把两个矩阵 进行几个相乘。这样得到最终的卷积结果； </p>
</li>
<li>
<p>flatten input feature map region拉成行向量，kernel拉成列向量</p>
</li>
<li>把每次滑动相乘 这个input region拉直，拉成一个向量，把9个向量 拼成一个矩阵，再跟kernel，把kernel 也拉成一个列向量，进行两个矩阵的相乘；</li>
</ol>
</blockquote>
<ul>
<li>pad &amp; flatten kernel</li>
</ul>
<blockquote>
<ol>
<li>首先是把整个input，input是5×5，把整个input拉成一个25×1的向量，再把每一步的kernel，也把它变成一个长度为25的向量，方法是把每一步的kernel填充成5×5的大小</li>
</ol>
<p><img alt="image-20241125144755231" src="../images/image-20241125144755231.png" /></p>
<ol>
<li>
<p>9个kernel 跟 同一个 input 进行内积操作</p>
</li>
<li>
<p>把9个kernel 拼成一个矩阵的话，相当于是一个 9×25的 kernel矩阵，跟25×1的input feature map进行矩阵相乘，最终得到 9×1，我们再把 9×1的输出 reshape一下，变成 3×3；</p>
</li>
<li>
<p>kernel 拉成行向量，input拉成列向量</p>
</li>
<li>
<p>again：把卷积看成 每一步 都是 5×5 的kernel 跟 5×5 的input 进行内积，然后求和的操作；为什么是5×5，因为我们把每一步 kernel填充成 5×5的，具体怎么 填充  看kernel的位置，按照 input的形状 进行填</p>
</li>
</ol>
</blockquote>
<h3 id="kernel-flatten-convolution">从 kernel flatten convolution 开始<a class="headerlink" href="#kernel-flatten-convolution" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-6-1"><span class="c1"># step4 通过对kernel进行展开来实现二维卷积，并推导出转置卷积，不考虑batch、channel大小，不考虑padding，假设stride=1</span>
</span><span id="__span-6-2"><span class="k">def</span><span class="w"> </span><span class="nf">get_kernel_matrix</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="n">input_size</span><span class="p">):</span>
</span><span id="__span-6-3">    <span class="c1"># 基于kernel和输入特征图的大小来得到填充拉直后的kernel堆叠后的矩阵</span>
</span><span id="__span-6-4">    <span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-6-5">    <span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-6-6">    <span class="n">num_out_fea_map</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积公式</span>
</span><span id="__span-6-7">    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_out_fea_map</span><span class="p">,</span><span class="n">input_h</span><span class="o">*</span><span class="n">input_w</span><span class="p">))</span> <span class="c1">#初始化结果矩阵，输出特征图元素个数*输入特征图元素个数</span>
</span><span id="__span-6-8">    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-6-9">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-6-10">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-6-11">            <span class="c1"># 填充成 跟 输入特征图一样大小</span>
</span><span id="__span-6-12">            <span class="c1"># padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j))</span>
</span><span id="__span-6-13">            <span class="n">padded_kernel</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">kernel</span><span class="p">,(</span><span class="n">j</span><span class="p">,</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">-</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="o">-</span><span class="n">i</span><span class="p">))</span>
</span><span id="__span-6-14">            <span class="n">result</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="n">padded_kernel</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span id="__span-6-15">            <span class="n">count</span> <span class="o">+=</span><span class="mi">1</span>
</span><span id="__span-6-16">    <span class="k">return</span> <span class="n">result</span>  
</span><span id="__span-6-17">
</span><span id="__span-6-18">
</span><span id="__span-6-19">
</span><span id="__span-6-20"><span class="c1"># 测试1：验证 二维卷积</span>
</span><span id="__span-6-21"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span><span id="__span-6-22"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</span><span id="__span-6-23"><span class="n">kernel_matrix</span> <span class="o">=</span> <span class="n">get_kernel_matrix</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 4*16</span>
</span><span id="__span-6-24">
</span><span id="__span-6-25"><span class="c1"># 通过矩阵相乘来计算卷积</span>
</span><span id="__span-6-26"><span class="n">mm_conv2d_output</span> <span class="o">=</span> <span class="n">kernel_matrix</span> <span class="o">@</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>  
</span><span id="__span-6-27">
</span><span id="__span-6-28"><span class="c1"># pytorch conv2d API</span>
</span><span id="__span-6-29"><span class="n">pytorch_conv2d_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="n">kernel</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-6-30"><span class="nb">print</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
</span><span id="__span-6-31"><span class="nb">print</span><span class="p">(</span><span class="n">kernel_matrix</span><span class="p">)</span>
</span><span id="__span-6-32"><span class="nb">print</span><span class="p">(</span><span class="n">mm_conv2d_output</span><span class="p">)</span>
</span><span id="__span-6-33"><span class="nb">print</span><span class="p">(</span><span class="n">pytorch_conv2d_output</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-7-1"><span class="n">kernel</span>
</span><span id="__span-7-2"><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.3170</span><span class="p">,</span>  <span class="mf">2.4005</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2991</span><span class="p">],</span>
</span><span id="__span-7-3">        <span class="p">[</span> <span class="mf">1.1566</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3610</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7246</span><span class="p">],</span>
</span><span id="__span-7-4">        <span class="p">[</span><span class="o">-</span><span class="mf">0.5764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7988</span><span class="p">,</span>  <span class="mf">1.5611</span><span class="p">]])</span>
</span><span id="__span-7-5"><span class="n">kernel_matrix</span>
</span><span id="__span-7-6"><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.3170</span><span class="p">,</span>  <span class="mf">2.4005</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2991</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">1.1566</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3610</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7246</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
</span><span id="__span-7-7">         <span class="o">-</span><span class="mf">0.5764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7988</span><span class="p">,</span>  <span class="mf">1.5611</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
</span><span id="__span-7-8">        <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.3170</span><span class="p">,</span>  <span class="mf">2.4005</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2991</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">1.1566</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3610</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7246</span><span class="p">,</span>
</span><span id="__span-7-9">          <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7988</span><span class="p">,</span>  <span class="mf">1.5611</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
</span><span id="__span-7-10">        <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.3170</span><span class="p">,</span>  <span class="mf">2.4005</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2991</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>
</span><span id="__span-7-11">          <span class="mf">1.1566</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3610</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7246</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7988</span><span class="p">,</span>  <span class="mf">1.5611</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
</span><span id="__span-7-12">        <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.3170</span><span class="p">,</span>  <span class="mf">2.4005</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2991</span><span class="p">,</span>
</span><span id="__span-7-13">          <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">1.1566</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3610</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7246</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7988</span><span class="p">,</span>  <span class="mf">1.5611</span><span class="p">]])</span>
</span><span id="__span-7-14"><span class="n">mm_conv2d_output</span>
</span><span id="__span-7-15"><span class="n">tensor</span><span class="p">([[</span> <span class="mf">5.3770</span><span class="p">],</span>
</span><span id="__span-7-16">        <span class="p">[</span><span class="o">-</span><span class="mf">2.0131</span><span class="p">],</span>
</span><span id="__span-7-17">        <span class="p">[</span><span class="o">-</span><span class="mf">5.9471</span><span class="p">],</span>
</span><span id="__span-7-18">        <span class="p">[</span><span class="o">-</span><span class="mf">2.7944</span><span class="p">]])</span>
</span><span id="__span-7-19"><span class="n">pytorch_conv2d_output</span>
</span><span id="__span-7-20"><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">5.3770</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0131</span><span class="p">],</span>
</span><span id="__span-7-21">          <span class="p">[</span><span class="o">-</span><span class="mf">5.9471</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7944</span><span class="p">]]]])</span>
</span></code></pre></div></td></tr></table></div>
<h3 id="_4">转置卷积<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<ul>
<li>输入：4×4，kernel：3×3，output：2×2   <ul>
<li>flatten input feature map region：4×9  @ 9×1 = 4×1</li>
<li>padding &amp; flatten kernel ：4×16 @ 16×1 = 4×1</li>
</ul>
</li>
<li>转置卷积：   <ul>
<li>16×4 @ 4×1 = 16×1  $ reshape \rightarrow $ 4 × 4 </li>
</ul>
</li>
</ul>
<blockquote>
<p>转置卷积是怎么做的呢？</p>
<p>其实做法很简单，就是把kernel matrix 首先转置一下；比方说本来是4×16的 矩阵；我们转置一下；转置成16×4的矩阵；</p>
<p>然后我们也讲了output是一个2×2的 矩阵，我们也把它拉直一下，变成4×1的矩阵；于是16×4的矩阵，跟4×1的矩阵，相乘，就变成了一个16×1的矩阵，我们在reshape一下，就变成了4×4，这样我们就把一个 2×2的特征图，变成了一个4×4的特征图；这是从原理上的解释</p>
<p>另外还有一种，我们这里实现了二维卷积，就类似于 y=wx(w乘以x这样的一个过程)；</p>
<p>w跟x之间 是一个矩阵乘法；然后我们求后向梯度的时候，偏y，偏x，刚好就是w的一个转置，所以说在pytorch中，实现转置卷积 或者叫 deconvolution 或者叫transpose convolution，都是基于后向传播 来实现的；</p>
<p>y=wx</p>
<p>dy dx就等于w的转置</p>
<p>这个就是转置卷积的原理部分</p>
</blockquote>
<ul>
<li>三点需要特别注意：</li>
</ul>
<blockquote>
<p>第一点</p>
<blockquote>
<p>转置卷积一般用在上采样的过程；因为普通的卷积会用在下采样，比方说这里的例子，把4×4的特征图，通过卷积变成了一个2×2的，这是常规的操作，这是下采样</p>
<p>那有时候，在生成的模型中，我们可能需要，输入是2×2的，输出变成4×4的，这个时候，我们可以用转置卷积实现，这是第一点；
</p>
</blockquote>
<p>第二点</p>
<blockquote>
<p>转置卷积 或者 后向 卷积 梯度；意思就是说 我们通过后向传播 来实现转置卷积的</p>
</blockquote>
<p>第三点</p>
<blockquote>
<p>转置卷积也可以通过 填充的方式来实现，什么意思呢？就是可以把2×2的输入 填充到6×6的大小；然后再去用3×3的kernel 进行一个卷积；也能实现一个上采样的效果；但这种方法并不是框架中使用的方法；框架中的实现 是通过 后向传播的方法来实现 转置卷积的；</p>
</blockquote>
</blockquote>
<p>代码实现：</p>
<blockquote>
<p>首先对kernel matrix进行一个转置，transpose，-1维，-2维转置一下</p>
<p>kernel_matrix.transpose(-1,-2)，这样得到w的一个转置，我们再把这个转置跟上面这个output <code>mm_conv2d_output</code> 进行一个矩阵相乘操作</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-8-1"><span class="n">kernel_matrix</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="n">mm_conv2d_output</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>
<p>mm_conv2d_output 是一个 4×1 的矩阵，前面转置后是一个 16×4的，得到一个 16×1的结果 </p>
</li>
<li>
<p>定义为 mm_transposed_conv2d_output  </p>
</li>
<li>
<p>这个就是通过矩阵相乘 得到的转置卷积，也叫做反卷积； </p>
</li>
<li>
<p>这个反卷积 或者叫 转置卷积，并不是一个可逆的，不是一个逆计算，这里的output并不是当初的input，只是形状跟input一样而已</p>
</li>
</ul>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-9-1"><span class="n">mm_transposed_conv2d_output</span> <span class="o">=</span> <span class="n">kernel_matrix</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="n">mm_conv2d_output</span>
</span></code></pre></div></td></tr></table></div>
<p>以上是矩阵乘积得到转置卷积的；</p>
</blockquote>
<p>为了验证，我们可以调用pytorch转置卷积的api</p>
<p><img alt="image-20241125174156737" src="../images/image-20241125174156737.png" /></p>
<ol>
<li>类形式</li>
<li>函数形式</li>
</ol>
<p><img alt="image-20241125174308099" src="../images/image-20241125174308099.png" /></p>
<ul>
<li>实例化class，调用的还是函数形式；现在我们来调用一下这个函数</li>
<li>就是F.conv_transpose2d()一样的，首先传入上面的output，就是把上面的pytorch_conv2d_output作为输入，kernel也要传进去，kernel就是之前写的kernel，同样也要对它进行两次的unsqueeze操作（batch size × channel × height × width），这样得到pytorch_transposed_conv2d_output API</li>
</ul>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-10-1"><span class="c1"># 测试2  通过矩阵成绩来计算转置卷积</span>
</span><span id="__span-10-2"><span class="n">mm_transposed_conv2d_output</span> <span class="o">=</span> <span class="n">kernel_matrix</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="n">mm_conv2d_output</span>
</span><span id="__span-10-3"><span class="n">pytorch_transposed_conv2d_conv2d</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">pytorch_conv2d_output</span><span class="p">,</span><span class="n">kernel</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1">#API</span>
</span><span id="__span-10-4"><span class="nb">print</span><span class="p">(</span><span class="n">mm_transposed_conv2d_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span><span id="__span-10-5"><span class="nb">print</span><span class="p">(</span><span class="n">pytorch_transposed_conv2d_conv2d</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p><strong><u>关于转置卷积要说明的：</u></strong></p>
<ul>
<li>我们把卷积看成是 填充后的kernel跟input，得到 kernel_matrix之后，再把kernel matrix转置一下，跟convolution output进行矩阵相乘，这样得到了一个新的output，刚好output的大小和input的大小是一样的；成功实现了上采样，因为mm conv2d output是 2×2的，左边mm transposed conv2d output是 4×4的，我们就实现了上采样；</li>
<li>F.conv_transposed2d()的输入，就是普通卷积的输出，kernel还是那个kernel，把它扩充一下</li>
</ul>
<p><strong><u>关于上采样的两个角度：</u></strong> </p>
<ul>
<li><mark>（第一种实现：把kernel转置 16 × 4  $ \rightarrow $ 4×16 ）</mark>  首先要把普通卷积的kernel matrix写出来，然后再把matrix转置一下，再跟普通卷积的输出 相乘一下；就实现了</li>
<li><mark>（第二种实现：把input变大）</mark>   直接把input进行填充；比如现在input是2×2，我们为了实现4×4，为了用普通的卷积，我们可以把2×2的填充成5×5 或者 6×6的；假如说是6×6的，我们就把上下左右 填充两行0就好了，再用普通卷积实现 也是可以的；因为反正参数都是要学习的，我们的目的就是做上采样；无论是从后向传播的角度，还是直接对input进行填充，把input变大，都能实现 上采样，不过数值是不一样的，不过没关系，反正都是要学习的</li>
</ul>
<p>转置卷积 反卷积=transpose conv2d</p>
<h2 id="4">4 膨胀卷积 &amp; 空洞卷积<a class="headerlink" href="#4" title="Permanent link">&para;</a></h2>
<p>intro，官方api：</p>
<p><img alt="image-20241125180526663" src="../images/image-20241125180526663.png" /></p>
<p>在默认的api中 dilation的值等于1，groups的值 也是等于1 的，也就是我们常用的卷积都没有指定，常用的值都为1</p>
<p>什么是dilation？</p>
<blockquote>
<p>dilation的意思就是说，我们普通的卷积，比如说3×3的卷积核，在一个输入特征图上 进行 卷积的话，我们每次，从输入特征图上取一块 3×3的 面积，取9个元素，并且这9个元素，都是紧挨着彼此的，就是3×3的区域，一个方形区域，这种情况，我们成为dilation=1，也就是说彼此之间间隔为1，可以这么理解，彼此的索引，差距为1，比方说第一个元素 索引为1，第二个元素 索引 就是2，那如果dilation不是等于1，而是2的话呢，说明第一个元素和第二个元素 索引相差了2，那就说明 中间还多了一个元素；</p>
<p>也就是说 dilation 是控制着我们输入 特征图 要取得那部分面积 是否是紧凑的，如果它的值大于1的话，它就不是紧凑的，它中间是有一些，跳过的元素的；</p>
</blockquote>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-11-1"><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p><img alt="image-20241125180705640" src="../images/image-20241125180705640.png" /></p>
<ul>
<li>dilation=2  a[0:5:2,0:5:2] 索引0到索引5，跳过一个取一个，最后一个取不到</li>
<li>dilation=3 用索引表示的话 就是 0到7，然后间隔是3；a[0:7:3,0:7:3] # dilation=3 同样列数也是一样的 0到7 间隔是3；索引间隔为3</li>
</ul>
<p><img alt="image-20241125180926621" src="../images/image-20241125180926621.png" /></p>
<p>一句话说清dilation是什么？卷积的覆盖区域 索引间隔多少</p>
<blockquote>
<p>如果 input size=7×7 kernel size=3×3，dilation=3，我们只需要 取一次就好了；</p>
<p>取一次 就刚好 已经到 边界了</p>
<p>所以7×7的input 跟 3×3的kernel 进行 卷积的话，我们不做padding stride=1的话，那么输出就是一个数，就是一个标量；这就是dilation 取 不同值 具体的运算规则</p>
<p>那为什么要用dilation大于1的这些情况呢？就是因为我们 增大dilation 但是并没有增大运算量；我们还是3×3的矩阵，跟3×3的矩阵 进行元素相乘；并没有因为 感受野变大 计算量 变大；所以一般 增大 dilation的目的 就是我们在 保持运算量不变的前提下，希望 增大 感受野的面积；这就是dilation</p>
</blockquote>
<p>一句话为什么dilation：在不增加运算量的情况下，增大感受野</p>
<h2 id="5">5 分组卷积 &amp; 群卷积<a class="headerlink" href="#5" title="Permanent link">&para;</a></h2>
<h3 id="_5">什么是分组卷积？<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<blockquote>
<p>分组卷积 group convolution；是对输入通道进行分组；输出通道并不是由所有的输入通道共同作用的；会有一种情况，比如输入通道是4，输出通道是2，输出通道的第一个通道只跟输入通道的第1、3个通道有关；输出通道的第二个通道只跟输入通道的第2、4个通道有关；如果输入通道有这样的关系时，我们可以采用分组卷积，设置组数group=2，这时有几个组就会有几个输出通道；这种情况是我们对每个组进行一次卷积，如果我们对每个组进行多次卷积，那么卷积核的个数就会增加了；这样也有一个问题，就是输入特征图的通道之间没有交互，所以这种情况下，在后面的卷积过程中，会有通道之间的随机混合或者用1×1的卷积；poinwise convolution；</p>
</blockquote>
<h3 id="depthwise-pointwise">补充深度可分离卷积 depthwise &amp; pointwise：<a class="headerlink" href="#depthwise-pointwise" title="Permanent link">&para;</a></h3>
<blockquote>
<p>深度可分离卷积，是特殊的分组卷积，有几个输入通道，就分成几个组，输入通道之间完全相互独立，deepwise convolution；这种情况下，后面通常会跟着 pointwise  convolution；</p>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/80041030">深度可分离卷积 &amp; 1×1卷积</a></p>
<p><a href="https://yinguobing.com/separable-convolution/#fn2">卷积神经网络中的Separable Convolution</a></p>
<p>一张图看懂深度可分离卷积：</p>
<p><img alt="image-20241125200302038" src="../images/image-20241125200302038.png" /></p>
<p>Depthwise Convolution完成后的Feature map数量与输入层的depth相同，但是这种运算对输入层的每个channel独立进行卷积运算后就结束了，没有有效的利用不同map在相同空间位置上的信息。因此需要增加另外一步操作来将这些map进行组合生成新的Feature map，即接下来的Pointwise Convolution。（<a href="https://yinguobing.com/separable-convolution/#fn2">摘自</a>）</p>
<p>一张图看懂1×1卷积：</p>
<p><img alt="image-20241125200417890" src="../images/image-20241125200417890.png" /></p>
<p>Pointwise Convolution的运算与常规卷积运算非常相似，不同之处在于卷积核的尺寸为 1×1×M，M为上一层的depth。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个Filter就有几个Feature map。（<a href="https://yinguobing.com/separable-convolution/#fn2">摘自</a>）</p>
<p>补充普通卷积：</p>
<p><img alt="image-20241125201354298" src="../images/image-20241125201354298.png" /></p>
<h3 id="_6">为什么需要分组卷积？<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<p>归纳偏置：</p>
<p>每一个模型 都有自己的假设，或者叫 归纳偏置 inductive bias；</p>
<ul>
<li>CNN的归纳偏置就是 局部建模性 和 平移不变性</li>
<li>RNN就是前后关联性</li>
<li>Transformer没有什么假设，只是引入了一个position embedding而已</li>
</ul>
<p>在我们这里引入的 group&gt;1的话，引入的假设是什么呢？</p>
<blockquote>
<p>我们只需要一小部分，只需要做一小部分 通道之间的建模就好了，不需要考虑 每个通道 跟所有通道的 关系；其实本质上 group=1的话，就是说 in channel，每个通道 都需要 跟 其他 通道 进行一个混合；但是当我们把 groups，设置成&gt;1的话，就是把它们分组来考虑，就是每次呢，只在几个通道做一下卷积；然后下次 再另外的通道 做卷积；然后把结果拼起来 就好了；也就是说 通道融合 并不充分；简单说 就是 这样的</p>
<p>再重复：groups&gt;1，就是说 通道融合 不需要 完全 充分，我们只需要在一个个group内进行融合，最后拼接，这就是group convolution 引入的一个偏置</p>
<p>其实这个偏置也很好解决，我们只需要在group convolution后面，再加上一个 1×1 point wise卷积就好了
</p>
<p>就是说 1×1的逐点卷积，虽然没有考虑 局部建模，但是它能对通道之间 进行融合；所以最后 我们还是能够把 通道之间 进行融合的</p>
</blockquote>
<p>分组卷积 &amp; 逐点卷积</p>
<p><strong>add 各种wise</strong> </p>
<ul>
<li>我们再说一下 这里的wise，一旦看到各种 wise，就是说 我们只考虑wise前面这个东西；</li>
<li>比方说；point wise就是说 我们只对 一个点 去算 相乘，而不是说 像 普通的卷积一样，取一个3×3的区域；那就不是一个点；</li>
<li>还比如说 channel wise，我们只对一个通道；（有点像 深度可分离卷积）</li>
<li>比如说layer wise，我们只对一层考虑等等；</li>
<li>各种 wise，比如element wise 只对元素跟元素之间；相同位置的元素进行考虑；</li>
</ul>
<h3 id="_7">分组卷积中的变与不变<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h3>
<p>题设： in channel和out channel分别等于2和4，分析group=1和group=2</p>
<p><strong><u>case1 ：group=1，一共是8张kernel map，（4个卷积核，每个kernel通道数等于2）</u></strong> </p>
<blockquote>
<p>首先我们拿出两张kernel map 分别与input进行卷积，然后加起来，加起来的结果赋给第一个通道；再拿两个卷积核，同样跟输入的两个通道进行卷积，然后加起来，赋值给第二个通道，以此类推，直到我们拿出最后的两个卷积核 跟 输入两个特征图 进行卷积，然后再求和 赋值给 最后一个通道；所以一共是8张kernel map</p>
</blockquote>
<p><u><strong>case2 ：groups=2，一共是4张kernel map，（4个卷积核，每个kernel的通道数=1）</strong></u> </p>
<blockquote>
<p>：in channels=2，groups=2，如果还让output channel=4，那么kernel map有几张？卷积核有几个？</p>
<p>首先，<code>#卷积核</code>   $ \stackrel{决定}{\rightarrow} $ <code>#输出通道数</code>  、<code>#输入通道数</code>   $ \stackrel{决定}{\rightarrow} $  <code>#单个卷积核通道数</code></p>
<p>∴ 有4个卷积核，每个卷积核的channels=1，（∵把输入通道数分成2组，所以输入通道数变成 2÷2=1 ）</p>
<p>∴有4张kernel map
</p>
</blockquote>
<p><u><strong>综上：</strong></u> </p>
<ol>
<li>kernel map减少一半 || 在每一组中，其实有两个卷积核，所以两组 一共是 4个 kernel map，相比上面 8个kernel map 就少了一半（kernel map、参数量、运算量减半）</li>
<li>输出特征图的高度 &amp; 宽度 不变，batch size不变</li>
</ol>
<h3 id="dilationgroups">代码实现 dilation&amp;groups 手撕 &amp; 库函数<a class="headerlink" href="#dilationgroups" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-12-1"><span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiplication_for_conv2d_finall</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-12-2">    <span class="k">if</span> <span class="n">padding</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
</span><span id="__span-12-3">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,(</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-12-4">
</span><span id="__span-12-5">    <span class="n">bs</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-12-6">    <span class="n">out_channel</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-12-7">
</span><span id="__span-12-8">    <span class="k">assert</span> <span class="n">out_channel</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">in_channel</span> <span class="o">%</span> <span class="n">groups</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="s2">&quot;groups必须要同时被输入通道和输出通道数整除！&quot;</span>
</span><span id="__span-12-9">    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span><span class="n">groups</span><span class="p">,</span><span class="n">in_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span><span class="p">))</span>
</span><span id="__span-12-10">    <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">groups</span><span class="p">,</span><span class="n">out_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">in_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span><span class="p">))</span>
</span><span id="__span-12-11">
</span><span id="__span-12-12">    <span class="n">kernel_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_h</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dilation</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">kernel_h</span>
</span><span id="__span-12-13">    <span class="n">kernel_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_w</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dilation</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">kernel_w</span>
</span><span id="__span-12-14">
</span><span id="__span-12-15">    <span class="n">output_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
</span><span id="__span-12-16">    <span class="n">output_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
</span><span id="__span-12-17">
</span><span id="__span-12-18">    <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">groups</span><span class="p">,</span><span class="n">out_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">)</span>
</span><span id="__span-12-19">    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>
</span><span id="__span-12-20">
</span><span id="__span-12-21">    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-12-22">        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_channel</span><span class="p">)</span>
</span><span id="__span-12-23">
</span><span id="__span-12-24">    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span> <span class="c1"># 对batch size进行遍历</span>
</span><span id="__span-12-25">        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">groups</span><span class="p">):</span> <span class="c1"># 对群组进行遍历</span>
</span><span id="__span-12-26">            <span class="k">for</span> <span class="n">oc</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">):</span> <span class="c1"># 对分组后的输出通道进行遍历</span>
</span><span id="__span-12-27">                <span class="k">for</span> <span class="n">ic</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">):</span> <span class="c1"># 对分组后的输入通道进行遍历</span>
</span><span id="__span-12-28">                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1">#对高度遍历</span>
</span><span id="__span-12-29">                        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1"># 对宽度遍历</span>
</span><span id="__span-12-30">                            <span class="n">region</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">ic</span><span class="p">,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">kernel_h</span><span class="p">:</span><span class="n">dilation</span><span class="p">,</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">kernel_w</span><span class="p">:</span><span class="n">dilation</span><span class="p">]</span> <span class="c1">#特征区域</span>
</span><span id="__span-12-31">                            <span class="n">output</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">oc</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">stride</span><span class="p">),</span><span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="o">/</span><span class="n">stride</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span><span class="o">*</span><span class="n">kernel</span><span class="p">[</span><span class="n">g</span><span class="p">,</span><span class="n">oc</span><span class="p">,</span><span class="n">ic</span><span class="p">])</span>
</span><span id="__span-12-32">
</span><span id="__span-12-33">                <span class="n">output</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">oc</span><span class="p">]</span> <span class="o">+=</span> <span class="n">bias</span><span class="p">[</span><span class="n">g</span><span class="o">*</span><span class="p">(</span><span class="n">out_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">)</span><span class="o">+</span><span class="n">oc</span><span class="p">]</span>  <span class="c1"># 考虑偏置项</span>
</span><span id="__span-12-34">    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span><span class="n">out_channel</span><span class="p">,</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">))</span>  <span class="c1"># 还原成四维张量</span>
</span><span id="__span-12-35">    <span class="k">return</span> <span class="n">output</span>
</span><span id="__span-12-36">
</span><span id="__span-12-37"><span class="c1"># 验证测试的代码</span>
</span><span id="__span-12-38"><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span>
</span><span id="__span-12-39"><span class="n">bs</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span>
</span><span id="__span-12-40"><span class="n">out_channel</span><span class="o">=</span><span class="mi">4</span>
</span><span id="__span-12-41"><span class="n">groups</span><span class="p">,</span><span class="n">dilation</span><span class="p">,</span><span class="n">stride</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span>
</span><span id="__span-12-42">
</span><span id="__span-12-43"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span><span class="p">)</span>
</span><span id="__span-12-44"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span><span class="n">in_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">)</span>
</span><span id="__span-12-45"><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_channel</span><span class="p">)</span>
</span><span id="__span-12-46">
</span><span id="__span-12-47"><span class="c1"># pytorch API的结果</span>
</span><span id="__span-12-48"><span class="n">pytorch_conv2d_api_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
</span><span id="__span-12-49">                                     <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
</span><span id="__span-12-50"><span class="n">mm_conv2d_finall_output</span> <span class="o">=</span> <span class="n">matrix_multiplication_for_conv2d_finall</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
</span><span id="__span-12-51">                                                                  <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
</span><span id="__span-12-52">
</span><span id="__span-12-53"><span class="n">flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">pytorch_conv2d_api_output</span><span class="p">,</span><span class="n">mm_conv2d_finall_output</span><span class="p">)</span>
</span><span id="__span-12-54"><span class="nb">print</span><span class="p">(</span><span class="n">flag</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="6">6 汇总代码<a class="headerlink" href="#6" title="Permanent link">&para;</a></h2>
<p>库函数实现卷积</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-13-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-13-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-13-3"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
</span><span id="__span-13-4"><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
</span><span id="__span-13-5">
</span><span id="__span-13-6"><span class="n">in_channels</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-13-7"><span class="n">out_channels</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-13-8"><span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">3</span>
</span><span id="__span-13-9"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-13-10"><span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="__span-13-11">
</span><span id="__span-13-12"><span class="n">input_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span><span class="n">in_channels</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
</span><span id="__span-13-13">
</span><span id="__span-13-14"><span class="c1"># 第一种实现</span>
</span><span id="__span-13-15"><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span><span class="n">out_channels</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
</span><span id="__span-13-16">
</span><span id="__span-13-17"><span class="n">input_feature_map</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
</span><span id="__span-13-18"><span class="n">out_feature_map</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">input_feature_map</span><span class="p">)</span>
</span><span id="__span-13-19"><span class="c1"># print(input_feature_map)</span>
</span><span id="__span-13-20"><span class="c1"># print(conv_layer.weight)  # 1*1*3*3=out_channels*in_channels*height*width</span>
</span><span id="__span-13-21">
</span><span id="__span-13-22"><span class="nb">print</span><span class="p">(</span><span class="n">out_feature_map</span><span class="p">)</span>
</span><span id="__span-13-23">
</span><span id="__span-13-24"><span class="n">out_feature_map1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">input_feature_map</span><span class="p">,</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span><span id="__span-13-25">
</span><span id="__span-13-26"><span class="nb">print</span><span class="p">(</span><span class="n">out_feature_map1</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>step1 用原始的矩阵运算来实现二维卷积，先不考虑 batch size维度 和 channel维度</p>
<p>step2 用原始的矩阵运算来实现二维卷积，先不考虑 batch size维度 和 channel维度，flatten版本</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-14-1"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 卷积 输入特征图</span>
</span><span id="__span-14-2"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># 卷积核</span>
</span><span id="__span-14-3"><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 卷积偏置，默认输出通道数目等于1</span>
</span><span id="__span-14-4">
</span><span id="__span-14-5"><span class="c1"># step1 用原始的矩阵运算来实现二维卷积，先不考虑 batch size维度 和 channel维度</span>
</span><span id="__span-14-6"><span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiplication_for_conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="__span-14-7">
</span><span id="__span-14-8">  <span class="k">if</span> <span class="n">padding</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
</span><span id="__span-14-9">    <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,(</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">))</span>
</span><span id="__span-14-10">
</span><span id="__span-14-11">
</span><span id="__span-14-12">  <span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-14-13">  <span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-14-14">
</span><span id="__span-14-15">  <span class="n">output_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的高度</span>
</span><span id="__span-14-16">  <span class="n">output_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的宽度 </span>
</span><span id="__span-14-17">  <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">)</span> <span class="c1"># 初始化 输出矩阵</span>
</span><span id="__span-14-18">
</span><span id="__span-14-19">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1"># 对高度进行遍历</span>
</span><span id="__span-14-20">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span>  <span class="c1"># 对宽度维进行遍历</span>
</span><span id="__span-14-21">      <span class="n">region</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">kernel_h</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">kernel_w</span><span class="p">]</span>  <span class="c1"># 取出被核滑动到的区域</span>
</span><span id="__span-14-22">      <span class="n">output</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">stride</span><span class="p">),</span><span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="o">/</span><span class="n">stride</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span> <span class="c1"># 点乘 并赋值给输出位置的元素 </span>
</span><span id="__span-14-23">
</span><span id="__span-14-24">  <span class="k">return</span> <span class="n">output</span>
</span><span id="__span-14-25">
</span><span id="__span-14-26">
</span><span id="__span-14-27"><span class="c1"># step2 用原始的矩阵运算来实现二维卷积，先不考虑 batch size维度 和 channel维度，flatten版本</span>
</span><span id="__span-14-28"><span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiplication_for_conv2d_flatten</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="__span-14-29">
</span><span id="__span-14-30">  <span class="k">if</span> <span class="n">padding</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
</span><span id="__span-14-31">    <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,(</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">))</span>
</span><span id="__span-14-32">
</span><span id="__span-14-33">
</span><span id="__span-14-34">  <span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-14-35">  <span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-14-36">
</span><span id="__span-14-37">  <span class="n">output_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的高度</span>
</span><span id="__span-14-38">  <span class="n">output_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的宽度 </span>
</span><span id="__span-14-39">  <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">)</span> <span class="c1"># 初始化 输出矩阵</span>
</span><span id="__span-14-40">
</span><span id="__span-14-41">  <span class="n">region_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span><span class="n">kernel</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span> <span class="c1">#存储着所有拉平后特征区域</span>
</span><span id="__span-14-42">  <span class="n">kernel_matrix</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">kernel</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 存储着kernel的 列向量（矩阵）形式</span>
</span><span id="__span-14-43">  <span class="n">row_index</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-14-44">
</span><span id="__span-14-45">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1"># 对高度进行遍历</span>
</span><span id="__span-14-46">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span>  <span class="c1"># 对宽度维进行遍历</span>
</span><span id="__span-14-47">      <span class="n">region</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">kernel_h</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">kernel_w</span><span class="p">]</span>  <span class="c1"># 取出被核滑动到的区域</span>
</span><span id="__span-14-48">      <span class="n">region_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">region</span><span class="p">)</span>
</span><span id="__span-14-49">      <span class="n">region_matrix</span><span class="p">[</span><span class="n">row_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">region_vector</span>
</span><span id="__span-14-50">      <span class="n">row_index</span> <span class="o">+=</span><span class="mi">1</span>
</span><span id="__span-14-51">
</span><span id="__span-14-52">  <span class="n">output_matrix</span> <span class="o">=</span> <span class="n">region_matrix</span> <span class="o">@</span> <span class="n">kernel_matrix</span>
</span><span id="__span-14-53">  <span class="n">output</span> <span class="o">=</span> <span class="n">output_matrix</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">))</span><span class="o">+</span><span class="n">bias</span>
</span><span id="__span-14-54">
</span><span id="__span-14-55">  <span class="k">return</span> <span class="n">output</span>
</span><span id="__span-14-56">
</span><span id="__span-14-57">
</span><span id="__span-14-58"><span class="c1"># 矩阵运算实现卷积的结果</span>
</span><span id="__span-14-59"><span class="n">mat_mul_conv_output</span> <span class="o">=</span> <span class="n">matrix_multiplication_for_conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-14-60"><span class="c1"># print(mat_mul_conv_output)</span>
</span><span id="__span-14-61">
</span><span id="__span-14-62"><span class="c1"># 调用pytorch api卷积的结果</span>
</span><span id="__span-14-63"><span class="n">pytorch_api_conv_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
</span><span id="__span-14-64">                                   <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
</span><span id="__span-14-65">                                   <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-14-66">
</span><span id="__span-14-67"><span class="c1"># 矩阵运算实现卷积的结果 flatten input版本</span>
</span><span id="__span-14-68"><span class="n">mat_mul_conv_output_flatten</span> <span class="o">=</span> <span class="n">matrix_multiplication_for_conv2d_flatten</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-14-69"><span class="c1"># 验证了 flatten版本卷积 与 pytorch 官方卷积的结果，正确</span>
</span><span id="__span-14-70"><span class="n">flag1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mat_mul_conv_output</span><span class="p">,</span><span class="n">pytorch_api_conv_output</span><span class="p">)</span>
</span><span id="__span-14-71"><span class="n">flag2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mat_mul_conv_output_flatten</span><span class="p">,</span><span class="n">pytorch_api_conv_output</span><span class="p">)</span>
</span><span id="__span-14-72"><span class="nb">print</span><span class="p">(</span><span class="n">flag1</span><span class="p">)</span>
</span><span id="__span-14-73"><span class="nb">print</span><span class="p">(</span><span class="n">flag2</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>step3 用原始的矩阵运算来实现二维卷积，考虑 batch size维度 和 channel维度</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-15-1"><span class="c1"># step3 用原始的矩阵运算来实现二维卷积，考虑 batch size维度 和 channel维度</span>
</span><span id="__span-15-2"><span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiplication_for_conv2d_full</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="__span-15-3">
</span><span id="__span-15-4">  <span class="c1"># input kernel 都是4维张量</span>
</span><span id="__span-15-5">  <span class="k">if</span> <span class="n">padding</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
</span><span id="__span-15-6">    <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,(</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-15-7">
</span><span id="__span-15-8">  <span class="n">bs</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-15-9">  <span class="n">out_channel</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-15-10">
</span><span id="__span-15-11">  <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-15-12">    <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_channel</span><span class="p">)</span>
</span><span id="__span-15-13">
</span><span id="__span-15-14">
</span><span id="__span-15-15">  <span class="n">output_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的高度</span>
</span><span id="__span-15-16">  <span class="n">output_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积输出的宽度 </span>
</span><span id="__span-15-17">  <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">out_channel</span><span class="p">,</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">)</span> <span class="c1"># 初始化 输出矩阵</span>
</span><span id="__span-15-18">
</span><span id="__span-15-19">
</span><span id="__span-15-20">  <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
</span><span id="__span-15-21">    <span class="k">for</span> <span class="n">oc</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_channel</span><span class="p">):</span>
</span><span id="__span-15-22">      <span class="k">for</span> <span class="n">ic</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_channel</span><span class="p">):</span>
</span><span id="__span-15-23">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span> <span class="o">-</span> <span class="n">kernel_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1"># 对高度进行遍历</span>
</span><span id="__span-15-24">          <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span>  <span class="c1"># 对宽度维进行遍历</span>
</span><span id="__span-15-25">            <span class="n">region</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">ic</span><span class="p">,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">kernel_h</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">kernel_w</span><span class="p">]</span>  <span class="c1"># 取出被核滑动到的区域</span>
</span><span id="__span-15-26">            <span class="n">output</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">oc</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">stride</span><span class="p">),</span><span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="o">/</span><span class="n">stride</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">[</span><span class="n">oc</span><span class="p">,</span><span class="n">ic</span><span class="p">])</span> <span class="c1"># 点乘 并赋值给输出位置的元素 </span>
</span><span id="__span-15-27">      <span class="n">output</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">oc</span><span class="p">]</span> <span class="o">+=</span> <span class="n">bias</span><span class="p">[</span><span class="n">oc</span><span class="p">]</span>
</span><span id="__span-15-28">  <span class="k">return</span> <span class="n">output</span>
</span><span id="__span-15-29">
</span><span id="__span-15-30"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># bs*in_channel*in_h*in_w</span>
</span><span id="__span-15-31"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># out_channel*in_channel*kernel_h*kernel_w</span>
</span><span id="__span-15-32"><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</span><span id="__span-15-33">
</span><span id="__span-15-34"><span class="c1"># 验证matrxi_multiplication_for_conv2d_full与官方API结果是否一致</span>
</span><span id="__span-15-35"><span class="n">pytorch_api_conv_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-15-36"><span class="n">mm_conv2d_full_output</span> <span class="o">=</span> <span class="n">matrix_multiplication_for_conv2d_full</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-15-37"><span class="n">flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">pytorch_api_conv_output</span><span class="p">,</span><span class="n">mm_conv2d_full_output</span><span class="p">)</span>
</span><span id="__span-15-38"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;all close:&quot;</span><span class="p">,</span><span class="n">flag</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>step4 通过对kernel进行展开来实现二维卷积，并推导出转置卷积，不考虑batch、channel大小，不考虑padding，假设stride=1</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-16-1"><span class="c1"># step4 通过对kernel进行展开来实现二维卷积，并推导出转置卷积，不考虑batch、channel大小，不考虑padding，假设stride=1</span>
</span><span id="__span-16-2"><span class="k">def</span><span class="w"> </span><span class="nf">get_kernel_matrix</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="n">input_size</span><span class="p">):</span>
</span><span id="__span-16-3">    <span class="c1"># 基于kernel和输入特征图的大小来得到填充拉直后的kernel堆叠后的矩阵</span>
</span><span id="__span-16-4">    <span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-16-5">    <span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-16-6">    <span class="n">num_out_fea_map</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 卷积公式</span>
</span><span id="__span-16-7">    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_out_fea_map</span><span class="p">,</span><span class="n">input_h</span><span class="o">*</span><span class="n">input_w</span><span class="p">))</span> <span class="c1">#初始化结果矩阵，输出特征图元素个数*输入特征图元素个数</span>
</span><span id="__span-16-8">    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-16-9">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-16-10">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span> <span class="o">-</span> <span class="n">kernel_w</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-16-11">            <span class="c1"># 填充成 跟 输入特征图一样大小</span>
</span><span id="__span-16-12">            <span class="c1"># padded_kernel = F.pad(kernel,(i,input_h-kernel_h-i,j,input_w-kernel_w-j))</span>
</span><span id="__span-16-13">            <span class="n">padded_kernel</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">kernel</span><span class="p">,(</span><span class="n">j</span><span class="p">,</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">-</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="o">-</span><span class="n">i</span><span class="p">))</span>
</span><span id="__span-16-14">            <span class="n">result</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="n">padded_kernel</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span id="__span-16-15">            <span class="n">count</span> <span class="o">+=</span><span class="mi">1</span>
</span><span id="__span-16-16">    <span class="k">return</span> <span class="n">result</span>  
</span><span id="__span-16-17">
</span><span id="__span-16-18">
</span><span id="__span-16-19">
</span><span id="__span-16-20"><span class="c1"># 测试1：验证 二维卷积</span>
</span><span id="__span-16-21"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span><span id="__span-16-22"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</span><span id="__span-16-23"><span class="n">kernel_matrix</span> <span class="o">=</span> <span class="n">get_kernel_matrix</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 4*16</span>
</span><span id="__span-16-24">
</span><span id="__span-16-25"><span class="c1"># 通过矩阵相乘来计算卷积</span>
</span><span id="__span-16-26"><span class="n">mm_conv2d_output</span> <span class="o">=</span> <span class="n">kernel_matrix</span> <span class="o">@</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>  
</span><span id="__span-16-27">
</span><span id="__span-16-28"><span class="c1"># pytorch conv2d API</span>
</span><span id="__span-16-29"><span class="n">pytorch_conv2d_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="n">kernel</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-16-30"><span class="c1"># print(kernel)</span>
</span><span id="__span-16-31"><span class="c1"># print(kernel_matrix)</span>
</span><span id="__span-16-32"><span class="c1"># print(mm_conv2d_output)</span>
</span><span id="__span-16-33"><span class="c1"># print(pytorch_conv2d_output)</span>
</span><span id="__span-16-34">
</span><span id="__span-16-35"><span class="c1"># 测试2  通过矩阵乘积来计算转置卷积 || 验证二维转置卷积</span>
</span><span id="__span-16-36"><span class="n">mm_transposed_conv2d_output</span> <span class="o">=</span> <span class="n">kernel_matrix</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="n">mm_conv2d_output</span>
</span><span id="__span-16-37"><span class="n">pytorch_transposed_conv2d_conv2d</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">pytorch_conv2d_output</span><span class="p">,</span><span class="n">kernel</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1">#API</span>
</span><span id="__span-16-38"><span class="nb">print</span><span class="p">(</span><span class="n">mm_transposed_conv2d_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span><span id="__span-16-39"><span class="nb">print</span><span class="p">(</span><span class="n">pytorch_transposed_conv2d_conv2d</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>分组卷积&amp;膨胀卷积</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-17-1"><span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiplication_for_conv2d_finall</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-17-2">                                            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-17-3">    <span class="k">if</span> <span class="n">padding</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
</span><span id="__span-17-4">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,(</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="n">padding</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-17-5">
</span><span id="__span-17-6">    <span class="n">bs</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-17-7">    <span class="n">out_channel</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-17-8">
</span><span id="__span-17-9">    <span class="k">assert</span> <span class="n">out_channel</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">in_channel</span> <span class="o">%</span> <span class="n">groups</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="s2">&quot;groups必须要同时被输入通道和输出通道数整除！&quot;</span>
</span><span id="__span-17-10">    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span><span class="n">groups</span><span class="p">,</span><span class="n">in_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span><span class="p">))</span>
</span><span id="__span-17-11">    <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">groups</span><span class="p">,</span><span class="n">out_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">in_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">kernel_h</span><span class="p">,</span><span class="n">kernel_w</span><span class="p">))</span>
</span><span id="__span-17-12">
</span><span id="__span-17-13">    <span class="n">kernel_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_h</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dilation</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">kernel_h</span>
</span><span id="__span-17-14">    <span class="n">kernel_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_w</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dilation</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">kernel_w</span>
</span><span id="__span-17-15">
</span><span id="__span-17-16">    <span class="n">output_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
</span><span id="__span-17-17">    <span class="n">output_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
</span><span id="__span-17-18">
</span><span id="__span-17-19">    <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">groups</span><span class="p">,</span><span class="n">out_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">)</span>
</span><span id="__span-17-20">    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>
</span><span id="__span-17-21">
</span><span id="__span-17-22">    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-17-23">        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_channel</span><span class="p">)</span>
</span><span id="__span-17-24">
</span><span id="__span-17-25">    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span> <span class="c1"># 对batch size进行遍历</span>
</span><span id="__span-17-26">        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">groups</span><span class="p">):</span> <span class="c1"># 对群组进行遍历</span>
</span><span id="__span-17-27">            <span class="k">for</span> <span class="n">oc</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">):</span> <span class="c1"># 对分组后的输出通道进行遍历</span>
</span><span id="__span-17-28">                <span class="k">for</span> <span class="n">ic</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">):</span> <span class="c1"># 对分组后的输入通道进行遍历</span>
</span><span id="__span-17-29">                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_h</span><span class="o">-</span><span class="n">kernel_h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1">#对高度遍历</span>
</span><span id="__span-17-30">                        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">input_w</span><span class="o">-</span><span class="n">kernel_w</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stride</span><span class="p">):</span> <span class="c1"># 对宽度遍历</span>
</span><span id="__span-17-31">                            <span class="n">region</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">ic</span><span class="p">,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">kernel_h</span><span class="p">:</span><span class="n">dilation</span><span class="p">,</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">kernel_w</span><span class="p">:</span><span class="n">dilation</span><span class="p">]</span> <span class="c1">#特征区域</span>
</span><span id="__span-17-32">                            <span class="n">output</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">oc</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">stride</span><span class="p">),</span><span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="o">/</span><span class="n">stride</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span><span class="o">*</span><span class="n">kernel</span><span class="p">[</span><span class="n">g</span><span class="p">,</span><span class="n">oc</span><span class="p">,</span><span class="n">ic</span><span class="p">])</span>
</span><span id="__span-17-33">
</span><span id="__span-17-34">                <span class="n">output</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">oc</span><span class="p">]</span> <span class="o">+=</span> <span class="n">bias</span><span class="p">[</span><span class="n">g</span><span class="o">*</span><span class="p">(</span><span class="n">out_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">)</span><span class="o">+</span><span class="n">oc</span><span class="p">]</span>  <span class="c1"># 考虑偏置项</span>
</span><span id="__span-17-35">    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span><span class="n">out_channel</span><span class="p">,</span><span class="n">output_h</span><span class="p">,</span><span class="n">output_w</span><span class="p">))</span>  <span class="c1"># 还原成四维张量</span>
</span><span id="__span-17-36">    <span class="k">return</span> <span class="n">output</span>
</span><span id="__span-17-37">
</span><span id="__span-17-38"><span class="c1"># 验证测试的代码</span>
</span><span id="__span-17-39"><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span>
</span><span id="__span-17-40"><span class="n">bs</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span>
</span><span id="__span-17-41"><span class="n">out_channel</span><span class="o">=</span><span class="mi">4</span>
</span><span id="__span-17-42"><span class="n">groups</span><span class="p">,</span><span class="n">dilation</span><span class="p">,</span><span class="n">stride</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span>
</span><span id="__span-17-43">
</span><span id="__span-17-44"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">in_channel</span><span class="p">,</span><span class="n">input_h</span><span class="p">,</span><span class="n">input_w</span><span class="p">)</span>
</span><span id="__span-17-45"><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span><span class="n">in_channel</span><span class="o">//</span><span class="n">groups</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">)</span>
</span><span id="__span-17-46"><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_channel</span><span class="p">)</span>
</span><span id="__span-17-47">
</span><span id="__span-17-48"><span class="c1"># pytorch API的结果</span>
</span><span id="__span-17-49"><span class="n">pytorch_conv2d_api_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
</span><span id="__span-17-50">                                     <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
</span><span id="__span-17-51"><span class="n">mm_conv2d_finall_output</span> <span class="o">=</span> <span class="n">matrix_multiplication_for_conv2d_finall</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
</span><span id="__span-17-52">                                                                  <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
</span><span id="__span-17-53">
</span><span id="__span-17-54"><span class="n">flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">pytorch_conv2d_api_output</span><span class="p">,</span><span class="n">mm_conv2d_finall_output</span><span class="p">)</span>
</span><span id="__span-17-55"><span class="nb">print</span><span class="p">(</span><span class="n">flag</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="7-1d">7 1D 卷积<a class="headerlink" href="#7-1d" title="Permanent link">&para;</a></h2>
<p><img alt="image-20250216193620221" src="../images/image-20250216193620221.png" /></p>
<p><img alt="image-20250216193732681" src="../images/image-20250216193732681.png" /></p>
<h2 id="8">8 深度可分离卷积<a class="headerlink" href="#8" title="Permanent link">&para;</a></h2>
<p>2025.2.20</p>
<p>深度可分离卷积（Depthwise Separable Convolution）是一种高效的卷积操作，它将标准卷积分解为两个更简单的操作：深度卷积（Depthwise Convolution）和逐点卷积（Pointwise Convolution）。</p>
<p><strong>深度可分离卷积的定义</strong></p>
<p><strong>深度卷积（Depthwise Convolution）</strong>：</p>
<ul>
<li>对每个输入通道分别进行卷积操作，而不是对所有通道进行卷积。</li>
<li>这意味着每个卷积核只作用于一个输入通道，输出的通道数与输入的通道数相同。</li>
</ul>
<p><strong>逐点卷积（Pointwise Convolution）</strong>：</p>
<ul>
<li>使用 <code>1x1</code> 卷积核对深度卷积的输出进行卷积操作。</li>
<li>逐点卷积用于将不同通道的信息进行线性组合，从而生成新的输出通道。</li>
</ul>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-18-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-18-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-18-3">
</span><span id="__span-18-4"><span class="k">class</span><span class="w"> </span><span class="nc">DepthwiseSeparableConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-18-5">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-18-6">        <span class="nb">super</span><span class="p">(</span><span class="n">DepthwiseSeparableConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-18-7">        <span class="c1"># 深度卷积</span>
</span><span id="__span-18-8">        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> 
</span><span id="__span-18-9">                                   <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">in_channels</span><span class="p">)</span>
</span><span id="__span-18-10">        <span class="c1"># 逐点卷积</span>
</span><span id="__span-18-11">        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-18-12">
</span><span id="__span-18-13">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-18-14">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-18-15">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-18-16">        <span class="k">return</span> <span class="n">x</span>
</span><span id="__span-18-17">
</span><span id="__span-18-18"><span class="c1"># 示例输入</span>
</span><span id="__span-18-19"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>  <span class="c1"># (batch_size, in_channels, height, width)</span>
</span><span id="__span-18-20">
</span><span id="__span-18-21"><span class="c1"># 实例化深度可分离卷积</span>
</span><span id="__span-18-22"><span class="n">model</span> <span class="o">=</span> <span class="n">DepthwiseSeparableConv</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</span><span id="__span-18-23">
</span><span id="__span-18-24"><span class="c1"># 前向传播</span>
</span><span id="__span-18-25"><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-18-26"><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 输出形状应为 (1, 128, 32, 32)</span>
</span></code></pre></div></td></tr></table></div>
<p>深度卷积：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-19-1"><span class="bp">self</span><span class="o">.</span><span class="n">depthwise</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> 
</span><span id="__span-19-2">                           <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">in_channels</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>groups=in_channels表示每个输入通道都有一个独立的卷积核。</li>
<li>这一步的输出通道数与输入通道数相同。</li>
</ul>
<p><strong>逐点卷积：</strong></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-20-1"><span class="bp">self</span><span class="o">.</span><span class="n">pointwise</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>使用 <code>1x1</code> 卷积核将深度卷积的输出通道数转换为所需的输出通道数。</p>
<p>前向传播：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-21-1"><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-21-2">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-21-3">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-21-4">    <span class="k">return</span> <span class="n">x</span>
</span></code></pre></div></td></tr></table></div>
<p>先进行深度卷积，再进行逐点卷积。</p>
<p>深度可分离卷积广泛应用于轻量级神经网络架构中，如 MobileNet 和 Xception，用于减少计算量和参数量，同时保持较好的性能。</p>
<p><img alt="image-20250220192443698" src="../images/image-20250220192443698.png" /></p>
<h2 id="_8">卷积过后输出特征图的大小<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h2>
<p>分组其实不影响输出特征图的大小，会影响卷积核的通道数，也不影响卷积核的个数，会影响卷积的参数量，因为通道变少了</p>
<p>正常卷积：</p>
<p><span class="arithmatex">\(output_h = \frac{h-k+2p+s}{s}\)</span></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-22-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-22-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-22-3">
</span><span id="__span-22-4"><span class="c1"># 定义分组卷积</span>
</span><span id="__span-22-5"><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">5</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-22-6">
</span><span id="__span-22-7"><span class="c1"># 示例输入</span>
</span><span id="__span-22-8"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
</span><span id="__span-22-9">
</span><span id="__span-22-10"><span class="c1"># 前向传播</span>
</span><span id="__span-22-11"><span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-22-12">
</span><span id="__span-22-13"><span class="c1"># 打印输出特征图的大小</span>
</span><span id="__span-22-14"><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 输出形状应为 (1, 64, 4, 4)</span>
</span></code></pre></div></td></tr></table></div>
<p><span class="arithmatex">\(output_h = \frac{input_h-k+s+2p}{s} =\frac{7-5+2+2*p}{2}=\frac{7-5+2+2*2}{2}=4\)</span> </p>
<p>这里需要注意的是 $ p = 5//2 = 2$</p>
<p>所以当 <span class="arithmatex">\(stride = 1\)</span> 时，<span class="arithmatex">\(padding = kernel\_size //2\)</span> 时，是不变卷积（输入特征图尺寸 和 输出特征图尺寸相同）</p>
<p>分组只是卷积核的参数变少了。</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> 膨胀卷积与输出特征图的尺寸？</li>
</ul>
<hr />
<div class="arithmatex">\[ \text{Output Size} = \left\lfloor \frac{\text{Input Size} + 2 \times \text{Padding} - \text{Kernel Size}}{\text{Stride}} \right\rfloor + 1 \]</div>
<p>对于输入特征图尺寸为 80x80，卷积核大小为 3x3，步幅为 2，填充为 1 的情况</p>
<div class="arithmatex">\[ \text{Output Size} = \left\lfloor \frac{80 + 2 \times 1 - 3}{2} \right\rfloor + 1 \]</div>
<ul>
<li>计算括号内的值:  <span class="arithmatex">\(80 + 2 \times 1 - 3 = 80 + 2 - 3 = 79\)</span> </li>
<li>除以步幅: <span class="arithmatex">\(\frac{79}{2}\)</span> = 39.5 </li>
<li>取整:  <span class="arithmatex">\(\left\lfloor 39.5 \right\rfloor = 39\)</span> </li>
<li>加 1:  <span class="arithmatex">\(39 + 1 = 40\)</span> </li>
</ul>
<p>因此，输出特征图的尺寸为 <span class="arithmatex">\(40×40\)</span></p>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="2025年2月23日 05:08:30"><span class="timeago" datetime="2025-02-23T05:08:30+00:00" locale="zh"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="2025年2月23日 05:08:30">2025-02-23</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="2024年11月25日 14:33:46"><span class="timeago" datetime="2024-11-25T14:33:46+00:00" locale="zh"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="2024年11月25日 14:33:46">2024-11-25</span>
  </span>

    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["toc.follow", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../js/timeago.min.js"></script>
      
        <script src="../../js/timeago_mkdocs_material.js"></script>
      
        <script src="../../mkdocs/javascripts/katex.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>