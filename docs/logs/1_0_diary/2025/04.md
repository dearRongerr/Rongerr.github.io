# 4æœˆ

## 2025-04-15 Tuesday 

TSB ç»™äº† docker è¿è¡Œï¼Œä½†æ˜¯å¿˜äº† docker æ€ä¹ˆç”¨ï¼Œï¼ˆæˆ‘è¿™è¯¥æ­»çš„è®°æ€§ï¼Œä¸€ç‚¹éƒ½ä¸è®°å¾—äº†ï¼Œäºæˆ‘ä¸Šä¸ªæœˆè¿˜ç ”ç©¶äº†ä¸€ä¸ªæ˜ŸæœŸã€‚

- docker run

- docker build

- docker image

æƒ³è®¾è®¡å®éªŒäº†ï¼Œè®ºæ–‡çœ‹å¾—äººå¤´å¤§

- [x] DLinear(ä»£ç è¿˜æ²¡çœ‹) 

## 2025-04-14 Monday 

- [ ] iTransformer 
- [x] WITRAN(å¤ªæœ‰åŠŸåŠ›äº†ï¼Œå…¬å¼)(ä»£ç æ²¡çœ‹å®Œ)
- [ ] PatchTST
- [ ] TimesNet

çµå…‰ä¹ç°ï¼Œå…³äºå¬å›ç‡ä¸ºä»€ä¹ˆå¯ä»¥æ˜¯ 1 çš„ç›´è§‚è§£é‡Šï¼Œå¦‚æœ 100 ä¸ªæ ·æœ¬ï¼Œ99 ä¸ªæ­£ä¾‹ï¼Œ1 ä¸ªè´Ÿä¾‹ï¼Œä½†æ˜¯é¢„æµ‹çš„æ—¶å€™æŠŠæ‰€æœ‰çš„æ ·æœ¬çš„éƒ½é¢„æµ‹ä¸ºæ­£ä¾‹ï¼Œæ­£ç¡®ç‡ 99%ï¼Œå¬å›ç‡=1ã€‚ç”¨ä¾‹å­æ¸…æ™°æ˜äº†ã€‚

## 2025-04-02 Wednesday 

åŠ æ²¹ã€‚

è£‚å¼€ï¼Œä¸çŸ¥é“å¹²å•¥

æˆ‘è¯¥æ€ä¹ˆåšï¼Œä»å“ªå„¿å¼€å§‹ã€‚

é¦–å…ˆï¼Œæˆ‘å·²ç»å¯¹ Electricity æ•°æ®é›†è¿›è¡Œäº† èšç±»ï¼Œèšæˆå››ç±»ï¼Œæ•ˆæœæ¯”è¾ƒå¥½ï¼ŒåŒæ—¶ç”¨ä¸»æˆåˆ†åˆ†å¯¹æ•°æ®é›†è¿›è¡Œäº†é™ç»´å¯è§†åŒ–ã€‚é•¿åºåˆ—æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œä¹Ÿè¦æµ‹è¯•åŸè®ºæ–‡çš„ç»“æœï¼Œæ‰€ä»¥æˆ‘åº”è¯¥å¤ç°å‡ºè®ºæ–‡çš„ç»“æœã€‚ç°åœ¨ SegRNN çš„ä»£ç çœ‹æ˜ç™½äº†ï¼Œæˆ‘å·²ç»è·‘äº†å¾ˆå¤š SegRNN çš„ä»£ç ï¼Œç°åœ¨çœ‹ï¼Œå¤ç°å‡ºæ¥çš„è®ºæ–‡ç»“æœï¼Œç„¶åå† SegRNN çš„åŸºç¡€ä¸Šæ”¹ã€‚

==é—®é¢˜ 1ï¼šSegRNN çš„ä½œè€…éƒ½åšäº†å“ªäº›å®éªŒï¼Ÿç”¨äº†ä»€ä¹ˆæ•°æ®é›†ï¼Ÿç”¨äº†ä»€ä¹ˆå¯¹æ¯”æ¨¡å‹ï¼Ÿ==

table1ï¼Œæ•°æ®é›†æ¦‚è¿°ï¼š

è¡¨2ï¼šæœ¬æ–‡åšçš„æ‰€æœ‰å¯¹æ¯”å®éªŒï¼š

![image-20250402131817042](images/image-20250402131817042.png)  

æˆ‘ç°åœ¨ï¼Œéœ€è¦å¤ç°åŸè®ºæ–‡çš„ç»“æœã€‚

## 2025-04-04 Friday 

### æ–‡çŒ®é˜…è¯»

Long time series forecasting aims to utilize historical information to forecast future states over extended horizons. Traditional RNN-based series forecasting methods struggle to effectively address long-term dependencies and gradient issues in long time series problems. Recently, SegRNN has emerged as a leading RNN-based model tailored for long-term series forecasting, demonstrating state-of-the-art performance while maintaining a streamlined architecture through innovative segmentation and parallel decoding techniques. Nevertheless, SegRNN has several limitations: its fixed segmentation disrupts data continuity and fails to effectively leverage information across different segments, the segmentation strategy employed by SegRNN does not fundamentally address the issue of information loss within the recurrent structure. To address these issues, we propose the ISMRNN method with three key enhancements: we introduce an implicit segmentation structure to decompose the time series and map it to segmented hidden states, resulting in denser information exchange during the segmentation phase. Additionally, we incorporate residual structures in the encoding layer to mitigate information loss within the recurrent structure. To extract information more effectively, we further integrate the Mamba architecture to enhance time series information extraction. Experiments on several real-world long time series forecasting datasets demonstrate that our model surpasses the performance of current state-of-the-art models.

é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹æ—¨åœ¨åˆ©ç”¨å†å²ä¿¡æ¯æ¥é¢„æµ‹æœªæ¥åœ¨è¾ƒé•¿æ—¶é—´èŒƒå›´å†…çš„çŠ¶æ€ã€‚ä¼ ç»Ÿçš„åŸºäºRNNçš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•åœ¨å¤„ç†é•¿æœŸæ—¶é—´åºåˆ—é—®é¢˜æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆè§£å†³é•¿æœŸä¾èµ–æ€§å’Œæ¢¯åº¦é—®é¢˜ã€‚æœ€è¿‘ï¼ŒSegRNNä½œä¸ºä¸€ç§é’ˆå¯¹é•¿æœŸåºåˆ—é¢„æµ‹çš„é¢†å…ˆRNNæ¨¡å‹å‡ºç°ï¼Œé€šè¿‡åˆ›æ–°çš„åˆ†æ®µå’Œå¹¶è¡Œè§£ç æŠ€æœ¯ï¼Œä»¥ç²¾ç®€çš„æ¶æ„å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒSegRNNå­˜åœ¨å‡ ä¸ªå±€é™æ€§ï¼šå…¶å›ºå®šçš„åˆ†æ®µæ–¹å¼ç ´åäº†æ•°æ®çš„è¿ç»­æ€§ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨ä¸åŒåˆ†æ®µä¹‹é—´çš„ä¿¡æ¯ï¼ŒSegRNNé‡‡ç”¨çš„åˆ†æ®µç­–ç•¥ä¹Ÿæœªèƒ½ä»æ ¹æœ¬ä¸Šè§£å†³å¾ªç¯ç»“æ„ä¸­çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ISMRNNæ–¹æ³•ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®å¢å¼ºç‚¹ï¼šæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éšå¼åˆ†æ®µç»“æ„ï¼Œå°†æ—¶é—´åºåˆ—åˆ†è§£å¹¶æ˜ å°„åˆ°åˆ†æ®µéšè—çŠ¶æ€ï¼Œåœ¨åˆ†æ®µé˜¶æ®µå®ç°äº†æ›´å¯†é›†çš„ä¿¡æ¯äº¤æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç¼–ç å±‚ä¸­å¼•å…¥æ®‹å·®ç»“æ„ï¼Œä»¥å‡è½»å¾ªç¯ç»“æ„ä¸­çš„ä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°æå–ä¿¡æ¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ•´åˆäº†Mambaæ¶æ„æ¥å¢å¼ºæ—¶é—´åºåˆ—ä¿¡æ¯æå–ã€‚åœ¨å‡ ä¸ªçœŸå®ä¸–ç•Œé•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹æ€§èƒ½ã€‚

In this work, we propose a novel model named ISMRNN to address the issues associated with SegRNN. Specifically, ISMRNN introduces an implicit segmentation structure that decomposes and maps the time series into encoded vectors through two linear transformations. This method facilitates more continuous processing during segmentation and enhances the utilization of information between different segments. Additionally, ISMRNN incorporates a residual structure with a linear layer, allowing some information to bypass the recurrent encoding structure, thus reducing information loss within the recurrent framework. Furthermore, we employ the Mamba structure[Gu and Dao, 2023] for preprocessing the time series, which aids in capturing long-term dependencies more effectively. The main contributions of ISMRNN can be summarized as follows:  

â€¢ Utilizing implicit segmentation for denser information exchange during the segmentation phase. 

 â€¢ Incorporating the Mamba structure to improve information preprocessing.  

â€¢ The residual structure reduces information loss within the recurrent structure.

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º ISMRNN çš„æ–°å‹æ¨¡å‹ï¼Œä»¥è§£å†³ä¸ SegRNN ç›¸å…³çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒISMRNN å¼•å…¥äº†ä¸€ç§éšå¼åˆ†æ®µç»“æ„ï¼Œé€šè¿‡ä¸¤æ¬¡çº¿æ€§å˜æ¢å°†æ—¶é—´åºåˆ—åˆ†è§£å¹¶æ˜ å°„ä¸ºç¼–ç å‘é‡ã€‚è¿™ç§æ–¹æ³•åœ¨åˆ†æ®µè¿‡ç¨‹ä¸­å®ç°äº†æ›´è¿ç»­çš„å¤„ç†ï¼Œå¹¶å¢å¼ºäº†ä¸åŒåˆ†æ®µä¹‹é—´ä¿¡æ¯çš„åˆ©ç”¨ã€‚æ­¤å¤–ï¼ŒISMRNN åœ¨ç¼–ç ç»“æ„ä¸­å¼•å…¥äº†å¸¦æœ‰çº¿æ€§å±‚çš„æ®‹å·®ç»“æ„ï¼Œä½¿å¾—éƒ¨åˆ†ä¿¡æ¯èƒ½å¤Ÿç»•è¿‡å¾ªç¯ç¼–ç ç»“æ„ï¼Œä»è€Œå‡å°‘å¾ªç¯æ¡†æ¶å†…çš„ä¿¡æ¯ä¸¢å¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨ Mamba ç»“æ„ [Gu and Dao, 2023] å¯¹æ—¶é—´åºåˆ—è¿›è¡Œé¢„å¤„ç†ï¼Œè¿™æœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°æ•æ‰é•¿æœŸä¾èµ–æ€§ã€‚ISMRNN çš„ä¸»è¦è´¡çŒ®å¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼š
- åˆ©ç”¨éšå¼åˆ†æ®µï¼Œåœ¨åˆ†æ®µé˜¶æ®µå®ç°æ›´å¯†é›†çš„ä¿¡æ¯äº¤æ¢ã€‚
- å¼•å…¥ Mamba ç»“æ„ä»¥æ”¹è¿›ä¿¡æ¯é¢„å¤„ç†ã€‚
- æ®‹å·®ç»“æ„å‡å°‘äº†å¾ªç¯ç»“æ„å†…çš„ä¿¡æ¯ä¸¢å¤±ã€‚



==è¾“å…¥ï¼š==

x.shape=torch.Size([16, 720, 321])  batchSize SequenceLength FeatureDim

==reshape== 

x.reshape(-1, self.seg_num_x, self.seg_len) â†’ [16Ã—321=5136, 15,48]

==self.valueEmbedding== 

```python
# build model
self.valueEmbedding = nn.Sequential(
    nn.Linear(self.seg_len, self.d_model),
    nn.ReLU()
)

Sequential(
  (0): Linear(in_features=48, out_features=512, bias=True)
  (1): ReLU()
)
```

â†’ [5136, 15,512]

## 2025-04-05 Saturday 

time Unet forward æ¥æ”¶ xï¼Œè¾“å…¥ x çš„å½¢çŠ¶æ˜¯(32,720,7)ï¼Œ

`x = self.revin_layer(x, 'norm')` è¿›è¡Œå¯é€†å®ä¾‹å½’ä¸€åŒ– ï¼Œå½¢çŠ¶ä¸å˜(32,720,7)

`x1 = x.permute(0,2,1)` æ¥ä¸‹æ¥äº¤æ¢ç»´åº¦ï¼Œå½¢çŠ¶ç”±(32,720,7)å˜ä¸º(32,7,720)

```python
i = 0
for down_block in self.down_blocks:
    e_out.append(down_block(x1))
    x1 = self.Maxpools[i](x1)
    i = i+1
```

==self.down_blocks== 

```
ModuleList(
  (0): block_model(
    (Linear_channel): ModuleList(
      (0-6): 7 x Linear(in_features=720, out_features=720, bias=True)
    )
    (ln): LayerNorm((720,), eps=1e-05, elementwise_affine=True)
    (relu): ReLU(inplace=True)
  )
  (1): block_model(
    (Linear_channel): ModuleList(
      (0-6): 7 x Linear(in_features=359, out_features=359, bias=True)
    )
    (ln): LayerNorm((359,), eps=1e-05, elementwise_affine=True)
    (relu): ReLU(inplace=True)
  )
  (2): block_model(
    (Linear_channel): ModuleList(
      (0-6): 7 x Linear(in_features=179, out_features=179, bias=True)
    )
    (ln): LayerNorm((179,), eps=1e-05, elementwise_affine=True)
    (relu): ReLU(inplace=True)
  )
  (3): block_model(
    (Linear_channel): ModuleList(
      (0-6): 7 x Linear(in_features=89, out_features=89, bias=True)
    )
    (ln): LayerNorm((89,), eps=1e-05, elementwise_affine=True)
    (relu): ReLU(inplace=True)
  )
)
```

==self.Maxpools== 

```python
ModuleList(
  (0-3): 4 x AvgPool1d(kernel_size=(3,), stride=(2,), padding=(0,))
)
```

==ç¬¬1æ¬¡å¾ªç¯== 

è¾“å…¥ x1 å½¢çŠ¶: [32, 7, 720]

`down_block(x1)` è¾“å‡ºå½¢çŠ¶: [32, 7, 720]

`Maxpools[0](x1)` è¾“å‡ºå½¢çŠ¶: [32, 7, 359]

æ± åŒ–å‚æ•°: kernel_size=3, stride=2, padding=0

è®¡ç®—: `(720 + 2*0 - 3)/2 + 1 = 359`

==ç¬¬2æ¬¡å¾ªç¯==

è¾“å…¥ x1 å½¢çŠ¶: [32, 7, 359]

down_block(x1) è¾“å‡ºå½¢çŠ¶: [32, 7, 359]

`Maxpools[1](x1)` è¾“å‡ºå½¢çŠ¶: [32, 7, 179]
è®¡ç®—: `(359 + 2*0 - 3)/2 + 1 = 179`

==ç¬¬3æ¬¡å¾ªç¯== 

è¾“å…¥ x1 å½¢çŠ¶: [32, 7, 179]

`down_block(x1)` è¾“å‡ºå½¢çŠ¶: [32, 7, 179]

`Maxpools[2](x1)` è¾“å‡ºå½¢çŠ¶: [32, 7, 89]

è®¡ç®—: `(179 + 2*0 - 3)/2 + 1 = 89`

==ç¬¬4æ¬¡å¾ªç¯== 

è¾“å…¥ x1 å½¢çŠ¶: [32, 7, 89]

down_block(x1) è¾“å‡ºå½¢çŠ¶: [32, 7, 89]
`Maxpools[3](x1)`è¾“å‡ºå½¢çŠ¶: [32, 7, 44]

è®¡ç®—: (89 + 2*0 - 3)/2 + 1 = 44

---

å¾ªç¯ç»“æŸåï¼Œ`e_out` åˆ—è¡¨åŒ…å«`4`ä¸ªå¼ é‡ï¼Œå¯¹åº”`4`ä¸ªå°ºåº¦çš„ç‰¹å¾ï¼š

e_out[0]: [32, 7, 720] (åŸå§‹å°ºåº¦)

e_out[1]: [32, 7, 359] (ç¬¬ä¸€æ¬¡ä¸‹é‡‡æ ·)

e_out[2]: [32, 7, 179] (ç¬¬äºŒæ¬¡ä¸‹é‡‡æ ·)

e_out[3]: [32, 7, 89] (ç¬¬ä¸‰æ¬¡ä¸‹é‡‡æ ·)

- éšç€å±‚æ¬¡åŠ æ·±ï¼Œæ—¶é—´ç»´åº¦é€æ¸å‡å°ï¼Œæ¯å±‚æ•è·ä¸åŒæ—¶é—´å°ºåº¦çš„ç‰¹å¾
- `down_block` æ“ä½œä¸æ”¹å˜é€šé“ç»´åº¦å’Œæ‰¹æ¬¡ç»´åº¦ï¼Œåªåœ¨æ—¶é—´ç»´åº¦ä¸Šè¿›è¡Œæ˜ å°„
- æ¯æ¬¡æ± åŒ–æ“ä½œä¼šä½¿æ—¶é—´ç»´åº¦å¤§çº¦å‡åŠ (å–å†³äºç²¾ç¡®çš„æ± åŒ–å‚æ•°)

(cvä¸­ï¼Œé€šè¿‡å·ç§¯é€šé“æ•°ç¿»å€ï¼Œå°ºåº¦ä¸å˜ï¼›æ± åŒ–æˆé€šé“æ•°ä¸å˜ï¼Œå°ºåº¦å‡åŠ)

---

`e_last = e_out[self.stage_num - 1]` å…¶ä¸­ `self.stage_num=4`ï¼Œè·å–ç¬¬ä¸‰æ¬¡ä¸‹é‡‡æ ·å±‚è¾“å‡º[32, 7, 89] ï¼Œä¹Ÿå°±æ˜¯ `e_last.shape = [32, 7, 89]` 

<details>
<summary>æ€ä¹ˆä»è®ºæ–‡ä¸­æ‰’æ¨¡å—ï¼Ÿ</summary>
<p>
	ï¼ˆ1ï¼‰æ‰¾åˆ°åŸè®ºæ–‡ï¼Œæå‡ºçš„æ¨¡å—ï¼Œç»™å‡ºçš„ä»£ç 
    ï¼ˆ2ï¼‰å‡†å¤‡è‡ªå·±çš„æµ‹è¯•æ–‡ä»¶
    åˆ†æï¼ŒåŸæ¨¡å—çš„ initã€forward åˆ†åˆ«éœ€è¦ä»€ä¹ˆå‚æ•°ï¼Œå½¢çŠ¶çš„å«ä¹‰
    åˆ†æï¼Œè‡ªå·±ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ¨¡å—ã€‚è‡ªå·±çš„æ•°æ®æ˜¯ä»€ä¹ˆå½¢çŠ¶çš„ï¼Œå°†è‡ªå·±çš„æ•°æ®ç»´åº¦æ‹¿å‡ºæ¥ï¼Œåˆ†æå’ŒåŸæ¨¡å—çš„ç»´åº¦å¯¹é½ã€‚æµ‹è¯•å¯¹è±¡æ˜¯è‡ªå·±æ‰€éœ€è¦çš„æ•°æ®ã€‚æµ‹è¯•æˆåŠŸã€‚
</p>
</details>

```python
e_last = e_out[self.stage_num - 1]
for i in range(self.stage_num - 1):
    e_last = torch.cat((e_out[self.stage_num - i - 2], e_last), dim=2)
    e_last = self.up_blocks[i](e_last)
e_last = e_last.permute(0,2,1)
e_last = self.revin_layer(e_last, 'denorm')
return e_last
```

åˆå§‹æ¡ä»¶ï¼š

- stage_num = 4 (é»˜è®¤è®¾ç½®)
- e_out åŒ…å«4ä¸ªä¸åŒå°ºåº¦çš„ç‰¹å¾å›¾:
  * e_out[0]: [32, 7, 720]
  * e_out[1]: [32, 7, 359]
  * e_out[2]: [32, 7, 179]
  * e_out[3]: [32, 7, 89]

==ç¬¬1æ¬¡å¾ªç¯ (i=0)== 

e_last åˆå§‹å€¼ä¸º e_out[3]: [32, 7, 89]

`e_last = torch.cat((e_out[self.stage_num - i -2], e_last), dim=2)`

è®¡ç®— `self.stage_num - i - 2 = 4 - 0 - 2 = 2`

<details>
<summary>
è¯´æ˜ï¼šæ‹¼æ¥ e_out[2] å’Œ e_last: [32, 7, 179] å’Œ [32, 7, 89] 
</summary>
<p>
æ¥ï¼Œè¯»ï¼Œ [32, 7, 179] ï¼Œ32 ä¸ª 7ï¼Œ7 ä¸ª 179ï¼Œç°åœ¨æ‹¼æ¥æ²¿ç€ dim=2ï¼Œç°åœ¨æ˜¯ 32 ä¸ª 7ï¼Œ7 ä¸ª 179+89
</p>
</details>

æ²¿ dim=2 (æ—¶é—´ç»´åº¦) æ‹¼æ¥ï¼Œå¾—åˆ° [32, 7, 179+89] = [32, 7, 268]

é€šè¿‡ up_blocks[0] æ˜ å°„ï¼Œè¾“å‡ºå½¢çŠ¶å˜ä¸º [32, 7, 179]

<details>
<summary>è¯´æ˜ï¼šself.up_blocks</summary>
<p>
```python
    ModuleList(
      (0): block_model(
        (Linear_channel): ModuleList(
          (0-6): 7 x Linear(in_features=268, out_features=179, bias=True)
        )
        (ln): LayerNorm((179,), eps=1e-05, elementwise_affine=True)
        (relu): ReLU(inplace=True)
      )
      (1): block_model(
        (Linear_channel): ModuleList(
          (0-6): 7 x Linear(in_features=538, out_features=359, bias=True)
        )
        (ln): LayerNorm((359,), eps=1e-05, elementwise_affine=True)
        (relu): ReLU(inplace=True)
      )
      (2): block_model(
        (Linear_channel): ModuleList(
          (0-6): 7 x Linear(in_features=1079, out_features=720, bias=True)
        )
        (ln): LayerNorm((720,), eps=1e-05, elementwise_affine=True)
        (relu): ReLU(inplace=True)
      )
    )
```
</p>
</details>

==ç¬¬2æ¬¡å¾ªç¯ (i=1)==

e_last å½“å‰å€¼: [32, 7, 179]  

`e_last = torch.cat((e_out[self.stage_num - i -2], e_last), dim=2)`

è®¡ç®— `self.stage_num - i - 2 = 4 - 1 - 2 = 1`

æ‹¼æ¥ `e_out[1]` å’Œ `e_last`: [32, 7, 359] å’Œ [32, 7, 179]

æ²¿ dim=2 æ‹¼æ¥ï¼Œå¾—åˆ° [32, 7, 359+179] = [32, 7, 538]

é€šè¿‡ up_blocks[1] æ˜ å°„ï¼Œè¾“å‡ºå½¢çŠ¶å˜ä¸º [32, 7, 359]

==ç¬¬3æ¬¡å¾ªç¯ (i=2)==

e_last å½“å‰å€¼: [32, 7, 359]

è®¡ç®— self.stage_num - i - 2 = 4 - 2 - 2 = 0

æ‹¼æ¥ e_out[0] å’Œ e_last: [32, 7, 720] å’Œ [32, 7, 359]

æ²¿ dim=2 æ‹¼æ¥ï¼Œå¾—åˆ° [32, 7, 720+359] = [32, 7, 1079]

é€šè¿‡ up_blocks[2] æ˜ å°„ï¼Œè¾“å‡ºå½¢çŠ¶å˜ä¸º [32, 7, 720]

---

 `e_last = e_last.permute(0,2,1)`

è°ƒæ•´ç»´åº¦: e_last.permute(0,2,1) å°† [32, 7, 720] å˜ä¸º [32, 720, 7]

åº”ç”¨é€†å½’ä¸€åŒ–: `revin_layer(e_last, 'denorm')` å½¢çŠ¶ä¸å˜ï¼Œä»ä¸º `[32, 720, 7]`

è¿”å›æœ€ç»ˆè¾“å‡º: å½¢çŠ¶ä¸º `[32, 720, 7]`

---

è¾“å…¥ BSCï¼Œ

å¯é€†å®ä¾‹å½’ä¸€åŒ– BSC

permute BCS

1.1 `èšåˆä¿¡æ¯`

maxPool BC $\frac{S}{2}$

AvgPool BC $\frac{S}{2}$

Add(MaxPool + AvgPool)  BC  $\frac{S}{2}$

ä¿å­˜Pool åçš„ BC  $\frac{S}{2}$

Linear  BC  $\frac{S}{2}$

ğŸŸ¢ 1.2 `Seg`  ç»†ç²’åº¦æå–ä¿¡æ¯ï¼Œæ®‹å·®è¿æ¥

ç›´æ¥å¯¹è¾“å…¥ BCS åˆ†æˆ 2 æ®µï¼Œå½¢çŠ¶ä¸º BCÃ—2Ã— $\frac{S}{2}$

ç»è¿‡ GRU æå–æœ€åä¸€å±‚éšå«å±‚çŠ¶æ€ ï¼Œå½¢çŠ¶ä¸º 1Ã—BCÃ— $\frac{S}{2}$  ï¼ˆ1è¡¨ç¤º num\_layers  å•å±‚ GRUï¼‰

ğŸŸ¢ èåˆä¿¡æ¯



---



## 2025-04-07 Monday 

è§£ç é˜¶æ®µï¼š

`decoder_input_4 = x_AddNorm4.permute(0, 2, 1)`  

è¾“å…¥å½¢çŠ¶ [B,S/8,d_model] ç»è¿‡ permute [B,d_model,S/8]

ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨1/8åˆ†è¾¨ç‡ç‰¹å¾é¢„æµ‹æ€»é¢„æµ‹é•¿åº¦çš„å‰1/8éƒ¨åˆ†

é¦–å…ˆæ‹¿åˆ°æœ€åä¸€ä¸ªå†å²æ—¶é—´ç‚¹  `last_state_4 = decoder_input_4[:, :, -1]`

ç»´åº¦ `[B,d_model]`

æ¥ä¸‹æ¥é¢„æµ‹ï¼ŒæŠ•å½±ï¼ˆåœ¨æƒ³ä¼šä¸ä¼šç”¨ GRU æ•ˆæœæ›´å¥½ä¸€äº›ï¼‰

`pred_4 = *self*.predictor_4(last_state_4) `

ç›´æ¥ä» `[B,d_model]`  æŠ•å½±åˆ° `[B,pred_len/4*enc_in]`

æ¥ä¸‹æ¥ reshape æˆæ­£å¸¸çš„å½¢çŠ¶ï¼š`[B,enc_in,pred_len/4]`

ä¸‹é¢å¼€å§‹å‘ä¸Šèåˆï¼Œä½¿ç”¨ `x_fused3`  èåˆäº† `seq_len/8`  å’Œ `seq_len/4` çš„ä¿¡æ¯

å½¢çŠ¶æ˜¯ `[B,S/4,d_model]`

permute ç»´åº¦è½¬æ¢ä¸º `[B,d_model,S/4]`

==æ¥ä¸‹æ¥è¿›è¡Œæ¸è¿›å¼çš„è§£ç ç»“æ„ï¼Œèåˆä¸Šä¸€å±‚çš„è§£ç ï¼Œåˆ›å»ºæ¸è¿›å¼ è‡ªå›å½’çš„è§£ç ç»“æ„==

æ‹¿åˆ° `x_fused3` å½¢çŠ¶ `[B,S/4,d_model]`

`permute`  å½¢çŠ¶å˜ä¸º `[B,d_model,S/4]`

èåˆä¸Šä¸€å±‚è§£ç  `pred_len/8` çš„è¾“å‡º  `pred_4` å½¢çŠ¶ `[B,enc_in,pred_len_4]`

ä¸ºäº†å¯ä»¥èåˆï¼Œé¦–å…ˆç»´åº¦è¦å¯¹é½ï¼Œæ‰€ä»¥å¯¹   `pred_4`  çš„ `enc_in` ç»´åº¦è¿›è¡ŒæŠ•å½±ï¼ŒæŠ•å½±åˆ° `d_model` ç»´åº¦ å½¢çŠ¶å˜ä¸º   `[B,d_model,pred_len_4]`

æ¥ä¸‹æ¥ï¼Œå°±å¯ä»¥ concat äº†ï¼Œ `[B,d_model,S/4]` concat     `[B,d_model,pred_len/4]`å˜æˆ   `[B,d_model,S/4]` 

å…¶å®è¿™é‡Œæ˜¯ä¸¤ä¸ªä¸å¯¹é½ï¼Œé¢„æµ‹çš„é•¿åº¦å’Œç»´åº¦éƒ½ä¸å¯¹é½ã€‚

å¯è¿˜æ˜¯æƒ³è®¾è®¡ä¸€ä¸ªæ¸è¿›è§£ç ï¼Œæ¸è¿›è§£ç ä¸å¥½å¼„ï¼Œå› ä¸ºç»´åº¦å’Œé•¿åº¦éƒ½ä¸å¯¹é½ã€‚

è‡ªåº•å‘ä¸Šçš„èåˆå’Œè§£ç ã€‚

---

## 2025-04-13 Sunday 

- [x] TimeMixerï¼ˆçº¿æ€§ç³»æ¨¡å‹ï¼‰
- [x] Pyraformer ï¼ˆæœ‰è¯æ˜ï¼‰
- [x] Fedformerï¼ˆè®¾è®¡é¢‘åŸŸçš„éƒ½æœ‰ç‚¹è€ƒéªŒæ•°å­¦åŠŸåº•ï¼Œå°æ³¢å˜æ¢ç¡®å®ä¸æ˜ç™½ï¼‰

