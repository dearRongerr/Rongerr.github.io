
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mydomain.org/mysite/literature/Reproduction/1/">
      
      
        <link rel="prev" href="../DAVE/">
      
      
        <link rel="next" href="../2/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.7">
    
    
      
        <title>一些模块 - 溶err</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8608ea7d.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/timeago.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
      <link rel="stylesheet" href="../../../mkdocs/css/no-footer.css">
    
      <link rel="stylesheet" href="../../../mkdocs/css/unordered-list-symbols.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="溶err" class="md-header__button md-logo" aria-label="溶err" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            溶err
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              一些模块
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../sticks/mkdocs_learn/" class="md-tabs__link">
          
  
    
  
  便签

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../bagu/questions/1_questions/" class="md-tabs__link">
          
  
    
  
  面试

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Error/github/" class="md-tabs__link">
          
  
    
  
  捉个虫

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../learning/3_ViT/" class="md-tabs__link">
          
  
    
  
  笔记

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
    
  
  文献

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../logs/" class="md-tabs__link">
          
  
    
  
  杂

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="溶err" class="md-nav__button md-logo" aria-label="溶err" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    溶err
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    便签
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            便签
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/mkdocs_learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MkDocs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/markdwon_learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    markdown
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/latex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LaTex
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/GitHub/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GitHub
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/MacOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MacOS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/shell/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/screen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    screen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/docker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/writting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    写作
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/1_github_v1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    github v1.0
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/2_python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sticks/3_vscode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    面试
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            面试
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    题目
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            题目
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/questions/1_questions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    面试问题
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../bagu/leetcode/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    力扣
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            力扣
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/leetcode/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1 两数之和
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/leetcode/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2 两数相加
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../bagu/deeplearning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    深度学习
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            深度学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/deeplearning/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕Transformer代码
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/deeplearning/former1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    空
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/deeplearning/former2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    空
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/deeplearning/pytorch_shape_function/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pytorch的维度变换函数
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/deeplearning/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visionTransformer代码
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    机器学习
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/machinelearning/kmeans/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕kmeans
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../bagu/machinelearning/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手撕反向传播
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    捉个虫
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            捉个虫
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/github/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    github
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/latex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latex
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/macos/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Error/docker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    docker
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    笔记
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            笔记
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/3_ViT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ViT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/1_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/2_MOCO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MOCO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    图解LayerNorm &amp; BatchNorm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5种归一化方法
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision Transformer的原理与难点源码实现
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/swintransformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SwinTransformer 学习笔记
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/pe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4种位置编码
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/convs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    卷积
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    李沐 目标检测部分
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/4_GAN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/5_Bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BERT从零详细解读
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/6_Diffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDPM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/6_Diffusion1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/7_Clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Clip
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/8_WeightNorm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    WeightNorm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/9_cGAN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN 变体
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/10_ResNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    项目实战：ResNet果蔬分类
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/11_excelcsvtensor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基础：excel\csv文件→tensor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/12_KLdivergence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KL divergence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/13_RNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/14_LSTM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LSTM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/15_ContrastiveLearning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对比学习
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/16_YOLO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    YOLO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/17_DETR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DETR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/18_DINO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DINO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/19_GPT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/20_distill/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    知识蒸馏
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learning/21_FastRCNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    21 FastRCNN
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    文献
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            文献
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../TSP/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    时间序列预测
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_2" id="__nav_5_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            时间序列预测
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TSP/0_note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NOTE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TSP/1_SegRNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SegRNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TSP/2_DLinear/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DLinear
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TSP/3_TimesNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TimesNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TSP/4_Informer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Informer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../ObejectCounting/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    目标计数
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            目标计数
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank1%20CountGD/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank1 CountGD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank2%20GeCo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank2 GeCo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank3%20DAVE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank3 DAVE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank4%20CACViT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank4 CACViT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank5%20SSD/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank5 SSD
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank6%20LOCA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank6 LOCA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank7%20SemAug_CountTR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank7 SemAug CountTR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank8%20CounTR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank8 CounTR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank9%20SemAug_SAFECount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank9 SemAug SAFECount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank10%20SPDCN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank10 SPDCN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank11%20GCA_SUN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank11 GCA SUN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank12%20SAFECount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank12 SAFECount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank13%20BMNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank13 BMNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank14%20LaoNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank14 LaoNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank15%20CounTX/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank15 CounTX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank16%20Counting_DETR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank16 Counting DETR
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank17%20RCC/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank17 RCC
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank18%20Omnicount/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank18 Omnicount
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObejectCounting/rank19%20FamNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rank19 FamNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    复现&代码
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_4" id="__nav_5_4_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            复现&代码
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../DAVE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DAVE复现
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    一些模块
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    一些模块
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      通道注意力及其变体
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#senet" class="md-nav__link">
    <span class="md-ellipsis">
      √ SENet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sknet" class="md-nav__link">
    <span class="md-ellipsis">
      √ SKNet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cbam" class="md-nav__link">
    <span class="md-ellipsis">
      √ CBAM
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eca" class="md-nav__link">
    <span class="md-ellipsis">
      √ECA
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coordinate-attention" class="md-nav__link">
    <span class="md-ellipsis">
      √ Coordinate Attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cotattention" class="md-nav__link">
    <span class="md-ellipsis">
      CoTAttention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tripletattention" class="md-nav__link">
    <span class="md-ellipsis">
      √ TripletAttention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#danet" class="md-nav__link">
    <span class="md-ellipsis">
      √DANet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      特征融合
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#asff" class="md-nav__link">
    <span class="md-ellipsis">
      √⭐ASFF 不同尺度特征融合
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fpn" class="md-nav__link">
    <span class="md-ellipsis">
      √ FPN 特征金字塔网络
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#panet" class="md-nav__link">
    <span class="md-ellipsis">
      PANet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      空间注意力及变体
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      √Attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention_1" class="md-nav__link">
    <span class="md-ellipsis">
      Attention 拓展
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cot" class="md-nav__link">
    <span class="md-ellipsis">
      上下文 CoT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    特征融合方式
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一些感悟
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    预训练权重
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../ObjectDetection/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    目标检测
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_5" id="__nav_5_5_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_5">
            <span class="md-nav__icon md-icon"></span>
            目标检测
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObjectDetection/2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    目标检测基础知识
    
  </span>
  

      </a>
    </li>
  

              
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObjectDetection/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DETR论文系列
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObjectDetection/3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    （DETR）End-to-End Object Detection with Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ObjectDetection/4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../MultiModal/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    多模态
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_6" id="__nav_5_6_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6">
            <span class="md-nav__icon md-icon"></span>
            多模态
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../MultiModal/1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../logs/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    杂
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            杂
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../logs/diary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    乐观 &amp; 坚强
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      通道注意力及其变体
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#senet" class="md-nav__link">
    <span class="md-ellipsis">
      √ SENet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sknet" class="md-nav__link">
    <span class="md-ellipsis">
      √ SKNet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cbam" class="md-nav__link">
    <span class="md-ellipsis">
      √ CBAM
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eca" class="md-nav__link">
    <span class="md-ellipsis">
      √ECA
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coordinate-attention" class="md-nav__link">
    <span class="md-ellipsis">
      √ Coordinate Attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cotattention" class="md-nav__link">
    <span class="md-ellipsis">
      CoTAttention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tripletattention" class="md-nav__link">
    <span class="md-ellipsis">
      √ TripletAttention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#danet" class="md-nav__link">
    <span class="md-ellipsis">
      √DANet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      特征融合
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#asff" class="md-nav__link">
    <span class="md-ellipsis">
      √⭐ASFF 不同尺度特征融合
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fpn" class="md-nav__link">
    <span class="md-ellipsis">
      √ FPN 特征金字塔网络
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#panet" class="md-nav__link">
    <span class="md-ellipsis">
      PANet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      空间注意力及变体
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      √Attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention_1" class="md-nav__link">
    <span class="md-ellipsis">
      Attention 拓展
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cot" class="md-nav__link">
    <span class="md-ellipsis">
      上下文 CoT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="_1">一些模块<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<h2 id="_2">通道注意力及其变体<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<h2 id="senet">√ SENet<a class="headerlink" href="#senet" title="Permanent link">&para;</a></h2>
<p><img alt="image-20250216195102911" src="../images/image-20250216195102911.png" /></p>
<p><img alt="image-20250216195120988" src="../images/image-20250216195120988.png" /></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-0-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-0-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-0-4"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">init</span>
</span><span id="__span-0-5">
</span><span id="__span-0-6"><span class="s2">&quot;Squeeze-and-Excitation Networks&quot;</span>
</span><span id="__span-0-7">
</span><span id="__span-0-8"><span class="k">class</span><span class="w"> </span><span class="nc">SEAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-0-9">
</span><span id="__span-0-10">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
</span><span id="__span-0-11">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-0-12">        <span class="c1"># 在空间维度上,将H×W压缩为1×1</span>
</span><span id="__span-0-13">        <span class="bp">self</span><span class="o">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-0-14">        <span class="c1"># 包含两层全连接,先降维,后升维。最后接一个sigmoid函数</span>
</span><span id="__span-0-15">        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="__span-0-16">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span> <span class="n">channel</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-0-17">            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span><span id="__span-0-18">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channel</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-0-19">            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
</span><span id="__span-0-20">        <span class="p">)</span>
</span><span id="__span-0-21">
</span><span id="__span-0-22">
</span><span id="__span-0-23">    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-0-24">        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="__span-0-25">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span><span id="__span-0-26">                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>
</span><span id="__span-0-27">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-0-28">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-0-29">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
</span><span id="__span-0-30">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-0-31">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-0-32">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="__span-0-33">                <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</span><span id="__span-0-34">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-0-35">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-0-36">
</span><span id="__span-0-37">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-0-38">        <span class="c1"># (B,C,H,W)</span>
</span><span id="__span-0-39">        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span><span id="__span-0-40">        <span class="c1"># Squeeze: (B,C,H,W)--&gt;avg_pool--&gt;(B,C,1,1)--&gt;view--&gt;(B,C)</span>
</span><span id="__span-0-41">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</span><span id="__span-0-42">        <span class="c1"># Excitation: (B,C)--&gt;fc--&gt;(B,C)--&gt;(B, C, 1, 1)</span>
</span><span id="__span-0-43">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-0-44">        <span class="c1"># scale: (B,C,H,W) * (B, C, 1, 1) == (B,C,H,W)</span>
</span><span id="__span-0-45">        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
</span><span id="__span-0-46">        <span class="k">return</span> <span class="n">out</span>
</span><span id="__span-0-47">
</span><span id="__span-0-48">
</span><span id="__span-0-49"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-0-50">    <span class="c1"># (B,C,H,W)</span>
</span><span id="__span-0-51">    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-0-52">    <span class="c1"># 定义通道注意力</span>
</span><span id="__span-0-53">    <span class="n">Model</span> <span class="o">=</span> <span class="n">SEAttention</span><span class="p">(</span><span class="n">channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</span><span id="__span-0-54">    <span class="n">output</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-0-55">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</span></code></pre></div></td></tr></table></div>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-1-1"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-1-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-1-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-1-4"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">init</span>
</span><span id="__span-1-5">
</span><span id="__span-1-6"><span class="k">class</span><span class="w"> </span><span class="nc">SEAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-1-7">    <span class="c1"># 初始化SE模块，channel为通道数，reduction为降维比率</span>
</span><span id="__span-1-8">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
</span><span id="__span-1-9">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-1-10">        <span class="bp">self</span><span class="o">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 自适应平均池化层，将特征图的空间维度压缩为1x1</span>
</span><span id="__span-1-11">        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>  <span class="c1"># 定义两个全连接层作为激励操作，通过降维和升维调整通道重要性</span>
</span><span id="__span-1-12">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span> <span class="n">channel</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>  <span class="c1"># 降维，减少参数数量和计算量</span>
</span><span id="__span-1-13">            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1"># ReLU激活函数，引入非线性</span>
</span><span id="__span-1-14">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channel</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>  <span class="c1"># 升维，恢复到原始通道数</span>
</span><span id="__span-1-15">            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>  <span class="c1"># Sigmoid激活函数，输出每个通道的重要性系数</span>
</span><span id="__span-1-16">        <span class="p">)</span>
</span><span id="__span-1-17">
</span><span id="__span-1-18">    <span class="c1"># 权重初始化方法</span>
</span><span id="__span-1-19">    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-1-20">        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>  <span class="c1"># 遍历模块中的所有子模块</span>
</span><span id="__span-1-21">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>  <span class="c1"># 对于卷积层</span>
</span><span id="__span-1-22">                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>  <span class="c1"># 使用Kaiming初始化方法初始化权重</span>
</span><span id="__span-1-23">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-1-24">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 如果有偏置项，则初始化为0</span>
</span><span id="__span-1-25">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>  <span class="c1"># 对于批归一化层</span>
</span><span id="__span-1-26">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 权重初始化为1</span>
</span><span id="__span-1-27">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 偏置初始化为0</span>
</span><span id="__span-1-28">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>  <span class="c1"># 对于全连接层</span>
</span><span id="__span-1-29">                <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># 权重使用正态分布初始化</span>
</span><span id="__span-1-30">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-1-31">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 偏置初始化为0</span>
</span><span id="__span-1-32">
</span><span id="__span-1-33">    <span class="c1"># 前向传播方法</span>
</span><span id="__span-1-34">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-1-35">        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># 获取输入x的批量大小b和通道数c</span>
</span><span id="__span-1-36">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>  <span class="c1"># 通过自适应平均池化层后，调整形状以匹配全连接层的输入</span>
</span><span id="__span-1-37">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 通过全连接层计算通道重要性，调整形状以匹配原始特征图的形状</span>
</span><span id="__span-1-38">        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 将通道重要性系数应用到原始特征图上，进行特征重新校准</span>
</span><span id="__span-1-39">
</span><span id="__span-1-40"><span class="c1"># 示例使用</span>
</span><span id="__span-1-41"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-1-42">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>  <span class="c1"># 随机生成一个输入特征图</span>
</span><span id="__span-1-43">    <span class="n">se</span> <span class="o">=</span> <span class="n">SEAttention</span><span class="p">(</span><span class="n">channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>  <span class="c1"># 实例化SE模块，设置降维比率为8</span>
</span><span id="__span-1-44">    <span class="n">output</span> <span class="o">=</span> <span class="n">se</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># 将输入特征图通过SE模块进行处理</span>
</span><span id="__span-1-45">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 打印处理后的特征图形状，验证SE模块的作用</span>
</span></code></pre></div></td></tr></table></div>
<ul>
<li>SE模块首先通过全局平均池化操作对输入特征图的空间维度（高度H和宽度W）进行聚合，为每个通道生成一个通道描述符。这一步有效地将全局空间信息压缩成一个通道向量，捕获了通道特征响应的全局分布。这一全局信息对于接下来的重新校准过程至关重要。</li>
<li>在压缩步骤之后，应用一个激励机制，该机制本质上是由两个全连接（FC）层和一个非线性激活函数（通常是sigmoid）组成的自门控机制。第一个FC层降低了通道描述符的维度，应用ReLU非线性激活，随后第二个FC层将其投影回原始通道维度。这个过程建模了通道间的非线性交互，并产生了一组通道权重。</li>
<li>激励操作的输出用于重新校准原始输入特征图。输入特征图的每个通道都由激励输出中对应的标量进行缩放。这一步骤有选择地强调信息丰富的特征，同时抑制不太有用的特征，使模型能够专注于任务中最相关的特征。</li>
</ul>
<p><strong>SE Net的核心贡献是通过SE块显式建模通道间的依赖关系</strong></p>
<h2 id="sknet">√ SKNet<a class="headerlink" href="#sknet" title="Permanent link">&para;</a></h2>
<p><img alt="image-20250216201417591" src="../images/image-20250216201417591.png" /></p>
<p><img alt="image-20250222123945075" src="../images/image-20250222123945075.png" /></p>
<p><img alt="image-20250222124005731" src="../images/image-20250222124005731.png" /></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-2-1"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-2-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-2-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-2-4"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">init</span>
</span><span id="__span-2-5"><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
</span><span id="__span-2-6">
</span><span id="__span-2-7"><span class="s2">&quot;Selective Kernel Networks&quot;</span>
</span><span id="__span-2-8">
</span><span id="__span-2-9"><span class="k">class</span><span class="w"> </span><span class="nc">SKAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-2-10">
</span><span id="__span-2-11">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">],</span><span class="n">reduction</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">L</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
</span><span id="__span-2-12">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-2-13">        <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">channel</span><span class="o">//</span><span class="n">reduction</span><span class="p">)</span>
</span><span id="__span-2-14">        <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([])</span>
</span><span id="__span-2-15">        <span class="c1"># 有几个卷积核,就有几个尺度, 每个尺度对应的卷积层由Conv-bn-relu实现</span>
</span><span id="__span-2-16">        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">kernels</span><span class="p">:</span>
</span><span id="__span-2-17">            <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-2-18">                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
</span><span id="__span-2-19">                    <span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span><span class="n">channel</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">k</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="n">group</span><span class="p">)),</span>
</span><span id="__span-2-20">                    <span class="p">(</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">channel</span><span class="p">)),</span>
</span><span id="__span-2-21">                    <span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
</span><span id="__span-2-22">                <span class="p">]))</span>
</span><span id="__span-2-23">            <span class="p">)</span>
</span><span id="__span-2-24">        <span class="c1"># 将全局向量降维</span>
</span><span id="__span-2-25">        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
</span><span id="__span-2-26">        <span class="bp">self</span><span class="o">.</span><span class="n">fcs</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([])</span>
</span><span id="__span-2-27">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernels</span><span class="p">)):</span>
</span><span id="__span-2-28">            <span class="bp">self</span><span class="o">.</span><span class="n">fcs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span><span class="n">channel</span><span class="p">))</span>
</span><span id="__span-2-29">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-2-30">
</span><span id="__span-2-31">
</span><span id="__span-2-32">
</span><span id="__span-2-33">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-2-34">        <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-2-35">        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span><span id="__span-2-36">        <span class="c1"># 存放多尺度的输出</span>
</span><span id="__span-2-37">        <span class="n">conv_outs</span><span class="o">=</span><span class="p">[]</span>
</span><span id="__span-2-38">        <span class="c1"># Split: 执行K个尺度对应的卷积操作</span>
</span><span id="__span-2-39">        <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">:</span>
</span><span id="__span-2-40">            <span class="n">scale</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1">#每一个尺度的输出shape都是: (B, C, H, W),是因为使用了padding操作</span>
</span><span id="__span-2-41">            <span class="n">conv_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
</span><span id="__span-2-42">        <span class="n">feats</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">conv_outs</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 将K个尺度的输出在第0个维度上拼接: (K,B,C,H,W)</span>
</span><span id="__span-2-43">
</span><span id="__span-2-44">        <span class="c1"># Fuse: 首先将多尺度的信息进行相加,sum()默认在第一个维度进行求和</span>
</span><span id="__span-2-45">        <span class="n">U</span><span class="o">=</span><span class="nb">sum</span><span class="p">(</span><span class="n">conv_outs</span><span class="p">)</span> <span class="c1">#(K,B,C,H,W)--&gt;(B,C,H,W)</span>
</span><span id="__span-2-46">        <span class="c1"># 全局平均池化操作: (B,C,H,W)--&gt;mean--&gt;(B,C,H)--&gt;mean--&gt;(B,C)  【mean操作等价于全局平均池化的操作】</span>
</span><span id="__span-2-47">        <span class="n">S</span><span class="o">=</span><span class="n">U</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-2-48">        <span class="c1"># 降低通道数,提高计算效率: (B,C)--&gt;(B,d)</span>
</span><span id="__span-2-49">        <span class="n">Z</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
</span><span id="__span-2-50">
</span><span id="__span-2-51">        <span class="c1"># 将紧凑特征Z通过K个全连接层得到K个尺度对应的通道描述符表示, 然后基于K个通道描述符计算注意力权重</span>
</span><span id="__span-2-52">        <span class="n">weights</span><span class="o">=</span><span class="p">[]</span>
</span><span id="__span-2-53">        <span class="k">for</span> <span class="n">fc</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fcs</span><span class="p">:</span>
</span><span id="__span-2-54">            <span class="n">weight</span><span class="o">=</span><span class="n">fc</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="c1">#恢复预输入相同的通道数: (B,d)--&gt;(B,C)</span>
</span><span id="__span-2-55">            <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># (B,C)--&gt;(B,C,1,1)</span>
</span><span id="__span-2-56">        <span class="n">scale_weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#将K个通道描述符在0个维度上拼接: (K,B,C,1,1)</span>
</span><span id="__span-2-57">        <span class="n">scale_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scale_weight</span><span class="p">)</span> <span class="c1">#在第0个维度上执行softmax,获得每个尺度的权重: (K,B,C,1,1)</span>
</span><span id="__span-2-58">
</span><span id="__span-2-59">        <span class="c1"># Select</span>
</span><span id="__span-2-60">        <span class="n">V</span><span class="o">=</span><span class="p">(</span><span class="n">scale_weight</span><span class="o">*</span><span class="n">feats</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 将每个尺度的权重与对应的特征进行加权求和,第一步是加权，第二步是求和：(K,B,C,1,1) * (K,B,C,H,W) = (K,B,C,H,W)--&gt;sum--&gt;(B,C,H,W)</span>
</span><span id="__span-2-61">        <span class="k">return</span> <span class="n">V</span>
</span><span id="__span-2-62">
</span><span id="__span-2-63">
</span><span id="__span-2-64">
</span><span id="__span-2-65"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-2-66">    <span class="c1"># (B,C,H,W)</span>
</span><span id="__span-2-67">    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-2-68">    <span class="n">Model</span> <span class="o">=</span> <span class="n">SKAttention</span><span class="p">(</span><span class="n">channel</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</span><span id="__span-2-69">    <span class="n">output</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-2-70">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="cbam">√ CBAM<a class="headerlink" href="#cbam" title="Permanent link">&para;</a></h2>
<p>论文《CBAM: Convolutional Block Attention Module》</p>
<p><img alt="image-20250222123629324" src="../images/image-20250222123629324.png" /></p>
<p><img alt="image-20250222123709287" src="../images/image-20250222123709287.png" /></p>
<p><img alt="image-20250222123740942" src="../images/image-20250222123740942.png" /></p>
<p><strong>作用：</strong></p>
<p>是为了提升前馈卷积神经网络性能而提出的一种简单而有效的注意力模块。CBAM通过顺序地推断两个维度上的注意力图（通道和空间），然后将这些注意力图乘以输入特征图进行自适应特征精炼。</p>
<p>1、<strong>通道注意力模块（Channel Attention Module）</strong>：</p>
<p>通过利用特征之间的通道关系来生成通道注意力图。每个通道的特征图被视为一个特征探测器，通道注意力关注于给定输入图像中“什么”是有意义的。为了有效地计算通道注意力，CBAM首先对输入特征图的空间维度进行压缩，同时使用平均池化和最大池化操作来捕获不同的空间上下文描述符，这些被送入共享的多层感知机（MLP）以产生通道注意力图。</p>
<p>2、<strong>空间注意力模块（Spatial Attention Module）</strong>：</p>
<p>利用特征之间的空间关系来生成空间注意力图。与通道注意力不同，空间注意力关注于“在哪里”是一个有信息的部分，这与通道注意力是互补的。为了计算空间注意力，CBAM首先沿着通道轴应用平均池化和最大池化操作，然后将它们连接起来生成一个高效的特征描述符。在该描述符上应用一个卷积层来生成空间注意力图。</p>
<p><strong>好处：</strong></p>
<p><strong>双重注意力机制</strong>：</p>
<p>CBAM首次将 <strong>通道注意力（Channel Attention）和空间注意力（Spatial Attention）顺序</strong> 结合起来，对输入特征进行两阶段的精炼。这种设计让模型先关注于“哪些通道是重要的”，然后再关注于“空间上哪些位置是重要的”，从而更加全面地捕获特征中的关键信息。</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-3-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-3-2"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-3-3">
</span><span id="__span-3-4"><span class="c1"># 通道注意力模块</span>
</span><span id="__span-3-5"><span class="k">class</span><span class="w"> </span><span class="nc">ChannelAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-3-6">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
</span><span id="__span-3-7">        <span class="nb">super</span><span class="p">(</span><span class="n">ChannelAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-3-8">        <span class="bp">self</span><span class="o">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 自适应平均池化</span>
</span><span id="__span-3-9">        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 自适应最大池化</span>
</span><span id="__span-3-10">
</span><span id="__span-3-11">        <span class="c1"># 两个卷积层用于从池化后的特征中学习注意力权重</span>
</span><span id="__span-3-12">        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">in_planes</span> <span class="o">//</span> <span class="n">ratio</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># 第一个卷积层，降维</span>
</span><span id="__span-3-13">        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>  <span class="c1"># ReLU激活函数</span>
</span><span id="__span-3-14">        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span> <span class="o">//</span> <span class="n">ratio</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># 第二个卷积层，升维</span>
</span><span id="__span-3-15">        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>  <span class="c1"># Sigmoid函数生成最终的注意力权重</span>
</span><span id="__span-3-16">
</span><span id="__span-3-17">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-3-18">        <span class="n">avg_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>  <span class="c1"># 对平均池化的特征进行处理</span>
</span><span id="__span-3-19">        <span class="n">max_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>  <span class="c1"># 对最大池化的特征进行处理</span>
</span><span id="__span-3-20">        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_out</span> <span class="o">+</span> <span class="n">max_out</span>  <span class="c1"># 将两种池化的特征加权和作为输出</span>
</span><span id="__span-3-21">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># 使用sigmoid激活函数计算注意力权重</span>
</span><span id="__span-3-22">
</span><span id="__span-3-23"><span class="c1"># 空间注意力模块</span>
</span><span id="__span-3-24"><span class="k">class</span><span class="w"> </span><span class="nc">SpatialAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-3-25">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
</span><span id="__span-3-26">        <span class="nb">super</span><span class="p">(</span><span class="n">SpatialAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-3-27">
</span><span id="__span-3-28">        <span class="k">assert</span> <span class="n">kernel_size</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="s1">&#39;kernel size must be 3 or 7&#39;</span>  <span class="c1"># 核心大小只能是3或7</span>
</span><span id="__span-3-29">        <span class="n">padding</span> <span class="o">=</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="mi">7</span> <span class="k">else</span> <span class="mi">1</span>  <span class="c1"># 根据核心大小设置填充</span>
</span><span id="__span-3-30">
</span><span id="__span-3-31">        <span class="c1"># 卷积层用于从连接的平均池化和最大池化特征图中学习空间注意力权重</span>
</span><span id="__span-3-32">        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  
</span><span id="__span-3-33">        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>  <span class="c1"># Sigmoid函数生成最终的注意力权重</span>
</span><span id="__span-3-34">
</span><span id="__span-3-35">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-3-36">        <span class="n">avg_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 对输入特征图执行平均池化</span>
</span><span id="__span-3-37">        <span class="n">max_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 对输入特征图执行最大池化</span>
</span><span id="__span-3-38">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">avg_out</span><span class="p">,</span> <span class="n">max_out</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 将两种池化的特征图连接起来</span>
</span><span id="__span-3-39">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 通过卷积层处理连接后的特征图</span>
</span><span id="__span-3-40">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 使用sigmoid激活函数计算注意力权重</span>
</span><span id="__span-3-41">
</span><span id="__span-3-42"><span class="c1"># CBAM模块</span>
</span><span id="__span-3-43"><span class="k">class</span><span class="w"> </span><span class="nc">CBAM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-3-44">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
</span><span id="__span-3-45">        <span class="nb">super</span><span class="p">(</span><span class="n">CBAM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-3-46">        <span class="bp">self</span><span class="o">.</span><span class="n">ca</span> <span class="o">=</span> <span class="n">ChannelAttention</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">ratio</span><span class="p">)</span>  <span class="c1"># 通道注意力实例</span>
</span><span id="__span-3-47">        <span class="bp">self</span><span class="o">.</span><span class="n">sa</span> <span class="o">=</span> <span class="n">SpatialAttention</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>  <span class="c1"># 空间注意力实例</span>
</span><span id="__span-3-48">
</span><span id="__span-3-49">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-3-50">        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ca</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 使用通道注意力加权输入特征图</span>
</span><span id="__span-3-51">        <span class="n">result</span> <span class="o">=</span> <span class="n">out</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># 使用空间注意力进一步加权特征图</span>
</span><span id="__span-3-52">        <span class="k">return</span> <span class="n">result</span>  <span class="c1"># 返回最终的特征图</span>
</span><span id="__span-3-53">
</span><span id="__span-3-54"><span class="c1"># 示例使用</span>
</span><span id="__span-3-55"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-3-56">    <span class="n">block</span> <span class="o">=</span> <span class="n">CBAM</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>  <span class="c1"># 创建一个CBAM模块，输入通道为64</span>
</span><span id="__span-3-57">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># 随机生成一个输入特征图</span>
</span><span id="__span-3-58">    <span class="n">output</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># 通过CBAM模块处理输入特征图</span>
</span><span id="__span-3-59">    <span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># 打印输入和输出的</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="eca">√ECA<a class="headerlink" href="#eca" title="Permanent link">&para;</a></h2>
<p>论文《ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks》</p>
<p><img alt="image-20250222124459369" src="../images/image-20250222124459369.png" /></p>
<p><img alt="image-20250222124515086" src="../images/image-20250222124515086.png" /></p>
<p><strong>作用：</strong></p>
<p>ECA模块旨在通过引入一种高效的通道注意力机制来增强深度卷积神经网络的特征表示能力。它着重于捕获通道间的动态依赖关系，从而使网络能够更加精确地重视对当前任务更重要的特征，提升模型在各种视觉任务上的性能。</p>
<p><strong>机制：</strong></p>
<p>ECA模块的核心机制是通过一个简单而高效的**一维卷积**来自适应地捕捉通道之间的依赖性，而**无需降维和升维**的过程。这种设计避免了传统注意力机制中复杂的多层感知机（MLP）结构，减少了模型复杂度和计算负担。ECA通过计算一个自适应的核大小，直接在通道特征上应用一维卷积，从而学习到每个通道相对于其他通道的重要性。</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-4-1"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-4-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-4-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-4-4"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">init</span>
</span><span id="__span-4-5"><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
</span><span id="__span-4-6">
</span><span id="__span-4-7"><span class="s2">&quot;ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks&quot;</span>
</span><span id="__span-4-8">
</span><span id="__span-4-9"><span class="k">class</span><span class="w"> </span><span class="nc">ECAAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-4-10">
</span><span id="__span-4-11">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span><span id="__span-4-12">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-4-13">        <span class="bp">self</span><span class="o">.</span><span class="n">gap</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-4-14">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-4-15"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-4-16"><span class="sd">            参数说明：</span>
</span><span id="__span-4-17"><span class="sd">            in_channels=1：输入通道数为1</span>
</span><span id="__span-4-18"><span class="sd">            out_channels=1：输出通道数为1</span>
</span><span id="__span-4-19"><span class="sd">            kernel_size=kernel_size：卷积核的大小，这里由构造函数的参数 kernel_size 指定</span>
</span><span id="__span-4-20"><span class="sd">            padding=(kernel_size-1)//2：填充大小，这里使用了对称填充，使得卷积操作后输出的长度与输入的长度相同</span>
</span><span id="__span-4-21"><span class="sd">            问题：1D 卷积和 2D卷积的区别是什么？</span>
</span><span id="__span-4-22"><span class="sd">            答：1D 卷积和 2D 卷积都有通道的概念，不同的是，1D 卷积，卷积的是序列，2D 卷积卷积的图</span>
</span><span id="__span-4-23"><span class="sd">            区别就是卷积的对象不同</span>
</span><span id="__span-4-24">
</span><span id="__span-4-25"><span class="sd">            这个 padding = （kernel_size - 1）//2 也是不变卷积</span>
</span><span id="__span-4-26"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-4-27">        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
</span><span id="__span-4-28">
</span><span id="__span-4-29">    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-4-30">        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="__span-4-31">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span><span id="__span-4-32">                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>
</span><span id="__span-4-33">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-4-34">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-4-35">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
</span><span id="__span-4-36">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-4-37">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-4-38">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="__span-4-39">                <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</span><span id="__span-4-40">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-4-41">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-4-42">
</span><span id="__span-4-43">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-4-44">        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gap</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 在空间方向执行全局平均池化: (B,C,H,W)--&gt;(B,C,1,1)</span>
</span><span id="__span-4-45">        <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 将通道描述符去掉一维,便于在通道上执行卷积操作:(B,C,1,1)--&gt;(B,C,1)--&gt;(B,1,C)</span>
</span><span id="__span-4-46">        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 在通道维度上执行1D卷积操作,建模局部通道之间的相关性: (B,1,C)--&gt;(B,1,C) 1: 表示单通道，C : 表示每个通道 C 个元素</span>
</span><span id="__span-4-47">        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 生成权重表示: (B,1,C) 对所有元素 sigmoid，因为 sigmoid 生成的是绝对权重</span>
</span><span id="__span-4-48">        <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 重塑shape: (B,1,C)--&gt;(B,C,1)--&gt;(B,C,1,1)</span>
</span><span id="__span-4-49">        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 权重对输入的通道进行重新加权: (B,C,H,W) * (B,C,1,1) = (B,C,H,W)</span>
</span><span id="__span-4-50">
</span><span id="__span-4-51">
</span><span id="__span-4-52">
</span><span id="__span-4-53">
</span><span id="__span-4-54"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-4-55">    <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-4-56">    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-4-57">    <span class="n">Model</span> <span class="o">=</span> <span class="n">ECAAttention</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span id="__span-4-58">    <span class="n">output</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-4-59">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p><strong>优势：</strong></p>
<p>1、<strong>无需降维升维</strong>：</p>
<p>与传统的注意力机制相比，ECA模块无需进行降维和升维的操作，这样不仅保留了原始通道特征的信息完整性，还进一步减少了模型复杂度。</p>
<p>、<strong>自适应核大小</strong>：</p>
<p>ECA模块根据通道数自适应地调整一维卷积的核大小，使其能够灵活地捕捉不同范围内的通道依赖性，这种自适应机制使得ECA在不同规模的网络和不同深度的层次中都能有效工作。</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-5-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-5-2"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-5-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">init</span>
</span><span id="__span-5-4">
</span><span id="__span-5-5"><span class="c1"># 定义ECA注意力模块的类</span>
</span><span id="__span-5-6"><span class="k">class</span><span class="w"> </span><span class="nc">ECAAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-5-7">
</span><span id="__span-5-8">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span><span id="__span-5-9">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-5-10">        <span class="bp">self</span><span class="o">.</span><span class="n">gap</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 定义全局平均池化层，将空间维度压缩为1x1</span>
</span><span id="__span-5-11">        <span class="c1"># 定义一个1D卷积，用于处理通道间的关系，核大小可调，padding保证输出通道数不变</span>
</span><span id="__span-5-12">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="__span-5-13">        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>  <span class="c1"># Sigmoid函数，用于激活最终的注意力权重</span>
</span><span id="__span-5-14">
</span><span id="__span-5-15">    <span class="c1"># 权重初始化方法</span>
</span><span id="__span-5-16">    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-5-17">        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="__span-5-18">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span><span id="__span-5-19">                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>  <span class="c1"># 对Conv2d层使用Kaiming初始化</span>
</span><span id="__span-5-20">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-5-21">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 如果有偏置项，则初始化为0</span>
</span><span id="__span-5-22">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
</span><span id="__span-5-23">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 批归一化层权重初始化为1</span>
</span><span id="__span-5-24">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 批归一化层偏置初始化为0</span>
</span><span id="__span-5-25">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="__span-5-26">                <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># 全连接层权重使用正态分布初始化</span>
</span><span id="__span-5-27">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-5-28">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 全连接层偏置初始化为0</span>
</span><span id="__span-5-29">
</span><span id="__span-5-30">    <span class="c1"># 前向传播方法</span>
</span><span id="__span-5-31">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-5-32">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gap</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 对输入x应用全局平均池化，得到bs,c,1,1维度的输出</span>
</span><span id="__span-5-33">        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 移除最后一个维度并转置，为1D卷积准备，变为bs,1,c</span>
</span><span id="__span-5-34">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 对转置后的y应用1D卷积，得到bs,1,c维度的输出</span>
</span><span id="__span-5-35">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 应用Sigmoid函数激活，得到最终的注意力权重</span>
</span><span id="__span-5-36">        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 再次转置并增加一个维度，以匹配原始输入x的维度</span>
</span><span id="__span-5-37">        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 将注意力权重应用到原始输入x上，通过广播机制扩展维度并执行逐元素乘法</span>
</span><span id="__span-5-38">
</span><span id="__span-5-39"><span class="c1"># 示例使用</span>
</span><span id="__span-5-40"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-5-41">    <span class="n">block</span> <span class="o">=</span> <span class="n">ECAAttention</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># 实例化ECA注意力模块，指定核大小为3</span>
</span><span id="__span-5-42">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># 生成一个随机输入</span>
</span><span id="__span-5-43">    <span class="n">output</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># 将输入通过ECA模块处理</span>
</span><span id="__span-5-44">    <span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># 打印输入和输出的尺寸，验证ECA模块的作用</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="coordinate-attention">√ Coordinate Attention<a class="headerlink" href="#coordinate-attention" title="Permanent link">&para;</a></h2>
<p>论文《Coordinate Attention for Efficient Mobile Network Design》</p>
<p><img alt="image-20250222170038241" src="../images/image-20250222170038241.png" /></p>
<p><img alt="image-20250222170042846" src="../images/image-20250222170042846.png" /></p>
<ul>
<li>Coordinate Attention提出了一种新的注意力机制，用于在移动网络中嵌入位置信息到通道注意力中。这种方法不仅关注“哪些通道是重要的”，而且关注“在哪里”关注，通过更精细地控制空间选择性注意力图的生成，进一步提升模型性能。</li>
</ul>
<p><strong>机制：</strong></p>
<p>1、<strong>坐标信息嵌入</strong>：</p>
<p>与传统的通道注意力通过2D全局池化将特征张量转换为单一特征向量不同，Coordinate Attention将通道注意力分解为两个1D特征编码过程，分别沿两个空间方向聚合特征。这种方法能够捕捉沿一个空间方向的长程依赖性，同时保留沿另一个空间方向的精确位置信息。</p>
<p>2、<strong>坐标注意力生成</strong>：</p>
<p>将沿垂直和水平方向聚合的特征图编码成一对方向感知和位置敏感的注意力图，这两个注意力图被互补地应用到输入特征图上，增强了对兴趣对象的表示。</p>
<p><strong>优势：</strong> </p>
<p>1、<strong>方向感知和位置敏感</strong>：</p>
<p>Coordinate Attention通过生成方向感知和位置敏感的注意力图，使模型能够更准确地定位和识别兴趣对象。这种注意力图能够精确地高亮兴趣区域，提升了模型对空间结构的理解能力。</p>
<p>2、<strong>灵活性和轻量级</strong>：</p>
<p>Coordinate Attention的设计简洁而高效，可以轻松嵌入到经典的移动网络结构中，如MobileNetV2、MobileNeXt和EfficientNet，几乎不增加计算开销，适用于计算资源受限的环境。</p>
<p>3、<strong>跨任务性能提升</strong>：</p>
<p>Coordinate Attention不仅在ImageNet分类任务上有效，更在下游任务如对象检测和语义分割上展现出更好的性能。这证明了其对于捕捉关键信息的能力，尤其在需要密集预测的任务中表现出色。</p>
<p><strong>优质注释》</strong> 记得学学</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-6-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-6-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-6-3"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
</span><span id="__span-6-4">
</span><span id="__span-6-5"><span class="s2">&quot;Coordinate Attention for Efficient Mobile Network Design&quot;</span>
</span><span id="__span-6-6">
</span><span id="__span-6-7"><span class="k">class</span><span class="w"> </span><span class="nc">h_sigmoid</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-6-8">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span><span id="__span-6-9">        <span class="nb">super</span><span class="p">(</span><span class="n">h_sigmoid</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-6-10">        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
</span><span id="__span-6-11">
</span><span id="__span-6-12">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-6-13">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">6</span>
</span><span id="__span-6-14">
</span><span id="__span-6-15"><span class="k">class</span><span class="w"> </span><span class="nc">h_swish</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-6-16">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span><span id="__span-6-17">        <span class="nb">super</span><span class="p">(</span><span class="n">h_swish</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-6-18">        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">h_sigmoid</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
</span><span id="__span-6-19">
</span><span id="__span-6-20">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-6-21">        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-6-22">
</span><span id="__span-6-23"><span class="k">class</span><span class="w"> </span><span class="nc">CoordAtt</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-6-24">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">oup</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
</span><span id="__span-6-25">        <span class="nb">super</span><span class="p">(</span><span class="n">CoordAtt</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-6-26">        <span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="__span-6-27">
</span><span id="__span-6-28"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-6-29"><span class="sd">            自适应平均池化：</span>
</span><span id="__span-6-30"><span class="sd">            自适应平均池化不需要指定池化窗口的大小和步幅，而是直接指定输出的特征图大小</span>
</span><span id="__span-6-31"><span class="sd">            (None, 1)</span>
</span><span id="__span-6-32"><span class="sd">            None: 在第一个维度（通常是高度）上，输出的尺寸将自动调整以匹配输入的高度。也就是说，输入的高度是多少，输出的高度就是多少</span>
</span><span id="__span-6-33"><span class="sd">            1: 在第二个维度（通常是宽度）上，输出的宽度将被调整为 1。</span>
</span><span id="__span-6-34"><span class="sd">            经过 nn.AdaptiveAvgPool2d((None, 1)) 操作后，输出的特征图大小将变为 (C, H, 1)。也就是说，宽度被压缩为 1，而高度保持不变</span>
</span><span id="__span-6-35"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-6-36">        <span class="bp">self</span><span class="o">.</span><span class="n">pool_w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
</span><span id="__span-6-37">        <span class="n">mip</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">inp</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">)</span>
</span><span id="__span-6-38">
</span><span id="__span-6-39">        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">mip</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-6-40">        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mip</span><span class="p">)</span>
</span><span id="__span-6-41">        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">h_swish</span><span class="p">()</span>
</span><span id="__span-6-42">        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</span><span id="__span-6-43">
</span><span id="__span-6-44">        <span class="bp">self</span><span class="o">.</span><span class="n">conv_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mip</span><span class="p">,</span> <span class="n">oup</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-6-45">        <span class="bp">self</span><span class="o">.</span><span class="n">conv_w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mip</span><span class="p">,</span> <span class="n">oup</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-6-46">
</span><span id="__span-6-47">
</span><span id="__span-6-48">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-6-49">        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="__span-6-50">
</span><span id="__span-6-51">        <span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span><span id="__span-6-52">        <span class="n">x_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 压缩水平方向: (B, C, H, W) --&gt; (B, C, H, 1)</span>
</span><span id="__span-6-53">        <span class="n">x_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_w</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 压缩垂直方向: (B, C, H, W) --&gt; (B, C, 1, W) --&gt; (B,C,W,1)</span>
</span><span id="__span-6-54">
</span><span id="__span-6-55">        <span class="c1"># 坐标注意力生成</span>
</span><span id="__span-6-56">        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x_h</span><span class="p">,</span> <span class="n">x_w</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 拼接水平和垂直方向的向量: (B,C,H+W,1)</span>
</span><span id="__span-6-57">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 通过Conv进行变换,并降维: (B,C,H+W,1)--&gt; (B,d,H+W,1)</span>
</span><span id="__span-6-58">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>   <span class="c1"># BatchNorm操作: (B,d,H+W,1)</span>
</span><span id="__span-6-59">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Relu操作: (B,d,H+W,1)</span>
</span><span id="__span-6-60">
</span><span id="__span-6-61">        <span class="n">x_h</span><span class="p">,</span> <span class="n">x_w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 沿着空间方向重新分割为两部分: (B,d,H+W,1)--&gt; x_h:(B,d,H,1); x_w:(B,d,W,1)</span>
</span><span id="__span-6-62">        <span class="n">x_w</span> <span class="o">=</span> <span class="n">x_w</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># x_w: (B,d,W,1)--&gt; (B,d,1,W)</span>
</span><span id="__span-6-63">
</span><span id="__span-6-64">        <span class="n">a_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_h</span><span class="p">(</span><span class="n">x_h</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="c1"># 恢复与输入相同的通道数,并生成垂直方向的权重: (B,d,H,1)--&gt;(B,C,H,1)</span>
</span><span id="__span-6-65">        <span class="n">a_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_w</span><span class="p">(</span><span class="n">x_w</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="c1"># 恢复与输入相同的通道数,并生成水平方向的权重: (B,d,1,W)--&gt;(B,C,1,W)</span>
</span><span id="__span-6-66"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-6-67"><span class="sd">            问题：sigmoid 在哪个方向进行？</span>
</span><span id="__span-6-68"><span class="sd">            答：</span>
</span><span id="__span-6-69"><span class="sd">                sigmoid 是逐元素应用的，不依赖于特定的方向</span>
</span><span id="__span-6-70"><span class="sd">                对每个输入元素独立地进行计算，而不是在某个特定的方向上进行</span>
</span><span id="__span-6-71"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-6-72">
</span><span id="__span-6-73">        <span class="n">out</span> <span class="o">=</span> <span class="n">identity</span> <span class="o">*</span> <span class="n">a_w</span> <span class="o">*</span> <span class="n">a_h</span> <span class="c1"># 将垂直、水平方向权重应用于输入,从而反映感兴趣的对象是否存在于相应的行和列中: (B,C,H,W) * (B,C,1,W) * (B,C,H,1) = (B,C,H,W)</span>
</span><span id="__span-6-74"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-6-75"><span class="sd">        广播机制：</span>
</span><span id="__span-6-76"><span class="sd">            identity 是一个形状为 (M, N) 的数组</span>
</span><span id="__span-6-77"><span class="sd">            a_w 是一个形状为 (M, 1) 的数组 → a_w 的形状会被广播为 (M, N)，即在列方向上复制 N 次</span>
</span><span id="__span-6-78"><span class="sd">            a_h 是一个形状为 (1, N) 的数组 → a_h 的形状会被广播为 (M, N)，即在行方向上复制 M 次</span>
</span><span id="__span-6-79"><span class="sd">            最后：逐元素相乘。identity、a_w 和 a_h 的形状都变为 (M, N)，可以进行逐元素相乘</span>
</span><span id="__span-6-80"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-6-81">
</span><span id="__span-6-82">        <span class="k">return</span> <span class="n">out</span>
</span><span id="__span-6-83">
</span><span id="__span-6-84"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-6-85">    <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-6-86">    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-6-87">    <span class="n">Model</span> <span class="o">=</span> <span class="n">CoordAtt</span><span class="p">(</span><span class="n">inp</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">oup</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span> <span class="c1"># input_channel,output_channel</span>
</span><span id="__span-6-88">    <span class="n">output</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-6-89">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>注释</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-7-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-7-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-7-3"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
</span><span id="__span-7-4">
</span><span id="__span-7-5"><span class="c1"># 定义h_sigmoid激活函数，这是一种硬Sigmoid函数</span>
</span><span id="__span-7-6"><span class="k">class</span><span class="w"> </span><span class="nc">h_sigmoid</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-7-7">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span><span id="__span-7-8">        <span class="nb">super</span><span class="p">(</span><span class="n">h_sigmoid</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-7-9">        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>  <span class="c1"># 使用ReLU6实现</span>
</span><span id="__span-7-10">
</span><span id="__span-7-11">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-7-12">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">6</span>  <span class="c1"># 公式为ReLU6(x+3)/6，模拟Sigmoid激活函数</span>
</span><span id="__span-7-13">
</span><span id="__span-7-14"><span class="c1"># 定义h_swish激活函数，这是基于h_sigmoid的Swish函数变体</span>
</span><span id="__span-7-15"><span class="k">class</span><span class="w"> </span><span class="nc">h_swish</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-7-16">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span><span id="__span-7-17">        <span class="nb">super</span><span class="p">(</span><span class="n">h_swish</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-7-18">        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">h_sigmoid</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>  <span class="c1"># 使用上面定义的h_sigmoid</span>
</span><span id="__span-7-19">
</span><span id="__span-7-20">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-7-21">        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 公式为x * h_sigmoid(x)</span>
</span><span id="__span-7-22">
</span><span id="__span-7-23"><span class="c1"># 定义Coordinate Attention模块</span>
</span><span id="__span-7-24"><span class="k">class</span><span class="w"> </span><span class="nc">CoordAtt</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-7-25">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">oup</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
</span><span id="__span-7-26">        <span class="nb">super</span><span class="p">(</span><span class="n">CoordAtt</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-7-27">        <span class="c1"># 定义水平和垂直方向的自适应平均池化</span>
</span><span id="__span-7-28">        <span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># 水平方向</span>
</span><span id="__span-7-29">        <span class="bp">self</span><span class="o">.</span><span class="n">pool_w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>  <span class="c1"># 垂直方向</span>
</span><span id="__span-7-30">
</span><span id="__span-7-31">        <span class="n">mip</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">inp</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">)</span>  <span class="c1"># 计算中间层的通道数</span>
</span><span id="__span-7-32">
</span><span id="__span-7-33">        <span class="c1"># 1x1卷积用于降维</span>
</span><span id="__span-7-34">        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">mip</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-7-35">        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mip</span><span class="p">)</span>  <span class="c1"># 批归一化</span>
</span><span id="__span-7-36">        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">h_swish</span><span class="p">()</span>  <span class="c1"># 激活函数</span>
</span><span id="__span-7-37">
</span><span id="__span-7-38">        <span class="c1"># 两个1x1卷积，分别对应水平和垂直方向</span>
</span><span id="__span-7-39">        <span class="bp">self</span><span class="o">.</span><span class="n">conv_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mip</span><span class="p">,</span> <span class="n">oup</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-7-40">        <span class="bp">self</span><span class="o">.</span><span class="n">conv_w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mip</span><span class="p">,</span> <span class="n">oup</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-7-41">
</span><span id="__span-7-42">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-7-43">        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># 保存输入作为残差连接</span>
</span><span id="__span-7-44">
</span><span id="__span-7-45">        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># 获取输入的尺寸</span>
</span><span id="__span-7-46">        <span class="n">x_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 水平方向池化</span>
</span><span id="__span-7-47">        <span class="n">x_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_w</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 垂直方向池化并交换维度以适应拼接</span>
</span><span id="__span-7-48">
</span><span id="__span-7-49">        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x_h</span><span class="p">,</span> <span class="n">x_w</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 拼接水平和垂直方向的特征</span>
</span><span id="__span-7-50">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 通过1x1卷积降维</span>
</span><span id="__span-7-51">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 批归一化</span>
</span><span id="__span-7-52">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 激活函数</span>
</span><span id="__span-7-53">
</span><span id="__span-7-54">        <span class="n">x_h</span><span class="p">,</span> <span class="n">x_w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 将特征拆分回水平和垂直方向</span>
</span><span id="__span-7-55">        <span class="n">x_w</span> <span class="o">=</span> <span class="n">x_w</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 恢复x_w的原始维度</span>
</span><span id="__span-7-56">
</span><span id="__span-7-57">        <span class="n">a_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_h</span><span class="p">(</span><span class="n">x_h</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>  <span class="c1"># 通过1x1卷积并应用Sigmoid获取水平方向的注意力权重</span>
</span><span id="__span-7-58">        <span class="n">a_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_w</span><span class="p">(</span><span class="n">x_w</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>  <span class="c1"># 通过1x1卷积并应用Sigmoid获取垂直方向的注意力权重</span>
</span><span id="__span-7-59">
</span><span id="__span-7-60">        <span class="n">out</span> <span class="o">=</span> <span class="n">identity</span> <span class="o">*</span> <span class="n">a_w</span> <span class="o">*</span> <span class="n">a_h</span>  <span class="c1"># 应用注意力权重到输入特征，并与残差连接相乘</span>
</span><span id="__span-7-61">
</span><span id="__span-7-62">        <span class="k">return</span> <span class="n">out</span>  <span class="c1"># 返回输出</span>
</span><span id="__span-7-63">
</span><span id="__span-7-64"><span class="c1"># 示例使用</span>
</span><span id="__span-7-65"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-7-66">    <span class="n">block</span> <span class="o">=</span> <span class="n">CoordAtt</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># 实例化Coordinate Attention模块</span>
</span><span id="__span-7-67">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># 创建一个随机输入</span>
</span><span id="__span-7-68">    <span class="n">output</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># 通过模块处理输入</span>
</span><span id="__span-7-69">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>  <span class="c1"># 打印输入和输出的尺寸</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="cotattention">CoTAttention<a class="headerlink" href="#cotattention" title="Permanent link">&para;</a></h2>
<p>论文《Contextual Transformer Networks for Visual Recognition》</p>
<p><img alt="image-20250222124330864" src="../images/image-20250222124330864.png" /></p>
<p><img alt="image-20250222124345663" src="../images/image-20250222124345663.png" /></p>
<p><img alt="image-20250222124401106" src="../images/image-20250222124401106.png" /></p>
<blockquote>
<p>作用</p>
</blockquote>
<p>Contextual Transformer (CoT) block 设计为视觉识别的一种新颖的 Transformer 风格模块。该设计充分利用输入键之间的上下文信息指导动态注意力矩阵的学习，从而加强视觉表示的能力。CoT block 首先通过 3x3 卷积对输入键进行上下文编码，得到输入的静态上下文表示。然后，将编码后的键与输入查询合并，通过两个连续的 1x1 卷积学习动态多头注意力矩阵。学习到的注意力矩阵乘以输入值，实现输入的动态上下文表示。最终将静态和动态上下文表示的融合作为输出。</p>
<blockquote>
<p>机制</p>
</blockquote>
<p>1、<strong>上下文编码</strong>：</p>
<p>通过 3x3 卷积在所有邻居键内部空间上下文化每个键表示，捕获键之间的静态上下文信息。</p>
<p>2、<strong>动态注意力学习</strong>：</p>
<p>基于查询和上下文化的键的连接，通过两个连续的 1x1 卷积产生注意力矩阵，这一过程自然地利用每个查询和所有键之间的相互关系进行自我注意力学习，并由静态上下文指导。</p>
<p>3、<strong>静态和动态上下文的融合</strong>：</p>
<p>将静态上下文和通过上下文化自注意力得到的动态上下文结合，作为 CoT block 的最终输出。</p>
<blockquote>
<p>优势</p>
</blockquote>
<p>1、<strong>上下文感知</strong>：</p>
<p>CoT 通过在自注意力学习中探索输入键之间的富上下文信息，使模型能够更准确地捕获视觉内容的细微差异。</p>
<p>2、<strong>动静态上下文的统一</strong>：</p>
<p>CoT 设计巧妙地将上下文挖掘与自注意力学习统一到单一架构中，既利用键之间的静态关系又探索动态特征交互，提升了模型的表达能力。</p>
<p>3、<strong>灵活替换与优化</strong>：</p>
<p>CoT block 可以直接替换现有 ResNet 架构中的标准卷积，不增加参数和 FLOP 预算的情况下实现转换为 Transformer 风格的骨干网络（CoTNet），通过广泛的实验验证了其在多种应用（如图像识别、目标检测和实例分割）中的优越性。</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-8-1"><span class="c1"># 导入必要的PyTorch模块</span>
</span><span id="__span-8-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-8-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-8-4"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
</span><span id="__span-8-5">
</span><span id="__span-8-6"><span class="k">class</span><span class="w"> </span><span class="nc">CoTAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-8-7">    <span class="c1"># 初始化CoT注意力模块</span>
</span><span id="__span-8-8">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span><span id="__span-8-9">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-8-10">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>  <span class="c1"># 输入的通道数</span>
</span><span id="__span-8-11">        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>  <span class="c1"># 卷积核大小</span>
</span><span id="__span-8-12">
</span><span id="__span-8-13">        <span class="c1"># 定义用于键(key)的卷积层，包括一个分组卷积，BatchNorm和ReLU激活</span>
</span><span id="__span-8-14">        <span class="bp">self</span><span class="o">.</span><span class="n">key_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="__span-8-15">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">kernel_size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-8-16">            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span>
</span><span id="__span-8-17">            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</span><span id="__span-8-18">        <span class="p">)</span>
</span><span id="__span-8-19">
</span><span id="__span-8-20">        <span class="c1"># 定义用于值(value)的卷积层，包括一个1x1卷积和BatchNorm</span>
</span><span id="__span-8-21">        <span class="bp">self</span><span class="o">.</span><span class="n">value_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="__span-8-22">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-8-23">            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span><span id="__span-8-24">        <span class="p">)</span>
</span><span id="__span-8-25">
</span><span id="__span-8-26">        <span class="c1"># 缩小因子，用于降低注意力嵌入的维度</span>
</span><span id="__span-8-27">        <span class="n">factor</span> <span class="o">=</span> <span class="mi">4</span>
</span><span id="__span-8-28">        <span class="c1"># 定义注意力嵌入层，由两个卷积层、一个BatchNorm层和ReLU激活组成</span>
</span><span id="__span-8-29">        <span class="bp">self</span><span class="o">.</span><span class="n">attention_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="__span-8-30">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">dim</span><span class="o">//</span><span class="n">factor</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-8-31">            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">dim</span><span class="o">//</span><span class="n">factor</span><span class="p">),</span>
</span><span id="__span-8-32">            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span><span id="__span-8-33">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">dim</span><span class="o">//</span><span class="n">factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">*</span><span class="n">kernel_size</span><span class="o">*</span><span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-8-34">        <span class="p">)</span>
</span><span id="__span-8-35">
</span><span id="__span-8-36">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-8-37">        <span class="c1"># 前向传播函数</span>
</span><span id="__span-8-38">        <span class="n">bs</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># 输入特征的尺寸</span>
</span><span id="__span-8-39">        <span class="n">k1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 生成键的静态表示</span>
</span><span id="__span-8-40">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 生成值的表示并调整形状</span>
</span><span id="__span-8-41">
</span><span id="__span-8-42">        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k1</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 将键的静态表示和原始输入连接</span>
</span><span id="__span-8-43">        <span class="n">att</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_embed</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 生成动态注意力权重</span>
</span><span id="__span-8-44">        <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</span><span id="__span-8-45">        <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 计算注意力权重的均值并调整形状</span>
</span><span id="__span-8-46">        <span class="n">k2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">v</span>  <span class="c1"># 应用注意力权重到值上</span>
</span><span id="__span-8-47">        <span class="n">k2</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>  <span class="c1"># 调整形状以匹配输出</span>
</span><span id="__span-8-48">
</span><span id="__span-8-49">        <span class="k">return</span> <span class="n">k1</span> <span class="o">+</span> <span class="n">k2</span>  <span class="c1"># 返回键的静态和动态表示的总和</span>
</span><span id="__span-8-50">
</span><span id="__span-8-51"><span class="c1"># 实例化CoTAttention模块并测试</span>
</span><span id="__span-8-52"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-8-53">    <span class="n">block</span> <span class="o">=</span> <span class="n">CoTAttention</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>  <span class="c1"># 创建一个输入通道数为64的CoTAttention实例</span>
</span><span id="__span-8-54">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># 创建一个随机输入</span>
</span><span id="__span-8-55">    <span class="n">output</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># 通过CoTAttention模块处理输入</span>
</span><span id="__span-8-56">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 打印输入和输出的尺寸</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="tripletattention">√ TripletAttention<a class="headerlink" href="#tripletattention" title="Permanent link">&para;</a></h2>
<p>论文《Rotate to Attend: Convolutional Triplet Attention Module》三分支注意力</p>
<p><img alt="image-20250222124137926" src="../images/image-20250222124137926.png" /></p>
<p><img alt="image-20250222124159250" src="../images/image-20250222124159250.png" /></p>
<p><img alt="image-20250222172404187" src="../images/image-20250222172404187.png" /></p>
<p>Triplet Attention是一种新颖的注意力机制，它通过**捕获跨维度交互**，利用**三分支结构**来计算注意力权重。对于输入张量，Triplet Attention通过**旋转操作**建立维度间的依赖关系，随后通过残差变换对信道和空间信息进行编码，实现了几乎不增加计算成本的情况下，有效增强视觉表征的能力。</p>
<blockquote>
<p>机制</p>
</blockquote>
<p>1、<strong>三分支结构</strong>：</p>
<p>Triplet Attention包含三个分支，每个分支负责捕获输入的空间维度H或W与信道维度C之间的交互特征。</p>
<p>2、<strong>跨维度交互</strong>：</p>
<p>通过在每个分支中对输入张量进行排列（permute）操作，并通过Z-pool和k×k的卷积层处理，以捕获跨维度的交互特征。</p>
<p>3、<strong>注意力权重的生成</strong>：</p>
<p>利用sigmoid激活层生成注意力权重，并应用于排列后的输入张量，然后将其排列回原始输入形状。</p>
<blockquote>
<p>独特优势</p>
</blockquote>
<p>1、<strong>跨维度交互</strong>：</p>
<p>Triplet Attention通过捕获输入张量的跨维度交互，提供了丰富的判别特征表征，较之前的注意力机制（如SENet、CBAM等）能够更有效地增强网络的性能。</p>
<p>2、<strong>几乎无计算成本增加</strong>：</p>
<p>相比于传统的注意力机制，Triplet Attention在提升网络性能的同时，几乎不增加额外的计算成本和参数数量，使得它可以轻松地集成到经典的骨干网络中。</p>
<p>3、<strong>无需降维</strong>：</p>
<p>与其他注意力机制不同，Triplet Attention不进行维度降低处理，这避免了因降维可能导致的信息丢失，保证了信道与权重间的直接对应关系。</p>
<p>优质注释：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-9-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-9-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-9-3">
</span><span id="__span-9-4"><span class="s2">&quot;Rotate to Attend: Convolutional Triplet Attention Module&quot;</span>
</span><span id="__span-9-5">
</span><span id="__span-9-6"><span class="k">class</span><span class="w"> </span><span class="nc">BasicConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-9-7">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="__span-9-8">        <span class="nb">super</span><span class="p">(</span><span class="n">BasicConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-9-9">        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_planes</span>
</span><span id="__span-9-10">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
</span><span id="__span-9-11">        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_planes</span><span class="p">,</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">bn</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="__span-9-12">        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span> <span class="k">if</span> <span class="n">relu</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="__span-9-13">
</span><span id="__span-9-14">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-9-15">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-9-16">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-9-17">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-9-18">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-9-19">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-9-20">        <span class="k">return</span> <span class="n">x</span>
</span><span id="__span-9-21">
</span><span id="__span-9-22"><span class="k">class</span><span class="w"> </span><span class="nc">ZPool</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-9-23">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-9-24">        <span class="c1"># 以建立CW之间的交互为例, x:(B, H, C, W)</span>
</span><span id="__span-9-25">        <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 全局最大池化: (B, H, C, W)-&gt;(B, 1, C, W);  torch.max返回的是数组:[最大值,对应索引]</span>
</span><span id="__span-9-26"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-9-27"><span class="sd">            torch.max 计算张量的最大值</span>
</span><span id="__span-9-28"><span class="sd">            x 是 输入张量</span>
</span><span id="__span-9-29"><span class="sd">            1 是 dim 参数，表示沿着第 1 维度（通常是行方向）计算最大值</span>
</span><span id="__span-9-30"><span class="sd">            torch.max(x, 1) 返回一个元组 (values, indices)，其中 values 是每行的最大值，indices 是对应的索引</span>
</span><span id="__span-9-31"><span class="sd">            [0] : [0] 取出元组中的第一个元素，即 values，也就是最大值</span>
</span><span id="__span-9-32"><span class="sd">            .unsqueeze(1) 在指定维度上增加一个大小为 1 的维度</span>
</span><span id="__span-9-33"><span class="sd">            values 的形状是 (n,)，那么 values.unsqueeze(1) 的形状将变为 (n, 1)</span>
</span><span id="__span-9-34">
</span><span id="__span-9-35"><span class="sd">            我懂了，想象的时候，要想成一张彩色的 RGB 图，不要想成数表，因此 torch.max(x,1)[0]的形状是 (B, 1, C, W)；对于一张图片来说有 W×C 个最大值，本来是彩色的 RGB 图，变成了一张灰度图，沿着通道方向取最大值</span>
</span><span id="__span-9-36"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-9-37">        <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># 全局平均池化: (B, H, C, W)-&gt;(B, 1, C, W);</span>
</span><span id="__span-9-38">        <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>       <span class="c1"># 在对应维度拼接最大和平均特征: (B, 2, C, W)</span>
</span><span id="__span-9-39">        <span class="k">return</span> <span class="n">c</span>
</span><span id="__span-9-40">
</span><span id="__span-9-41"><span class="k">class</span><span class="w"> </span><span class="nc">AttentionGate</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-9-42">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-9-43">        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionGate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-9-44">        <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">7</span>
</span><span id="__span-9-45">        <span class="bp">self</span><span class="o">.</span><span class="n">compress</span> <span class="o">=</span> <span class="n">ZPool</span><span class="p">()</span>
</span><span id="__span-9-46">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">BasicConv</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-9-47">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-9-48">        <span class="c1"># 以建立CW之间的交互为例, x:(B, H, C, W)</span>
</span><span id="__span-9-49">        <span class="n">x_compress</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 在对应维度上执行最大池化和平均池化,并将其拼接: (B, H, C, W) --&gt; (B, 2, C, W);</span>
</span><span id="__span-9-50">        <span class="n">x_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x_compress</span><span class="p">)</span> <span class="c1"># 通过conv操作将最大池化和平均池化特征映射到一维: (B, 2, C, W) --&gt; (B, 1, C, W);</span>
</span><span id="__span-9-51">        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid_</span><span class="p">(</span><span class="n">x_out</span><span class="p">)</span> <span class="c1"># 通过sigmoid函数生成权重: (B, 1, C, W);</span>
</span><span id="__span-9-52">        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">scale</span>              <span class="c1"># 对输入进行重新加权表示: (B, H, C, W) * (B, 1, C, W) = (B, H, C, W) 广播，复制，沿着通道方向复制成一样的</span>
</span><span id="__span-9-53">
</span><span id="__span-9-54"><span class="k">class</span><span class="w"> </span><span class="nc">TripletAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-9-55">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">no_spatial</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="__span-9-56">        <span class="nb">super</span><span class="p">(</span><span class="n">TripletAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-9-57">        <span class="bp">self</span><span class="o">.</span><span class="n">cw</span> <span class="o">=</span> <span class="n">AttentionGate</span><span class="p">()</span>
</span><span id="__span-9-58">        <span class="bp">self</span><span class="o">.</span><span class="n">hc</span> <span class="o">=</span> <span class="n">AttentionGate</span><span class="p">()</span>
</span><span id="__span-9-59">        <span class="bp">self</span><span class="o">.</span><span class="n">no_spatial</span><span class="o">=</span><span class="n">no_spatial</span>
</span><span id="__span-9-60">        <span class="k">if</span> <span class="ow">not</span> <span class="n">no_spatial</span><span class="p">:</span>
</span><span id="__span-9-61">            <span class="bp">self</span><span class="o">.</span><span class="n">hw</span> <span class="o">=</span> <span class="n">AttentionGate</span><span class="p">()</span>
</span><span id="__span-9-62">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-9-63">        <span class="c1"># 建立C和W之间的交互:</span>
</span><span id="__span-9-64">        <span class="n">x_perm1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># (B, C, H, W)--&gt; (B, H, C, W);  执行“旋转操作”,建立C和W之间的交互,所以要在H维度上压缩</span>
</span><span id="__span-9-65">        <span class="n">x_out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cw</span><span class="p">(</span><span class="n">x_perm1</span><span class="p">)</span> <span class="c1"># (B, H, C, W)--&gt;(B, H, C, W);  在H维度上进行压缩、拼接、Conv、sigmoid操作, 然后通过权重重新加权</span>
</span><span id="__span-9-66">        <span class="n">x_out11</span> <span class="o">=</span> <span class="n">x_out1</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># 恢复与输入相同的shape,也就是重新旋转回来: (B, H, C, W)--&gt;(B, C, H, W)</span>
</span><span id="__span-9-67">
</span><span id="__span-9-68">        <span class="c1"># 建立H和C之间的交互:</span>
</span><span id="__span-9-69">        <span class="n">x_perm2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># (B, C, H, W)--&gt; (B, W, H, C); 执行“旋转操作”,建立H和C之间的交互,所以要在W维度上压缩</span>
</span><span id="__span-9-70">        <span class="n">x_out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hc</span><span class="p">(</span><span class="n">x_perm2</span><span class="p">)</span> <span class="c1"># (B, W, H, C)--&gt;(B, W, H, C);  在W维度上进行压缩、拼接、Conv、sigmoid操作, 然后通过权重重新加权</span>
</span><span id="__span-9-71">        <span class="n">x_out21</span> <span class="o">=</span> <span class="n">x_out2</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># 恢复与输入相同的shape,也就是重新旋转回来: (B, W, H, C)--&gt;(B, C, H, W)</span>
</span><span id="__span-9-72">
</span><span id="__span-9-73">        <span class="c1"># 建立H和W之间的交互:</span>
</span><span id="__span-9-74">        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_spatial</span><span class="p">:</span>
</span><span id="__span-9-75">            <span class="n">x_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, C, H, W)--&gt;(B, C, H, W);  在C维度上进行压缩、拼接、Conv、sigmoid操作, 然后通过权重重新加权</span>
</span><span id="__span-9-76">            <span class="n">x_out</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_out</span> <span class="o">+</span> <span class="n">x_out11</span> <span class="o">+</span> <span class="n">x_out21</span><span class="p">)</span> <span class="c1"># 取三部分的平均值进行输出</span>
</span><span id="__span-9-77">        <span class="k">else</span><span class="p">:</span>
</span><span id="__span-9-78">            <span class="n">x_out</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_out11</span> <span class="o">+</span> <span class="n">x_out21</span><span class="p">)</span>
</span><span id="__span-9-79">        <span class="k">return</span> <span class="n">x_out</span>
</span><span id="__span-9-80">
</span><span id="__span-9-81"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-9-82">    <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-9-83">    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-9-84">    <span class="n">Model</span> <span class="o">=</span> <span class="n">TripletAttention</span><span class="p">()</span>
</span><span id="__span-9-85">    <span class="n">output</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-9-86">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<p>注释</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-10-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-10-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-10-3">
</span><span id="__span-10-4"><span class="c1"># 定义一个基本的卷积模块，包括卷积、批归一化和ReLU激活</span>
</span><span id="__span-10-5"><span class="k">class</span><span class="w"> </span><span class="nc">BasicConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-10-6">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="__span-10-7">        <span class="nb">super</span><span class="p">(</span><span class="n">BasicConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-10-8">        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_planes</span>
</span><span id="__span-10-9">        <span class="c1"># 定义卷积层</span>
</span><span id="__span-10-10">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
</span><span id="__span-10-11">        <span class="c1"># 条件性地添加批归一化层</span>
</span><span id="__span-10-12">        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_planes</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">bn</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="__span-10-13">        <span class="c1"># 条件性地添加ReLU激活函数</span>
</span><span id="__span-10-14">        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span> <span class="k">if</span> <span class="n">relu</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="__span-10-15">
</span><span id="__span-10-16">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-10-17">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 应用卷积</span>
</span><span id="__span-10-18">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-10-19">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 应用批归一化</span>
</span><span id="__span-10-20">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-10-21">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 应用ReLU</span>
</span><span id="__span-10-22">        <span class="k">return</span> <span class="n">x</span>
</span><span id="__span-10-23">
</span><span id="__span-10-24"><span class="c1"># 定义ZPool模块，结合最大池化和平均池化结果</span>
</span><span id="__span-10-25"><span class="k">class</span><span class="w"> </span><span class="nc">ZPool</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-10-26">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-10-27">        <span class="c1"># 结合最大值和平均值</span>
</span><span id="__span-10-28">        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-10-29">
</span><span id="__span-10-30"><span class="c1"># 定义注意力门，用于根据输入特征生成注意力权重</span>
</span><span id="__span-10-31"><span class="k">class</span><span class="w"> </span><span class="nc">AttentionGate</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-10-32">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-10-33">        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionGate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-10-34">        <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">7</span>  <span class="c1"># 设定卷积核大小</span>
</span><span id="__span-10-35">        <span class="bp">self</span><span class="o">.</span><span class="n">compress</span> <span class="o">=</span> <span class="n">ZPool</span><span class="p">()</span>  <span class="c1"># 使用ZPool模块</span>
</span><span id="__span-10-36">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">BasicConv</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># 通过卷积调整通道数</span>
</span><span id="__span-10-37">
</span><span id="__span-10-38">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-10-39">        <span class="n">x_compress</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 应用ZPool</span>
</span><span id="__span-10-40">        <span class="n">x_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x_compress</span><span class="p">)</span>  <span class="c1"># 通过卷积生成注意力权重</span>
</span><span id="__span-10-41">        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid_</span><span class="p">(</span><span class="n">x_out</span><span class="p">)</span>  <span class="c1"># 应用Sigmoid激活</span>
</span><span id="__span-10-42">        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">scale</span>  <span class="c1"># 将注意力权重乘以原始特征</span>
</span><span id="__span-10-43">
</span><span id="__span-10-44"><span class="c1"># 定义TripletAttention模块，结合了三种不同方向的注意力门</span>
</span><span id="__span-10-45"><span class="k">class</span><span class="w"> </span><span class="nc">TripletAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-10-46">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">no_spatial</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="__span-10-47">        <span class="nb">super</span><span class="p">(</span><span class="n">TripletAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-10-48">        <span class="bp">self</span><span class="o">.</span><span class="n">cw</span> <span class="o">=</span> <span class="n">AttentionGate</span><span class="p">()</span>  <span class="c1"># 定义宽度方向的注意力门</span>
</span><span id="__span-10-49">        <span class="bp">self</span><span class="o">.</span><span class="n">hc</span> <span class="o">=</span> <span class="n">AttentionGate</span><span class="p">()</span>  <span class="c1"># 定义高度方向的注意力门</span>
</span><span id="__span-10-50">        <span class="bp">self</span><span class="o">.</span><span class="n">no_spatial</span> <span class="o">=</span> <span class="n">no_spatial</span>  <span class="c1"># 是否忽略空间注意力</span>
</span><span id="__span-10-51">        <span class="k">if</span> <span class="ow">not</span> <span class="n">no_spatial</span><span class="p">:</span>
</span><span id="__span-10-52">            <span class="bp">self</span><span class="o">.</span><span class="n">hw</span> <span class="o">=</span> <span class="n">AttentionGate</span><span class="p">()</span>  <span class="c1"># 定义空间方向的注意力门</span>
</span><span id="__span-10-53">
</span><span id="__span-10-54">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-10-55">        <span class="c1"># 应用注意力门并结合结果</span>
</span><span id="__span-10-56">        <span class="n">x_perm1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># 转置以应用宽度方向的注意力</span>
</span><span id="__span-10-57">        <span class="n">x_out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cw</span><span class="p">(</span><span class="n">x_perm1</span><span class="p">)</span>
</span><span id="__span-10-58">        <span class="n">x_out11</span> <span class="o">=</span> <span class="n">x_out1</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># 还原转置</span>
</span><span id="__span-10-59">        <span class="n">x_perm2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># 转置以应用高度方向的注意力</span>
</span><span id="__span-10-60">        <span class="n">x_out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hc</span><span class="p">(</span><span class="n">x_perm2</span><span class="p">)</span>
</span><span id="__span-10-61">        <span class="n">x_out21</span> <span class="o">=</span> <span class="n">x_out2</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># 还原转置</span>
</span><span id="__span-10-62">        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_spatial</span><span class="p">:</span>
</span><span id="__span-10-63">            <span class="n">x_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 应用空间注意力</span>
</span><span id="__span-10-64">            <span class="n">x_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_out</span> <span class="o">+</span> <span class="n">x_out11</span> <span class="o">+</span> <span class="n">x_out21</span><span class="p">)</span>  <span class="c1"># 结合三个方向的结果</span>
</span><span id="__span-10-65">        <span class="k">else</span><span class="p">:</span>
</span><span id="__span-10-66">            <span class="n">x_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_out11</span> <span class="o">+</span> <span class="n">x_out21</span><span class="p">)</span>  <span class="c1"># 结合两个方向的结果（如果no_spatial为True）</span>
</span><span id="__span-10-67">        <span class="k">return</span> <span class="n">x_out</span>
</span><span id="__span-10-68">
</span><span id="__span-10-69"><span class="c1"># 示例代码</span>
</span><span id="__span-10-70"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-10-71">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>  <span class="c1"># 生成随机输入</span>
</span><span id="__span-10-72">    <span class="n">triplet</span> <span class="o">=</span> <span class="n">TripletAttention</span><span class="p">()</span>  <span class="c1"># 实例化TripletAttention</span>
</span><span id="__span-10-73">    <span class="n">output</span> <span class="o">=</span> <span class="n">triplet</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># 应用TripletAttention</span>
</span><span id="__span-10-74">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 打印输出形状</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="danet">√DANet<a class="headerlink" href="#danet" title="Permanent link">&para;</a></h2>
<p>标题：Dual Attention Network for Scene Segmentation</p>
<p>双重注意力网络</p>
<p><img alt="image-20250222124748077" src="../images/image-20250222124748077.png" /></p>
<p><img alt="image-20250222124830648" src="../images/image-20250222124830648.png" /></p>
<p><img alt="image-20250222124841327" src="../images/image-20250222124841327.png" /></p>
<p>我应该也学一下 自己绘制 计算流程图</p>
<p><img alt="image-20250222124930464" src="../images/image-20250222124930464.png" /></p>
<p><img alt="image-20250222124949328" src="../images/image-20250222124949328.png" /></p>
<p><img alt="image-20250222125003225" src="../images/image-20250222125003225.png" /></p>
<p><img alt="image-20250222125024697" src="../images/image-20250222125024697.png" /></p>
<p>优质注释</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-11-1"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-11-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-11-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-11-4"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">init</span>
</span><span id="__span-11-5">
</span><span id="__span-11-6"><span class="s2">&quot;Dual Attention Network for Scene Segmentation&quot;</span>
</span><span id="__span-11-7">
</span><span id="__span-11-8">
</span><span id="__span-11-9"><span class="k">class</span><span class="w"> </span><span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-11-10"><span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-11-11"><span class="sd">    Scaled dot-product attention</span>
</span><span id="__span-11-12"><span class="sd">    &#39;&#39;&#39;</span>
</span><span id="__span-11-13">
</span><span id="__span-11-14">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">.1</span><span class="p">):</span>
</span><span id="__span-11-15"><span class="w">        </span><span class="sd">&#39;&#39;&#39; 调用init：self.pa=ScaledDotProductAttention(d_model,d_k=d_model,d_v=d_model,h=1)</span>
</span><span id="__span-11-16"><span class="sd">        :param d_model: Output dimensionality of the model</span>
</span><span id="__span-11-17"><span class="sd">        :param d_k: Dimensionality of queries and keys</span>
</span><span id="__span-11-18"><span class="sd">        :param d_v: Dimensionality of values</span>
</span><span id="__span-11-19"><span class="sd">        :param h: Number of heads  也许我关于 QKV 的理解有误：nd ///  n_q×d_q . d_q × n_k . n_k × d_v</span>
</span><span id="__span-11-20"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-11-21">        <span class="nb">super</span><span class="p">(</span><span class="n">ScaledDotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-11-22">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">)</span>
</span><span id="__span-11-23">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">)</span>
</span><span id="__span-11-24">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">)</span>
</span><span id="__span-11-25">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span><span id="__span-11-26">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span><span id="__span-11-27">
</span><span id="__span-11-28">        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span id="__span-11-29">        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
</span><span id="__span-11-30">        <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_v</span>
</span><span id="__span-11-31">        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
</span><span id="__span-11-32">
</span><span id="__span-11-33">        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
</span><span id="__span-11-34">
</span><span id="__span-11-35">
</span><span id="__span-11-36">    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-11-37">        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="__span-11-38">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span><span id="__span-11-39">                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>
</span><span id="__span-11-40">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-11-41">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-11-42">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
</span><span id="__span-11-43">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-11-44">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-11-45">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="__span-11-46">                <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</span><span id="__span-11-47">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-11-48">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-11-49">
</span><span id="__span-11-50">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="__span-11-51"><span class="w">        </span><span class="sd">&#39;&#39;&#39;这里的调用 y=B,N(HW),C → y=self.pa(y,y,y) → self.pa=ScaledDotProductAttention(d_model,d_k=d_model,d_v=d_model,h=1)</span>
</span><span id="__span-11-52"><span class="sd">        Computes</span>
</span><span id="__span-11-53"><span class="sd">        :param queries: Queries (b_s, nq, d_model) == (B,N,C)</span>
</span><span id="__span-11-54"><span class="sd">        :param keys: Keys (b_s, nk, d_model) == (B,N,C)</span>
</span><span id="__span-11-55"><span class="sd">        :param values: Values (b_s, nk, d_model) == (B,N,C)</span>
</span><span id="__span-11-56"><span class="sd">        :param attention_mask: Mask over attention values (b_s, h, nq, nk); C=h*nk. True indicates masking.</span>
</span><span id="__span-11-57"><span class="sd">        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).</span>
</span><span id="__span-11-58"><span class="sd">        :return:</span>
</span><span id="__span-11-59"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-11-60">        <span class="n">b_s</span><span class="p">,</span> <span class="n">nq</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># n_q 是 query 的序列长度</span>
</span><span id="__span-11-61">        <span class="n">nk</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># n_k key 和 value 的序列长度=像素个数=HW；再次强调，QKV 嵌入维度相同，序列长度可以不同</span>
</span><span id="__span-11-62">
</span><span id="__span-11-63">        <span class="c1"># 注意力与卷积相结合是我的不明白</span>
</span><span id="__span-11-64">        <span class="c1"># QK^TV Q n1×d K=V n2×d </span>
</span><span id="__span-11-65">        <span class="c1"># Q的来源可以和 KV 不同，KV 的来源 shape 必须一样，但是嵌入空间必须一致</span>
</span><span id="__span-11-66">        <span class="c1"># n1×d . d×n2 . n2×d = n1×d</span>
</span><span id="__span-11-67">
</span><span id="__span-11-68">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_s</span><span class="p">,</span> <span class="n">nq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># (b_s, h, nq, d_k) 其实这里的d_k 嵌入维度 应该是 embedding_dim//h 因为 head=1，所以这里的  d_k = embedding_dim // → self.fc_q = nn.Linear(d_model, h * d_k) → self.pa=ScaledDotProductAttention(d_model,d_k=d_model,d_v=d_model,h=1) </span>
</span><span id="__span-11-69">        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_s</span><span class="p">,</span> <span class="n">nk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (b_s, h, d_k, nk)</span>
</span><span id="__span-11-70">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_s</span><span class="p">,</span> <span class="n">nk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># (b_s, h, nk, d_v) 这几个形状的注释很准确，甚至过分准确了</span>
</span><span id="__span-11-71">
</span><span id="__span-11-72">        <span class="n">att</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># (b_s, h, nq, nk)</span>
</span><span id="__span-11-73">        <span class="k">if</span> <span class="n">attention_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-11-74">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span> <span class="o">*</span> <span class="n">attention_weights</span>
</span><span id="__span-11-75">        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-11-76">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
</span><span id="__span-11-77">        <span class="n">att</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-11-78">        <span class="n">att</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
</span><span id="__span-11-79">
</span><span id="__span-11-80">        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_s</span><span class="p">,</span> <span class="n">nq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span>  <span class="c1"># (b_s, nq, h*d_v)</span>
</span><span id="__span-11-81">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># (b_s, nq, d_model) 再把嵌入维度降回去 恢复原来的嵌入维度</span>
</span><span id="__span-11-82">        <span class="k">return</span> <span class="n">out</span>
</span><span id="__span-11-83">
</span><span id="__span-11-84">
</span><span id="__span-11-85"><span class="k">class</span><span class="w"> </span><span class="nc">SimplifiedScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-11-86"><span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-11-87"><span class="sd">    Scaled dot-product attention</span>
</span><span id="__span-11-88"><span class="sd">    &#39;&#39;&#39;</span>
</span><span id="__span-11-89">
</span><span id="__span-11-90">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">.1</span><span class="p">):</span>
</span><span id="__span-11-91"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-11-92"><span class="sd">        :param d_model: Output dimensionality of the model</span>
</span><span id="__span-11-93"><span class="sd">        :param d_k: Dimensionality of queries and keys</span>
</span><span id="__span-11-94"><span class="sd">        :param d_v: Dimensionality of values</span>
</span><span id="__span-11-95"><span class="sd">        :param h: Number of heads</span>
</span><span id="__span-11-96"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-11-97">        <span class="nb">super</span><span class="p">(</span><span class="n">SimplifiedScaledDotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-11-98">
</span><span id="__span-11-99">        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span id="__span-11-100">        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span><span class="o">//</span><span class="n">h</span>
</span><span id="__span-11-101">        <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_model</span><span class="o">//</span><span class="n">h</span>
</span><span id="__span-11-102">        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
</span><span id="__span-11-103">
</span><span id="__span-11-104">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span><span id="__span-11-105">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span><span id="__span-11-106">
</span><span id="__span-11-107">
</span><span id="__span-11-108">
</span><span id="__span-11-109">        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
</span><span id="__span-11-110">
</span><span id="__span-11-111">
</span><span id="__span-11-112">    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-11-113">        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="__span-11-114">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span><span id="__span-11-115">                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>
</span><span id="__span-11-116">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-11-117">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-11-118">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
</span><span id="__span-11-119">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-11-120">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-11-121">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="__span-11-122">                <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</span><span id="__span-11-123">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-11-124">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-11-125">
</span><span id="__span-11-126">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="__span-11-127"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-11-128"><span class="sd">        Computes</span>
</span><span id="__span-11-129"><span class="sd">        :param queries: Queries (b_s, nq, d_model)</span>
</span><span id="__span-11-130"><span class="sd">        :param keys: Keys (b_s, nk, d_model)</span>
</span><span id="__span-11-131"><span class="sd">        :param values: Values (b_s, nk, d_model)</span>
</span><span id="__span-11-132"><span class="sd">        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.</span>
</span><span id="__span-11-133"><span class="sd">        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).</span>
</span><span id="__span-11-134"><span class="sd">        :return:</span>
</span><span id="__span-11-135"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-11-136">        <span class="n">b_s</span><span class="p">,</span> <span class="n">nq</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</span><span id="__span-11-137">        <span class="n">nk</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="__span-11-138">
</span><span id="__span-11-139">        <span class="n">q</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_s</span><span class="p">,</span> <span class="n">nq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># (b_s, h, nq, d_k)</span>
</span><span id="__span-11-140">        <span class="n">k</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_s</span><span class="p">,</span> <span class="n">nk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (b_s, h, d_k, nk)</span>
</span><span id="__span-11-141">        <span class="n">v</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_s</span><span class="p">,</span> <span class="n">nk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># (b_s, h, nk, d_v)</span>
</span><span id="__span-11-142">
</span><span id="__span-11-143">        <span class="n">att</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># (b_s, h, nq, nk)</span>
</span><span id="__span-11-144">        <span class="k">if</span> <span class="n">attention_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-11-145">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span> <span class="o">*</span> <span class="n">attention_weights</span>
</span><span id="__span-11-146">        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-11-147">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
</span><span id="__span-11-148">        <span class="n">att</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-11-149">        <span class="n">att</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
</span><span id="__span-11-150">
</span><span id="__span-11-151">        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_s</span><span class="p">,</span> <span class="n">nq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span>  <span class="c1"># (b_s, nq, h*d_v)</span>
</span><span id="__span-11-152">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># (b_s, nq, d_model)</span>
</span><span id="__span-11-153">        <span class="k">return</span> <span class="n">out</span>
</span><span id="__span-11-154">
</span><span id="__span-11-155">
</span><span id="__span-11-156"><span class="k">class</span><span class="w"> </span><span class="nc">PositionAttentionModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-11-157">
</span><span id="__span-11-158">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">H</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="n">W</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
</span><span id="__span-11-159">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-11-160">        <span class="bp">self</span><span class="o">.</span><span class="n">cnn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span><span class="n">d_model</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 不变卷积，通道数相当于嵌入维度 512；in_channel=d_modle out_channel=d_model;这一步就是聚合一下局部特征</span>
</span><span id="__span-11-161">        <span class="bp">self</span><span class="o">.</span><span class="n">pa</span><span class="o">=</span><span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span><span class="n">d_k</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span><span class="n">d_v</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span><span class="n">h</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-11-162">
</span><span id="__span-11-163">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
</span><span id="__span-11-164">        <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-11-165">        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-11-166">        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, C, H, W) --&gt; (B, C, H, W)</span>
</span><span id="__span-11-167">        <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, C, H, W) --&gt; (B,C,N)--&gt;(B,N,C)   N=H*W 序列长度就是 H*W</span>
</span><span id="__span-11-168">        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pa</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1">#(B,N,C)</span>
</span><span id="__span-11-169">        <span class="k">return</span> <span class="n">y</span>
</span><span id="__span-11-170">
</span><span id="__span-11-171">
</span><span id="__span-11-172"><span class="k">class</span><span class="w"> </span><span class="nc">ChannelAttentionModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-11-173">
</span><span id="__span-11-174">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">H</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="n">W</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
</span><span id="__span-11-175">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-11-176">        <span class="bp">self</span><span class="o">.</span><span class="n">cnn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span><span class="n">d_model</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-11-177">        <span class="bp">self</span><span class="o">.</span><span class="n">pa</span><span class="o">=</span><span class="n">SimplifiedScaledDotProductAttention</span><span class="p">(</span><span class="n">H</span><span class="o">*</span><span class="n">W</span><span class="p">,</span><span class="n">h</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 区别在哪儿？为什么没有复用</span>
</span><span id="__span-11-178">
</span><span id="__span-11-179">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
</span><span id="__span-11-180">        <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-11-181">        <span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-11-182">        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, C, H, W) --&gt; (B, C, H, W)</span>
</span><span id="__span-11-183">        <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, C, H, W)--&gt;(B, C, N)  N=H*W</span>
</span><span id="__span-11-184">        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pa</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># (B, C, N) 图像数据在进入到注意力计算之前就已经展平维度了。</span>
</span><span id="__span-11-185">        <span class="k">return</span> <span class="n">y</span>
</span><span id="__span-11-186">
</span><span id="__span-11-187">
</span><span id="__span-11-188">
</span><span id="__span-11-189">
</span><span id="__span-11-190"><span class="k">class</span><span class="w"> </span><span class="nc">DAModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-11-191">
</span><span id="__span-11-192">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">H</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="n">W</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
</span><span id="__span-11-193">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-11-194">        <span class="c1"># 位置注意力和通道注意力的区别就是：通道注意力没有通过卷积操作生成qkv</span>
</span><span id="__span-11-195">        <span class="bp">self</span><span class="o">.</span><span class="n">position_attention_module</span><span class="o">=</span><span class="n">PositionAttentionModule</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">H</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="n">W</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-11-196">        <span class="bp">self</span><span class="o">.</span><span class="n">channel_attention_module</span><span class="o">=</span><span class="n">ChannelAttentionModule</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">H</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="n">W</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-11-197">
</span><span id="__span-11-198">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="nb">input</span><span class="p">):</span>
</span><span id="__span-11-199">        <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-11-200">        <span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-11-201">        <span class="n">p_out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position_attention_module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># 执行位置注意力: (B, C, H, W)--&gt;(B,N,C) 位置注意力和空间注意力很像</span>
</span><span id="__span-11-202">        <span class="n">c_out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">channel_attention_module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># 执行通道注意力:(B, C, H, W)--&gt; (B, C, N)</span>
</span><span id="__span-11-203">        <span class="n">p_out</span><span class="o">=</span><span class="n">p_out</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">)</span> <span class="c1">#(B,N,C)--&gt;(B,C,N)--&gt;(B,C,H,W)</span>
</span><span id="__span-11-204">        <span class="n">c_out</span><span class="o">=</span><span class="n">c_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">)</span> <span class="c1"># (B,C,N)--&gt;(B,C,H,W)</span>
</span><span id="__span-11-205">
</span><span id="__span-11-206">        <span class="n">p_out</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">p_out</span>
</span><span id="__span-11-207">        <span class="n">c_out</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">c_out</span>
</span><span id="__span-11-208">
</span><span id="__span-11-209">        <span class="k">return</span> <span class="n">p_out</span><span class="o">+</span><span class="n">c_out</span>
</span><span id="__span-11-210">
</span><span id="__span-11-211">
</span><span id="__span-11-212"><span class="c1"># 两个注意力机制就不细讲了哦, 基本一模一样,只不过通道注意力没有通过卷积生成新的qkv,作者说会破坏原有通道之间的相关性。</span>
</span><span id="__span-11-213"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-11-214">    <span class="c1"># (B, C, H, W)</span>
</span><span id="__span-11-215">    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-11-216">    <span class="n">Model</span><span class="o">=</span><span class="n">DAModule</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">H</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="n">W</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-11-217">    <span class="n">output</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-11-218">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="_3">特征融合<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<h2 id="asff">√⭐ASFF 不同尺度特征融合<a class="headerlink" href="#asff" title="Permanent link">&para;</a></h2>
<p>论文《Learning Spatial Fusion for Single-Shot Object Detection》</p>
<p>自适应空间特征融合</p>
<p><a href="https://mp.weixin.qq.com/s/LzAFyaH5Pj42uGHS8_0DWQ">ASFF</a></p>
<p><img alt="image-20250223122423399" src="../images/image-20250223122423399.png" /></p>
<p><img alt="image-20250223122919658" src="../images/image-20250223122919658.png" /></p>
<p><img alt="image-20250223122932770" src="../images/image-20250223122932770.png" /></p>
<p><strong>主要解决：不同特征尺度不一致的问题</strong> </p>
<p>softmax：</p>
<p><img alt="image-20250223122755770" src="../images/image-20250223122755770.png" /></p>
<p><img alt="image-20250223122819398" src="../images/image-20250223122819398.png" /></p>
<p>（实践出真知）</p>
<p><strong>优质注释</strong></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-12-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-12-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-12-3"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
</span><span id="__span-12-4">
</span><span id="__span-12-5">
</span><span id="__span-12-6"><span class="k">def</span><span class="w"> </span><span class="nf">autopad</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># kernel, padding</span>
</span><span id="__span-12-7">    <span class="c1"># Pad to &#39;same&#39;</span>
</span><span id="__span-12-8">    <span class="k">if</span> <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-12-9">        <span class="n">p</span> <span class="o">=</span> <span class="n">k</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span><span class="n">x</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span><span class="p">]</span>  <span class="c1"># auto-pad</span>
</span><span id="__span-12-10">    <span class="k">return</span> <span class="n">p</span>
</span><span id="__span-12-11">
</span><span id="__span-12-12">
</span><span id="__span-12-13"><span class="k">class</span><span class="w"> </span><span class="nc">Conv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-12-14">    <span class="c1"># Standard convolution</span>
</span><span id="__span-12-15">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>  <span class="c1"># ch_in, ch_out, kernel, stride, padding, groups</span>
</span><span id="__span-12-16">        <span class="nb">super</span><span class="p">(</span><span class="n">Conv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-12-17">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">autopad</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-12-18">        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">c2</span><span class="p">)</span>
</span><span id="__span-12-19">        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span> <span class="k">if</span> <span class="n">act</span> <span class="ow">is</span> <span class="kc">True</span> <span class="k">else</span> <span class="p">(</span><span class="n">act</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">())</span>
</span><span id="__span-12-20">
</span><span id="__span-12-21">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-12-22">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span><span id="__span-12-23">
</span><span id="__span-12-24">    <span class="k">def</span><span class="w"> </span><span class="nf">forward_fuse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-12-25">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span id="__span-12-26">
</span><span id="__span-12-27">
</span><span id="__span-12-28"><span class="k">class</span><span class="w"> </span><span class="nc">ASFF</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-12-29">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">level</span><span class="p">,</span> <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rfb</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vis</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">act_cfg</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span><span id="__span-12-30"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-12-31"><span class="sd">        multiplier should be 1, 0.5</span>
</span><span id="__span-12-32"><span class="sd">        which means, the channel of ASFF can be</span>
</span><span id="__span-12-33"><span class="sd">        512, 256, 128 -&gt; multiplier=0.5</span>
</span><span id="__span-12-34"><span class="sd">        1024, 512, 256 -&gt; multiplier=1</span>
</span><span id="__span-12-35"><span class="sd">        For even smaller, you need change code manually.</span>
</span><span id="__span-12-36"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-12-37">        <span class="c1"># init asff_module = ASFF(level=1, multiplier=1, rfb=False, vis=False)</span>
</span><span id="__span-12-38">        <span class="nb">super</span><span class="p">(</span><span class="n">ASFF</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-12-39">        <span class="bp">self</span><span class="o">.</span><span class="n">level</span> <span class="o">=</span> <span class="n">level</span>
</span><span id="__span-12-40">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span>
</span><span id="__span-12-41">                    <span class="nb">int</span><span class="p">(</span><span class="mi">256</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">)]</span>
</span><span id="__span-12-42">        <span class="c1"># print(self.dim)</span>
</span><span id="__span-12-43">
</span><span id="__span-12-44">        <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">level</span><span class="p">]</span>
</span><span id="__span-12-45">        <span class="k">if</span> <span class="n">level</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="__span-12-46">            <span class="bp">self</span><span class="o">.</span><span class="n">stride_level_1</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="__span-12-47">
</span><span id="__span-12-48">            <span class="bp">self</span><span class="o">.</span><span class="n">stride_level_2</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mi">256</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="__span-12-49">
</span><span id="__span-12-50">            <span class="bp">self</span><span class="o">.</span><span class="n">expand</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span>
</span><span id="__span-12-51">                <span class="mi">1024</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-52">        <span class="k">elif</span> <span class="n">level</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="__span-12-53">            <span class="bp">self</span><span class="o">.</span><span class="n">compress_level_0</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span>
</span><span id="__span-12-54">                <span class="nb">int</span><span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-55">            <span class="bp">self</span><span class="o">.</span><span class="n">stride_level_2</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span>
</span><span id="__span-12-56">                <span class="nb">int</span><span class="p">(</span><span class="mi">256</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="__span-12-57">            <span class="bp">self</span><span class="o">.</span><span class="n">expand</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-58">        <span class="k">elif</span> <span class="n">level</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="__span-12-59">            <span class="bp">self</span><span class="o">.</span><span class="n">compress_level_0</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span>
</span><span id="__span-12-60">                <span class="nb">int</span><span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-61">            <span class="bp">self</span><span class="o">.</span><span class="n">compress_level_1</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span>
</span><span id="__span-12-62">                <span class="nb">int</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-63">            <span class="bp">self</span><span class="o">.</span><span class="n">expand</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span>
</span><span id="__span-12-64">                <span class="mi">256</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-65">
</span><span id="__span-12-66">        <span class="c1"># when adding rfb, we use half number of channels to save memory</span>
</span><span id="__span-12-67">        <span class="n">compress_c</span> <span class="o">=</span> <span class="mi">8</span> <span class="k">if</span> <span class="n">rfb</span> <span class="k">else</span> <span class="mi">16</span>
</span><span id="__span-12-68">        <span class="bp">self</span><span class="o">.</span><span class="n">weight_level_0</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span>
</span><span id="__span-12-69">            <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="n">compress_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-70">        <span class="bp">self</span><span class="o">.</span><span class="n">weight_level_1</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span>
</span><span id="__span-12-71">            <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="n">compress_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-72">        <span class="bp">self</span><span class="o">.</span><span class="n">weight_level_2</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span>
</span><span id="__span-12-73">            <span class="bp">self</span><span class="o">.</span><span class="n">inter_dim</span><span class="p">,</span> <span class="n">compress_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-74">
</span><span id="__span-12-75">        <span class="bp">self</span><span class="o">.</span><span class="n">weight_levels</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">(</span>
</span><span id="__span-12-76">            <span class="n">compress_c</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-77">        <span class="bp">self</span><span class="o">.</span><span class="n">vis</span> <span class="o">=</span> <span class="n">vis</span>
</span><span id="__span-12-78">
</span><span id="__span-12-79">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>  <span class="c1"># l,m,s</span>
</span><span id="__span-12-80"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-12-81"><span class="sd">        #</span>
</span><span id="__span-12-82"><span class="sd">        256, 512, 1024</span>
</span><span id="__span-12-83"><span class="sd">        from small -&gt; large</span>
</span><span id="__span-12-84"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-12-85">        <span class="c1"># forward output_feature = asff_module([level_2_feature, level_1_feature, level_0_feature])</span>
</span><span id="__span-12-86">        <span class="n">x_level_0</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># 最大特征层 level_0_feature = (1, 1024, 20, 20)  # 大尺寸特征图 尺寸小通道多</span>
</span><span id="__span-12-87">        <span class="n">x_level_1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 中间特征层 level_1_feature = (1, 512, 40, 40)   # 中尺寸特征图</span>
</span><span id="__span-12-88">        <span class="n">x_level_2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 最小特征层 level_2_feature = (1, 256, 80, 80)   # 小尺寸特征图</span>
</span><span id="__span-12-89">
</span><span id="__span-12-90">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">level</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="__span-12-91">            <span class="n">level_0_resized</span> <span class="o">=</span> <span class="n">x_level_0</span>
</span><span id="__span-12-92">            <span class="n">level_1_resized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride_level_1</span><span class="p">(</span><span class="n">x_level_1</span><span class="p">)</span>
</span><span id="__span-12-93">            <span class="n">level_2_downsampled_inter</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span>
</span><span id="__span-12-94">                <span class="n">x_level_2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-95">            <span class="n">level_2_resized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride_level_2</span><span class="p">(</span><span class="n">level_2_downsampled_inter</span><span class="p">)</span>
</span><span id="__span-12-96">        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">level</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="__span-12-97">            <span class="n">level_0_compressed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compress_level_0</span><span class="p">(</span><span class="n">x_level_0</span><span class="p">)</span> <span class="c1"># (1, 1024, 20, 20) → self.compress_level_0 =  Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) → (1, 512, 20, 20)</span>
</span><span id="__span-12-98">
</span><span id="__span-12-99">            <span class="n">level_0_resized</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
</span><span id="__span-12-100">                <span class="n">level_0_compressed</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span> <span class="c1"># (1, 512, 20, 20) → F.interpolate→ [1, 512, 40, 40]</span>
</span><span id="__span-12-101">            <span class="n">level_1_resized</span> <span class="o">=</span> <span class="n">x_level_1</span> <span class="c1">#  [1, 512, 40, 40] → = → [1, 512, 40, 40]</span>
</span><span id="__span-12-102">            <span class="n">level_2_resized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride_level_2</span><span class="p">(</span><span class="n">x_level_2</span><span class="p">)</span> <span class="c1"># [1, 256, 80, 80] → self.stride_level_2 = Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) → [1, 512, 40, 40]</span>
</span><span id="__span-12-103">        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">level</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="__span-12-104">            <span class="n">level_0_compressed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compress_level_0</span><span class="p">(</span><span class="n">x_level_0</span><span class="p">)</span>
</span><span id="__span-12-105">            <span class="n">level_0_resized</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
</span><span id="__span-12-106">                <span class="n">level_0_compressed</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
</span><span id="__span-12-107">            <span class="n">x_level_1_compressed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compress_level_1</span><span class="p">(</span><span class="n">x_level_1</span><span class="p">)</span>
</span><span id="__span-12-108">            <span class="n">level_1_resized</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
</span><span id="__span-12-109">                <span class="n">x_level_1_compressed</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
</span><span id="__span-12-110">            <span class="n">level_2_resized</span> <span class="o">=</span> <span class="n">x_level_2</span>
</span><span id="__span-12-111">
</span><span id="__span-12-112">        <span class="n">level_0_weight_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_level_0</span><span class="p">(</span><span class="n">level_0_resized</span><span class="p">)</span> <span class="c1"># [1, 512, 40, 40] → self.weight_level_0 = (conv): Conv2d(512, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) → [1, 16, 40, 40]</span>
</span><span id="__span-12-113">        <span class="n">level_1_weight_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_level_1</span><span class="p">(</span><span class="n">level_1_resized</span><span class="p">)</span> <span class="c1"># [1, 512, 40, 40] → self.weight_level_1 = Conv2d(512, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) → [1, 16, 40, 40]</span>
</span><span id="__span-12-114">        <span class="n">level_2_weight_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_level_2</span><span class="p">(</span><span class="n">level_2_resized</span><span class="p">)</span> <span class="c1"># [1, 512, 40, 40] → self.weight_level_2 = Conv2d(512, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) → [1, 16, 40, 40]</span>
</span><span id="__span-12-115">
</span><span id="__span-12-116">        <span class="n">levels_weight_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span><span id="__span-12-117">            <span class="p">(</span><span class="n">level_0_weight_v</span><span class="p">,</span> <span class="n">level_1_weight_v</span><span class="p">,</span> <span class="n">level_2_weight_v</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># [1, 16, 40, 40],[1, 16, 40, 40],[1, 16, 40, 40] → cat → [1, 48, 40, 40]</span>
</span><span id="__span-12-118">        <span class="n">levels_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_levels</span><span class="p">(</span><span class="n">levels_weight_v</span><span class="p">)</span> <span class="c1"># [1, 48, 40, 40] → self.weight_levels = (conv): Conv2d(48, 3, kernel_size=(1, 1), stride=(1, 1), bias=False) → [1, 3, 40, 40]</span>
</span><span id="__span-12-119">        <span class="n">levels_weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">levels_weight</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [1, 3, 40, 40] → F.softmax → [1, 3, 40, 40]</span>
</span><span id="__span-12-120">
</span><span id="__span-12-121">        <span class="n">fused_out_reduced</span> <span class="o">=</span> <span class="n">level_0_resized</span> <span class="o">*</span> <span class="n">levels_weight</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+</span> \
</span><span id="__span-12-122">                            <span class="n">level_1_resized</span> <span class="o">*</span> <span class="n">levels_weight</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+</span> \
</span><span id="__span-12-123">                            <span class="n">level_2_resized</span> <span class="o">*</span> <span class="n">levels_weight</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span>
</span><span id="__span-12-124">        <span class="c1"># [1, 512, 40, 40] * [1, 1, 40, 40] + [1, 512, 40, 40] * [1, 1, 40, 40] + [1, 512, 40, 40] * [1, 1, 40, 40] →  [1, 512, 40, 40]</span>
</span><span id="__span-12-125">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">fused_out_reduced</span><span class="p">)</span> <span class="c1"># [1, 512, 40, 40] → self.expand = (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) → [1, 512, 40, 40]</span>
</span><span id="__span-12-126">
</span><span id="__span-12-127">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vis</span><span class="p">:</span><span class="c1"># self.vis = False</span>
</span><span id="__span-12-128">            <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">levels_weight</span><span class="p">,</span> <span class="n">fused_out_reduced</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-12-129">        <span class="k">else</span><span class="p">:</span>
</span><span id="__span-12-130">            <span class="k">return</span> <span class="n">out</span>
</span><span id="__span-12-131">
</span><span id="__span-12-132">
</span><span id="__span-12-133"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span id="__span-12-134">    <span class="c1"># 模拟的输入特征图，模拟三个不同尺度的特征图，例如来自一个多尺度特征提取网络的输出</span>
</span><span id="__span-12-135">    <span class="n">level_0_feature</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>  <span class="c1"># 大尺寸特征图</span>
</span><span id="__span-12-136">    <span class="n">level_1_feature</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>   <span class="c1"># 中尺寸特征图</span>
</span><span id="__span-12-137">    <span class="n">level_2_feature</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>   <span class="c1"># 小尺寸特征图</span>
</span><span id="__span-12-138">
</span><span id="__span-12-139">    <span class="c1"># 初始化ASFF模块，level表示当前ASFF模块处理的是哪个尺度的特征层，这里以处理中尺寸特征层为例</span>
</span><span id="__span-12-140">    <span class="c1"># multiplier用于调整通道数，rfb和vis分别表示是否使用更丰富的特征表示和是否可视化</span>
</span><span id="__span-12-141">    <span class="n">asff_module</span> <span class="o">=</span> <span class="n">ASFF</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rfb</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vis</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-12-142">
</span><span id="__span-12-143">    <span class="c1"># 通过ASFF模块传递特征图</span>
</span><span id="__span-12-144">    <span class="n">output_feature</span> <span class="o">=</span> <span class="n">asff_module</span><span class="p">([</span><span class="n">level_2_feature</span><span class="p">,</span> <span class="n">level_1_feature</span><span class="p">,</span> <span class="n">level_0_feature</span><span class="p">])</span>
</span><span id="__span-12-145">
</span><span id="__span-12-146">    <span class="c1"># 打印输出特征图的形状，确保ASFF模块正常工作</span>
</span><span id="__span-12-147">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output feature shape: </span><span class="si">{</span><span class="n">output_feature</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-12-148">
</span><span id="__span-12-149"><span class="c1"># TODO 计算流程图、原文框架图、公式表示</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="fpn">√ <a href="https://arxiv.org/pdf/1612.03144">FPN</a> 特征金字塔网络<a class="headerlink" href="#fpn" title="Permanent link">&para;</a></h2>
<p><img alt="IMG_0295" src="../images/IMG_0295.jpg" /></p>
<p><strong><a href="https://mp.weixin.qq.com/s?__biz=MzkzODQ0Nzk5Nw==&amp;mid=2247484987&amp;idx=1&amp;sn=e7fea22826dbda623f23e1a2f00f5552&amp;chksm=c2814329f5f6ca3f0c5cb6bab48a6cc719bf2f83f774c941c4960b30fba8d2c2b2c58e3dd618&amp;cur_album_id=3010807818071064579&amp;scene=190#rd">Feature Pyramid Network（FPN）</a></strong></p>
<p><img alt="image-20250223135703949" src="../images/image-20250223135703949.png" /></p>
<p><img alt="image-20250223160541609" src="../images/image-20250223160541609.png" /></p>
<p><img alt="image-20250223160600857" src="../images/image-20250223160600857.png" /></p>
<p><a href="https://www.bilibili.com/video/BV1dh411U7D9/?spm_id_from=333.337.search-card.all.click&amp;vd_source=ddd7d236ab3e9b123c4086c415f4939e">讲解</a></p>
<p><img alt="image-20250223135758297" src="../images/image-20250223135758297.png" /></p>
<p>2016 年论文</p>
<p><img alt="image-20250223160755182" src="../images/image-20250223160755182.png" /></p>
<p><img alt="image-20250223160922542" src="../images/image-20250223160922542.png" /></p>
<p><img alt="image-20250223160954522" src="../images/image-20250223160954522.png" /></p>
<p><img alt="image-20250223161125762" src="../images/image-20250223161125762.png" /></p>
<p><img alt="image-20250223161229485" src="../images/image-20250223161229485.png" /></p>
<p><a href="https://blog.csdn.net/weixin_41552975/article/details/135530473">代码</a>（已注释）：</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-13-1"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="__span-13-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-13-3"><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
</span><span id="__span-13-4"><span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.model_zoo</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">model_zoo</span>
</span><span id="__span-13-5"><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.ops</span><span class="w"> </span><span class="kn">import</span> <span class="n">nms</span>
</span><span id="__span-13-6"><span class="c1"># from retinanet.utils import BasicBlock, Bottleneck, BBoxTransform, ClipBoxes</span>
</span><span id="__span-13-7"><span class="c1"># from retinanet.anchors import Anchors</span>
</span><span id="__span-13-8"><span class="c1"># from retinanet import losses</span>
</span><span id="__span-13-9">
</span><span id="__span-13-10"><span class="k">class</span><span class="w"> </span><span class="nc">PyramidFeatures</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-13-11">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C3_size</span><span class="p">,</span> <span class="n">C4_size</span><span class="p">,</span> <span class="n">C5_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
</span><span id="__span-13-12">        <span class="nb">super</span><span class="p">(</span><span class="n">PyramidFeatures</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-13-13">
</span><span id="__span-13-14">        <span class="c1"># upsample C5 to get P5 from the FPN paper</span>
</span><span id="__span-13-15">        <span class="bp">self</span><span class="o">.</span><span class="n">P5_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C5_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-13-16">        <span class="bp">self</span><span class="o">.</span><span class="n">P5_upsampled</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
</span><span id="__span-13-17">        <span class="c1">#将C5的特征图尺寸放大2倍用于跟C4相加</span>
</span><span id="__span-13-18">        <span class="bp">self</span><span class="o">.</span><span class="n">P5_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-13-19">
</span><span id="__span-13-20">        <span class="c1"># add P5 elementwise to C4</span>
</span><span id="__span-13-21">        <span class="bp">self</span><span class="o">.</span><span class="n">P4_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C4_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-13-22">        <span class="bp">self</span><span class="o">.</span><span class="n">P4_upsampled</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
</span><span id="__span-13-23">        <span class="bp">self</span><span class="o">.</span><span class="n">P4_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-13-24">
</span><span id="__span-13-25">        <span class="c1"># add P4 elementwise to C3</span>
</span><span id="__span-13-26">        <span class="bp">self</span><span class="o">.</span><span class="n">P3_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C3_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-13-27">        <span class="bp">self</span><span class="o">.</span><span class="n">P3_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-13-28">
</span><span id="__span-13-29">        <span class="c1"># &quot;P6 is obtained via a 3x3 stride-2 conv on C5&quot;</span>
</span><span id="__span-13-30">        <span class="bp">self</span><span class="o">.</span><span class="n">P6</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C5_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-13-31">
</span><span id="__span-13-32">        <span class="c1"># &quot;P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6&quot;</span>
</span><span id="__span-13-33">        <span class="bp">self</span><span class="o">.</span><span class="n">P7_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</span><span id="__span-13-34">        <span class="bp">self</span><span class="o">.</span><span class="n">P7_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-13-35">
</span><span id="__span-13-36">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="__span-13-37">        <span class="c1">#注意理解这里的inputs，其表示的是一个列表</span>
</span><span id="__span-13-38">        <span class="n">C3</span><span class="p">,</span> <span class="n">C4</span><span class="p">,</span> <span class="n">C5</span> <span class="o">=</span> <span class="n">inputs</span>
</span><span id="__span-13-39">        <span class="c1"># input = [torch.randn(1, 32, 640, 640), torch.randn(1, 64, 320, 320), torch.randn(1, 96, 160, 160)]</span>
</span><span id="__span-13-40">        <span class="c1"># 特点：channel：32→64→96（无明显规律），feature_sizw：640*640→320*320→160*160</span>
</span><span id="__span-13-41">
</span><span id="__span-13-42">        <span class="n">P5_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P5_1</span><span class="p">(</span><span class="n">C5</span><span class="p">)</span> <span class="c1">#  conv 升维，尺寸不变 torch.Size([1, 96, 160, 160])→ Conv2d →torch.Size([1, 256, 160, 160]) /// (96, 256, kernel_size=(1, 1), stride=(1, 1))</span>
</span><span id="__span-13-43">        <span class="n">P5_upsampled_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P5_upsampled</span><span class="p">(</span><span class="n">P5_x</span><span class="p">)</span> <span class="c1"># 上采样，维度不变，尺寸翻倍 [1, 256, 160, 160] → [1, 256, 320, 320] // Upsample(scale_factor=2.0, mode=&#39;nearest&#39;)</span>
</span><span id="__span-13-44">        <span class="n">P5_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P5_2</span><span class="p">(</span><span class="n">P5_x</span><span class="p">)</span> <span class="c1"># 311conv 维度不变，尺寸不变 [1, 256, 160, 160] → Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))尺寸不变卷积 → [1, 256, 160, 160]</span>
</span><span id="__span-13-45">
</span><span id="__span-13-46">        <span class="n">P4_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P4_1</span><span class="p">(</span><span class="n">C4</span><span class="p">)</span> <span class="c1">#1x1conv 升维 尺寸不变  [1, 64, 320, 320] → Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)) → [1, 256, 320, 320]</span>
</span><span id="__span-13-47">        <span class="n">P4_x</span> <span class="o">=</span> <span class="n">P5_upsampled_x</span> <span class="o">+</span> <span class="n">P4_x</span> <span class="c1"># [1, 256, 320, 320] + [1, 256, 320, 320] → [1, 256, 320, 320]</span>
</span><span id="__span-13-48">        <span class="n">P4_upsampled_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P4_upsampled</span><span class="p">(</span><span class="n">P4_x</span><span class="p">)</span> <span class="c1"># 上采样，维度不变，尺寸翻倍 [1, 256, 320, 320] → Upsample(scale_factor=2.0, mode=&#39;nearest&#39;) → [1, 256, 640, 640]</span>
</span><span id="__span-13-49">        <span class="n">P4_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P4_2</span><span class="p">(</span><span class="n">P4_x</span><span class="p">)</span> <span class="c1"># 3x3 311conv 卷积操作，维度不变，尺寸不变。[1, 256, 320, 320] → Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) → torch.Size([1, 256, 320, 320]) ksp 311卷积核为 3 的不变卷积</span>
</span><span id="__span-13-50">
</span><span id="__span-13-51">        <span class="n">P3_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P3_1</span><span class="p">(</span><span class="n">C3</span><span class="p">)</span> <span class="c1"># 卷积升维，尺寸不变 [1, 32, 640, 640] Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1)) [1, 256, 640, 640]</span>
</span><span id="__span-13-52">        <span class="n">P3_x</span> <span class="o">=</span> <span class="n">P3_x</span> <span class="o">+</span> <span class="n">P4_upsampled_x</span> <span class="c1"># add 混合 [1, 256, 640, 640] + [1, 256, 640, 640] --&gt; [1, 256, 640, 640]</span>
</span><span id="__span-13-53">        <span class="n">P3_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P3_2</span><span class="p">(</span><span class="n">P3_x</span><span class="p">)</span> <span class="c1"># 3x3conv  维度不变，尺寸不变 [1, 256, 640, 640] Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) [1, 256, 640, 640]</span>
</span><span id="__span-13-54">
</span><span id="__span-13-55">        <span class="n">P6_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P6</span><span class="p">(</span><span class="n">C5</span><span class="p">)</span> <span class="c1"># 3x3 conv(ksp=321) 升维，特征图尺寸减半 [1, 96, 160, 160] Conv2d(96, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) [1, 256, 80, 80]</span>
</span><span id="__span-13-56">
</span><span id="__span-13-57">        <span class="n">P7_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P7_1</span><span class="p">(</span><span class="n">P6_x</span><span class="p">)</span> <span class="c1"># 非线性变换 ReLU激活层  [1, 256, 80, 80] ReLU() [1, 256, 80, 80]</span>
</span><span id="__span-13-58">        <span class="n">P7_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P7_2</span><span class="p">(</span><span class="n">P7_x</span><span class="p">)</span> <span class="c1"># 3x3conv(ksp=321)维度不变，特征图尺寸减半 [1, 256, 80, 80] Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) [1, 256, 40, 40]</span>
</span><span id="__span-13-59">
</span><span id="__span-13-60">        <span class="k">return</span> <span class="p">[</span><span class="n">P3_x</span><span class="p">,</span> <span class="n">P4_x</span><span class="p">,</span> <span class="n">P5_x</span><span class="p">,</span> <span class="n">P6_x</span><span class="p">,</span> <span class="n">P7_x</span><span class="p">]</span> <span class="c1"># [P3_x [1, 256, 640, 640],P4_x [1, 256, 320, 320],P5_x [1, 256, 160, 160],P6_x [1, 256, 80, 80],P7_x [1, 256, 40, 40]]</span>
</span><span id="__span-13-61">
</span><span id="__span-13-62">
</span><span id="__span-13-63"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-13-64">    <span class="n">model</span> <span class="o">=</span> <span class="n">PyramidFeatures</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">96</span><span class="p">)</span>
</span><span id="__span-13-65">    <span class="c1"># print(model)</span>
</span><span id="__span-13-66">    <span class="c1">##这里假设输入是三层不同尺寸的特征图，输入的形状是[batch_size, 256, height, width]</span>
</span><span id="__span-13-67">    <span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">320</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">160</span><span class="p">)]</span>
</span><span id="__span-13-68">    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-13-69">    <span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="c1"># 返回的是一个列表</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="panet">PANet<a class="headerlink" href="#panet" title="Permanent link">&para;</a></h2>
<p><strong>PANet（Path Aggregation Network）</strong></p>
<p>ICCV 2019</p>
<p>小样本图像分割</p>
<p><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.pdf">PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment</a></p>
<h2 id="_4">空间注意力及变体<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<h2 id="attention">√Attention<a class="headerlink" href="#attention" title="Permanent link">&para;</a></h2>
<p><img alt="image-20250223172648964" src="../images/image-20250223172648964.png" /></p>
<p><img alt="image-20250223172826491" src="../images/image-20250223172826491.png" /></p>
<p>QKV 是 X 在三种不同向量空间的表示，增强了模型的表达能力</p>
<p><img alt="image-20250223172907126" src="../images/image-20250223172907126.png" /></p>
<p><mark>为什么 QK 之间做乘法？</mark></p>
<ul>
<li>向量乘法可以用来衡量向量之间的相似度</li>
<li>向量A和向量B的点积等于向量A的模乘上向量B的模乘上夹角的余弦值</li>
<li>当给定两个确定向量的时候，也就是当给定一个A向量和B向量的时候：</li>
</ul>
<p>如果它们之间的夹角越小，那 <span class="arithmatex">\(cos\theta\)</span> 就越大，那么相应的这个点积的值就越大</p>
<p>那么如果它们的夹角越大，这个 <span class="arithmatex">\(cos\theta\)</span> 越小，相应的点击值就越小</p>
<ul>
<li>从整体上来看呀，点积操作，既有长度信息，也就是模长，也有方向信息，就是 <span class="arithmatex">\(cos\theta\)</span></li>
</ul>
<p><strong>所以，用向量点积来衡量向量之间的相似度</strong></p>
<p><mark>为什么不用余弦相似度呢？</mark> </p>
<p>要计算余弦相似度，得求向量A和向量B的模，计算是非常耗时耗力的</p>
<p>点积的话就非常的高效，只需要对应元素相乘并求和即可</p>
<p><mark>为什么要执行缩放操作</mark></p>
<p>把向量的维度设置为64和设置为512</p>
<p>在计算点积的时候，维度越大点积的数值越大，此时在计算 softmax 也就是 e 的 x 次方的时候，指数函数的数值会变大，会造成梯度不稳定，所以除以一个缩放因子，也就是根号下<span class="arithmatex">\(D_k\)</span>，既可以保持数值上的稳定性，也可以保持梯度上的稳定性。</p>
<p><mark>多头注意力机制</mark></p>
<p><img alt="image-20250223173913249" src="../images/image-20250223173913249.png" /></p>
<p>多头自注意力机制是由多个自注意、多个缩放点击注意力构成的，大家各计算各的互不相干，最后只要将每个自注意力机制的一个输出进行一个拼接，再通过一个线性层融合就可以了</p>
<p><mark>第一个点，如何划分多头？</mark></p>
<p>最简单的方式，当给定一个BCHW矩阵的时候，在这个通道C上平均划分为M组，每组通道数量是K，M乘K等于C</p>
<p>为了便于计算，通常会将M，迁移到第一个维度上面，然后让它重新变为一个四维矩阵，每一组的计算，都是独立的</p>
<p><mark>第二个点在输出的时候，concat和 Linear？</mark></p>
<p>在这个输出的时候，首先 多个头的输出，在通道上进行一个拼接，并且恢复和输入相同的shape，然后再通过一个线性层</p>
<p><strong>关于 拼接和线性层</strong> </p>
<p>多头自注意力机制，每一个头都在不同的向量空间进行计算，在不同的向量空间提取有用的特征。</p>
<p>例如要提取一个人的特征，多头自注意力机制，可以看作是在不同的头，分别关注身高年龄长相工作等特征，最后，将这些不同向量空间的特征进行拼接，然后再通过一个线性层进行融合，得到更新后的特征。</p>
<p>代码</p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-14-1"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-14-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-14-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="__span-14-4"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">init</span>
</span><span id="__span-14-5">
</span><span id="__span-14-6"><span class="s2">&quot;Attention Is All You Need&quot;</span>
</span><span id="__span-14-7">
</span><span id="__span-14-8"><span class="k">class</span><span class="w"> </span><span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-14-9"><span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-14-10"><span class="sd">    Scaled dot-product attention</span>
</span><span id="__span-14-11"><span class="sd">    &#39;&#39;&#39;</span>
</span><span id="__span-14-12">
</span><span id="__span-14-13">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="mf">.1</span><span class="p">):</span>
</span><span id="__span-14-14"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-14-15"><span class="sd">        :param d_model: Output dimensionality of the model</span>
</span><span id="__span-14-16"><span class="sd">        :param d_k: Dimensionality of queries and keys</span>
</span><span id="__span-14-17"><span class="sd">        :param d_v: Dimensionality of values</span>
</span><span id="__span-14-18"><span class="sd">        :param h: Number of heads</span>
</span><span id="__span-14-19"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-14-20">        <span class="nb">super</span><span class="p">(</span><span class="n">ScaledDotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-14-21">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">)</span>
</span><span id="__span-14-22">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">)</span>
</span><span id="__span-14-23">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">)</span>
</span><span id="__span-14-24">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span><span id="__span-14-25">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span><span id="__span-14-26">
</span><span id="__span-14-27">        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span id="__span-14-28">        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
</span><span id="__span-14-29">        <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_v</span>
</span><span id="__span-14-30">        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
</span><span id="__span-14-31">
</span><span id="__span-14-32">        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
</span><span id="__span-14-33">
</span><span id="__span-14-34">
</span><span id="__span-14-35">    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-14-36">        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="__span-14-37">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span><span id="__span-14-38">                <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>
</span><span id="__span-14-39">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-14-40">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-14-41">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
</span><span id="__span-14-42">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-14-43">                <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-14-44">            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="__span-14-45">                <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</span><span id="__span-14-46">                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-14-47">                    <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="__span-14-48">
</span><span id="__span-14-49">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="__span-14-50"><span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-14-51"><span class="sd">        Computes</span>
</span><span id="__span-14-52"><span class="sd">        :param queries: Queries (b_s, nq, d_model)</span>
</span><span id="__span-14-53"><span class="sd">        :param keys: Keys (b_s, nk, d_model)</span>
</span><span id="__span-14-54"><span class="sd">        :param values: Values (b_s, nk, d_model)</span>
</span><span id="__span-14-55"><span class="sd">        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.</span>
</span><span id="__span-14-56"><span class="sd">        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).</span>
</span><span id="__span-14-57"><span class="sd">        :return:</span>
</span><span id="__span-14-58"><span class="sd">        &#39;&#39;&#39;</span>
</span><span id="__span-14-59">        <span class="c1"># (B, N, C), N=nq</span>
</span><span id="__span-14-60">        <span class="n">B</span><span class="p">,</span> <span class="n">nq</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># B=2，nq=50</span>
</span><span id="__span-14-61">        <span class="n">nk</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># nk=50</span>
</span><span id="__span-14-62">
</span><span id="__span-14-63">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">nq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  
</span><span id="__span-14-64">        <span class="c1"># (B,N,C)--&gt;(B,nq,h*d_k)--&gt;(B,nq,h,d_k)--&gt;(B,h,nq,d_k)  h:注意力头的个数, d_k:QK每一个注意力头的通道数</span>
</span><span id="__span-14-65">        <span class="c1"># [2, 50, 64]-&gt; self.fc_q=Linear(in_features=64, out_features=512, bias=True)-&gt;[2,50,512]-&gt;view-&gt;(2,50,8,64)-&gt;premute-&gt;(2,8,50,64)</span>
</span><span id="__span-14-66">
</span><span id="__span-14-67">        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">nk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  
</span><span id="__span-14-68">        <span class="c1">#  (B,N,C)--&gt;(B,nq,h*d_k)--&gt;(B,nk,h,d_k)--&gt;(B,h,d_k,nk)</span>
</span><span id="__span-14-69">        <span class="c1"># [2, 50, 64]-&gt;Linear(in_features=64, out_features=512, bias=True)-&gt;[2,50,512]-&gt;view-&gt;[2,50,8,64]-&gt;permute-&gt;[2,8,64,50]</span>
</span><span id="__span-14-70">
</span><span id="__span-14-71">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">nk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  
</span><span id="__span-14-72">        <span class="c1"># (B,N,C)--&gt;(B,nk,h*d_v)--&gt;(B,nk,h,d_v)--&gt;(B,h,nk,d_v)      </span>
</span><span id="__span-14-73">        <span class="c1"># [2, 50, 64]-&gt;Linear(in_features=64, out_features=512, bias=True)-&gt;[2,50,512]-&gt;view-&gt;[2,50,8,64]-&gt;[2,8,50,64]</span>
</span><span id="__span-14-74">
</span><span id="__span-14-75">        <span class="n">att</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># (B, h, nq, nk)</span>
</span><span id="__span-14-76">        <span class="c1"># q (2,8,50,64) k [2,8,64,50] -&gt; torch.matmul -&gt; [2, 8, 50, 50]</span>
</span><span id="__span-14-77">
</span><span id="__span-14-78">        <span class="c1"># 如果需要为注意力矩阵额外添加一个参数矩阵,那么执行逐点相乘即可</span>
</span><span id="__span-14-79">        <span class="k">if</span> <span class="n">attention_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="c1"># attention_weights = None</span>
</span><span id="__span-14-80">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span> <span class="o">*</span> <span class="n">attention_weights</span>
</span><span id="__span-14-81">        <span class="c1"># 如果需要为注意力矩阵添加mask,那么在对应需要mask的地方填充为负无穷数值,这样在计算softmax的时候,负无穷的归一化得分将趋近于0</span>
</span><span id="__span-14-82">        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="c1"># attention_mask = None</span>
</span><span id="__span-14-83">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
</span><span id="__span-14-84">        <span class="n">att</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> 
</span><span id="__span-14-85">        <span class="c1"># [2, 8, 50, 50] -&gt; torch.softmax -&gt; [2, 8, 50, 50]</span>
</span><span id="__span-14-86">        <span class="c1"># attn n_q d_q × d_k n_k = n_q × n_k 在 n_k 列进行softmax</span>
</span><span id="__span-14-87">
</span><span id="__span-14-88">        <span class="n">att</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
</span><span id="__span-14-89">
</span><span id="__span-14-90">
</span><span id="__span-14-91">        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">nq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span>  
</span><span id="__span-14-92">        <span class="c1"># (B,h,nq,nk)@(B,h,nk,d_v)=(B,h,nq,d_v)--&gt;(B,nq,h,d_v)--&gt;(B,nq,h*d_v)</span>
</span><span id="__span-14-93">        <span class="c1"># [2, 8, 50, 50] @ [2, 8, 50, 64] -&gt; [2, 8, 50, 64] -&gt; permute -&gt; [2,50,8,64] -&gt; contiguous -&gt; view -&gt; [2,50,512]</span>
</span><span id="__span-14-94">        <span class="c1"># 这里有多头注意力需要注意的点：为什么要进行 concat 和 Linear 因为每个头分别学习人的一个特征，最后的注意力是需要交互的</span>
</span><span id="__span-14-95">
</span><span id="__span-14-96">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># (B,nq,C) [2, 50, 512]-&gt;Linear(in_features=512, out_features=64, bias=True) -&gt; [2, 50, 64]</span>
</span><span id="__span-14-97">        <span class="k">return</span> <span class="n">out</span>
</span><span id="__span-14-98">
</span><span id="__span-14-99">
</span><span id="__span-14-100"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-14-101">    <span class="c1"># (B, N, C)</span>
</span><span id="__span-14-102">    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">64</span><span class="p">)</span>
</span><span id="__span-14-103">    <span class="n">Model</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">d_v</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</span><span id="__span-14-104">    <span class="n">output</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="nb">input</span><span class="p">,</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-14-105">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="attention_1">Attention 拓展<a class="headerlink" href="#attention_1" title="Permanent link">&para;</a></h2>
<p><img alt="image-20250223213343729" src="../images/image-20250223213343729.png" /></p>
<p><img alt="image-20250223213743962" src="../images/image-20250223213743962.png" /></p>
<p><img alt="image-20250223214012716" src="../images/image-20250223214012716.png" /></p>
<h2 id="cot">上下文 CoT<a class="headerlink" href="#cot" title="Permanent link">&para;</a></h2>
<ul>
<li>首先 query 并没有经过 1x1 的 conv 变换（也就是对应序列的 linear），而是直接 x = query</li>
<li>key 是聚合了 3 x 3 窗口范围内的上下文信息</li>
<li>query 和 key 在通道上进行拼接，经过两个 1x1 的卷积，得到权重矩阵</li>
</ul>
<p><img alt="image-20250223223714588" src="../images/image-20250223223714588.png" /></p>
<p>难点：<mark>怎么理解 query 和 key 在通道上进行拼接，经过两个 1x1 的卷积，得到权重矩阵？</mark>  </p>
<p><mark>一个拼接后的向量，它怎么就能变成注意力权重矩阵？</mark></p>
<p><img alt="image-20250223223642221" src="../images/image-20250223223642221.png" /></p>
<div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-15-1"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-15-2"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-15-3"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">flatten</span><span class="p">,</span> <span class="n">nn</span>
</span><span id="__span-15-4"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">init</span>
</span><span id="__span-15-5"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.modules.activation</span><span class="w"> </span><span class="kn">import</span> <span class="n">ReLU</span>
</span><span id="__span-15-6"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.modules.batchnorm</span><span class="w"> </span><span class="kn">import</span> <span class="n">BatchNorm2d</span>
</span><span id="__span-15-7"><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
</span><span id="__span-15-8">
</span><span id="__span-15-9"><span class="s2">&quot;Contextual Transformer Networks for Visual Recognition&quot;</span>
</span><span id="__span-15-10">
</span><span id="__span-15-11"><span class="k">class</span><span class="w"> </span><span class="nc">CoTAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-15-12">
</span><span id="__span-15-13">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span><span id="__span-15-14">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-15-15">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
</span><span id="__span-15-16">        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span>
</span><span id="__span-15-17">
</span><span id="__span-15-18">        <span class="bp">self</span><span class="o">.</span><span class="n">key_embed</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="__span-15-19">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span><span class="n">dim</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">kernel_size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-15-20">            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span>
</span><span id="__span-15-21">            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</span><span id="__span-15-22">        <span class="p">)</span>
</span><span id="__span-15-23">        <span class="bp">self</span><span class="o">.</span><span class="n">value_embed</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="__span-15-24">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span><span class="n">dim</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-15-25">            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span><span id="__span-15-26">        <span class="p">)</span>
</span><span id="__span-15-27">
</span><span id="__span-15-28">        <span class="n">factor</span><span class="o">=</span><span class="mi">4</span>
</span><span id="__span-15-29">        <span class="bp">self</span><span class="o">.</span><span class="n">attention_embed</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="__span-15-30">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">dim</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">dim</span><span class="o">//</span><span class="n">factor</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="__span-15-31">            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">dim</span><span class="o">//</span><span class="n">factor</span><span class="p">),</span>
</span><span id="__span-15-32">            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span><span id="__span-15-33">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">dim</span><span class="o">//</span><span class="n">factor</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">*</span><span class="n">kernel_size</span><span class="o">*</span><span class="n">dim</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-15-34">        <span class="p">)</span>
</span><span id="__span-15-35">
</span><span id="__span-15-36">
</span><span id="__span-15-37">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-15-38">        <span class="c1"># 除了要明白形状变化，还要明白为什么做这一步</span>
</span><span id="__span-15-39">        <span class="n">bs</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># [1, 512, 7, 7]</span>
</span><span id="__span-15-40">
</span><span id="__span-15-41">        <span class="n">k1</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  
</span><span id="__span-15-42">        <span class="c1"># 311分组卷积，聚合上下文信息，得到静态上下文表示 [1, 512, 7, 7] Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False) [1, 512, 7, 7]</span>
</span><span id="__span-15-43">        <span class="c1"># 编码静态上下文信息key,表示为k1: (B,C,H,W) --&gt; (B,C,H,W)</span>
</span><span id="__span-15-44">
</span><span id="__span-15-45">        <span class="n">v</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  
</span><span id="__span-15-46">        <span class="c1"># 1×1 卷积得到 value 并 reshape [1, 512, 7, 7]  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) [1, 512, 7, 7] .view(bs,c,-1) [1,512,49]</span>
</span><span id="__span-15-47">        <span class="c1"># 编码value矩阵: (B,C,H,W) --&gt; (B,C,H,W) --&gt; (B,C,HW)</span>
</span><span id="__span-15-48">
</span><span id="__span-15-49">        <span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k1</span><span class="p">,</span><span class="n">x</span><span class="p">],</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
</span><span id="__span-15-50">        <span class="c1"># [1, 512, 7, 7] [1, 512, 7, 7] torch.cat [1,1024,7,7]</span>
</span><span id="__span-15-51">        <span class="c1"># 将上下文信息key和query在通道上进行拼接: (B,2C,H,W) [人家的注释真的写得好] x=query 而没有进行变换</span>
</span><span id="__span-15-52">
</span><span id="__span-15-53">        <span class="n">att</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_embed</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> 
</span><span id="__span-15-54">        <span class="c1"># 两个连续的 1x1 卷积，得到 每个 对于坐标(i,j) 的位置，它的第 Ch 个注意⼒头的的局部注意⼒矩阵为 3x3</span>
</span><span id="__span-15-55">        <span class="c1"># [1,1024,7,7]-&gt; 1x1conv-&gt;[1,256,7,7]-&gt;1x1conv-&gt;[1,4608=512（头的个数）*3*3,7,7]</span>
</span><span id="__span-15-56">        <span class="c1"># 通过两个连续的1×1卷积操作: (B,2C,H,W)--&gt;(B,D,H,W)--&gt;(B,C×k×k,H,W)   4608/512 = 9</span>
</span><span id="__span-15-57">        <span class="c1"># 这里的C:把它看作是注意力头的个数</span>
</span><span id="__span-15-58">        <span class="c1"># 不管怎么想 这里把 C×k×k全部放到 第 1 维都是想不通的，HW 像素的语义信息被嵌入了 512*3*3 的这么多维上...(9个 512 这么理解，似懂非懂)</span>
</span><span id="__span-15-59">
</span><span id="__span-15-60">        <span class="n">att</span><span class="o">=</span><span class="n">att</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> 
</span><span id="__span-15-61">        <span class="c1"># 中文说明目的?（每个头都学到了 3x3 小格子的局部注意力，为后面取平均聚合 注意力值做准备）</span>
</span><span id="__span-15-62">        <span class="c1">#  [1,4608,7,7] -&gt; reshape -&gt; [1,512,9,7,7] </span>
</span><span id="__span-15-63">        <span class="c1"># (B,C×k×k,H,W) --&gt; (B,C,k×k,H,W)</span>
</span><span id="__span-15-64">
</span><span id="__span-15-65">        <span class="c1"># (B,C,k×k,H,W) --&gt; (B,C,H,W) --&gt; (B,C,HW)   </span>
</span><span id="__span-15-66">        <span class="c1"># 每个坐标点在每个注意力头的的注意力矩阵为：k×k, 然后对窗口内的值取平均, 因此: (k×k,HW)-&gt; (1,HW), 每个坐标点只有一个值 （忽略BC维度）</span>
</span><span id="__span-15-67">        <span class="c1"># [1,512,9,7,7] -&gt; mean -&gt; [1,512,7,7]-&gt; view -&gt; [1,512,49]</span>
</span><span id="__span-15-68">        <span class="n">att</span><span class="o">=</span><span class="n">att</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-15-69">
</span><span id="__span-15-70">        <span class="c1"># 对N=HW个坐标点(虽然每个坐标点现在只有一个值,但是是通过k×k窗口内的值共同获得的,利用了上下文信息),使用softmax求权重, 然后使用权重与Value相乘，生成动态上下文表示</span>
</span><span id="__span-15-71">        <span class="n">k2</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">v</span>  <span class="c1"># [1, 512, 49] * [1, 512, 49] -&gt; [1, 512, 49]</span>
</span><span id="__span-15-72">        <span class="c1"># 得到动态上下文表示k2: (B,C,HW) * (B,C,HW) =  (B,C,HW)    权重*Value 这里的 * 表示 element-wise 逐元素相乘</span>
</span><span id="__span-15-73">        <span class="n">k2</span><span class="o">=</span><span class="n">k2</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="c1"># [1, 512, 49] -&gt; [1,512,7,7]</span>
</span><span id="__span-15-74">
</span><span id="__span-15-75">        <span class="k">return</span> <span class="n">k1</span><span class="o">+</span><span class="n">k2</span> <span class="c1"># 融合静态上下文信息k1 和 动态上下文信息k2 [1, 512, 7, 7] + [1, 512, 7, 7]</span>
</span><span id="__span-15-76">
</span><span id="__span-15-77">
</span><span id="__span-15-78"><span class="c1"># 简单来讲, 43-49行代码的含义就是: 融合静态上下文信息k1和query信息,来生成每个像素点的权重。 这个权重是基于上下文信息获得的,所以是有效的。</span>
</span><span id="__span-15-79"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span><span id="__span-15-80">    <span class="c1"># (B,C,H,W)</span>
</span><span id="__span-15-81">    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</span><span id="__span-15-82">    <span class="n">Model</span> <span class="o">=</span> <span class="n">CoTAttention</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span id="__span-15-83">    <span class="n">output</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="__span-15-84">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># [1, 512, 7, 7]</span>
</span></code></pre></div></td></tr></table></div>
<h2 id="_5"><a class="headerlink" href="#_5" title="Permanent link">&para;</a></h2>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="2025年2月24日 02:39:20"><span class="timeago" datetime="2025-02-24T02:39:20+00:00" locale="zh"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="2025年2月24日 02:39:20">2025-02-24</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="2025年2月20日 14:28:38"><span class="timeago" datetime="2025-02-20T14:28:38+00:00" locale="zh"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="2025年2月20日 14:28:38">2025-02-20</span>
  </span>

    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["toc.follow", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../../js/timeago.min.js"></script>
      
        <script src="../../../js/timeago_mkdocs_material.js"></script>
      
        <script src="../../../mkdocs/javascripts/katex.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>